Associative Web Document Classification Based on Word Mixed Weight

Abstract-There are two shortages when the method of classification based on association rules is applied to classify  the web documents: one is that the method process the web document as a plain text, ignoring the HTML tags information of the web page; another is that either item of the association rules is only the word in the web page, without considering the weight of the word, or it quantifies the weight of the word frequency, ignoring the importance of the location of the word in the web document Therefore, a new efficient method is proposed in the paper. It calculates the word's mixed weight by the information of the HTML tags feature, and then mines the classification rules based on the mixed weight to classify the web pages. The result of experiment shows that the performance of this approach is better than the traditional associated classification methods.

Keywords-web document classifICation; association rules; HTML tags; mixed weight.



I. INTRODUCTION  With the rapid development of the network, a number of pages are surging in the internet and the information has been explored in the form of them. Using the search engines to crawl, the result is usually very large and lots of information is irrelevant to the needs of the users. So it is a great challenge for technology that how to organize and process the large amount of web document data, and find the interesting information for the users quickly, exactly and fully. As a result, a possible strategy is presented by importing web document classification which has gradually been a hot spot in the field of machine learning as well as the text categorization.

At present, the research about the technology of web document classification is mainly on the basis of the text classification methods, such as support vector machine (SVM) [2], Native Bayesian classification (NB) [3], k? Nearest Neighbor algorithm (KNN) [4] and so on. Although these methods have achieved a certain effect, they still fall short of our expectations. And then the classification based on association rules has been recently presented which has a high degree of accuracy in classification and forecasting.

Lots of literatures do the research about it. Taking the literatures [5,6] for instance, the idea of these methods is to use the existing mining algorithm of association rules to generate all the frequent item sets, and then build the classifier according to these sets in various categories. But these are just designed for classification of non-text area. A    approach is proposed in the studies [7,8] which mine the associative rules to classify the text through treating the text as a transaction and taking the word as a item. In spite of the fact that it is effective to some extent, there is still a drawback that it ignores the item's weight in the text. While the study [9] imports the word frequency as the weight, makes a concept of the weighted frequent item-sets and presents a corresponding mining algorithm, but it is only suit for text categorization. Compared with the normal text, web pages have their own characteristics, for example, web pages are rich in the tags, and the structural information is more clearly. Because of these, the classification of web documents is more difficult than the text classification.

Therefore we must make further study in the view of the characteristic feature of the web pages. In brief, when the above methods are applied to classify the web documents, there exit two shortcomings: (1) to treat the web page as a plain text, without considering the information of the HTML tags; (2) for the items in the association rules, either ignoring the word's weight or only quantifying the weight of the word frequency, without considering the great effect of locations of the words in the web pages.

So in order to truly classify the web pages, a method of associative web document classification based on word mixed weight is presented in this paper. Among the method, we express the web page of a collection of triples in accordance with the location feature embodied in the information of the HTML tags, word frequency and so on, then calculate the word mixed weight by the method of weighted frequency based on the word location feature, and use it to weight the item in the association rules, finally mine the sets of the frequent weighted items, moreover, build the classification rules to classify the unlabelled web pages.



II. WEB DOCUMENT PREPROCESSING  A. Expression of the web document As different from the common text, web document is a  semi-structured data based on the HTML syntax rules. The structure of the web pages is embodied in the HTML tags.

The content, belong to the different tags, has a different ability to express the subject of the web page. So it is a good instruction for the web document classification. For example, if the word "books" is contained in the title of the web page, it indicates that the category of this page is closely related to the kind of books. Therefore, it is necessary for us to not    only take into account the infonnation of the words in the web pages, but also pay attention to the characteristics of the word location. Generally speaking, the locations, which are in accordance with HTML tags of the page, include Title, META, HI-H6, Strong, Anchor, and Plain Text and so on.

The words in the Plain text are words that do not appear in the text enclosed by the title, header, or emphasized structures of the web document.

According to the different infonnation of the categories contained in the locations [10], we have grouped locations into the following three classes (see Table I).

TABLE I. THE THREE CLASSES AND ASSOCIATED HTML TAGS  Class name: p HTMLTags  I Title,Strong,HI-H6,META  2 Anchor  3 Plain text (N one of the above)  Definition l(expression of the web document): a web document can be defined as a set of triples, such  asR = {(Ci,p,/ci,P) I Ci E S,p E P,/ci,p E Z}, where Ci is a word in the web page, p is the class of the location for Ci, /ci, P is the frequency of the word Ci which is located in the location class p, S is a set of words in the page, P is a set of location class, Z is a set of integer.

The method using sets of triples is a new process for expressing the web document. It has many advantages. On the one hand, this approach pays attention to the difference between the common text and the web document, and takes into account the characteristics of the web pages clearly. On the other hand, it makes a difference between the words located in different locations in the page, reflecting the importance of the location feature for the classification.

B. Word frequency count When counting the word frequency in the traditional text  classification methods, we often ignore the semantic relations of the tenns, like a situation that different words have the same or nearly meaning. As a result, for one thing it causes the dimension of the words so large that affect the efficiency of the classification, for other thing it also reduces the degree of accuracy in classifying. Consequently, .

a statistical method of the frequency based on semantIc IS presented in the paper.

The steps are as follows: (1) to extract the text content from the web page according to the class of locations, obtain the words by segmenting them and process the words to a set of triples; (2) to get the synonyms by using the HowNet [11] to extend the words, scan the triples and combine two or more triples into a triple with adding the word frequency together if they have the same class of locations and mean?g; (3) finally obtain a collection of triples based on the semantIC which stands for the web page ..

C. Mixed weight of the word The weight of the word in the web page not only reflects  the contribution degree of the word to identify the subject of the page, but also stands for a capacity of distinguishing between the different pages. As the characteristic of the web document, three factors are responsible for the word weight in the web page: (1) the word frequency in the web page; (2) the location feature of the word in the page; (3) the quantity of the web pages containing this word which is also called inverse document frequency.

Definition 2(weighted frequency of the word): the calculating fonnula for the weighted frequency of the word C is defined as  tfc = ?)A(p)x/c,p) (1)  p=l /C, p is the frequency of the word C which locates in the location class p, A is a function of location contribution.

Let p denote a class of the locations, then {4 p = 1  A(p) = 2 :p = 2 1, p = 3  (2)  Therefore, let C denote a word in the web page, we propose a new calculating fonnula of the word mixed weight Wc , and it is defined as following: Wc = tfc x id/c = tfc x log(N / Nc + 0.01) (3)  Where tfc is the weighted frequency of the word C calculated by the method of weighted frequency based on the  word location feature, using the fonnula(1), id/c is the inverse document frequency, N is the total of the sample web pages, Nc is the number of these pages which contain the word c.

For convenience of the classifying latter, after using the fonnula (3) to calculate the mixed weight of the words in the page, we will use a vector to represent the web page, just  like V = {( Cl, WCI), (C2, WC2), (C3, WC3), . . ?  , (Cn, Wcn)} , where Ci is a word, and WCi is the mixed weight of this word.



III. ASSOCIATIVE WEB DOCUMENT CLASSIFICATION BASED ON WORD MIXED WEIGHT  As is well known, in the traditional methods of associative classification, the items, which compose the association rules, usually are only the words in the web page, ignoring its weight. Or the means of computing the weight ?s unreasonable, as they just use the word frequency as therr weight, without considering the characteristics of the web pages. In order to solve this problem, an efficient method is presented on the premise of pay a great attention to the web characteristic feature in the paper. Firstly, we compute the word mixed weight with the infonnation of the HTML tags, and then weight the items of the association rules by the mixed weight. At last, we mine the sets of the frequent    weighted items; moreover, building the classification rules to classify the unlabelled web pages.

Definition3:Let X = {(C!, Wc,), (C2, WC,), "', (Ci, WC;)} be a set which stands for a web page, then the element (Ci, We;) is called a weighted item, one or more elements are called a weighted item sets, such as X . If there are two weighted items, (Ci,We;) and (Ci,We;l) , and We;> We;' , then (Ci, We;) is called a supper set of (Ci, We; ').

Definition 4: Let X={(a,l1a1(o,"W21";(a,Y1<l)} be a weighted item sets ande is a threshold of minimum support, then the support of X is the minimum mixed weight of the item which is contained in the set X , marked as stqpJrl(X)=Mn(We"l1b, .. ?,Wc;). If support(X) > e, then X is called a frequent weighted item sets.

Associative web document classification based on word mixed weight contains two steps:  Stepl: mining all the frequent weighted item sets from the sample pages.

Step2: generating the classification rules based on the frequent weighted item sets, and classifying the unlabelled web pages.

Apriori algorithm [12] is the classic algorithm in the mining of associate rule. In view of the first step, we improve this algorithm, and propose a new mining algorithm of frequent weighted item sets, called E-Apriori algorithm.

E-Apriori algorithm  Input: a kind of training web pages setD; a threshold of minimum support e  Output: frequent weighted item sets L of this class pages (1) For each dE D (2) { To extract the text content from the web page d ,  count the word frequency, and then obtain the set R of triples based on the semantic;  (3) To get the set of weighted items 1d = { (Ct, Wet) I Ct ESt} from R through the method of weighted frequency based on the word location feature;  (4) add(Td, T ); Iladd the elements of Td into the set T (5) } C = {Ct I (Ct, Wet) E T}; II get the item sets from the  weighted item sets  (6) For each Ct E C LKd;(Ct)  (7) Wet = { d;  } ; II calculate the support of the  I di I Ct E di I item Ct , function Kd;( Ct ) is stands for the mixed weight of item Ct in the web page d i  (8) Ll={(Ct,Wet) IWet>e} ; II Ll is the frequent weighted I_ item sets  (9) for(k = 2;Lk - d = null;k + +)   (10) { get k-l_ item sets G-l={CiI(Ci,Wc;)Ell-l} from Lk-l;  (11) Ck = Ck -1 X Ck -1 ; Ilaccording to Apriori algorithm, get k_ item sets by the connecting of k-l_ item sets  (12) For \;fX.Ct E Ck (13) { For each Ct E X  LKdi(Ct) (14) Wet =  { d;  } ; Ilcalculate the weight over again  I dilXEdi I  (15) Lk = {(Ct, Wet) I (Ct, Wet) E X, Support(X) > e} Ilgenerate frequent weighted k_ item sets  (16) }}return L = {L I i = 1, 2, .. ? ,k} ; The initial classification rules of the web pages are  derived from the frequent weighted item sets which are generated by the E-Apriori algorithm. When selecting some rules as meeting a certain threshold of the confidence, we can build a classifier by these rules.

In general condition, using the classifier, we can classify different web pages. However, because of a lot of rules in it, the classifier will be inefficient if it matches the rules in turn.

So with the property of the supper set defmed in the paper before, we adopt the CR-tree to store, search and match the rules, and then to classify the web pages. This is because all the child nodes in the tree are a supper set of its father node.

If the rule in the father node can't match the unlabelled web pages, neither can the rules in its child nodes. In situation as this, we not only reduce many times of matching the rules, but also do not affect the accuracy of the classifying. The steps of web document classification is that firstly, to extract the words from the page and calculate the word's mixed weight, secondly, to use the rules in the CR-tree to match the page, and then get the category of the page.



IV. EXPERIMENTAL RESULTS  The experimental data is downloaded from the internet, and divided into six categories: political, military, computer, sports, entertainment and books. The total of the training sample web pages is 900, for each category, there are 150.

And the quantity of testing samples is 300, for each category, there are 50. The tool of segmenting words is the ICTCLAS system.

To evaluate our method, the measures are Precision, Recall and Fl. Their definitions are as follows:  D 11 number of correct positive predictions  ?eca = (4) number of positive examples  P .. number of correct positive predictions ( ) reClSIon= 5 number of positive predictions  F 1 = 2 x Recall x Precision Recall+ Precision  (6)  To evaluate the performance of the method presented in the paper, we make the approach of associative web    document classification based on word mixed weight (called MW _ARC) in comparison with the traditional classification of association rules( called N _ ARC) which without considering the weight of the words . In the experiment, the threshold of minimum support is 12. And the results are as follows (see Figures 1, 2, 3).

100 ,--------------,  ? 60 r----------? .:?

'" . ? '10  0..

20 r----------?  poiil. nilil. conpu. sports cnte. books  I-+-MW ARC --N ARCl Figure I. Precision Comparison  100 ,-------------,  80 ? ? 60  ?  ? 40r-----------? '"  20r-----------?  pol i t. mi 1 i t. compu. sports entc. books  I -+-MW_ARC __ N_ARC 1 Figure 2. Recall Comparison  pol i l. mi I i l. compu. sports enle. books  I -+-MW ARC --N ARC I  Figure 3. FI Comparison  As a result, we can see from the figures above that the method of MW ARC is more accurate than the method of N ARC.



V. CONCLUSIONS  In view of the disadvantages of the classification based on association rules applied to classify the web documents, we have presented an approach of associative web document classification based on word mixed weight in the paper. And the experiment results show that it can enhance the accuracy by using the information of HTML tags to compute the mixed weight and weighting the items. Moreover, it will also promote the efficiency by using the CR-tree. In summation,   it sets off the faults of the traditional methods and proves that it is a better method.

