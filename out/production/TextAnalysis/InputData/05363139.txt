An Association Rule Algorithm Generated By  Minimal Head-Item Set

Abstract?A lot of association rules may be generated in the process of association rules mining and much time may be wasted when a user analyzes the association rules. To solve this problem, an improved algorithm which based on the set- enumeration tree for mining the association rules is introduced; and the algorithm reduces the number of association rules remarkably and generates a subset of association rules without any information loss. The number of association rules is decreased very much, so the space consumption in rules? storage will be declined greatly, and the efficiency of analysis for association rules will be enhanced. ?  Keywords- frequent itemset ; association rule ; head-item set ; set-enumeration ; key rule

I. INTRODUCTION Association rule is a very important branch in the area of  data mining. The mining of association rule is implemented by two steps, the first one is mining the frequent items; the second one is generating association rules that meet the condition of min-confidence (mc) [1-3]. The first step is usually the core step in the mining process of association rules, and the efficiency of data mining is affected mainly by finding of frequent item.

In experience, there are a great number of association rules that generated via frequent items. For instance, there are more than 10 thousands of association rules results in one mining case on medical treatment, therefore, users have to spend a great deal of time on analyzing so many rules if they want to analyze an association relation on an illnesses case,  that means, it waste not only  time but also space. Actually, it is not necessary to generate all the rules, on the contrary, it is enough for us to generate some key rules only, which contain all the others information, that is to say, those key association rules are able to deduce all rules without any data loss.

* Supported by the Fund of Program for Tackling Key Problems in Science and Technology of the Department of Science and Technology of Henan Government (Grant No. 072102210066).

GRSET is a kind of algorithm based on set-enumeration, its function is to generate all the association rules which based on one frequent item, and this method generates large numbers of association rules. For example, in table 1, the item I3I2I8I5 is frequent item because it meets the condition of min- confidence (mc) and min-support (ms). And according to GRSET algorithm, only one frequent item generates 10 association rules, they are  {I5 I3I2I8 I3I2 I8I5 I2I8 I3I5 I2I5 I3I8 I3I5 I2I8 I8I5 I3I2 I3I2I8 I5 I3I2I5 I8 I3I8I5 I2 I2I8I5 I3}.

But in fact, it is a huge waste to generate so many rules.

Each expression is a rule, and here, we call the string before the ?  a head-item, and the string after the ?  a tail- item. For example, in the second line of the set, 'I3I2 I8I5', we call I3, I2 the head-item of the rule 'I3I2 I5I8', and I5, I8 the tail-item of the rule 'I3I2 I5I8'.[4] Therefore, the whole set  {I3I5 I2I8 I2I5 I3I8 I8I5 I3I2 I3I2I8 I5 I3I2I5 I8 I3I8I5 I2 I2I8I5 I3}                                 (1)  can be deduced by a set only includes 3 rules via a tail-to-head method , the set is  {I5 I3I2I8 I3I2 I8I5  I2I8 I3I5}                        (2)  For instance, the rule 'I5 I3I2I8',  'I5I3 I2I8' can be obtained via moving I3 to head; and 'I3I8I5 I2' can be obtained via moving I3,I8. So, it is not difficult to find that all the rules in (1) can be deduced by its subset (2).

That indicates that no information will be lost in the moving process, and any rule in (1) is implied in the subset (key rules set).

GSHS (Generate Subset of the Head-item Set) algorithm introduced in this paper is to reduce the number of association rules without any data loss, so as to save more storage consumption and time for analysis.



II. CONCEPTS AND THEOREMS Some new concepts, theorems and symbols are introduced  here to make it easy to explain the new algorithm, besides, we also inherit some concepts in GRSET algorithm in [1].

Definition 1 The support of itemset A is denoted as sup (a), which means the number of transactions that include the itemset. Take quasi association rule [4] c  (l-c) as an example, l is a frequent itemset, and c is the subset of l.

Definition 2 If quasi association rules c  (1-c) meets the condition of the min-confidence.

Theorem 1 If itemset C is the subset of itemset D, then sup(c) sup(d).

Theorem 2 If quasi association rule c  (l-c) meets the condition of min-confidence (mc), then all the quasi association rules that head-item include C and be the subset of L meets the condition of mc.

Proof: Let c be a proper subset d, and d is the subset of l, if c  (l-c) is true, then association rule d l-d is true.

c l-c is true, sup(l)/sup(c)  mc, and because c is the proper subset of d, sup c sup d , sup(l)/sup(d) mc, quasi association rule d l-d is true too. QED.

Therefore, by the theorem 2, given a frequent itemset l, if one of its proper subset c, as a head-item, meets the condition of association rule c  (l-c), then for any itemset d, which is the subset of l and includes c, as a head-item, meets the condition of association rule d  (l-d).

By the property of Theorem 1 and 2, given an association rule, if move some tail-items to head-items, then the new quasi association rules meet the condition of mc. For example, let ?abcde? be a frequent itemset, if ab cde meets the condition of mc, and move the tail-item c to head, then the new association rule abc de meet the condition of mc as well.

Given a frequent itemset l, x denotes the head-item of quasi association rules that generated by l, if quasi association rule x  (l-x) is tenable, then sup(l)/sup(x) mc, namely  sup(x) sup(l)/mc. According to the analysis above, the problem to find the head-item that makes the association rules which generated by frequent itemset l tenable, in fact, is the problem to find the subset of item set  sub-item set for short  whose support is less than or equal to sup(l)/mc according to theorem 1 and 2.

Therefore, in the process of finding the sub-itemset, if the support of a certain sub-itemset less than or equal to sup (l)/mc, then it is not necessary to judge whether the support of the superset of sub-itemset meets the condition. However, if the support of the sub-itemset is bigger than sup (l)/mc, it is still necessary to judge the support of the superset of the sub- itemset by depth first algorithm.

Example 1: in table 1, I3I2I8I5 is a frequent itemset, and its support is 9, because mc=80%, the condition meet x I3I2I5I8-x is just sup(x) sup(l)/mc, and sup(l)/mc = 9/80% = 11.5. So sup(x) 11.5 is necessary and sufficient condition in this case.

The GSHS algorithm in this paper is based on the theorem 1 and 2, and its main idea is: firstly, generates the subset of the head-item set that makes quasi association rules that generated by frequent itemset l tenable; and secondly, adds the subset of the head-item set to the Subset of the Head-item Set (SHS ).



III. GENERATE SUBSET OF THE HEAD-ITEM SET ALGORITHM  GSHS is acronyms of Generate Subset of the Head-item Set, and it is a recursive algorithm for the function of generating a minimal expression-a subset of the head-itemset.

Algorithm 1 GSHS  Input: frequent itemset l, min-confidence mc;  Output: Subset of the Head-item Set (SHS) of association rules  Method:  1  m=||l||  2  SHS=?;  3  get SHS ? ;  procedure getSHS(h s)  Parameters:  H: substring of l, the quasi association rule with the head- item h, and the rule meets the condition of mc.

S: the beginning location to be connected in l, and p (l, h[||h||])+1  s  m.

Method:  1  n=||h||;  2  if n < m-1;  3    if sup (h+l[s])  sup (l)/mc  4      if (! (Superset Check (SHS, h+l[s]) then  5         SHS=SHS ? {h+l[s]}  6       else SHS=SHS ? {SuperSet of h+l[s]}- {h+l[s] }  7    else {  8          j=s,  9          h=h+l[s]  10         do  11          j=j+1  12         while sup (h+l[j]) > sup (l)/mc  13            if (! (Superset Check (SHS, h+l[j]))  14              SHS=SHS ? {h+l[s]}    15             else SHS=SHS ? {SuperSet of h+l[s]}-{h+l[s]  16  }  Procedure supsetcheck(SHS, a)  Parameters:  SHS: Subset of the Head-item Set  A: an itemset found just recently  Method:  (1) If   Superset (a) SHS  (2)   {  (3)     Sus=Superset (a);  (4)     Prune (sus) according to Theorem 1, 2;  (5)     Return 1;  (6)    }  (7) Else  (8)   Return 0  Explanation: In the line 4,5,6,13,14,15 of algorithm GSHS, the Superset Check(SHS, a) is run because if the sub-itemset in a certain recursive procedure meets the condition of sup(x) sup(l)/mc, it needs to judge whether SHS has superset, if it is true, then the superset will be pruned according to Theorem 1 and 2, and at the same time, adds the new head-item to the set SHS, and a new set SHS will be obtained.



IV. EXAMPLE AND ANALYSIS  Example 2 Given a data set, table 1, Let ms=50% mc=80%. Sup (I3I2I8I5) =9.

Question: output the minimal expression of association rules  generated by frequent item sets f=I3I2I8I5.

The mining process is as follows  TABLE I. TRANSACTIONAL DATA SET  TID          Item T1 I1I2I3I4I5I7I8 T2 I2I3I5I6I7I8 T3 I3I4I6 T4 I1I2I3I4I5I6I7I8 T5 I1I2I3I5I6I7I8 T6 I1I3I4I5I7I8 T7 I2I3I4I6I7I8 T8 I1I2I3I5I6I8 T9 I1I2I3I4I5I6I7I8 T10 I3I5I7I8 T11 I1I2I4I6 T12 I1I2I4I6 T13 I3I4I7 T14 I1I2I3I4I5I6I7I8 T15 I2I3I4I5I6I7I8 T16 I2I3I5I6I8  (1) m=4 (2) SHS=? (3) Sup(f)/mc=11 (4) Sup(I3)=14 > 11 (5) Sup(I3I2)=10 < 11 SHS= SHS ? {I3I2}={I3,I2} (6) Sup(I3I8)=12 > 11 (7) Sup(I3I8I5)=11  11 SHS= SHS ? {  I3I8I5}={I3I2,I3I8I5} (8) Sup(I3I5)=11  11  SHS= SHS ? { I3I5}-  {I3I8I5}={I3I2,I3I5} (9) Sup(I2)=12 > 11 (10)Sup(I2I8)=10  11 SHS= SHS ? {  I2I8}={I3I2,I3I5,I2I8} (11)Sup(I2I5)=9  < 11 SHS= SHS ? {  I2I5}={I3I2,I3I5,I2I8,I2I5} (12)Sup(I8)=12 > 11 (13)Sup(I8I5)=11 11 SHS= SHS ? {  I8I5}={I3I2,I3I5,I2I8,I2I5,I8I5} (14)Sup(I5)=11  11 (15)SHS=SHS?{I5}-{I3I5,I2I5,I8I5} ={I3I2,I2I8,I5} (16)According to GSHS, the subset {I3I2,I2I8,I5} of  head-item set of association rule can be generated via the frequent itemset I3I2I8I5, and the association rules with the head-items in {I3I2,I2I8,I5} are  { I3I2 I8I5 I2I8 I3I5 I5 I3I2I8}.

Algorithm analysis In example 2, in regard to the frequent itemset {I3I2I8I5},  if by GRSET algorithm, then all the 10 association rules should be generated, namely  {I5 I3I2I8 I3I2 I5I8  I3I5 I2I8 I2I8 I3I5 I2I5 I3I8  I8I5 I3I2 I3I2I8 I5 I3I2I5 I8 I3I8I5 I2 I2I8I5 I3}.

So as to the whole database, the number of association rules will be huge, and the consumption of time, space and power will be enormous.

However, the GSHS algorithm here just generates 3 rules with head-item in set {I3I2 I2I8 I5}, the rules are  {I3I2 I8I5 I2I8 I3I5 I5 I3I2I8}, and other 7 rules generated by I3I2I8I5 in GRSET can be obtained by moving tail-items to head-items via the 3 rules above. For example, 2 rules {I3I2I8 I5 I3I2I5 I8} can be obtained by tail-to-head method based on rule I3I2 I8I5; and {I2I8I3 I5, I2I8I5 I3} can be deduced by I2I8 I3I5; and rules  {I5I3 I2I8 I5I2 I3I8  I5I8 I3I2 I5I3I2 I8 I5I3I8 I2 I5I2I8 I3} can also be deduced by I5 I3I2I8.

So it is not difficult to find that the 3 key rules {I3I2 I8I5 I2I8 I3I5 I5 I3I2I8} implies totally all other 7 rules without any data loss.

A Hasse diagram, Fig.1, is able to indicate the structure of Boolean association rules set [5] that generated by the  frequent itemset I3I2I8I5, the first line in brackets indicates the head-items of association rules, and the second line in brackets means the tail-items of association rules. It is not    Figure 1. Hasse diagram  difficult to get a conclusion that the association rules with head-items {I3I2 I2I8 I5} constitute the maximal element in Fig.1.



V. PERFORMANCE ANALYSIS The performance of the GSHS algorithm was tested  comparing with the GREST and Apriori algorithm with the same data set. The algorithm is implemented by Java, and the running environment mainly includes: a CPU of P4 2.0GHz, storage memory of 1.0G and the Windows XP system. We have tested the number of rules that generated via GRSET algorithm and GSHS algorithm respectively, the data set used in this case are shown in table 1.

Let ms=50% ,and mc=80%, then Table1 and Fig.2 show the number of rules in GRSET and GSHS algorithm for the data in Table 1.

As it shown from the experimental results in Table 2 and Fig.2, for the same dataset and the minimum support threshold, the number of the key rules generated by GSHS  TABLE II. NUMBER OF ASSOCIATION RULES  Apriori GRSET GSHS Number of  Rules 258 140 86  Figure 2. Number of Association Rules  algorithm are much less than the rules that generated by GRSET algorithm, so the storage usage are decreased than that of GRSET, the GSHS algorithm almost saves 40% storage consumption compared with the GRSET, and saves 66% space to Apriori. Therefore, the experiment indicates that the GSHS algorithm is more effective in saving storage consumption than the GRSET and Apriori algorithm.



VI. CONLUSIONS In this paper, a new algorithm named GSHS is introduced;  it only generates some key rules of association rules comparing with GRSET algorithm. It is shown on theory and experiments that GSHS has three advantages. Firstly, the key rules generated by GSHS are much less than that generated by GRSET, it can save more space consumption for user.

Secondly, the subset built by key rules is a maximal element of all association rules which generated by the same frequent itemset, so it implies all the rules. Thirdly, the key rules can deduce all the association rules base on the same frequent itemset without any information loss.

Because the property of frequent closed itemset is helpful for reducing the number of redundant rules in the process of generating the association rules, the studies on generating association rules based on frequent closed itemset will be carried out.

