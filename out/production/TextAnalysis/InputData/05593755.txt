

Abstract?Association rule mining is one of the most popular  data mining techniques to find associations among items in a set by mining necessary patterns in a large database. Typical association rules consider only items enumerated in transactions.

Such rules are referred to as positive association rules. Negative association rules also consider the same items, but in addition consider negated items (i.e. absent from transactions). Negative association rules are useful in market-basket analysis to identify products that conflict with each other or products that complement each other. They are also very useful for constructing associative classifiers. In this paper, we propose an algorithm that mines positive and negative association rules without adding any additional measure and extra database scans.

Index Terms?Data Mining, Negative Association Rules, Support, Confidence.



I. INTRODUCTION Association rule mining is a data mining task that  discovers associations among items in a transactional database. Association rules have been extensively studied in the literature for their usefulness in many application domains such as recommender systems, diagnosis decisions support, telecommunication, intrusion detection, etc. Efficient discovery of such rules has been a major  focus in the data mining research. From the celebrated Apriori algorithm [1] there have been a remarkable number of variants and improvements of association rule mining algorithms [2]. A typical example of association rule mining application is the market basket analysis. In this process, the behavior of the customers is studied with reference to buying different products in a shopping store. The discovery of interesting patterns in this collection of data can lead to important marketing and management strategic decisions. For instance, if a customer buys bread, what are chances that customer buys milk as well? Depending on some measure to represent the said chances of such an association, marketing personnel can develop better planning of the shelf space in the store or can base their discount strategies on such associations/correlations found in the data. All the traditional association rule mining algorithms were developed to find positive associations between items. By positive associations, we refer to associations between items exist in transactions containing the items bought together. What about associations of the type: ?customers that buy Coke do not buy Pepsi? or ?customers that buy juice do not buy bottled water?? In addition  to the positive associations, the negative association can provide  valuable information, in devising marketing strategies. This paper is structured as follows: the next section recalls preliminaries about Association Rules, In Section 3, existing strategies for mining negative Association Rules are reviewed.

The proposed algorithm is presented in Section 4 and this is a new Apriori-based algorithm for finding all valid positive and negative association rules. Section 5 contains conclusions and future work.



II. BASIC CONCEPTS AND TERMINOLOGY This section introduces association rules terminology and  some related work on negative association rules.

A. Association Rules Formally, association rules are defined as follows: Let  I = {i1, i2,?in} be a set of items. Let D be a set of transactions, where each transaction T is a set of items such that T ?  I.

Each transaction is associated with a unique identifier TID. A transaction T is said to contain X, a set of items in I, if X ?   T.

An association rule is an implication of the form ?X ? Y?, where X ? ? I; Y ? ? I, and X ? Y = ?. The rule X ? Y has support s in the transaction set D if s% of the transactions in D contain X U Y. In other words, the support of the rule is the probability that X and Y hold together among all the possible presented cases. It is said that the rule X ? Y holds in the transaction set D with confidence c if c% of transactions in D that contain X also contain Y. In other words, the confidence of the rule is the conditional probability that the consequent Y is true under the condition of the antecedent X. The problem of discovering all association rules from a set of transactions D consists of generating the rules that have a support and confidence greater than given thresholds. These rules are called strong rules, and the framework is known as the support-confidence framework for association rule mining.

Definition of Negative Association Rule A negative association rule is an implication of the form      X ? ?Y (or ? X ? Y or ? X ? ? Y), where X ?   I, Y ?   I and X ? Y = ? (Note that although rule in the form of ? X ? ? Y contains negative elements, it is equivalent to a positive association rule in the form of Y?X. Therefore it is not considered as a negative association rule.) In contrast to positive rules, a negative rule encapsulates relationship between the  occurrences of one set of items with the absence of the other set of items. The rule X ? ? Y has support s % in the data sets, if s %  of transactions in T contain itemset X while do not contain itemset Y. The support of a negative   Computer Science & Education Hefei, China. August 24?27, 2010   ThP9.20      association rule, supp( X ? ? Y), is the frequency of occurrence of transactions with item set X in the absence of item set Y. Let U be the set of transactions that contain all items in X. The rule      X ? ? Y holds in the given data set (database) with confidence c %, if c% of transactions in U do not contain item set Y. Confidence of negative association rule, conf ( X ?  ? Y), can be calculated with P( X ? Y )/P(X), where P(.) is the probability function. The support and confidence of itemsets are calculated during iterations.

However, it is difficult to count the support and confidence of non-existing items in transactions. To avoid counting them directly, we can compute the measures through those of positive rules.



III. RELATED WORK IN NEGATIVE ASSOCIATION RULE MINING  We give a short description of the existing algorithms that can generate positive and negative association rules.

The concept of negative relationships mentioned for the first time in the literature by Brin et.al [11]. To verify the independence between two variables, they use the statistical test. To verify the positive or negative relationship, a correlation metric was used. Their model is chi-squared based.

The chi-squared test rests on the normal approximation to the binomial distribution (more precisely, to the hyper geometric distribution). This approximation breaks down when the expected values are small.

A new idea to mine strong negative rules presented in [14].

They combine positive frequent itemsets with domain knowledge in the form of taxonomy to mine negative associations. However, their algorithm is hard to generalize since it is domain dependent and requires a predefined taxonomy. Finding negative itemsets involve following steps: (1) first find all the generalized large itemsets in the data (i.e., itemsets at all levels in the taxonomy whose support is greater than the user specified minimum support) (2) next identify the candidate negative itemsets based on the large itemsets and the taxonomy and assign them expected support. (3) in the last step,  count the actual support for the candidate itemsets and retain only the negative itemsets .The interest measure RI of negative association rule X ? ?Y, as follows RI=(E[support( X U Y )]-support( X U Y ))/support(X) Where E[support(X)] is the expected support of an itemset X.

A new measure called mininterest (the argument is that a rule A ? B is of interest only if      supp( A U B)-supp(A) supp(B) ? mininterest) added on top of the support-confidence framework[16]. They consider the itemsets (positive or negative) that exceed minimum support and minimum interest thresholds as itemsets of interest. Although, [8] introduces the ?mininterest? parameter, the authors do not discuss how to set it and what would be the impact on the results when changing this parameter.

A novel approach has proposed in [15]. In this, mining both positive and negative association rules of interest can be decomposed into the following two sub problems, (1) generate the set of frequent itemsets of interest (PL) and the set of infrequent itemsets of interest (NL) (2) extract positive rules of  the form A=>B in PL, and negative rules of the forms A ? ? B, ? A?B and ? A ? ? B in NL. To generate PL, NL and negative association rules they developed three functions namely,  fipi(), iipis() and CPIR().

The most common frame-work in the association rule generation is the ?Support-Confidence? one. In [13], authors considered another frame-work called correlation analysis that adds to the support-confidence. In this paper, they combined the two phases (mining frequent itemsets and generating strong association rules) and generated the relevant rules while analyzing the correlations within each candidate itemset. This avoids evaluating item combinations redundantly. Indeed, for each generated candidate itemset, they computed all possible combinations of items to analyze their correlations. At the end, they keep only those rules generated from item combinations with strong correlation. If the correlation is positive, a positive rule is discovered. If the correlation is negative, two negative rules are discovered. The negative rules produced are of the form X ? ?Y or ? X ? Y which the authors term as ?confined negative association rules?. Here the entire antecedent or consequent is either a conjunction of negated attributes or a conjunction of non-negated attributes.

An innovative approach has proposed in [12]. In this generating positive and negative association rules consists of four steps: (1) Generate all positive frequent itemsets L ( P1 ) (ii) for all itemsets I in L( P1 ), generate negative frequent itemsets of the form ? ( I1 I2 ) (iii) Generate all negative frequent  itemsets   ? I1 ?I2 (iv) Generate all negative frequent itemsets     I1 ? I2 and (v) Generate all valid positive and negative association rules . Authors generated negative rules without adding additional interesting measure(s) to support-confidence frame work.



IV. ALGORITHM In this section we propose and explain our algorithm.

Apriori algorithm has two steps, namely, the join step and the prune step. In join step all frequent itemsets of previous level joined itself to obtain candidate itemsets i.e., by joining Lk-1 to itself i.e.,  C k = L k -1   ?   L k ? 1.

In pruning step, for each I ? Ck, it applies Apriori Property (an itemset is frequent if all its subsets are also frequent). Itemsets satisfying Apriori Property are called as valid itemsets and it is denoted by PCK. NCK can be obtained by replacing each literal in PCK by its corresponding negated item. For a valid candidate with n literals it produces n negative itemsets.

Support of  negative itemsets can be obtained from positive itemsets.

1) Algorithm: Negative Association Rules (NAR).

INPUT:  D:Transactional database, ms: minimum support, mc: minimum confidence    ThP9.20      OUTPUT: Positive and Negative Association Rules Method:  (1) L1=frequent-1-positive-itemsets(D) (2) N1=frequent-1-Negative-itemsets(D)  // complement frequent-1-positive-itemsets(D) (3) L=L1 U N1; (4) for (k=2;     Lk-1 ? ?;    k++) (5) { (6)  // Generating Ck (7) for each l1,l2 ?  Lk-1 (8) If(l1[1]=l2[1]^????l1[k-2]=l2[k-2]^l1[k-  1]<l2[k-1]) (9) Ck=Ck  ?   {l1 [1]??.l1 [k-2], l1[k-1], l2[k-1]} (10) end if (11)         end for (12)         // Pruning using Apriori property (13)          for each (k-1)- subsets s of c in Ck (14)          If s is not a member of Lk-1 (15)          Ck=Ck ? {c} (16)          end if (17)         end for (18)         PCk= Ck; (19)         for each c in PCK (20)         NCk={ck1/  ck1 is obtained by replacing a literal  of c in PCk  by its negation} (21)       //Pruning using Support Count (22)       Scan the database and find support for all c in PCk (23)        Lk=candidates in PCk that pass support threshold (24)        Find support for all ck1 in NCk from supports of  members of PCK and Lk-1 (25)        Nk= candidates in NCk that pass support  threshold (26)       L= Lk U Nk (27)       }    ? Line 1 generates positive-frequent-1- itemsets  ? Line 2 generates negative-frequent-1- itemsets by complementing 1-itemsets obtained in Line  1  ? Line 8 and 9 generates candidate itemsets Ck using Apriori algorithm  ? Line 13-15 pruning candidate itemsets in Ck using Apriori property  ? Lines 18, after pruning, the remaining elements are treated as valid candidates and is denoted by PCk.

? Line 19-20, for each literal of this valid candidate, replace the literal with the corresponding negated literal, creates a new negative rule and denoted by NCk. Each valid candidate with n number of literals in the antecedent will generate n new negative itemsets. For example a 3-itemset ABC will give 3 negative items ?ABC, A?BC, and AB?C.

? Line 22-23, prune all items in PCk using support count and add to Lk, set of frequent k-itemsets  ? Line 24, find support count of all items in NCk using PCk and Lk-1.

? support(?A) = 1- suuport(A) ? support(AU?B) = support(A)-support(AU  B) ? support(?AUB) = support(B)-support(AU  B) ? support(?AU?B) = 1-support(A)-  support(B) + support(A U B) ? Line 25, Nk is the set of all elements whose  support ? minsupp.

?  The generation of positive rules continues  without disruption and the rich but valuable negative rules are produced as by-products of the Apriori process.



V. EXPERIMENTAL RESULTS We tested proposed algorithm on a synthetic dataset to  study the  performance of the algorithm.  However, to illustrate the algorithm, we hereunder provided the solution to a typical example.

Example. Let us consider a small transactional table with 15 transactions and 9 items. In Table 1 a small transactional database is given   TABLE   I.    A Transaction Table    TID   ITEMS   TID   ITEMS   TID   ITEMS   T1   I1,I5,I6, I8   T6   I2,I3,I4   T11   I3,I5,I7   T2   I2,I4,I8   T7   I2,I6,I7,I9   T12   I5,I6,I8   T3   I4,I5,I7   T8   I5   T13   I2,I4,I6,I7   T4   I2,I3   T9   I8   T14   I1,I3,I5,I7   T5   I5,I6,I7   T10   I3,I5,I7   T15   I2,I3,I9   In the following tables, LK denoted as positive frequent k-  itemsets and NK denoted as negative frequent k-itemsets.

Consider minimum support as 20%. In NK bold itemsets are infrequent negative itemsets.

Table II.   Frequent 1-Positive And Negative Itemsets   L1 SUPP N1 SUPP I2 0.4 ?I2 0.6 I3 0.4 ?I3 0.6 I4 0.27 ?I4 0.73 I5 0.53 ?I5 0.47 I6 0.33 ?I6 0.67 I7 0.47 ?I7 0.53 I8 0.27 ?I8 0.73   ThP9.20      Table III.     Frequent 2-Positive Itemsets      L2   I2 I3   I2 I4   I3 I5   I3 I7   I5 I6   I5 I7   I6 I7   SUPP   0.2   0.2   0.2   0.2   0.2   0.33   0.2   Table IV: Frequent 2-Negative Itemsets   N2 SUPP N2 SUPP  ?I2 I3 0.2 I2 ?I3 0.2 ?I2 I4 0.07 I2 ?I4 0.2 ?I3 I5 0.33 I3 ?I5 0.2 ?I3 I7 0.27 I3 ?I7 0.2 ?I5 I6 0.13 I5 ?I6 0.33 ?I5 I7 0.14 I5 ?I7 0.2 ?I6 I7 0.27 I6 ?I7 0.13    Table V: Frequent 3-Positive Itemsets  L3 SUPP I3I5I7 0.2     Table VI: Frequent 3-Negative Itemsets   N3 SUPP ?I3I5I7 0.13 I3?I5I7 0 I3I5?I7 0

VI. CONCLUSION AND FUTURE WORK  In this paper, we proposed an algorithm that mines both positive and negative association rules. Our method generates positive and negative rules with existing support-confidence framework and no extra scan is required for mining negative association rules. We conducted experiments on synthetic data set. It is producing larger number of negative rules. In future we wish to conduct experiments on real datasets and compare the performance of our algorithm with other related algorithms.

