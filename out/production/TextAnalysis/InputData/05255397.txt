Ranking Evaluation Functions to Improve Genetic Feature Selection in Content-Based Image Retrieval of Mammograms?

Abstract  The ranking problem is a crucial task in the information retrieval systems. In this paper, we take advantage of single valued ranking evaluation functions in order to develop a new method of genetic feature selection tailored to improve the accuracy of content-based image retrieval systems. We propose to boost the feature selection ability of the genetic algorithms (GA) by employing an evaluation criteria (fit- ness function) that relies on order-based ranking evalua- tion functions. The evaluation criteria are provided by the GA and has been successfully employed as a measure to evaluate the efficacy of content-based image retrieval pro- cess, improving up to 22% the precision of the query an- swers. Experiments on three medical datasets containing breast cancer diagnosis and breast tissue density analysis showed that fitness functions based on ranking evaluation functions occupy an essential role on the algorithms? per- formance, obtaining results significatively better than other fitness function designs. The experiments also showed that the proposed method obtains results superior than feature selection based on the traditional decision-tree C4.5, naive bayes, support vector machine, 1-nearest neighbor and as- sociation rule mining.

1. Introduction  Breast cancer is the second most common type of cancer in women [15]. Mammograms are a valuable tool to the early detection of the disease and may contribute to increase the patient survival rates. Due to the decreasing costs of mammographies and storage devices, the volume of digital mammograms has grown exponentially. The exploration of such data has become of great importance as they normally contain useful information on past cases, which can help  ?This work has been supported by FAPESP, CNPq and CAPES.

specialists to retrieve similar cases and also contribute to their analysis task and treatments.

Content-Based Image Retrieval (CBIR) refers to the technique to retrieve images based on their content, as op- posed to textual descriptions. In medical domain, the ob- jective of a CBIR system is to aid the specialist on diag- nosing patients by retrieving relevant past cases with proven pathology, along with the associated clinical diagnostic, and other information [1].

In traditional approaches of CBIR, each image in the dataset is represented by a set of feature values, the so- called feature vector, which conveys the essence of the im- age regarding specific criteria. Therefore, feature vectors can hold one or more sets of features generated by one or more image feature extractors, and they are used during the indexing and retrieval process. At retrieval, the images that are most similar to the query image according to some dis- tance measure (e.g. Euclidean distance) are returned. In this context, well-suited features are very important to improve the accuracy of similarity queries results. For most CBIR applications, including medicine, it is well-known that a set of image features computed from a single extractor is not usually the most appropriate way of characterizing im- ages. However, multiple feature extractors usually provide a large number of features, many times containing correlated and irrelevant information that deteriorates the efficiency of similarity queries and impairs data mining techniques, lead- ing to the dimensionality curse problem [12]. Feature selec- tion methods have been proposed in the literature aimed at dealing with the dimensionality curse.

Feature selection is one of the most important and fre- quently used preprocessing technique for data mining [13].

It reduces the number of features, removes irrelevant, re- dundant, or noisy data, and brings immediate benefits to ap- plications: speeds up data mining algorithm, improves min- ing performance, such as predictive accuracy, and improves results comprehensibility.

Genetic algorithms (GAs) have become quite popular due to their ability in dealing with very large search spaces     [18]. Genetic algorithms [8] were inspired on Darwin?s evo- lutionary theory: ?the better adapted individuals tend to sur- vive, transferring their genetic material to the next genera- tion, while the less adapted tend to disappear?.

For each problem kind solved by a GA, a fitness function must be supplied; and this choice is of crucial importance to maximize the GA?s performance [18]. Given an individual (represented as a chromosome), the fitness function must return a numerical value that represents how well adapted the individual is. This score will be used in the parents se- lection and survival selection processes for the next gener- ation thus that the better adapted individuals will have the greater likelihood of being chosen. Therefore, the fitness function must be tailored to the problem being dealt with.

The GA?s effectiveness is, in a large degree, determined by how faithfully the fitness function characterizes the problem to be optimized.

To better understand our proposal, it is necessary to de- fine the concepts of similarity query and relevant element.

A similarity query returns a set of elements ordered accord- ing to a similarity measure. This order is known as a rank- ing. In this work, feature selection is considered a super- vised task that uses a set of pre-labeled samples distributed among a set of different classes. A relevant element (an image, for example) is a sample returned by a query (re- gardless of its position in the ranking) that belongs to the expected class.

This work proposes using single valued ranking evalua- tion functions, henceforth referred to only as ranking eval- uation functions, in the design of evaluation criteria for ge- netic feature selection algorithms in CBIR. A ranking eval- uation function provides a measure of the quality of a simi- larity query result. This work employs order-based ranking evaluation functions which share the utility concept. The utility of a relevant element is proportional to its position in the ranking, i.e. the higher its position in the ranking (more similar), the higher its utility. If an element is not relevant (does not belong to the expected class) its utility score is set to zero.

To the best of our knowledge, this is the first work using ranking evaluation functions for feature selection. Perform- ing feature selection in CBIR using a measure of ranking quality as evaluation criteria is more adequate to evaluate the retrieval effectiveness than the actual approaches that use classification error, class separability or measures of in- trinsic information. Here, we propose to use a measure of ranking quality (ranking evaluation functions) to build fit- ness functions for genetic feature selection algorithms. Due to space limitations, we present results for three medical datasets showing that this new feature selection approach brings superior results when compared to evaluation cri- teria derived from classification (decision-tree C4.5, naive bayes, support vector machine, 1-nearest neighbor) and as-  sociation rule mining. We also show that it considerably increases the precision of the similarity queries while de- creasing the number of features selected. The cost to train the genetic algorithm is high, however it can be done of- fline, since the cost that effectvely counts to the user is the searching one.

The remainder of this paper is structured as follows. Sec- tion 2 gives the main concepts concerning this work. Sec- tion 3 presents related works. Section 4 describes the pro- posed framework for feature selection. Section 5 details the experimental evaluation, and Section 6 presents the conclu- sions.

2. Background  This Section discusses the main concepts necessary to follow this paper.

2.1. Feature Selection  Feature selection (FS) aims at choosing a reduced num- ber of features that preserves the most relevant information of the dataset. Consequently, it performs a dimensionality reduction of the dataset. Feature selection is usually applied as a preprocessing step in data mining tasks: remove irrel- evant or redundant features (dealing also with the curse of dimensionality), leading to more efficient and effective clas- sification, clustering and similarity search processes. It also diminishes the cost to process the queries, as well as the memory required.

A typical feature selection process consists of four basic steps, namely, subset generation, subset evaluation, stop- ping criterion, and result validation [13]. Subset generation is a search procedure that produces candidate feature sub- sets for evaluation based on a certain search strategy. Each candidate subset is evaluated and compared with the previ- ous best subsets according to a certain evaluation criterion.

If the new subset turns out to be better, it replaces the previ- ous one. The process of subset generation and evaluation is repeated until a given stopping criterion is satisfied. Then, the selected best subset usually needs to be validated by a test dataset.

In the wrapper feature selection process, the evaluation criterium is usually a performance measure of a predeter- mined mining algorithm. Works concerning feature selec- tion for CBIR with the wrapper approach were originally intended to optimize classification [22] and clustering per- formance [5]. In our work, we take advantage of traditional classifiers such as 1-nearest neighbor (1-NN), decision-tree (C4.5), Support Vector Machine (SVM) and Naive Bayes (NB) to build FS wrappers. These feature selectors are used as baselines for comparison.

Another approach for feature selection is the use of sta- tistical association rules, which led to the development of the StARMiner algorithm [14]. The goal of StARMiner is to implement statistical association rule mining to find fea- tures that best discriminate images into categorical classes.

It is also employed as a baseline for comparison with our method.

2.2. Genetic Algorithms  Genetic Algorithms (GAs) [8, 9] are based on the princi- ples of biological inheritance and evolution . Each potential solution is called an individual (i.e. a chromosome) in a population. GAs work iteratively applying the genetic op- erations of selection, crossover and mutation, to a popula- tion of individuals to create more diverse and better adapted individuals in subsequent generations. The fitness function assigns a fitness value for each individual, and it is known for playing an essential role in genetic evolution.

Associated with the characteristics of exploitation and exploration search, GAs can efficiently deal with large search spaces, and hence are less prone to stuck with a lo- cal optimal solution than other algorithms. This is due to its ability to allow multiple solutions representations (indi- viduals) in the search space and by applying probabilistic genetic operators.

2.3. Ranking Evaluation Functions  Ranking evaluation functions are measures of the rank- ing accuracy (a set of sorted elements according to a simi- larity measure). Ranking evaluation functions belong to two categories: order-based and non order-based. Non order- based ranking evaluation functions are measures in which the score of an element in the ranking has a fragile relation- ship to its position. An example is the R-precision measure (percentage of relevant images between the first R in rank- ing). Order-based ranking evaluation functions are based in the utility concept, where the score value of a relevant element in the ranking is usually inversely proportional to its position. The fact that users would rather see relevant elements appearing in the initial positions of the ranking, suggests that order-based ranking evaluation functions are more likely to be successful.

Various ranking evaluation functions have been proposed in the literature. However, as far as we know, ranking eval- uation functions have never been applied to the feature se- lection domain. A ranking evaluation function that presents promising results is [6]:  Fr(q, C) = ? ?i?I  ( r(i)  A  ( (A? 1) A  )(pos(i)?1)) (1)  where Fr(q, C) denotes the score value of the individual C (given by the chromosome coding, explained in the next section) for the image query q. I represents the entire im- age dataset, r(i) returns the relevance of the image i, where r(i) = 1 if the image i is relevant for that query q, and r(i) = 0 otherwise. A is a user-defined parameter with values larger than or equal to 2. It determines the relative importance of the positioning of an element in the ranking position pos(i). For small values of A, greater importance is given to relevant elements better positioned in the rank- ing (i.e. those at the initial positions). When A takes high values, i.e. the factor (A?1)A leads to values near 1, the rela- tive position of the elements in the ranking are not strongly reflected in the final score value. We have set A to 10, which leads to an intermediate behavior of the score value Fr(q, C). That is, the score values computed for all rel- evant elements at increasing positions are more uniformly distributed in the range (0, 1).

3. Related Works  Most of the works in data mining field regarding fea- ture selection technique aim at maximizing the classifica- tion accuracy. Also, there is various researches aim at opti- mizing the information retrieval process. Many works em- ployed evolutionary approaches to perform feature selec- tion [20, 21, 13, 5, 18] and others to improve information retrieval (IR) [7, 17, 4]. A wide review of the application of evolutionary computation to boost information retrieval is presented in [4].

The main focus of this work is to provide effective infor- mation retrieval (IR), specifically image retrieval of mam- mograms, employing feature selection. It is well-known that an image retrieval system can be affected by any of the three subsystems: images representations, queries rep- resentations and similarity functions. Previous approaches for effectively improving IR performance by manipulating queries representations have been developed in [2], and also dealing with relevance feedback techniques [3, 10, 16].

Recently, similarity functions and ranking functions have received more attention of the researchers. Many rank- ing functions were developed and applied to Web informa- tion retrieval, as well as similarity functions to image re- trieval. However, recent studies show that these functions do not perform consistently well across different contexts [7, 17]. According to [17], an image descriptor is a pair composed of an image feature extractor and a distance func- tion, since the similarity functions and the image represen- tations (feature vectors) have a crucial role in the retrieval performance.

The approaches presented in [7] and [17], respectively, showed solutions about how to combine multiples ranking functions and descriptors using evolutionary computation    optimization and an evaluation criterion based on ranking quality. Those works consider that there is not a best rank- ing function nor a best descriptor for a given problem. In the classification context, ensemble techniques combine predic- tion of multiple models to allow higher accuracy, which are often not achievable when using just a single model. The works presented in [18, 19] employed genetic algorithms to select feature subsets to construct ensembles using one base classifier.

Another approach to improve an image retrieval tech- nique is to modify the images representations. It is well- known that the choice of an adequate feature vector im- proves the accuracy of the image retrieval. Many filter fea- ture selection techniques are applied to improve image re- trieval. Such techniques are based on the idea that corre- lated or inconsistent feature deteriorate the ability of data differentiating. Wrapper feature selection techniques are also applied to improve image retrieval. However, the latter does not use an adequate evaluation criteria for image re- trieval, which normally is based on classification error or a measure of clustering separability [5].

In this is work it is proposed a method to improve im- age retrieval through feature selection. The quality of fea- tures subsets (chromosomes) is evaluated using a measure of ranking quality given by a ranking evaluation function and evolved by a genetic algorithm.

4. The Proposed Framework  The present framework uses GA with a fitness function based on the ranking evaluation concept to perform feature selection for CBIR. This decision stemmed from three rea- sons: (i) the large size of the search space in feature selec- tion; (ii) previous successfully usage of GA in the feature selection domain; and (iii) no prior works on feature selec- tion based on optimizing ranking quality applied for CBIR.

Figure 1 illustrates the steps from the pipeline that im- plemented the proposed method. Here, the feature selection step is a supervised process. In the training phase, image features are extracted from the image training set and then submitted to the feature selection process. The feature se- lection process guided by GA searchs for the best features subset according to an evaluation criteria based on the rank- ing quality ? given by a single valued ranking evaluation function (Fc). In the test phase, the features selected indi- cated by the best chromosome chosen the GA in the train- ing phase, will be used for representing the images of the test set. Similarity queries are performed considering each image from the test set as a query image, and finally the average precision-recall curve is built.

The corresponding GA designed for feature selection is described as follows.

The chromosome coding ? In order to apply GA to a given  Figure 1. Pipeline of the proposed method.

problem, it is necessary to define the genotype required by the problem, i.e. the chromosome representation. In other words, a decision must be made on how the parameters of the problem will be mapped into a finite string of symbols (genes), encoding a possible solution in the problem space.

In this work, a chromosome was coded by an n-bit string or binary-valued vector (n is the initial number of features), C = (g1, g2, . . . , gn), where gi takes value 0, if the i-th feature is excluded from the subset, and 1, if it is kept in the subset. We refer to C as a feature selection vector and gi as a feature selection variable.

The genetic operators ? GA searches for better solutions by genetic operations, including selection, crossover and mutation. Selection implements the survival of the best fit- ted individuals according to some predefined fitness mea- sure. Therefore, high-fitted individuals have a better chance of surviving and reproducing, while low-fitted ones are more likely to disappear. Crossover and mutation opera- tions represent an analogy to natural reproduction and ex- plore the solution space to find the best solution. The ge- netic operations used in this study are:  ? Selection for recombination: applied to select pairs of individuals that will go on reproducing (mating pool).

Linear Ranking Selection was used: the individuals are sorted according to their fitness and the last position is assigned to the best individual, while the first position    is allocated to the worst one. The selection probability is linearly assigned to the individuals according to their ranks.

? Selection for reinsertion: a total of (Sp ? 2) best off- springs and 2 best parents according to their fitness values survive from two consecutive generations. Sp is the population size.

? Crossover: represents the mating of two individuals to form two new individuals (offsprings). Uniform crossover was used in this work. Uniform crossover is a sort of multiple points crossover taken to the ex- treme, where instead of raffling crossover points, raffle a mask with the size of the chromosome that indicates which Chromosome-Parent will supply each gene to Offspring 1. Offspring 2 is generated by the comple- ment of the mask.

? Mutation: a gene (a feature selection variable) selected for mutation is substituted by its complement, i.e. each chosen bit will be changed from 0 to 1 and vice-versa.

This is known as uniform mutation.

Fitness function: The fitness function plays a very im- portant role in guiding a GA to obtain the best solutions within a large search space. Good fitness functions help a GA to explore the search space more effectively and effi- ciently. Not proper fitness functions, on the other hand, can easily make the GA get trapped in a local optimum solu- tion and lose the discovery power. Two main designs have been analysed in this work: classification error and fitness functions based on ranking evaluation functions. From the order-based ranking evaluation function Fr(q, C) (Eq. 1), a mechanism to derive the fitness function Fc(Q,C) for the GA has been devised. Fc(Q,C) (Eq. 2) is given by the average of the score values obtained from each image q of the training set Q as an image query. nQ is the number of images in the set Q. The output of the fitness function has been normalized within the range [0, 1], where 1 indicates maximum accuracy. According to this criterion, the prob- lem consists of seeking for the highest values. On the other hand, this can also be seen as a minimization problem if the output is subtracted from 1 (1 - output). This is the approach adopted in this work.

Fc(Q,C) =  ? ?q?Q Fr(q, C)  nQ (2)  We recall that the fundamental principle in wrapper fea- ture selection approaches is the minimization of the number of features, while optimizing (or preserving) the quality of the result. For a CBIR system, the best approach is to re- trieve images with a minimum amount of features and high- est accuracy. This concept led to the proposal of two distinct  fitness functions FcA(Q,C) and FcB(Q,C) (Equations 3 and 4) that combine two optimization criteria. The first cri- terion indicates the quality of the query results given by the term Fc(Q,C) present in both Equations. The second crite- rion, the minimization of the number of features, are given by the terms (|C|?d)n and  |C| n in Equations 3 and 4, respec-  tively.

In both Equations, |C| is the number of selected features  coded by chromosome C, Q represents the training set, d is the number of desired features specified by the user, and n is the dimensionality (number of features) of the entire dataset. The term (|C|?d)n yields high values when the num- ber of selected features |C| widely differs from the number of desired features d. The term |C|n of Equation 4 is also a penalizing factor that takes into account the number of selected features only. Finally, ? is an adjustment parame- ter in the range [0,1], which determines the importance as- signed to each criterion, in a complementary way.

FcA(Q,C) = ?(Fc(Q,C)) + (1? ?) (  (|C| ? d) n  ) (3)  FcB(Q,C) = ?(Fc(Q,C)) + (1? ?) ( |C| n  ) (4)  Control parameters: the experiments described in this work employed crossover probability pc = 0.8, mutation probability pm = 0.01 for each gene (a feature selection variable).

5. Experiments and Analysis  This section presents three experiments in which the pro- posed technique is employed to perform content-based im- age retrieval of mammograms. The datasets were split into training and test subsets. Feature selection has been car- ried out on the training subset, while performance evalu- ation has been conducted through precision?recall (P&R) curves [2] over the test subset for different feature selec- tion techniques, grouped as: (a) traditional methods (1-NN, C4.5, SVM and Naive Bayes - NB); (b) non order-based ranking evaluation functionR-precision (FR-Precision); (c) the association rules (StARMiner algorithm) ; (d) the pro- posed technique (Fc, FcAand FcB) and (e) all features combined (no feature selection). A rule of thumb when an- alyzing P&R curves is the closer to the top the better the technique.

All methods evaluated appear in the legends of Figures 2, 3 and 4. The values in parenthesis indicate the numbers of features selected (|C|). The methods of group (a) employ classification errors as minimization criteria for the GA.

Group (b) employs a fitness function based on R-precision.

Group (c) is based on association rules. Group (d) are those related to the proposed framework. Fc, FcAand FcB correspond to the proposed fitness functions Fc(Q,C), FcA(Q,C), FcB(Q,C) given by the Equations 2, 3, 4 and respectively.

The evolutionary search was set to 100 individuals, evolving along 400 generations in experiments 1 and 2. Ex- periment 3 employed 50 individuals and 250 generations.

According to the dimensionality of the datasets, the parame- ter d employed in FcA (Equation 3) was set to 50 in the first and second experiments, and to 20 in the third experiment.

The Euclidean distance was employed in the experiments to measure the similarity between the feature vectors.

5.1. Experiment 1 - ROI-102 image dataset  This dataset consists of 102 images with regions of interest (ROI), taken from mammograms collected from the Breast Imaging Reporting and Data System of the Department of Radiology of University of Vienna (http://www.birads.at) classified into three levels of BI- RADS (3, 4 and 5). The BIRADS (Breast Imaging Re- porting and Data System) categorization was developed by the American College of Radiology to standardize mam- mogram reports and procedures. The BIRADS categoriza- tion is summarized in Table 1. The dataset has been di- vided into a 68?image training set and a 34?image test set.

Each image has been characterized by a 850?dimensional feature vector, including features generated by Haralick de- scriptors (140 features), wavelets (64 features), zernike mo- ments (255 features), histogram (256 features), features of first order derived of histogram (6 features), Run length (44 features) and Edge Histogram MPEG7 (80 features). Fig- ure 2 shows the P&R curves for the test dataset and also the number of features selected for each method. It can be seen that the proposed framework with fitness function de- rived from order-based ranking evaluation function yielded superior results when compared with the traditional evalu- ation criteria given by the average classification error, im- proving up to 22% the precision of the query answers. The proposed framework also outperforms the fitness function derived from non order-based ranking evaluation function (FR-Precision) and all features combined. Also, the pro- posed framework finds the sets with the fewest features in comparison with the other methods.

5.2. Experiment 2 - ROI-250 image dataset  This experiment investigates the performance of the proposed technique for a 250?image mammog- raphy dataset with ROIs comprising lesions, taken from the Digital Database for Screening Mam-  Table 1. BIRADS categorization.

value description 0 Need Additional Imaging Evaluation.

1 Negative.

2 Benign Finding.

3 Probably Benign Finding. Short Interval Follow-  Up Suggested.

4 Suspicious Abnormality. Biopsy recommended.

5 Highly Suggestive of Malignancy. Proper Action  Must be Taken.

Figure 2. Precision-recall curve in ROI-102 image dataset.

mography of the University of South Carolina (http://marathon.csee.usf.edu/Mammography/), classi- fied into two classes: mass-benign and mass-malign. A 739?dimensional feature vector has been computed for each sample, including features generated by Haralick descriptors (140 features), zernike moments (255 features), histogram (256 features), features of first order derived of histogram (6 features), Run length (44 features) and invariant moments (38). The dataset was divided into a 166?image training subset and a 64-image test subset.

Figure 3 shows the precision-recall curves obtained and also the number of features selected in each method.

The graphs of Figure 3 show that the proposed methods (FcA, FcB and Fc) increased the precision of the queries in about 15% in the region of 5% of recall in comparison with the other methods, while decreasing the number of fea- tures from 739 to around 50. This is, using about 7% of the previous memory space for the images representation.

These results indicate that ranking evaluation functions are well-suited to be employed in genetic feature selection for CBIR.

Figure 3. Precision-recall curve in ROI-250 image dataset.

5.3. Experiment 3 - Mammograms-1080 im- age dataset  This experiment employed a dataset composed by 1080 mammograms images collected in the Clinical Hospital of University of Sao Paulo at Ribeiro Preto. The dataset was previously classified into 4 levels of breast tissue density: (1) mostly fatty (362 images); (2) partly fatty (446 images); (3) partly dense (200 images); and (4) mostly dense (72 im- ages).

Figure 4. Precision-recall curve in Mammography-1080 image dataset.

Breast density is an important risk factor in the devel- opment of breast cancer. In this experiment, the images are represented by the feature set proposed in [11], build-  ing a vector of 85 features, including shape and size of the breast, the conditions of the breast contour; nipple position, and the distribution of fibroglandular tissue. This dataset was divided in training set and test set. The training set is composed of 720 images and test set is composed of 360 images.

Figure 4 shows the P&R curves over test dataset and also the number of features selected in each method. Again, the proposed methods reached the highest values of precision and select the smallest number of features.

(a)  (b)  (c)  (d)  Figure 5. Queries in Mammography dataset: (a) ? is the query image; (b) using the fea- tures selected through fitness function FcB; (c) using the features selected through clas- sification error of C4.5; and (d) using the all features extracted  Results for the retrieval of the 5 most similar images from a query image are also provided in this experiment, as illustrated in Figure 5. The image 5.(a) is the query im- age taken from the mostly fatty image class. Images shown in 5.(b) are the 5 most similar images retrieved for the pro- posed fitness function FcB. The row (c) shows the results for C4.5 classifier, whereas (d) illustrate the images result- ing from all features (no selection applied). In Figure 5, the images surrounded by dashed lines are false positives (not relevant images). For this query, the proposed method achieved the highest precision (100%) when compared the results of C4.5 and the original feature vector (precision of 40%).

6. Conclusions  This work proposed a novel genetic feature selection framework for CBIRs. It employs a wrapper strategy that searches for the best reduced feature set, while optimizing (or preserving) the quality of the solution. From a rank- ing evaluation function, three new fitness functions namely, FcA, FcB and Fc have been proposed and evaluated in three experiments.

The proposed genetic feature selection approach, which encompasses FcA, FcB and Fc, has been compared with (a) traditional methods found in the literature, (b) the StARMiner feature selector and (c) the whole feature vec- tor, and significantly outperformed them.

The proposed approach has been able to optimize the ac- curacy of similarity queries while selecting a significatively reduced number of features. Additionally, the proposal of combining the quality of the query results with the crite- rion of minimizing the number of selected features, FcA and FcB, led to high accurate query answers while reduc- ing the number of features more than the fitness function Fc. Therefore, the final processing cost of the queries is also reduced.

