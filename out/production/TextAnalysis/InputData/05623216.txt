Based on rough set of associative rules improve algorithm of data mining

Abstract?According to information system theory and from the equivalent and support the concept of view, it is easy to find the frequency of collection and confirm the relevant rules of coarse.

Under in-depth and systematic research on RS theory and associate rules mining algorithms, This paper make some improvement based on original algorithms. The first and the foremost, this paper proposes an efficient algorithm for counting core and a reduction algorithm of attributes based on discernibility matrix which can handle the knowledge system and make the extraction of decision-rules convinent. Secondly, it put forward a mining model of association rules with decision attributes based on Apriori ,AprioriTid and AprioriHybrid algorithms,which also optimize them.

Keywords-Data Mining?Rough Set ?Asscoiate Rule

I. INTRODUCTION Mining (Data Mining) is extracted from the mass of data  known in advance, understandable, and ultimately the information available for the user, and knowledge. And the associated rules of research is a very hot of data mining.

Purpose of mining association rules from large transaction databases mining history is to found the association between the project, The association between the items, including two steps: (l) to identify all frequent itemsets. (2) frequent itemsets generated by the trust to meet the minimum threshold rule Currently, the vast majority of studies focus on the first step, in which the most classic Apriori algorithm, most descendants to improve on this basis  Rough set theory is a new deal with fuzzy and uncertain knowledge of the mathematical tools, the idea is to maintain the premise of the same classification ability, through knowledge reduction, export decision-making or classification rules.



II. CONCEPTS AND MODELS OF ASSOCIATION  RULES  Given a set of projects I=(I1,I2?Im) And a transaction database D=(t1,t2?tn) Among  ti={ Ii1,,Ii2,?,Iim} and Iij I? ?Association rule is of the form X=>Y?among  X,Y? I Is a collection of two projects, known as the project set and XnY=??  Association rule X => Y's support?sup (X => Y) is a database that contains X Y in the?  transaction account database the percentage of all transactions  that sup (X => Y) = p (X Y).? Association rule X => Y's confidence: conf (X => Y) is  the transaction that contains X  Y contains x number and ?  ratio of the number of services that conf (X => Y) = sup  (X  Y) / sup (X).?  Association rule X => Y's confidence: conf (X => Y) is  the transaction that contains X  Y contains x number and ? ratio of the number of services that conf (X => Y) = sup (X  Y) / sup (X).? Confidence of association rules is a measure of accuracy,  support is a measure of the importance of association rules.

Help support this rule in all matters the representation of the extent, obviously the greater the support, the more to association rules. Although some confidence association rules is high, but the support is very low, indicating that the association rules and practical opportunity to small, it is not important. the problem of mining association rules is found to have a user-specified minimum support and minimum confidence of all association rules, which found that the support of association rules and confidence are not less than the minimum support and minimum confidence.



III. ROUGH SET THEORY IN ASSOCIATION RULES MINING In practical application, a lot of knowledge are based on  information form, Information systems generally deal with the following main steps: First, data preparation, including data discretization, data cleaning, depending on the issue of the form given information table knowledge representation system, incompatible with the object and remove redundant objects, the decision to establish the compatibility table is prepare  for the data reduction. And then examine whether the conditional attribute can be omitted, get the simple attribute set, a multi-lateral compression of information table, if the information table reflects the control rules, then the equivalent of all the control rules to reduce the antecedent conditions. On this basis, on the basis of value reduction to reduce the number of properties and individuals, the final extraction rules is to access information systems inherent laws. Using rough set theory for data mining, extraction of knowledge rules, the most important thing is based on rough set attribute reduction and rule redundancy value reduction.

V11-236C978-1-4244-7237-6/$26.00     2010 IEEE    Through some simple operations, reduce the dimension attribute, summed up the knowledge for decision support in the rules is rough set theory is one of the most important applications.



IV. DECISION-MAKING SYSTEM AND DECISION RULES APPLIED TO THE ASSOCIATION RULE MINING  Rough set theory in decision-making systems and decision rules applied to the concept of mining association rules, attribute rules can also limit; before proceeding to association rules mining, to improve the efficiency of mining association rules. Mining Decision table or something before the association rules in the database, should be handled in accordance with the following general steps to reduce the mining complex, reduce errors, false, redundant rule generation  Processing steps are as follows: Step 1: data preprocessing, the continuous attributes in decision table will be converted to discrete values.

Step 2: Using rough set reducts of condition attributes, delete redundant attribute, use the resolution matrix (difference matrix) to complete the reduction and the core. When there are several core values, core values with the smallest component to extract simple rules.

Step 3: Mining association rules is based on Boolean, continuous attribute values to go through a cluster analysis to classify the property value, obtained after the reduction of decision table into a Boolean type. The decision table can be treated as transaction databases.



V. WITH CONCLUSIONS BASED ON ROUGH SET OF ASSOCIATION RULES MINING.

A. Algorithm Apriori-MARDA Introduction In this paper, research project constraints algorithm,  Apriori algorithm is improved by, a band of Association Rules Mining Algorithm Apriori-MARDA (Mining Associate Ruleswith Decision Atrribute) and optimized variants.The example indicates that this method can reduce the association rule mining of time and space complexity, improve efficiency of data mining.

How to improve the efficiency of the current rules mining algorithm of association rule mining is an important research topic, the relevance of the general conclusion is no domain, although it can try to find the relationship between  the various properties and dependence, and to fully explore the database .But aimlessly analysis have to pay a high price, when there is a definite goal, which is concluded when the domain is known (from the constraints of the project point of view are bound, that is, only certain items can appear in the rules right, as a rule after the piece), can reduce the number of candidate sets, the search space, and count, so to some extent improved the efficiency of the algorithm  Apriori-MARDA algorithm aims to use the reduction of rough directly from the decision table after the extract with the rules of Association. Some scholars have raised the rough set decision table reduction, and then extracted directly from decision rules (with the conclusion of Association Rules) This idea, and verified by this method for mining association rules are meaningful, without modeling, to achieve such further work  B. Apriori-MARDA algorithm model First, the decision table using the previously mentioned  steps to deal with common, the property values of all properties unique number, and then, using Apriori-MARDA algorithm to generate all the frequent item sets frequent itemsets generated by the length of the property does not exceed the conditions number with the number of decision attributes.

To facilitate the description below, are agreed as follows:  Decision table established by the transaction processing database (Transactions Database) to T.

Lk: with minimum support set of frequent k itemsets Ck: Candidate itemset of k-dimensional set of (potentially  the largest itemset) c: 1 ~ k-dimensional itemsets c ': 1 ~ k frequent itemsets d ': 1 ~ k frequent itemsets Count (C): condition attribute dimension Count (D): decision attribute dimension This Apriori-MARDA algorithm is as follows: Input: a database T, condition attribute set C, decision  attribute set D, minimum support threshold (minsup) Output: T of all frequent itemsets (1) L1 = find_frequent_Itemset (large 1-itemsets); / *  generate L1 * / (2) L2 = GenerateL2 (c '? L1, d' ? L1); / * generate C2  and L2, with the SM-MARDA to store and calculate all the possible frequent 2 item set support * /  (3) for (k = 3; k <= Count (D) + Count (C); k ++)/* produce L3 ~ frequent Count (D) + Count (C) itemset * /  (4) (Ck = apriori_genC (c '?  Lk-1, d' ?  Lk-1) ? apriori_genD (c '? Lk-1, d' ? Lk-1); / * generate new candidate itemsets * /  (5) for each transactions t ? T (6) (Ct = subset (Ck, t); (7) for each New candidates c ? C t (8) (c.count + +; (9) if (c.count> = minsup) / * c is greater than minsup is  added Lk * / (10) Lk = (c ? Ck | c.count> = minsup); )  Data Preprocessing  Decision Table Reduction  Rules acquisition  Original information sheet   V11-237    ) ) (11) Answer = ? Lk; Algorithm GenerateL2 function is used to generate L2,  and the function is divided into two steps: Join (link) and Prune (pruning). In the connection step, the frequent one-dimensional set of decision attribute value of item 1-D conditions and frequent item sets from the connection attribute values, until all the candidates generated two sets, functions described as follows: insert into C2 (Select p [1], q [1] / * p, q as frequent a set * /  from L1 p, L1 q where p [1] <> q [1] ) Pruning function described as follows:  for each transactions t ? T (Ct = subset (C2, t);  for each New candidates c ? Ct (C.count + +; if (c.count> = minsup) / * c is greater than minsup are  joined L1 * / L2 = (c ? C2 | c.count> = minsup)  ) ) ) ) Algorithm in the function apriori_genD and apriori_genC  can produce all the candidate itemsets. Their input parameters for the Lk-1, Lk-1 that all frequent k-1 itemsets, eventually returned to Ck, that all candidate k itemsets. Two numbers the same letter has two steps: Join and Prune.

First of all, apriori_genD in the Join step, through the Lk-1 in pre-k-2 Same as items of the item sets from the connection operations to obtain a superset of Ck, the steps described as follows:  insert into Ck (Select p [1], q [1], p [2], q [2], ..., p [k-1], q [k-1] / * p, q  k-1 for the frequent item sets * / from Lk-1 p, Lk-1 q where p [1] = q [1], ..., p [k-2] = q [k-2], p [k-1] <q [k-1] ) Then, in the Prune steps that have been obtained for each  k Ck itemsets c, if c k-1 some of the item set Not in Lk-1 in, it will delete c from Ck. The steps  described below: for each New candicate c ? Ck for each s ? subset (c, k-1) if (! (s ? Lk-1)) (Delete c from Ck;) Secondly, apriori_genC in the Join step, through the Lk-1  in Same as items after the k-2 item sets from the connection operations to obtain a superset of Ck, the steps described as follows:  insert into Ck (Select p [1], q [1], p [2], q [2] ..., p [k-1], q [k-1] / * p, q  k-1 for the frequent itemsets * / from Lk-1 p, Lk-1 q  where p [1] <q [1], ..., p [k-2] = q [k-2], p [k-1] = q [k-1] ) apriori_genC function Prune apriori_genD function  with the same step, not repeat them here.

AprioriTid-MARDA algorithm to set up a calculator to  store the candidate itemsets c ? Ck the number of transactions when dealing with a certain time T, the counter initial value set to 0, the time when the scan Hash tree, if the transaction T reaches Hash tree one leaf node, then the counter plus 1, when scanned Hash tree after the counter trading on T has been included in the candidate the number of frequent k itemsets.

In the worst case (all items are set to meet the minimum support, which are frequent itemsets), up to (C1n + C2n +...+ Cnn) (C1m + C2m +...+ Cmm) = (2n-1) ( 2m-1) = 2m + n-2m (2n-m +1) +1 second operation, get all the frequent itemsets (where m is the number of decision attribute value, n the number of attribute values for the conditions). If conditions do not distinguish between attributes and decision attribute, the simple use Apriori algorithm to generate all frequent itemsets when up to (C1m + n + C2m + n +...+ Cm + nm + n) = 2m + n-1 time operations, and improve algorithm to reduce the 2n +2 m-2 times operator (the above items in any length to satisfy the minimum support set all the time). Shows that the modified algorithm reduces the computation time and space complexity, improve efficiency of data mining.

Based measure of confidence from the frequent itemsets generated in the form of  Des (C1) ? Des (C2) ? ... ? Des (Cn) => Des (d1) ? Des (d2) ? ... ? Des (dn) association rules, in which Des (Cn) is one such condition attributes Cn price category values, Des (dn) for the decision attribute of an equivalence class values.



VI. APRIORI-MARDA ALGORITHM OPTIMIZATION Because of previous mining algorithm Apriori-MARDA  is based on the Apriori algorithm to adjust the efficiency of the traditional Apriori algorithm in mining on the shortage is also inherited, so the original algorithm of optimization is also essential.

Apriori algorithm is the major shortcomings of the following two points:  (1) k-1 by the frequent itemsets were generated from the connection candidate frequent k itemsets large quantities.

(2) verify the candidate frequent k itemsets when the need to scan the entire database is very time-consuming. Therefore, based on the Apriori another scholar suggested AprioriTid algorithm is only the first scan the transaction database D with the calculation of the candidate frequent itemsets support, and other first k scans generated with its last set of Ck-scan 1 'to calculate the candidate frequent itemsets support, support to reduce the computation time required to scan the total number of transactions, thereby reducing the computation time support.

Rough set is an effective tool for data mining, has a solid theoretical basis. Pawlak has been proposed since 1982, has been applied in many fields, but as a new, rough sets are also encountered in many practical difficulties. Currently   V11-238    there are two effective ways: First, the expansion of rough set theory, such as Ziarko's variable precision rough set model. Followed by the rough set combined with other methods.



