Performance Tuning in Distributed Processing of ETL

Abstract? Extract, transform, and load (ETL) is a very common and important technology for building data warehouse includes business intelligence. When people issue a very complex SQL query to acquit data from a transaction system into a data warehouse, it involves many procedures including table-joining, sort, and aggregation. Such procedures require significant retrieving step and huge data transferring from tables. The intensive querying very often causes performance issues to be concerned. Moreover, it commonly generates negative impacts on data instance resources. How to improve the performance for ETL becomes critical and challenging. This paper presents a parallel processing solution that splitting big and complex SQL query into small pieces in distributed computing manor. The proposed method aims at reducing cost of computation, while ensuring data integrity among joined tables. The innovative idea can be verified through selected test-beds of performance tuning.

Keywords- ETL; Data extraction; Capture data changes; load; Performance tuning

I. INTRODUCTION In processing of extract, transform, and load (ETL),  there exist three database functions that are integrated together to operate data transfer from one table to another to form data marts or warehouses. It also functions to convert databases from one format to another. Therefore, ETL is a very common technology utilized in business intelligence.

There is an example of unexpected ETL performance during an implementation of business analysis. The transactional data are extracted, transformed, and loaded from SIEBEL CRM system to a business intelligence data warehouse. As ETL?s components, ProC for extraction, Syncsort for capture data changes (CDC), and Informatica for loading delta changes (LDC) are all functioning well in the first phase. However, after the data volume rapidly increased in the system, the performance of complex extraction became worse. When sophisticated queries are executed and the system retrieved huge data on the system, takes about 90% of total ETL computing time. How to deal with such performance problem become critical to the business data extraction, transactions and distribution in real time. In the following section, we would like to demonstrate how to analyze such problem.



II. PROBLEM ANALYSIS First, we make sure that all data are read from the  Siebel CRM system using ProC. The data in previous and current steps are compared to check whether there exists any data change. Such process can be accomplished by using Syncsort. Then captured data changes are loaded into the data warehouse. Extensive analysis can be conducted to determine bottlenecks occurring in the whole process.  The major issues can be addressed as follows.

All transforms/aggregations can be executed in complex extract queries. For example, a most complex query can sometimes consists of more than 10 scalar sub queries and complex aggregations, loops, or sorts.

If using the SIEBEL-CRM software system, the performance may lack of tuning. As all know, most of setups in a transactional system is OLTP-dedicated, which only returns first row. It is uncontrollable to all the return rows (which are DW characteristics). In addition, the system also suffers negative performance due to the influence cause by huge complex extraction of retrieving data.

Rapidly-developed commerce markets and business generate huge amounts of data volume significantly. The corresponding queries become complex and not linearly scalable. The more volume increase, more surfer we may have. When dealing with big data, the intensive computing becomes necessary. Such computation causes low performance of operation using existing software. The performance issues are getting to be more concerned and complained by end-users, especially for Windows-based users on their daily PCs.

Not like a simple query on a huge amount of data resource, which can be accomplished within a tolerated time scale, a complex query on a big data can significantly reduce computing performance.

Fig. 1 shows the previous solution chart in which CM refers to a complex query involved both base table M and referred tables A1, A2, A3, B1, B2, etc. d(CM) stands for the change (or delta) between the current and previous CM. Everyday only delta records can executed on target table, while remaining the current CM on source table. M is the base table of source. It has the same primary key as in the target table. M*, A*, and B* represent the expected columns from M, A, and B in a target table T, respectively.

The d(M*), d(A*), and d(B*) are the delta data   DOI 10.1109/ICICSE.2013.24     corresponding to M*, A*, and B* . They are obtained after the CDC?s previous and current data steps in daily process.

One flexible solution is to reduce the complexity of extract queries, ensure the data integrity, and reduce the any influences on performance on source instance.

Figure 1. The previous solution chart  (CM query) Select M.rowid,  .

(select aggregation From A1,A2,A3..

Where A.rowid=M.rowid), .

(select aggregation From B1,B2.

Where b.rowid=M.rowid), .

.(select aggregation From C..

Where C.rowid=M.rowid), .

From M Where ...

(CM* query) Select M.rowid.

A.aggregation_Values,.

B.aggregation_Values,.

C.aggregation_vaules,.

From M, (select aggregation From A1,A2,A3..

group A.rowid) A, .

(select aggregation  From B1,B2.

Group by B.rowid) B,.

.(select aggregation From C..

Group C.rowid) C Where M.rowid=A.rowid(+) And M.rowid=B.rowid(+) And M.rowid=C.rowid(+);  (SM query) Select M.rowid,...

From M Where ?.

(B query) (select aggregation  From B1,B2.

group by B.rowid) B,  (A query) (select aggregation From A1,A2,A3..

group A.rowid) A    (C query) (select aggregation  From C Group by B.rowid) C     Target Table      Figure 2. The implementation steps  Sie be  lC RM  DB  M  A1 A3A2  B1 B2  In for  ma tic  a  Da ta  W ar  eh ou  seM*A* B* Target table  Previous M*  Current M*  Previous A*  Current A*  Previous B*  Current B*  Result of simpler SM query  Subset of refer A query  Subset of refer B query  Delta d(M*)  Delta d(A*)  Delta d(B*)  Implement d(M*)  Update d(A*)  Update d(B*)  Pro C Execute sql on  siebel DB to extract data to flat file.

Syncsort CDC Process Utilize Syncsort to Get delta data(insert, update, delete) by compare previous extracted file and current extract file  Proposed Solution  Figure 3. The proposed solution chart.

Sie be  lC RM  DB  Syncsort CDC Process Utilize Syncsort to calculate delta data (current versus previous extract file)  Pro C Execute sql on siebel DB to  extract data to flat file.

Result of complex query CM  M  A1 A3A2  B1  B2 d(CM)  M*A* B*  Current data  In for  ma tic  a  Da ta  W ar  eh ou  se  M*A* B*  Target table  M*A* B*  Previous data  d(M*)d(A*) d(B*)  Delta data  Previous Solution

III. SOLUTION How to reduce the complexity of extract query, while  still ensuring data integrity is an important issue. We first analyze what the kind of data are expected to be delivered from source in SIEBEL CRM system to data warehouse during a regular daily process. In such case, we have  d(T)=d(CM)= Current CM(M*,A*,B*) CDC Previous CM(M*,A*,B*). T is the target table, which has the current result from a complex query CM. It includes expected columns of M*, A*, and B*. The d(T) is identical to d(CM). The d(T) is executed to obtain the target table T to keep the current updated.

When consider ROWID as the primary key in both M and T, the d(T) includes the delta ROWIDS calculated in d(CM). In other words, the d(T) includes those ROWIDS which have been changed in the base table M or those ROWIDS possibly to be changed in M due to the influences of scalar sub queries A and B. Therefore, the d(T) could be split into small pieces based on ROWID, i.e,   d(T) =d(SM){ Current SM CDC Previous SM) + M*[impacted by d(A*) ] + M*[impacted by d(B*) ] }  SM is the result based on a simple query. It only fetches records from base table M (or the base table with simple joins)  The d(SM) stands for the delta data of the current SM and previous SM. It includes those that the ROWID are changed.

The M*(d(A*)),M*(d(B*)) includes those ROWIDS from base table M*. Whether or not being changed, they are always influenced by the delta data d(A*), d(B*),..

The d(SM) includes those changed columns from base table M. After d(SM) is executed, the target table T has the same ROWIDS as M has. Let T have the keys A* and B* referenced to M*. It is possible to be ROWID?s or other reference key. Then those delta data come from d(A*).

The d(B*) can be calculated from T. It turns out that the calculation of d(A*) and d(B*) are on the source side. The T(d(A*)) and T(d(B*)) can be update by EQUAL-JOIN on Data Warehouse side. Fig. 2 shows the execution steps:  Step 1: Restructure a complex query. Put in-line scalar sub queries down to select the location to store using OUT-JOIN.

Step 2: Split the complex query into main query (with base table) and referred queries  Step 3: After separating EXTRACT/CDC/LOAD jobs, for preparation of distributed main and referred queries, load the delta d(SM) into target table. The d(M*) must be executed firstly to ensure data integrity.

Step 4: After the d(M*) is executed, load delta M*(d(A*)), M*(d(B*)),  and M*(d(C*)).

Fig. 3 shows the proposed solution flow chart. A is a scalar sub query with tables A1, A2 and A3 which are joined each other. B is a scalar sub query with jointed tables B1 and B2.

TABLE1 SHOWS THE DIFFERENT COST  Process Extract CDC Load  Solution Sum(SM, A, B,C) CM Sum(d(M*), d(A*),d(B*), d(C*)) d(CM) Sum(d(M*), d(A*), d(B*),  d(C*)) d(CM)  Rows processed 24570 14750 24570 14750 3710 2100  Consistent gets 194842 355942343 X X X X  Physical reads 44328 44328 X X X X  Elapsed time 0:03:36 3:17:06 0:00:17 00:00:07 0:00:17 00:00:09  After a complex query being spitted into some simpler and smaller ones, the previous complex FULL-FULL tables OUT-JOIN can be converted to several separated simpler queries on source side. In this way, the querying can be executed in parallel. The OUT-JOIN (full M out-joins to full referred queries) becomes EQUAL-JOIN (target table T equal-joins delta data from referred several queries). The complexity can be balanced on two sides, while the computational performance can be reduced significantly.



IV. TEST-BED OF PROPOSED SOLUTION After implement the proposed solution in ETL, three  processes of extract, CDC, and loading have been changed.

Taking the most complex query as our test, we split a complex query CM into a simple query SM and several referred sub queries A, B, and C. Compare the results of computational cost using previous and proposed solutions, we find the differences which are listed in Table1.

The total elapsed time for extraction is decreased from about 3 hours to 3 minutes using proposed method.

Moreover, the distributed queries can be executed in parallel. Though multiple CDC?s loading jobs can be     present, the total time elapsed is not significantly large, compared with of single original CDC and loading jobs.



V. CONCLUSION Many methods have been tried on the complex extract  query (like adding hints) in ETL process with varying instance parameters in data transaction. The cost performance is a concern.

In this work, we propose and experiment a parallel method in which a complex query is spited into several simple and small ones for execution in parallel. The results of increasing performance are very promising. The total cost can distributed and decreased, with less influences on source database.

In future, we would like to consider reduce such cost through reference sub queries, if queries are very complex and cannot hosted on source instance. The relationship between main query and reference sub queries with data integrity is the most important issue to be considered and modeled, if we split complex queries.

