An Effective Algorithm and Modeling for Information Resources Scheduling in  Cloud Computing

Abstract?With the development of the Internet and Web technology and frequent cross-cultural communication, it is necessary for people to be able to access and manage information in many different languages. The scheduling of multilingual information resources become complex, as multilingual information resources stored in different cloud datacenters are heterogeneous and their distributions are uneven. In this paper, based on Min-Min algorithm, Least Languages First Min-Min (LLFMM), a scheduling algorithm that is suitable to multilingual information resources and can improve information resources scheduling performance in cloud computing is proposed. A task with least number of sites that can fulfill this task is firstly selected in LLFMM, rather than the task with the least execution time among all the tasks in Min-Min. Petri net is a powerful graphical and mathematics tool for describing the concurrent, asynchronous and dynamic events. This paper models and analyzes the multilingual information resources scheduling using colored dynamic timed Petri nets (CDTdPN). In CDTdPN model, the color represents different language information resource, and the time of a transition representing scheduling time is a function of tokens in input places of the transition. The concurrent scheduling marking graph (CSMG) of CDTdPN is defined and the algorithm for constructing the CSMG of CDTdPN is given in this paper. The CSMG can represent the concurrent relation of transitions in Petri nets and reduce the size of the graph to be constructed in some extent. Finally, the example and simulation show that our scheduling algorithm is effective in reducing makespan and our modeling and analyzing techniques are convenient for analyzing performance of scheduling system.

Keywords?cloud computing, multilingual; resources scheduling; algorithm;  modeling; Petri net

I.  INTRODUCTION Cloud computing [1] provides information resources for  users in "Cloud" through the Internet. Cloud computing utilizes large-scale virtualized data centers to manage such large volume of resources. Scheduling is considered to be an important issue in Cloud computing. Typically, it is difficult to find an optimal resource allocation which minimizes the makespan and effectively utilize the resources. The choice of the best pair of jobs and resources has been proved to be NP- complete problem. Large numbers of heuristic resources scheduling algorithms are available to minimize the  makespan. The simple well-known algorithm used for tasks scheduling in heterogeneous computing systems is Min-Min [2]. The Min-Min heuristic algorithm is becoming the benchmark of such kinds of task/host scheduling problems.

The numbers of languages for information stored in different datacenters are uneven. Some languages such as English and Spanish could be distributed in many datacenters, while some languages such as Korean and Japanese may be stored in a few datacenters. So, it is not suitable to the multilingual resources scheduling for existing mono-lingual resource algorithms. In this paper, we propose a different algorithm to the original Min-Min algorithm in order to adapt to the characteristics of multilingual information requirements. The algorithm proposed in this paper called Least Languages First Min-Min (LLFMM) considers the characteristics of uneven distribution of multilingual information resources in cloud datacenters. A task with least number of sites that can fulfill this task is firstly selected in LLFMM, rather than the task with the least execution time among all the tasks in Min-Min.

As the complexity of multilingual information resources scheduling in cloud computing, there is a need for modeling and analyzing techniques for scheduling flow of multilingual information resources. Petri nets are perfect mathematical and graphic tools for modeling and analysis resources scheduling. So it has received increasing application in modeling and analyzing resource scheduling. But, the model and analysis technology are not suitable to the multilingual information resources scheduling in cloud computing. We use colored dynamic timed Petri net (CDTdPN) to model and analyze multilingual information resources scheduling in cloud computing. In CDTdPN model, the color represents different language information resource, and the time of a transition representing scheduling time is a function of tokens in input places of the transition. In this paper, we define the concurrent scheduling marking graph (CSMG) of CDTdPN and give the algorithm for constructing the CSMG of CDTdPN. The CSMG can reduce the size of the graph to be constructed in some extent.

The rest of this paper is organized as follows. Section 2 presents the related works. In section 3, we give a scheduling algorithm LLFMM for multilingual information resources in cloud computing. Aiming at the problems of analysis and modeling of the multilingual information resources scheduling in cloud computing, colored dynamic timed Petri   DOI 10.1109/CBD.2013.11     net (CDTdPN) model and the concurrent scheduling marking graph (CSMG) are proposed in section 4. An example and simulation show the effectiveness of the scheduling model and algorithm in section 5. The conclusions of the paper are given in section 6.



II. RELATED WORK Min-Min algorithm starts with a set of all unmapped  tasks. The machine that has the minimum completion time for all jobs is selected. Then the job with the overall minimum completion time is selected and mapped to that resource. The ready time of the resource is updated. This process is repeated until all the unmapped tasks are assigned.

The Min-min heuristics is now becoming the benchmark of such kinds of task/host scheduling problems. However, the Min-min algorithm is unable to balance the load well since it usually maps small tasks first and did not deal with the Quality of Service (QoS).

Some improved algorithms for the resources scheduling in cloud computing have been proposed in the literature. All the problems discussed in those methods are taken and analyzed to give a more effective schedule. Yang, Zhi et al.

[3] present a cost-based resource scheduling paradigm in cloud computing by leveraging market theory to schedule compute resources to meet user's requirement. Yun Fei Cui et al. [4] give an improved genetic algorithm for cloud computing resource scheduling. Hao Li and Guo Tang [5] propose a cloud banking model based on multi-dimensional Pareto optimal theory for resource scheduling. Haihua Chang and Xinhuai Tang [6] present a resource scheduling algorithm based on dynamic load balance in cloud computing. Xin Lu and Zilong Gu [7] present a load- adaptive cloud resource scheduling model based on ant colony algorithm. Chen Ming et al. [8] present a scheduling model for cloud computing resource based on buffer-pool agent. Dr. M. Dakshayini and Dr. H. S. Guruprasad propose a priority and admission control based service scheduling policy that aims at serving the user requests satisfying the QoS [9]. El-Sayed T. El-kenawy et al. [10] proposed an improved Max-min algorithm is based on the expected execution time instead of complete time as a selection basis.

Although there exist many resource scheduling algorithms in cloud computing, none involves the scheduling of multilingual information resources. So it is not suitable to the multilingual resources scheduling for existing resource algorithms in cloud computing. An extension of Min-min algorithm called Least Languages First Min-Min is proposed in this paper.

As the complexity of cloud computing system, there is a need for modeling and analyzing techniques for multilingual information resources scheduling. Petri nets are perfect mathematical and graphic tools for modeling and analysis scheduling problems. W.M.P. van der Aalst modeled and analyzed scheduling problems using timed Petri nets in [11].

Some Petri nets models for the resources scheduling in grid computing have been proposed in the literature. El-Sayed T.

El-kenawy et al. [10] used Petri net to model the concurrent behavior of distributed systems. Mohammad Abdollahi Azgomi et al. [12] proposed CPN-based modeling pattern  formally describing the process of task distribution and execution within the grid environment. Xiangang Zhao et al.

[13] proposed a scheduling net based on a hierarchical color Petri net and a job net based on a Petri net with changeable structure. In our previous work [14-16], we proposed some Petri net models for resources scheduling in grid computing, transaction scheduling in grid database and multilingual information parallel downloads in data grid. But, above model and analysis technologies are not suitable to the multilingual information resources scheduling in cloud computing environment. Therefore, new models and performance analysis technologies for the multilingual information resources scheduling in cloud computing are proposed in this paper. We use colored dynamic timed Petri net (CDTdPN) to model and analyze multilingual information resources scheduling in cloud computing. In CDTdPN, the time interval of a transition is a function of tokens in input places of the transition, which is convenient for modeling and analyzing dynamic performance of multilingual information resources in cloud computing.

Reachable marking graph(RMG)[18] is a very important tool to analyze the dynamic properties of Petri nets, but the major weakness of the RMG is that the size of RMG grows with the number of processes and thus creates the state space explosion problem. Moreover, the concurrent relation of transitions in Petri nets cannot be represented by RMG. In this paper, we use the concurrent scheduling marking graph instead of RMG to analyze the correctness and effectiveness of the system.



III. SCHEDULING ALGORITHM FOR MULTILINGUAL INFORMATION RESOURCES IN CLOUD COMPUTING  In this section, we give a scheduling algorithm LLFMM of multilingual information resources for scheduling agent.

The algorithm LLFMM starts by sorting the requirements (i.e. tasks). It first identifies the number of datacenters storing each requirement and sorts the requirements in ascending order of the number of datacenters storing multilingual requirements. Thus the requirement with minimum number of datacenters that can fulfill the requirement is scheduled first. The main aim of the scheduling algorithm is to minimize the makespan.

The scheduling problem can be defined as follows: Let multilingual requirement set MR= mr1, mr2, mr3 , ?.

, mrn be the group of multilingual requirements submitted to scheduler.

Let multilingual information set MI= mi1, mi2, mi3 , ?. , mik be the group of multilingual information resources provided by cloud datacenters, where MI?MR.

Let datacenter set DC = dc1, dc2, dc3 , ?. , dcm be the set of cloud datacenters, where, dci is a sub-set of MI.

Let EST is a matrix, where ESTij is the estimated scheduling time of requirement i on datacenter j. If the requirement i do not exist in datacenter j, EST(i,j)=0.

Let TT is a matrix, where, TTij is the expected transmit time taken by transmitting requirement i to datacenter j from global scheduling agent.

Let RTj is the ready time or availability time of datacenter j after completing the previously assigned requirements.

Let CT is a matrix, where, ctij that is the expected completion time of requirement i on datacenter j is defined as the wall-clock time at which datacenter j completes requirement i.

Where,  CT(i,j)= EST(i,j)+ TT(i,j)+ R(j) Algorithm: LLFMM scheduling algorithm Input: EST, TT and MR.

Output: CT and Scheduling series {( mri, dcj)|i=1, 2, ?,  n; j=1, 2, ? m}, where, mri is some language requirement, dcj is datacenter j, (mr i, dcj) means requirement i is assigned to datacenter j.

Let RT=(0,0,...0); Define MR as a Queue; Calculate the number of datacenters storing each  requirement; Sort MR in ascending order of the number of datacenters  storing multilingual requirement; While (MR is not empty) { Get mri from MR; Find datacenter j such that min1?j?m{(RT(j)+ TT(i,j)+  EST(i,j)| EST(i,j)>0}; Assign requirement mri to datacenter j; Delete mri from MR; Let CT(i,j) = EST(i,j)+ TT(i,j) +RT(j); Let RT(j) = RT(j)+ EST(i,j); } Obviously, the time complexity of LLFMM algorithm is  same as one of Min-Min algorithm.



IV. PETRI NET MODEL AND PERFORMANCE ANALYSIS FOR MULTILINGUAL INFORMATION RESOURCES SCHEDULING IN  CLOUD COMPUTING Some concepts of Petri nets cannot be discussed here for  lack of space. See references [18,19,20]for the definitions of Petri nets.

A. Colored Dynamic Timed Petri Net for Multilingual Information Resources Scheduling in Cloud Computing Definition 1 Suppose that there are n datacenters in cloud  computing. The ith datacenter includes information resources of mi kinds of languages. The colored dynamic timed Petri net model (CDTdPN) for multilingual information resource scheduling in cloud computing is a eight-tuple CDTdPN =( P,T; F, C, W, I, M, D), where,  P={p0, pb, pf, pe} {pj1, pj2 |j=1,2,?,n} is a finite set of places, where, p0 represents that user is ready to submit a request, pb represents the request submitted by user, pf represents the expected results on datacenter j, pe represents the results expected by user, pj1 represent the requests scheduled to datacenter j for executing, pj2 represent the results returned from datacenter j.

T={tb, te} {tj1, tj2, tj3|j=1,2,?,n}is a finite set of transitions, where, tb is used to submit requests to schedule, te is used to return the results to user, tj1 is used to schedule a  request to datacenter j according to LLFMM algorithm, tj2 is used to execute the request on datacenter j, tj3 is used to finish the request on datacenter j.

F= {(p0,tb), (tb,pb), (te,pe)} {(pb,tj1), (tj1,pj1), (pj1,tj2), (tj2,pj2), (pj2,tj3), (tj3,pf), (pf, te) | j= 1,2,?,n} is a finite set of arcs.

C={LG} {(cj,ej) | j=1,2,?,n } is a set of colors, where, LG is a set of languages in the request submitted by user, cj and ej are communication time from user site to datacenter j and execution time of the request on datacenter j respectively.

W :  F?L(C)+.

I :   T?L(C)+.

M :  P?L(C).

L(C) is a non-negative integer coefficient liner function.

L(C)+ represents at least one coefficient in L(C) is not 0.

D(tj1)=[cj], D(tj2)=[ej], D(tj3)=0, j=1,2,?,n D(tb)= D(te) =0.

In the CDTdPN, the firing time delay associated with  transition t is a function of color token of input places of transition t.

The graphical representation of CDTdPN is shown as Fig.

1.

Figure 1.  CDTdPN model.

B. Performance analysis of concurrency transactions scheduling in Cloud Computing A major strength of Petri nets is their support for analysis  of many properties and problems associated with concurrent systems. After modeling the with Petri nets, we will analyze the correctness and performance of the multilingual information resource scheduling system in order to ensure the correctness and effectiveness of the system. We first define the concurrent scheduling marking graph and then use it to analyze the correctness and effectiveness of the system.

Definition 2 Let CDTdPN be a colored dynamic timed Petri net for multilingual information resource scheduling.

The concurrent scheduling marking graph (CSMG) of CDTdPN is defined as a directed graph with labeled directed edges and nodes.

CSMG (CDTdPN)= (V, E, F, FC), where, V={R(M0)}, E={( Mi, Mj)| Mi, Mj?R(M0), ?t?T: Mi [ t >Mj , F(E)= t/[td], where the marking Mj that results from  firing t at Mi , td is the time delay of transition t firing.

FC(E)= t1?t2? ??tk /[mt], where the marking Mj that  results from concurrently firing t1 , t2 ?, tk at Mi ,     mt=max{ td1 , td2 ?, dtk }, tdi is the time delay of transition ti firing, i=1,2, ?, k.

Definition 3 Let CDTdPN be a colored dynamic timed Petri net, ?P?=n, ? t?T, n-vector Xin(t)=(x1, x2,?, xn) and Yout(t)=(y1, y2,?, yn) are input vector and output vector of the transition t respectively, iff   The CSMG (CDTdPN) is constructed by the following  algorithm.

Algorithm 2 Construction of CSMG(CDTdPN) Input: CDTdPN.

Output: CSMG(CDTdPN) For each transition t?T  Construct the input vector Cin(t) and output vector Cout(t);  Let V={M0}; E={?}; CE={?}; tag M0 ?new?; While (exists ?new? node in V)  { Select a ?new? marking M from V and tag M ?old?; If there no exist enabled transition t at M then tag M  with ?end node?; Else  { For each enabled transition t?T at M Construct the concurrent transition set CT={ti1 , ti2  ?, tik} and non-concurrent transition set NT={tj1 , tj2 ?, tjk2}; For each transition t? NT at M  {Obtain M? that results from firing t at M; If M??V then V=V+{M?}; tag M? with ?new?; E=E+{(M,M?)}; tag (M,M?) with t/[td];  } For each concurrent transition set ct? CT at M  {Obtain M? that results from firing ct={ ti1 , ti2 ?, tik} at M;  If M??V then V=V+{M?}; tag M? with ?new?; E=E+{(M,M?)}; tag (M,M?) with t1?t2? ??tk /[mt]; where the marking M? that results from firing ct={ ti1 , ti2 ?, tik} at M, M?= M-(Cin(t1)+ Cin(t2)+ ?, + Cin(tk))+(Cout(t1)+ Cout(t2)+ ?, + Cout(tk)), mt=max{ tdi1 , tdi2 ?, dtik }, tdi is the time delay of transition tij firing, j=1,2, ?, k.

} Remove ?new? from M ;  } } The correctness of the algorithm 2 can be easily proven  according to the definitions of CDTdPN and CSMG.

Proposition 1 Let CDTdPN be a colored dynamic timed  Petri net model for a multilingual information resource scheduling process. The multilingual information resource scheduling process is correct iff for any end node M? CSMG (M0), M(pe)?0 and any p?pe, M(p)=0.

Proposition 2 Let CSMG(CDTdPN) be the concurrent scheduling marking graph of CDTdPN and The multilingual information resource scheduling process be correct. The makespan of the multilingual information resource scheduling process is equal to ?tdi, where, tdi is the tag of marking Mi in CSMG.

The proof of proposition 1 and 2 are obvious.



V. EXAMPLE AND SIMULATION In this section, we will give an example and a simulation  to show the effectiveness of our algorithm.

A. Example Consider a cloud environment with 4 datacenters named  as DL1, DL2, DL3 and DL4. The languages included in 4 datacenters and the download times of different languages information from 4 datacenters are listed in table 1.

TABLE I.  DOWNLOAD TIME OF DIFFERENT LANGUAGE INFORMATION FROM 4 DATACENTERS (UNIT: SECOND)  Datacenters DC1 DC2 DC3 DC4  Language a d a b c a b a  Download time 10 13 15 14 12 16 15 15   Suppose that there are two requirements R1 and R2  submitted by user. The R1 includes 3 languages: a, b and d.

The R2 includes 3 languages: a, c and d.

For R1, the scheduling process is a(DC1)? b(DC2)?d(DC1) and the final completion time (makespans) is 23 for Min-Min algorithm, and the scheduling process is d(DC1)? b(DC2)?a(DC4) and the final completion time (makespans) is 15 for LLFMM algorithm. For R2, the scheduling process is a(DC1)? c(DC3)?d(DC1) and the final completion time (makespan) is 23 for Min-Min algorithm, and the scheduling process is d(DC1)? c(DC3)?a(DC4) and the final completion time (makespans) is 15 for LLFMM algorithm. Therefore, the makespan for LLFMM algorithm is less than one for Min- Min algorithm.

Now we model and analyze the example using CDTdPN.

First, we construct the CDTdPN for multilingual  information resource scheduling of the task. The graphical representation of the CDTdPN for the example is similar to Fig. 1, where n=4.

Second, we construct CSMG, as show in Fig. 2.

Figure 2.  CSMG(CDTdPN) of the example with LLFMM algorithm  From Fig. 2, we know that the scheduling of this example is correct because for end node M5, M5(pe)?0 and any p?pe, M5(p)=0. Also we know that the makespan for the scheduling is 15 according to proposition 2.

B. Simulation To evaluate the efficiency of the proposed algorithm, we  design a program to execute the Min-Min and LLFMM algorithms. Suppose that there are 20 datacenters in cloud computing system and the total languages are 10. The number of languages stored in each datacenter and the download times of requirements on datacenters are produced randomly by the program. The experimental evaluation of the algorithms is performed for n={1, 2, 3, 4, 5, 6,7, 8, 9, 10} languages included in requirement. We run the program 100 times and get the average makespan for each requirement.

Table 2 and Fig. 3 show the comparison of makespans (Suppose the interval of download times for each requirement is 10-50s).

From table 2 and Fig. 3, we can observe that LLFMM algorithm produces same makespan as Min-Min when the number of language included in requirement is equal to 1.

LLFMM algorithm produces less makespan than the Min- Min algorithm when the number of language included in requirement is greater than 1. The greater number of language included in requirement, the bigger difference of makespans between two algorithms.

TABLE II.  MAKESPANS FOR TWO ALGORITHMS  Number of languages included in  requirement  Min-Min (sec)  LLFMM (sec )  1 15 15  2 18 17  3 21 19  4 23 20  5 25 21  6 27 22  7 29 22  8 32 24  9 34 24  10 36 26    Figure 3.  Makespans of Min-Min and LLFMM

VI. CONCLUSION In this paper, we propose a new algorithm LLFMM for  multilingual information resources scheduling. It uses the advantages of Min-Min algorithm and overcomes its disadvantages by choosing the requirement with the least number of sites that can fulfill this requirement rather than one with the least execution time. The example and simulation show that the proposed scheduling model and algorithm can reduce the makespan and increase the resource utilization. So, it is an effective algorithm for the scheduling of multilingual information resources in cloud computing.

We also construct a colored dynamic timed Petri net (CDTdPN) model for multilingual information resource scheduling in cloud computing and give a definition of the concurrent scheduling marking graph (CSMG) of CDTdPN.

We give two propositions for analyzing performances such as correctness and makespan of multilingual information resource scheduling in cloud computing using CSMG of CDTdPN.

