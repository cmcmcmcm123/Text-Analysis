Natural Language Processing and Big Data An Ontology-Based Approach for Cross-Lingual Information Retrieval

Abstract?Extracting relevant information in multilingual context  from  massive  amounts  of  unstructured, structured  and  semi-structured  data  is  a  challenging task. Various theories have been developed and applied to  ease  the  access  to  multicultural  and  multilingual resources. This papers describes a methodology for the development  of  an  ontology-based  Cross-Language Information  Retrieval  (CLIR)  application  and  shows how it is possible to achieve the translation of Natural Language (NL) queries in any language by means of a knowledge-driven  approach  which  allows  to  semi- automatically map natural language to formal language, simplifying  and  improving  in  this  way  the  human- computer interaction and communication. The outlined research activities are based on Lexicon-Grammar (LG), a  method  devised  for  natural  language  formalization, automatic  textual  analysis  and  parsing.  Thanks  to  its main  characteristics,  LG  is  independent  from  factors which are critical for other approaches, i.e. interaction type (voice or keyboard-based), length of sentences and propositions,  type  of  vocabulary  used  and  restrictions due to users?idiolects. The feasibility of our knowledge- based methodological framework, which allows mapping both  data  and  metadata,  will  be  tested  for  CLIR  by implementing a domain-specific early prototype system.

Keywords-Cross-Language  Information  Retrieval  (CLIR); Lexicon-grammar; Ontology

I.  INTRODUCTION Cross-language  Information  Retrieval  (CLIR)  allows  users  to  search  and  access  information  in  multilingual document collections on the web in languages different from their own. Usually, information is searched by means of a query expressed in the user?s mother tongue. This query is automatically translated in the desired foreign language and the results are translated back in the user?s mother tongue.

This  process  is  based  on  two  different  translation  stages: query  translation  and  document  translation.  The  query translation  concerns  the  translation  in  the  desired  foreign language of the query expressed in the user?s mother tongue,  whereas the document translation is the back translation in the  user?s  language  of  the  relevant  documents  found  by means of the translated query. For instance, an Italian user expresses a query in his/her native language and wishes to look for relevant information in English on the web: first of all the CLIR application translates the queries from Italian into  English  (query  translation)  and  searches  the  relevant information on the web, subsequently it translates again all the information and the documents detected in this way into the  user's  mother  tongue (document  translation),  i.e.  from English into Italian.

CLIR  success  clearly  depends  on  the  quality  of translations,  therefore  faulty  translations  cause  serious problems in retrieving the relevant information.  In domain specific  contexts,  indeed,  CLIR  applications  present significant shortcomings with reference to query translation, since  short  queries  do  not  provide  enough  context  for disambiguation  in  choosing  proper  translations  of  query words,  and  also  because  they  do  not  use  domain-specific semantic constraints. Furthermore, in document translation, translation inaccuracies  are  mainly  due to  lack of  domain adaptation  of  the  MT  systems  underlying  the  translation process.

This  paper  describes  a  knowledge-based  methodology aimed at the development of an ontology-based CLIR system to  overcome  the  current  limitations  of  the  state-of  the-art applications in this field, with a focus on proper processing and translation of domain specific queries.

The methodology has been set up for the Italian/English language pair and in the field of Cultural Heritage (CH) but can be easily extended to other language pairs and different domains.  The  remaining  of  this  paper  is  organized  as follows. The next section briefly explains the related work in the  area  of  CLIR.  Section  III  describes  the  methodology adopted  in  our  research.  Then,  section  IV  is  devoted  to system  overview,  and  section  V  describes  the  feasibility study presenting the Language Resources (LR), the semantic and  translation  model  used  in  the  research  work.  Finally, conclusions and future work are reported in section VI.



II. RELATED WORK There are several approaches to CLIR, either based on  bilingual  or  multilingual  Machine  Readable  Dictionaries  SocialCom/PASSAT/BigData/EconCom/BioMedCom 2013  DOI 10.1109/SocialCom.2013.108     (MRD),  Machine  Translation  (MT),  parallel  corpora  and finally ontologies [1, 2, 3].

MRD-based and MT-based CLIR are the most popular approaches  but  they  present  several  shortcomings, particularly  in  domain-specific  contexts,  because  of  the inaccurate  processing  and  translation  of  multi-word  units (MWU),  a  very  frequent  and  productive  linguistic phenomenon in languages for special purposes (LSP).

MWUs are lexical elements composed of more than one word which have a particular structural and semantic internal cohesion, act as single lexical units and belong to different lexical categories (to look at, heavy water, arsenic water; as soon as possible; in order to are examples of MWUs). They are constructions half way between morphology and syntax since  they  have  a  very  similar  structure  to  phrases,  but present distribution and cohesion characteristics  which are very close to words.

In  domain-specific  contexts,  the  most  frequent  MWU typology is represented by terminological compound words, such as  doric frieze or  spiral stem in the Cultural Heritage domain. Terminological compound words, with a limited or no  variability  of  co-occurence  among  constituents,  often have unpredictable, non-literal translations and are therefore particularly difficult to process and translate.

Various  techniques  have  been  proposed  to  reduce  the errors due to the presence of MWU introduced during query translation in CLIR applications.  Among these techniques, phrasal  translation,  co-occurrence  analysis,  and  query expansion are the most popular ones.

Concerning phrasal translation, techniques are often used to identify multi-word concepts in  the query and translate them as phrases [1, 4, 5, 6, 7].

Co-occurrence  statistics  is  used  to  identify  the  best translation(s)  among  all  translation  candidates  using  text collections  in  the  target  language  as  a  language  model, assuming that correct translations occur more frequently than wrong ones [6, 8, 9, 10].

As for query expansion techniques, [5, 11] assume that additional terms that are related to the primary concepts in the query are likely to be relevant and that phrases in query expansion via local context analysis and local feedback can be  used  to  reduce  the  error  associated  with  automatic dictionary translation.

Concerning  MT-based  CLIR,  in  spite  of  the  recent positive  developments  in  Machine  Translation,  the identification,  interpretation and translation of MWUs still represent  open  challenges,  both  from  a  theoretical  and  a practical point of view. There is an increasing attention to MWU processing in MT, as witnessed by the recent ?Multi- word  Units  in  Machine  Translation  and  Translation Technology? workshop at MT Summit XIV.

MWU processing and translation in Statistical Machine Translation  (SMT),  the  current  leading  approach  in  MT, started  being  addressed  only  very  recently  and  different solutions have been proposed so far, but basically they are considered either as a problem of automatically learning and integrating  translations,  word  alignment  and  finally  Word Sense Disambiguation (WSD).

Current approaches to MWU processing move towards hybrid  approaches,  where  phrase-based  models  are integrated with linguistic knowledge and scholars are starting to use linguistic  resources,  either  hand-crafted dictionaries and grammars or data-driven ones, in order to identify and process  MWUs as  single  units.  [12]  provides  a  thorough overview of the different approaches to MWU processing in MT.

Ontologies are also used in CLIR and are considered by several  scholars  a  promising research area to  improve the effectiveness of CLIR techniques particularly for technical- domain queries. [13] use ontologies as interlingua in cross- language information  retrieval  in  the  medical  domain  and show  that  the  semantic  annotation  outperforms  machine translation of the queries, but the best results are achieved by combining a similarity  thesaurus with the semantic  codes.

[14] perform ontology-based query expansion of  the most relevant terms exploiting the synonymy relation in WordNet.



III. METHODOLOGY Our  methodology  is  based  on  the  Lexicon-Grammar  (LG)  theoretical  and  practical  analytical  framework.  LG theory was conceived by the French linguist Maurice Gross during  the  ?60s  [15,  16,  17].  Unlike  Chomsky?s transformational grammar and its various offspring [18, 19], LG  assumes  that  linguistic  formal  descriptions  should  be based on the observation of the lexicon and the combinatory behaviors  of  its  elements,  encompassing in  this  way both syntax and lexicon. It has also reached important results in the domain of automatic textual analysis and parsing, with the creation of software and lingware fully oriented toward NLP, such as NooJ1, and former software packages used in the  LG  framework,  such  as  INTEX  and  UNITEX2.

Nowadays,  LG methodology  is  being  adopted  by  a  wide research community for  several  European (French,  Italian, Portuguese, Spanish; English,  German, Norwegian; Polish, Czech,  Russian,  Bulgarian;  Greek)  and  non-European languages (Arabic; Korean; Malagasy; Chinese; Thai...).

This  methodology  is  particularly  suitable  for  CLIR applications due to the fact that the quality of translation is guaranteed  by  a  linguistic  approach  aimed  at  the development  of  a  coherent  and  formalized  linguistic knowledge  basis.  This  knowledge  basis  is  composed  of Linguistic  Resources  (LRs),  mainly  electronic  dictionaries and grammars (described in Section IV) useful in achieving effective  Information  Retrieval  (IR)  Systems  [20] and  in overcoming  the  weaknesses  of  the  current  statistical approaches used in MT by Google Translate or Bing, where the lack of meaning context represents a serious obstacle to MWU disambiguation. LG linguistic framework is grounded in the analysis of  ?simple sentences?3, the minimal linguistic  1 See http://www.nooj4nlp.net/pages/nooj.html.

2 More information on the website http://www-igm.univ-mlv.fr/~unitex/.

3 In LG, a simple sentence is a context formed by a unique predicative  element (a verb, but also a name or an adjective) and all the necessary arguments selected by the predicate in order to obtain an acceptable and grammatical sentence.  For more specification on simple sentence definition (Gross, 1968)     meaning  contexts  within  which  word  combinatorial behaviors can be analyzed. The study of simple sentences is achieved by analyzing the so-called rules of co-occurrence and  selection  restriction,  i.e.  distributional  and transformational rules based on predicate syntactic-semantic properties.  Also,  LG  theory  is  prevalently  based  on  the concept  of  Operator-Argument  Grammar  [21],  therefore  it highlights  transformational  rules  (active/passive, positive/interrogative, etc.) and mutual relationships between simple sentences as they have been observed by Zellig Harris [22],  starting  from the bloomfieldian  notion of  morpheme and  the  method  of  commutation  or  equivalence  between different morphemes? available lexical stuffs [23].

Thanks to these above-mentioned research studies,  LG range  of  analysis  concerns  lexicon,  and  especially  the concept  of  MWU  as  ?meaning  unit?,  ?lexical  unit?  and ?word  group?,  for  which  LG  identifies  four  different combinatorial  behaviors  [24].  These  meaning  units  are collected  and  described  inside  LRs  consisting  of  (I) morphologically  and  semantically  tagged  electronic dictionaries, (ii) local grammars in the form of Finite State Transducers/Automata (FSTs/FSA), used to parse texts,  and finally (iii) binary tables which describe syntactic-semantic properties of lexical entries (refer to V.A and  V.B).

A. LG Methodology to Assess the Translation Quality The  quality  of  translations  is  guaranteed,  from  the  beginning,  by  the  development  of  highly  formalized  LRs according  to  morphological,  syntactical  and  semantic criteria.  Using  smart  technologies  for  translation technologies  often  implies  the  deterioration of  Translation Quality  (TQ).  In  LG  methodology,  instead,  we  take advantage of well-formed LRs to to keep a high level of TQ, since from the beginning, a supervised approach is carried out by expert linguists during the proper setting of resources.

Assessing  the  quality  of  resources  before  they  are translated  prevents  from  too  many  subsequent  checks  on translated  resources,  even  if  an  ex  post evaluation  of  TQ results is always necessary.

According to LG a valid evaluation methodology should be  based  on  a  hybrid  approach,  that  encompasses  both human and automatic evaluation.

The process is composed of two cycles. The first cycle can be outlined as follows (i) a query expressed in a Source Language (SL) is the input of the CLIR application, (ii) the MT system produces sample queries (i.e. sample texts) in the Target Language (TL), (iii) the resulting translated queries are  examined  by  humans  (Linguists,  Translators, Terminologists/Domain  Experts)  to  evaluate  their  quality.

The human judgements are based on common criteria of TQ ? i.e.  adequacy  and  fluency  ?  and  are  expressed  using  a Likert scale with scores 1-5 (for instance using follow-ing judgements:  1.  Strongly  disagree,  2.  Disagree,  3.  Neither agree nor disagree,  4. Agree, 5. Strongly agree),  (iv) only texts  which  obtained  scores  4-5  become  ?validated?  and ?supervised? texts which represent the gold standard, (v) this gold standard is the training set for the Automatic Evaluation  process, that can be carried out using METEOR4 and GTM5, that are the most suitable methods according to our opinion, as well as other ones6.

During the second cycle,  human evaluation is  skipped and the SL queries directly become the input for automatic evaluation.

It  is  necessary  to  periodically  repeat  the  first  cycle  in order to enrich the training set and to increase the quality cycle.



IV. SYSTEM OVERVIEW The proposed architecture aims to map data and metadata  exploiting  the  morph-syntactic  and  semantic  information stored  inside  both  electronic  dictionaries  and  Finite  State Automata/Finite State Transducers (FSA/FSTs) (presented in V.B).

Furthermore,  we use our application in order to define entities and to model relationships, mapping linguistic tags (i.e. POS) and structures (i.e. sentences, MWUs) to domain concepts.

Indeed, in CLIR systems the most frequent error is the assignment  of  wrong  Parts  Of  Speech  (POS)  to  lexical meaning  units,  as  stated  in  [25]:  ?the  complexity  of  the grammatical  structures  and  the  quality  of  parsing  are  the main cause of the errors?.

Our  research  framework  could  allow  achievement  of major  improvements  in  IR  and  IE,  both  in  recall  and precision.

The system is based on two workflows which are carried out simultaneously but independently. As described in Fig. 1, before the execution of a query against a knowledge base it is necessary to apply the Translation and the Transformation routines.

The  first  step  performed  is  a  linguistic  pre-processing phase  which  formalizes  (i.e.  converts)  natural  language strings  into  reusable  linguistic  resources.  During  this  first phase  we  also  extract  information  from  free-form  user queries, and match this information with already available ontological domain conceptualizations.

The advantages in keeping separate transformation and translation routines are:  ? the  development  of  an  architecture  with  a  central multilingual formalization of the lexicon, in which there  is  no  specific  target  language,  but  each language can be at the same time target and source language;  ? the  development  of  extraction  ontologies  and SPARQL/SERQL adaptation  systems  which  could represent  a  standard  not  only  for  our  multilingual electronic dictionaries, but also for any lexical and/or language data-base for which translation is required.

With  this  dual-structure  system,  it  is  easier  to successfully  achieve the  question  answering (QA) process  4 http://www.cs.cmu.edu/~alavie/METEOR/.

5 http://nlp.cs.nyu.edu/GTM/.

6 BLEU and NIST (based only on precision measure), F-Measure (based  also on recall).

since the answers are given explicitly in the target language chosen by the user and the translation process is separated from the matching with the RDF triples.



V. FEASIBILITY STUDY To test the feasibility of our architecture, we are carrying  out a translation experiment from Italian into English, using all  ontological  and  semantic  constraints  defined  for  the Italian model.

We have chosen the Archaeological domain to test the applicability  of  our  approach.  This  choice  allows  us  to demonstrate that the modularity of our architecture may be applied to a domain which is variable by type and properties and is semantically interlinked with other domains.

In  the  next  paragraphs,  we  will  present  the  LRs developed for our study, together with the a description of the semantic annotation and the translation routines used in query translation.

A. Electronic Dictionaries An  electronic  dictionary  is  a  lexical  database  homogeneously  structured,  in  which  the  morphologic  and grammatical  features  (gender,  number  and  inflection)  of lexical  entries  are  formalized  by means  of  distinctive and non-ambiguous  alphanumeric  tags  [26,  27].  All  electronic dictionaries, built according to the LG descriptive method, form the DELA7 system, which is also used as a linguistic knowledge-base embedded in several NLP applications (text mining, IR, QA systems among others) and parsers. DELA electronic dictionaries are of two types:  ? simple  word  dictionaries,  which  include semantically  autonomous  lexical  units  formed  by  7 Dictionnaire  ?lectronique  of  LADL  (Laboratoire  d'Automatique Documentaire et Linguistique).

character  sequences  delimited  by  blanks,  such  as home and chair;  ? compound word dictionaries, which include lexical units composed of two or more simple words with a non-compositional  meaning,  such as  nursing home and rocking chair.

Terminological  entries,  as  already  stated  the  most common obstacle in CLIR applications, are lemmatized in compound word electronic dictionaries.

The following example represents an excerpt extracted from the Italian dictionary of Archaeological Artifacts8  freccia di balestra, N+NPN+FLX=C45+ DOM=RA1SUOARAL+ EN=crossbow arrow, N+AN+FLX=EC3  freccia foliata, N+NA+FLX=C556+DOM=RA1SUOIL + EN=leafed arrow, N+AN+FLX=EC3  fregio con coronamento, N + NPN + FLX=C12 + DOM=RA1EDEAES + EN=frieze crown, N+AN+FLX=EC3  fregio dorico, N+NA+FLX=C523 + DOM=RA1EDEAES + EN=doric frieze, N+AN+FLX=EC3  fusto a spirale, N + NPN + FLX = C7 + DOM=RA1EDEAES + EN=spiral stem, N+AN+FLX=EC3  For instance, the compound word  fregio dorico (?Doric frieze?)  is  marked  with  the  domain  tag ?DOM=RA1EDEAES?,  which  stands  for  ?Archaeological Artifacts  ? Building ? Architectural  Elements  ? Structural Elements?.

For each entry, a formal and morphological description is also given, which includes:  ? the internal structure of each compound. This means that  in  the  compound  word  fregio  dorico  the  tag ?NA? indicates that the given compound is formed by a Noun, followed by an Adjective. At the same time, in the compound word fregio con coronamento the tag ?NPN?, indicates that the given compound is formed  by  a  Noun  followed  by  a  Preposition, followed by a Noun;  ? the  inflectional  class.  This  means  that  the  tag ?+FLX=C523? indicates the gender and the number of  the  compound  fregio  dorico,  together  with  its plural  form,  i.e.  it  indicates  that  fregio  dorico is  8 It?s important to specify that our domain dictionaries, collected in the DELAC/DELACF system,  cover about 180 different  semantic  tags.

The  most  important  dictionaries  are  those  of  Computer  Science (54,000  entries  ca.),  Medicine  (46,000  entries  ca.),  Law  (21,000 entries) and Engineering (19,000 entries ca.). Each dictionary has been created and verified under the supervision of domain experts. Subset tags  are  also  previewed  for  those  domains  that  include  specific subsectors.  This  is  the  case  of  Archaeological  Artefacts  dictionary (9,200 entries ca.) , for which a generic tag RA1 is used, while more explicit  tags  are  used  for  object  type,  subject,  primary  material, method of manufacture, object description. In order to develop Italian- English  dictionary  of  Archaeological  Artefacts,  we  relied  on  the Thesauri  and  Guidelines  of  the  Italian  Central  Institute  for  the Catalogue  and  Documentation  (ICCD)  available  at http://www.iccd.beniculturali.it/index.php?it/240/vocabolari.

Figure 1. System Workflow     masculine  singular,  does  not  have  any  feminine correspondent  form,  and  its  plural  form  is  fregi dorici.  Each inflectional class is associated to a local grammar which produces  the inflected forms of the word according to the inflection class associated to it.

On the  other  hand,  local  grammars  formally  describes syntactic features of natural languages and are used in NooJ9 to parse texts. Such grammars are defined ?local? because each one of  them account  for  one and only one syntactic profile of a given predicate. This means that for instance we build a local grammar only to describe the syntactic features of the verb to give. Local grammars are built and developed in the form of FSA/FSTs [28].

B. Semantic annotation As for ontologies, the formal definition we rely upon is  the one given by the International  Council  of  Museums - Conseil  Interational  des  Musees  (ICOM  ?  CIDOC) Conceptual Reference Model (CRM) [29]. CIDOC CRM is composed of two different hierarchies, one composed of 90 classes  (which  includes  subclasses  and  superclasses)  and another  of  148 unique properties  (and subproperties).  The object-oriented  semantic  model  and  its  terminology  are compatible  with  the  Resource  Description  Framework (RDF). Actually, this ontology was already available and is constantly developed.  At the same time,  our methodology shows  that  a  given  linguistic  knowledge  can  be  reused independently from the domain to which it pertains. Domain ontologies refer  to  mid-  and upper-level  ones,  which tend pragmatically  to  be  standardized.  Logically,  such  process indirectly involves also low-level ontologies, and this allows  the reuse of linguistic resources regardless of the domain in which they were developed or to which they pertain.

LRs are used for analyzing corpora to retrieve recursive phrase structures, in which combinatorial behaviours and co- occurrence between words identify properties, also denoting a  relationship.  Furthermore,  electronic  dictionaries  also include all inflected verb forms allowing to process queries expressed  also  with  passive  and  more  generally  non- declarative sentences.  Consequently we use FSA variables for identifying ontological classes and properties for subject, object and predicate within RDF graphs.

This matching of linguistic data to RDF triples and their translation  into  SPARQL/SERQL path  expressions  allows the use of specific meaning units to process natural language queries.

9 NooJ  is  an  NLP  software  environment  which  uses  electronic dictionaries and local grammars we use automatically read and parse digitized texts. For more on NooJ, see www.nooj4nlp.net.

Figure  2  is  a  sample  of  an  automaton  showing  an associated RDF graph for the following sentence:  Il  Partenone (subject)  presenta (predicate)  colonne doriche e ioniche (object).

According to our approach, electronic dictionaries entries (simple words and MWUs) are the subject and the object of the RDF triple.

In Figure 3 we develop an FSA with a variable which applies to the sentence the following classes and property:  ? E19 indicates ?Physical Object? class; ? P56 stands for ?Bears Feature? property; ? E26 indicates ?Physical Feature? class.

So, the FSA variables transform our sentence into: Il  Partenone  (E19) bears  feature colonne  doriche  e  ioniche (E26).

The  role  pairs  Physical  Object/name and  Physical  Feature/type are trigged by the RDF predicate presenta.

Besides in Fig. 3 we also indicate specific POS for the  first  noun phrase  Il  Partenone (DETerminer  + Noun),  the verb  presenta (V)  and  the  second  noun  phrase  colonne doriche e ioniche (Noun+Adjective+Conjuntion+Adjective).

By applying the automaton in Fig. 3 (built using the high variability of lexical class and not of the original form) we can recognize all instances included in E19 and E26 classes, the property of which is P56.

C. Query Translation In  our  model,  the  Translation  Routines  are  applied  independently of the mapping process of the pivot language.

This  allows  us  to  preserve  the  semantic  representation  in both languages. Indeed, identifying semantics through FSA guarantees the detection of all data and metadata expressed in any different language.

Figure 4 illustrate a FST in which a translation process from Italian to English is performed combining information coming  from  dictionaries  and  grammars  of  analysis  with lexical  transfer.  This  translation  FST,  recognizes  and annotates  the  different  linguistic  elements  of  declarative sentences such as Il Partenone presenta fregi dorici, I templi romani  hanno  fusti  a  spirale,  etc.,  with  their  morpho- syntactic and semantic information and performs automatic translations on the basis of an LG Italian-English bilingual dictionary.  For instance,  if  a grammar variable, say $E26, holds  the  value  fusti  a  spirale,  the  output  $E26$EN will  Figure 3. Sample of the use of the FSA variables for identifying classes for subject, predicate and object.

Figure 2. Simple FSA with RDF Graph.

produce the correct English translation ?spiral stems?, on the basis  of  the  value  associated  to  the  +EN  feature  in  the bilingual entry ?fusto a spirale, N + NPN + FLX = C7 + OM=RA1 + EN=spiral  stem,  N+AN+FLX=EC3?  and  the morpho-syntactic analysis performed by the graph in Figure 4,  which  identifies  and  produces  the  plural  form  of  the compound noun fusto a spirale.



VI. CONCLUSIONS AND FUTURE WORK The architecture described in the previous pages ensures  both the coverage of large quantities of knowledge and the preservation  of  any  deep  semantic  relations  which  may interlink different languages. Our future work aims therefore at  further  developing our  system:  we will  implement  new Linguistic Resources in order to test the accuracy of cross- language  information  retrieval.  Futhermore,  we  will  carry out a larger number of experiments in different domains on selected corpora.

NOTE Johanna Monti is author of sections I, II and V.C, Mario  Monteleone is author of sections V.A and VI, Maria Pia di Buono is  author  of  sections  IV,  V and V.B and Federica Marano is author of section III and III.A.

