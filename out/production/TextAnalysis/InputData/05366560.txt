Frequent Itemsets Discovery Algorithm and its application based on Frequent Matrix

Abstract: There are varieties of linkages among the  properties of the data in a large database,  These linkages are  hidden in the data of the database, the purpose of association  mining is to identify these hidden association rules. The  discovery of association rules is based on frequent itemsets, so  how could we make frequent itemsets fast and accurate is the  main research of association data mining. In this paper, a  matrix-based frequent discovery algorithm is raised according to  the problems in the existing algorithms in finding frequent  itemsets, and it has a practical application in the automobile  fault parts association analysis system of automotive industry  chain business intelligence platform..

Key words: frequent itemsets, association rules, data mining  ? INTRODUCTION  The quality of automobile is the base of the a automotive manufacturers? survival, there are many ways to improve the quality of products, but it is mainly by improving the quality of parts and vehicle design to achieve the objectives of demand of improving product quality for enterprises. One of the major premises is built on a scientific model fault analysis. Models? fault analysis is mainly performed by seeking the relationship between the fault and the information of vehicle mileage, the use of time, the territory of use, accessories brands and related fault accessories when some accessories error occurred.

The basic concepts and classical algorithm algorithm-Apriori of Association Rules Mining were proposed by R.Agrawal [1], algorithm-Apriori is comprised of looking for frequent itemsets and the discovery of association rules in two parts, the algorithm is simple, intuitive, easy to implement, but there are still two serious problems in its frequent itemsets search[2]: A great deal of  I/O load is needed when scanning the database several times; It may produce a large candidate itemsets, it?s a challenge to time and living space.

In this paper, Frequent Itemsets Mining Algorithms Based on Frequent Matrix is raised for the two problems.

This algorithm avoid the frequent scanning the databases, minimize the number of candidate itemsets and improve the efficiency of Association Rules Mining.

? FREQUENT ITEMSETS MINING ALGORITHMS BASED ON FREQUENT MATRIX  Frequent discovery algorithm based on binomial itemsets matrix raised in this paper is a kind of algorithm that discovers all the most likely Frequent itemsets and then verify whether they are frequent or not one by one in the constraints of special matrix.

A. The definition of frequent itemsets  Assume I = {i1?i2?? , im} is a collection of all the items. D is a collection of database transactions, transaction T is a subset of a project. Each transaction has a unique identifier TID. Assume A is a collection consisting of project, called itemset, transaction T contains A. If the project collection A contains k items, then called k-item, recorded as Length(A)=k, times that project A appears in transaction database D is called Itemset Support. If the Support is more than the minimum support threshold given by user, we call the itemset is a frequent itemset.

B. Definition and nature of binomial itemsets matrix  Definition 1: Set the total number of items in database to p, serious number of items followed by 1, 2, ?, p, matrix Ap?p?s element a[i,j]=Support, 1?i?p, 1?j?p (a[i,i] regarded as a special binomial itemset), we regard A as binomial itemsets phalanx of D, It is obviously that the matrix is symmetric on the main diagonal.

Definition 2: Assume X is an itemset of database, Length(X)=k (k?3),  Bk?k is a phalanx generated by Definition 1, matrix B?s element b[i,j]=Support, i, j is arbitrary item number in X, then we call B the     Corresponding binomial itemsets phalanx of itemset X and a subset of A matrix.

Definition 3: Assume B is a phalanx defined by 2, if arbitrary b[i,j]?Minsupport, we call B frequent binomial itemsets phalanx of X.

Theorem 1  Assume X is a database itemset and Length(x)=k(k?3), then the necessary condition for X of being frequent itemset is: all of its binomial subsets are frequent.

Proof: According to reference [3]: ??X ,if YX ? , if Y is a frequent itemset, then X is also a frequent itemset, therefore, if a k-itemset is frequent, its all nonempty sets are frequent, it is obviously that all binomial itemsets are frequent.Proof is completed.

Theorem 2  Assume X is a frequent itemset in D, and Length(X)=k(k?3), items of X are numbers defined by 4, then rule out the rows and columns of items in matrix A that do not belong to X, according to Definition 3: Sub-matrix B that consisted by the rest binomial itemsets will surely be a frequent binomial itemsets phalanx.

Proof: According to Definition 2, this matrix is corresponding binomial itemsets matrix of X, we can see from the conditions that X is frequent, according to theorem 1, we can see all binomial subsets of X are frequent, with the Definition 6 we know that B is frequent binomial itemset phalanx.

Therefore, the proposition is established and the proof is completed  For binomial set matrix is symmetrical to the main diagonal of matrix, we all use upper triangular matrix to improve budget speed, reduce storage space in practical applications,  Theorem 3  Assume X is an itemset in D, and Length(X)=k(k?3), then the necessary condition for X is a frequent set is: binomial phalanx of itemset X is frequent phalanx.

Proof of this theorem is a direct consequence from theorem 1 and will not be repeated.

The significance of theorem 2 and theorem 3 is that we can get the binomial matrix of the database by scanning it  once first, then search this matrix to get the number of frequent phalanx n which is number of maximum candidates of k(k?3) frequent itemsets; All itemsets collections corresponding to frequent phalanx is frequent candidate collections of D(excluding 1,2-frequency set), as the phalanx? generation is based on frequent binomial itemsets, in essence, it?s a kind of ?pruning? to the candidates based on frequent matrix, so the number of candidate sets is reduced greatly. The maximum crest reduction of unnecessary itemsets mining improved the mining efficiency.

C.  frequent candidate itemsets generation algorithm  For p?p matrix A defined in definition 1, we use breadth-first search algorithm to find out all possible full-frequency sub matrix, and store them with tree-structure(item candidate tree).

Step 1, Search main diagonal elements, if a[i,i]?Minsupport, then we create a child node i under the super-root node, corresponding to the special frequent itemset a[i,i].

Step 2, Search elements a[i,j](j?p) in the right side of matrix a[i,i],if a[i,j]?Minsupport, then we create a child node j under node i(a[i,j] corresponding to frequent itemset a[i,j]);  Step 3, Create grandchildren node for all child node of i in accordance with step 2, and so on, until we get the only child node.

Through this search, we finally get a tree, it?s obviously that matrix that consisted by items of the tree?s each path might be a frequent binomial itemset matrix. The longest path of the tree is the maximum frequent itemset that may exist.

Preorder traversal the tree, we find out all paths, all items of each path(except the root node) can form a binomial phalanx for each node is a frequent two sets, for example, we traverse to the third layer to get a 3?3 binomial matrix, to the third layer   we get a 4?4 binomial matrix, the k-th layer might get a k?k binomial matrix, Then, determine which of the phalanx is frequent according to Definition 3, itemsets corresponding to each frequent phalanx is the frequent candidate itemsets we are looking for, then we put the itemsets into a database of candidate frequent itemsets.

For 1,2-item frequent itemsets can be easily found from binomial itemsets matrix, and 1,2-item frequent itemsets are generally large, so that in order to enhance the research speed, we don?t generate 1,2-item by tree, and 2-item frequent sets will not be stored in the database of frequent candidate itemsets neither.

D.  frequent candidate itemsets support algorithm  After the frequent candidate itemsets database has been gotten by 2.3, these itemsets need to scan transaction database for a second time, then we calculate the support degree, it can be  true frequent itemsets if it is no less than Misupport, we calculate support degree of candidate itemsets fast through vector inner product calculation.

Definition 4 Assume total item number in transaction database D is p, serial numbers of items followed by 1,2, ?,p, then the number of item of frequent candidate itemsets database will not exceed p, transaction bit vector Tran_Vec is defined as follows:  Support algorithm is calculated as follows: Assume Ficd is frequent candidate itemset database, the  number of records(the number of candidate itemsets) is Fic_count.

Step 1, Deine a Fic_count?p matrix Ficm, each row of the matrix is p-dimensional bit vector that generated according to Definition 4, corresponding to a candidate itemset of database; Define a Fic_count?1 matrix for the storage of each candidate itemset support in Ficd, set the initial value of each row of the matrix to 0; Define a Fic_count?1 matrix inner_product_Result_Matrax to store the results of temporary calculation.

Step 2, Get the first record from the transaction database, transfer it to p-dimensional vector Trant_Vecr in accordance with the definition 4.

Step3,Inner_product_Result_Matrax=Ficm?Tran_VecT Step4,Itemsets_Support_Matrax=  Itemsets_Support_Matrax+ Inner_product_Result_Matrax Step 5, return to Step 2 until all transactions of records  are processed.

Algorithm takes a second scan on transaction database, calculate all candidate itemsets support in Ficd dand store them into matrix Itemsets_Support_Matrax, in this matrix, candidate itemsets corresponding to the elements whose values are bigger than Minsupport is the final frequent itemsets that we are looking for.

E. Association rules discovery algorithm  This paper raised a new frequent itemsets discovery algorithm and didn?t design special association rules discovery algorithm, in the following applications in this paper, association rules discovery algorithm uses the association rules discovery algorithm according to literature 1 directly, and not repeat any more.

?  ALGORITHM FOR SPECIFIC APPLICATION OF THE RESULTS  The quality of automobile is the base of the a automotive manufacturers? survival, a lot of decisions of enterprises are made around how to improve product quality, the quality of spare parts is good or bad and the design of product is good or bad are the main factors which affecting the quality of automobiles, how to make decisions has become a major problem. Using the method of association rules mining we can find the relationships among fault parts, the design department can find out the cause of problems according to the results of mining, then they change brand of parts or find out design flaws, and redesign it to achieve the objective of improving product quality.

By discussing with related decision makers of a Sichuan Automobile Co., Ltd, using three bags of their enterprise data for 2004-2007, the system development environment is Microsoft Visual Studio 2008, developing language is C# and database system is Microsoft SQL Server 2005. The analysis result is shown in Table 1.

?  CONCLUSION With the global economy is moving into an era of  information analysis, the ability of information processing and utilization is the key to make an enterprise succeed or defeat. Enterprises hope to transfer the transaction data into reliable information, then transfer the knowledge into profit.

Choosing the appropriate algorithm is very important, this paper improved mining efficiency by improving association  (1)p21i t1 t0  _ ][ ???= ? ? ?  ? ?  = project project  VecTran i    algorithm, and achieved the association mining of parts fault.

Table 1.  the result of parts association faults mining.

