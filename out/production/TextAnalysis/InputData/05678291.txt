An Algorithm of Fast Mining Long Frequent  Neighboring Class Set

Abstract?As present frequent neighboring class set mining algorithms inefficiently extract long frequent neighboring class set, and so this paper introduces an algorithm of fast mining long frequent neighboring class set. To fast search long frequent neighboring class set in large spatial data, this algorithm uses down search strategy to generate candidate frequent neighboring class set. But the course of down search strategy used by the algorithm isn?t different from present down search strategy, which need set position of k-subset when (k+1)-non frequent neighboring class set generates its all k-subset. By the method, the algorithm may delete repetitive candidate item sets and redundant computing. Because the algorithm creates digital database of neighboring class set via neighboring class weight, and so it computes support of candidate frequent neighboring class set by digit logical operation. The algorithm improves mining efficiency through these methods. The result of experiment indicates that the algorithm is faster and more efficient than present algorithms when mining long frequent neighboring class set in large spatial data.

Keywords- Long frequent neighboring class set; down search; setting position; neighboring class weight; spatial data mining

I.  INTRODUCTION Geographic Information Databases is an important spatial  database in spatial data mining, mining spatial association rules from Geographic Information Databases is one important part of spatial data mining and knowledge discovery (SDMKD), which is also known as spatial co-location pattern as in [1].

Spatial co-location pattern are some implicit rules expressing construct and association of spatial objects in Geographic Information Databases, and also expressing hierarchy and correlation of different subsets of spatial association or spatial data in Geographic Information Databases as in [2]. At present, in spatial data mining, three kinds approaches are mainly used to mining spatial association rules as in [3], such as, layer covered based on clustering as in [3], mining approach based on spatial transaction as in [2, 4, 5, and 6] and mining approach based on non-spatial transaction as in [3]. The first two methods may be also used to mining frequent neighboring class set, The spatial association as in [4, 5, and 6] is very single, because they only express spatial association among these objects which are all close to objective. However, neighboring class set expresses another spatial association among these objects which are close to each other. MFNCS as in [2] uses  the similar method of Apriori to search frequent neighboring class set, which gains some right instance of (k+1)-neighboring class set only through connecting right instance of k- neighboring class set according to down-top strategy, and so this algorithm is only suitable for mining short frequent neighboring class set according to character of Apriori. Hence, this paper introduces an algorithm of fast mining long frequent neighboring class set, denoted by MLFNCS, which is faster and more efficient than present algorithms when mining long frequent neighboring class set in large spatial data.



II. PRELIMINARY KNOWLEDGE According to these basic contents as in [2], a spatial data set  is made up of all these objects in spatial domain. These objects are expressed as <Object Identify, Class Identify and Spatial Location>. Here, identify of different class in spatial data set is denoted by Class Identify, identify of different object instance in the same class is denoted by Object Identify, location coordinate of object is denoted by Spatial Location. We regard an object as an instance of corresponding class and so these instances of spatial Class Identify constitute spatial data set.

Sets of Class Identify are thought as a class set, denoted by C = {C1, C2?Cm}, means there are m different classes.

A. Definition and Property Definition 1 Neighboring Class Set, it is a subset of spatial  class set in spatial data set, which is denoted by {Ct1, Ct2?Ctk} (tk ? m), denoted by NCS.

Let I = {it1, it2?itk} be an instance of neighboring class set expressed as NCS = {Ct1, Ct2?Ctk}, here, itj is an instance of Ctj (j?1, 2?k).

Example, let {V, W, X, Z} be a NCS, and I = {V2, W3, X4, Z1} is an instance of NCS.

Definition 2 Neighboring Class Set Length, its value is equal to the sum of class contained in neighboring class set. If the length of NCS is equal to k, it is expressed as k-NCS.

Example, let {W, X, Y, Z} be a NCS, and its length is 4.

Definition 3 Right Instance of Neighboring Class Set, here, suppose I = {it1, it2?itk} be an instance of NCS, if ?  ip and iq (ip, iq ? I), and distance (ip, iq) ? d, and then we think I be an right instance of NCS. Here, d is the minimal distance used by  This work was fully supported by science and technology research projects of Chongqing Education Commission (Project No. KJ061101), and it was also fully supported by science and technology research projects of Chongqing Three Gorges University (Project No. 10QN-22).

judging two spatial objects are close to each other, Euclidean distance is denoted by distance (ip?iq).

Definition 4 Neighboring Class Set Support, it is equal to the sum of right instance of neighboring class set, which is denoted by support (NCS).

Definition 5 Frequent Neighboring Class Set, its support is not less than the minimal support given by user.

Property If (k+1)-NCS is frequent neighboring class set, and then k-NCS will be also frequent neighboring class set.

Here, k-NCS ? (k+1)-NCS.

B. Problem Description According to above these definitions, property and these  contents as in [2], we describe frequent neighboring class set mining as follows:  Input:  (1) Class set is expressed as C = {C1, C2?Cm}, instance set is expressed as I = {i1, i2?in}, each ik (ik? I) is expressed as above defined data structure.

(2) Minimal distance is expressed as d.

(3) Minimal support is expressed as s.

Output:  Frequent neighboring class set.



III. THE ALGORITHM  OF MINING LONG FREQUENT NEIGHBORING CLASS SET  A. Creating Mining Database If we create digital database of neighboring class set, we  will turn every corresponding NCS of every right instance into a digit. The course of turning used by the algorithm is expressed as follows:  Step1, firstly, we define the order of class as C = {C1, C2? Cj?Cm}, Here j is defined as order of class, and so each class inside right instance has an order expressed as Noj.

Step2, and then, we use weight to denote every class, if order of this class is expressed as Noj, and this neighboring class weight is expressed as 12 ?jNo .

Step3, we will gain an integer by computing this value,  expressed as? =  ? L  j  Noj   12 , here L is Neighboring Class Set Length,  and namely, it is the sum of class inside this neighboring class set.

By this method, the algorithm will gain an integer through scanning every right instance and this integer also express this corresponding NCS of right instance.

Now we save these integers by this data structure expressed as follows:  Structure neighboring class set {  Int value; // saving integer expressed as NCS of right instance  Int count; // saving the sum of same integer expressed as NCS} NCS  Example, here class set is denoted by C = {V, W, X, Y, Z}, the first three right instances are denoted by I1 = {V5, W4, X3, Y2}, I2 = {X2, Y3, Z4}, I3 = {V2, W3, X4, Y5}.

Now we use the previous introductive method to create digital NCS database, this course is expressed as follows:  NCS of I1 is denoted by {V, W, X, Y}, and these orders of classes are denoted by {1, 2, 3, 4}, and then integer = 2(1-1) + 2(2-1) + 2(3-1) + 2(4-1) =15, NCS [0].value =15, NCS [0].count=1.

NCS of I2 is denoted by {X, Y, Z}, and these orders of classes are denoted by {3, 4, 5}, and then integer = 2(3-1) + 2(4-1) + 2(5-1) =28, NCS [1].value =28, NCS [1].count=1.

NCS of I3 is denoted by {V, W, X, Y}, and these orders of classes are denoted by {1, 2, 3, 4}, and then integer = 2(1-1) + 2(2-1) + 2(3-1) + 2(4-1) =15, because NCS [0] has already saved this information, and only NCS [0].count=NCS [0].count+1=2.

.......

B. The course of generating candidate frequent neighboring class set The algorithm generates candidate frequent neighboring  class set via down strategy. Namely, it uses computing k-subset of (k+1)-non frequent neighboring class set to generate k- candidate frequent neighboring class set. The strategy is similar to B_UDMA as in [7] and ITDASN as in [8], but the course of generating subset is not different from the two kinds of method.

Now we introduce the difference of generating course.

We save candidate frequent item by the structure as follows:  Structure Candidate Neighboring Class Set {  Int value; // saving integer expressed as NCS  Int length; //saving length of NCS  Int No; // saving setting position of generating subset  Int [m] location; //saving the order of class in NCS  } CNCSet, NFSet  As this knowledge in [9], the principle of setting position about k-subset of (k+1)-non frequent neighboring class set is expressed as follows:  Principle Each k-subset of (k+1)-non frequent neighboring class set is set position only once when it is firstly generated from (k+1)-non frequent neighboring class set, i.e. setting position of subset is only one when it is saved in CNCSet.No.

Example, here are two 4-NCS denoted by  {30, 23}, 3-NCS denoted by 22 is not only 3-subset of 30, but also 3-subset of 23, obviously, it is the third time that 30 generates 22 when 30 generates its all 3-subsets, it is the first time that 23 generates 22 when 23 generates its all 3-subsets, and so, we know that setting position of 22 is not 1 but 3 because 23 will need not    generate 22 according to the method as follows, this is the difference of generating k-subset.

Let C = {C1, C2? Cj?Cm} be NCS, and let CSet save neighboring class weight, namely CSet[t] = 2t (t = 0?m-1).

Suppose there are N (k+1)-non frequent neighboring class sets, let NFSet save them, and let CNCSet save k-subset. Aiming to each NFSet[i] which saves No.i k-non frequent neighboring class sets, these rules of generating subset are expressed as follows:  Rule 1 aiming to each (k+1)-non frequent neighboring class sets, namely NFSet[i], the sum of k-subset generated by NFSet[i] is denoted by Sum = NFSet [i].length- NFSet [i].No.

Rule 2 when NFSet[i] generates No.j k-subset, which is saved by CNCSet[j], and then value of its domain are expressed as follows:  CNCSet[j].value=NFSet[i].value&(~CSet[NFSet [i].location [ID]]);  CNCSet[j].length=NFSet[i].length-1;  CNCSet[j].No=NFSet[i].No + (j-1), namely ID;  CNCSet[j].location[0...(CNCSet[j].length-1)]=NFSet[i].location[0?ID-1, ID+1 ? (NFSet[i].length-1)].

If a NCS with all spatial classes, its information of data structure is expressed as follows:  NCS.value=2m-1, NCS.length=m, NCS.No=0;  NCS.location [0] = 0?NCS.location [m-1] = m-1.

C. The method of computing support Accordingly to chapter A and digit logical operation, we  may gain this property as follows:  Property Let p and q express two right instances, let Cp be a NCS denoted by p, let Cq be a NCS denoted by q, then Cp ? Cq ? p & q=p.

This algorithm uses logical operation to compute support according to this property. The process is expressed as follows:  Suppose class set is expressed as C = {V, W, X, Y, Z}, there are five neighboring class sets as follows:  NCS1 = {V, Y, Z};  NCS2 = {W, X, Y, Z};  NCS3= {V, W, X, Y, Z};  NCS4 = {W, X, Z};  NCS5 = {X, Y, Z};  These integers denoting neighboring class set of right instance are expressed as {19, 15, 31, 13 and 7}. Suppose a candidate is 3 which is denoted by C-NCS = {Y, Z}, and then 19&3=3, 15&3=3, 31&3=3, 13&3=1, 7&3=3. Because of this, we write 4 to support (C-NCS).

D. The process of mining frequent neighboring class set Input:  (1) Spatial class set is denoted by C = {C1, C2?Cm}.

(2) Instance set is denoted by I = {i1, i2?in}.

(3) The minimal distance is denoted by d.

(4) The minimal support is denoted by s.

Output: Frequent neighboring class set.

Step1: firstly, we computes all right instances denoted by I? from instance set in spatial database as I by the minimal distance as d.

Step2: and then, we create neighboring class set database as NCS after scanning once right instance set as I? via the presented method in chapter A of III.

Step3: we use down strategy to generate candidate frequent NCS according to this chapter B of III. Namely, if a (k+1)- candidate is frequent neighboring class set, and it is written to FNCS which saves frequent neighboring class set, otherwise, the algorithm will compute k-subset of (k+1)-candidate, and then continue to search frequent neighboring class set.

Step4: finally, the algorithm output FNCS according to chapter A of III.



IV. THE ANALYSIS AND COMPARING OF CAPABILITY At present, there are little documents of research frequent  neighboring class set. Let ATDSS express an algorithm of traditional down search strategy. Here we compare MFNCS as [2] and ATDSS with MLFNCS as follows:  A. The Analysis of Capability MFNCS: this algorithm uses idea of Apriori to find  frequent neighboring class set, which is made of three stages, firstly, computing all the frequent 1-NCS, secondly, generating all the 2-NCS by range query, and generating all the k-NCS (k>2) by iteration. The algorithm has some repeated computing and superfluous candidate frequent neighboring class set, which gains some right instance of (k+1)-neighboring class set only through connecting right instance of k-neighboring class set. As this algorithm adopts down-top strategy to generate candidate frequent neighboring class set, it is only suitable for mining short frequent neighboring class set.

ATDSS: this algorithm adopts down search strategy to generate candidate frequent neighboring class set, which is made of three stages, firstly, computing the 1st (k+1)-candidate frequent neighboring class set which contains all classes, and then generating k-candidate frequent neighboring class set, and generating all k-frequent neighboring class set (k>3) by iteration. But the algorithm has some repetitive candidate and redundancy computing, it is inefficient when mining long frequent neighboring class set.

MLFNCS: this algorithm also adopts down search strategy to generate candidate frequent neighboring class set, but it is different from ATDSS, the difference is that it can delete repetitive k-candidate frequent neighboring class set of (k+1)- non frequent neighboring class set to reduce redundancy computing via setting position of subset in B of III. And so this algorithm is more efficient than ATDSS when mining long frequent neighboring class set.

B. The comparing of experimental result Now we use experiment to testify above analyses and  comparison. Three mining algorithms are used to generate frequent neighboring class set from 12267 right instances, whose class sets are expressed as integer from 3 to 8191, neighboring class set does not include any simple class, namely, it has two classes at least, the number of spatial class is denoted by m=13, the number of right instance included by these neighboring class set observe the discipline which is expressed as follows:  NCS of integer denoted by 8191 has one right instance.

NCS of integer denoted by 8190 has two right instances.

NCS of integer denoted by 8189 has one right instance.

NCS of integer denoted by 8188 has two right instances.

?  Our experimental circumstances are expressed as follow: Intel(R) Celeron(R) M CPU 420 ?  1.60 GHz, 512MB, language of the procedure is Visual C# 2005.NET, OS is Windows XP Professional.

The experimental result of three algorithms is expressed as Fig.1, here support is absolute. The runtime of algorithms MFNCS and MLFNCS is expressed as Fig.2 as support and length of neighboring class set change. The runtime of algorithms ATDSS and MLFNCS is expressed as Fig.3 as support and length of neighboring class set change.

Figure 1.  The experimental result        1000(3) 500(4) 200(5) 100(6) 50(7) 20(8) 10(9) 5(10)  Absolute Support(Length)  Ru nt  i me  (M i ll  is e co  nd )  MFNCS MLFNCS    Figure 2.  The runtime of algorithms MFNCS and MLFNCS       1000(3) 500(4) 200(5) 100(6) 50(7) 20(8) 10(9) 5(10)  Absolute Support(Length)  R un  t i me  ( M il  l i se  c o nd  )  ATDSS MLFNCS    Figure 3.  The runtime of algorithms ATDSS and MLFNCS

V. CONCLUSION This paper introduces an algorithm of fast mining long  frequent neighboring class set. The result of experiment indicates that the algorithm is faster and more efficient than present algorithms when mining long frequent neighboring class set in large spatial data.

