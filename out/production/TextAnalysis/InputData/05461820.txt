An Efficient Online System of Concept Based  Association Rules Mining

Abstract? This paper presents a new text mining system for extracting association rules based on concepts from online textual documents. The system is called developed extracting association rules from textual documents. The mathematical formula of weighting schema is used for labeling the documents automatically and its named fuzzy weighting schema. A new algorithm is proposed for generating association rules based on concepts and it used a data structure of hash table for the mining process. The experiments are applied on a collection of scientific documents that selected from MEDLINE for breast cancer treatments and side effects. The performance of proposed system is compared with the previous Apriori-concept system for the execution time and the evaluation of the extracted association rules. The results show that the number of extracted association rules in the proposed system is always less than that in Apriori- concept system. Moreover, the execution time of proposed system is much better than Apriori-concept system in all cases.

Keywords? Text mining, Data mining, Association rules mining, Concept extraction, MEDLINE.



I. INTRODUCTION The explosive growth of information in textual documents  creates a great need of techniques for knowledge discovery from text collections. Collecting, analyzing and extracting useful information from a very large amount of medical texts are difficult tasks for researchers in the medicine who need to keep up with scientific advances. Nowadays several domains in medical practice, drug development, and health care require support for such actives such as bioinformatics, medical informatics, clinical genomics, and many other sectors.

Moreover, the examined textual data are generally unstructured as in the case of Medline abstracts in the available resources such as PubMed, search engine interfacing Medline and medical records [38]. All these resources do not provide adequate mechanisms for retrieving the required information and analyzing very large amount of text content.

Text Mining is a tool to support and automate the process of finding and extracting interesting information from the documents. Selecting features are necessary and sufficient for constructing a model that can accurately predict future events or describe a problem. The models based on informative features will be easier to interpret from the other models,  which are based on uninformative features. The quality of the features must be described in terms of semantic richness. For example, breast cancer is a disease occurring in a particular part of the body. If a text mining system represented this phrase using the two individual features breast and cancer, it would not capture the meaning of the phrase breast cancer.

Thus, the concept feature breast cancer is semantically richer than the individual features breast and cancer. Therefore increasing the information content or semantic richness of the features will increase the plausibility and usefulness of the extracted association rules.

In this paper, we present a new text mining system that called developed extracting association rules from textual documents (D-EART) for extracting association rules from online structured and unstructured documents. The design of the D-EART system is based on concepts representation.  D- EART is designed to overcome the drawbacks of the previous EART system that is presented in [24]-[25]. The mathematical weighting schema formula that used in the EART system is developed and is named fuzzy weighting schema. In addition, generation association rules based concept algorithm (GARC) is used for the mining process instead of word based as in the traditional data mining algorithms. In the D-EART system, MEDLINE abstracts are selected for the breast cancer treatments and side effects as the main domain of online collecting documents. The system is consists of three phases that are Text Preprocessing, Association Rule Mining (ARM), and visualization.

The reset of this paper is organized as follows. Section 2 presents the related works. Section 3 presents the D-EART system architecture. Experimental results are presented in section 4. Section 5 provides conclusions and future work.



II. RELATED WORKS There are several previous works in the field of association  rules mining from structured documents (XML data) [4], [6], [11], [29], [31]-[34].  More precisely the ability to extract useful knowledge from XML data is needed because the numerous data have been represented and exchanged by XML. Thought there are some works to exploit XML within the knowledge discovery tasks, and most of them rely on    legacy relational database with an XML interface. In addition, mining knowledge in XML world is faced with more challenges than in the traditional well-structured world because of the inherent flexibility of XML. Extracting association rules from native XML documents called ?XML association rules? was first introduced by Braga et al in [6].

All the previous works in this field are based on the word features or structured data, consequently all extracted association rules are the relations between words [31]-[34].

Recently, some works developed tools for extracting association rules from XML documents [7]-[35], but both of them are approaching from the view point of XML query language. This caused the problem of language-dependent association rules mining. Ding et. al in [11] developed a method to discover all of the possible rules, i.e. generalized association rules from XML documents. In this method, all of the possible combinations of XML nodes based on their multiple nesting are used to generate the relational transactions format. This method suffered from some shortcomings such as generation of redundant rules.

Moreover, it ignored the valuable tree structure of the documents.

A model for the effective extraction of generalized association rules from a collection of XML document is presented in [4]. This method does not used frequent subtree mining techniques in the discovery process and not ignored the tree structure of data in the final rules. The frequent subtrees based on the user that provide support and split to complement subtrees to form the rules. From the above previous works, we found that all works concentrated on the domain of Association Rules Mining (ARM) based on words from XML data documents. Therefore this research is concentrated on mining of association rules based on concepts from native XML text documents and deals with their tags.

In the field of ARM from unstructured documents, there is a large body of previous works. Identifying informative features from natural language (text) can be difficult so that the problem is that there are many approaches use semantically poor features, such as words [3], [14], [21]-[32]. These approaches take bag of words as input to the association rule mining algorithm such as Apriori algorithm, and finds associations among single isolated words. These approaches have the advantage of domain independent and easy to implement. There are two drawbacks in these approaches: (1) some concepts consist of multiple words, these multiple words concepts cannot be found as a unit in the association rules, and (2) the number of association rules is tremendously large.

There are some approaches was concentrated on extracted association rules based on concepts instead of words as in [17], [19], [20], [23], [27], [33]-[37]. The identified problems in these approaches are: (1) the ambiguity of the language and can be overcome with human interaction. (2) They used the Apriori algorithm to generate association rules based on concepts. (3) There are many systems based on word features representation and do not take into account the synonymy problem. These systems could cause a text mining system to generate a misleading model of association rules.

The earlier work of association rules mining from text has explored the use of manually assigned keywords [5], [12], [13]-[14]. They used keywords as features for generating association rules. The drawbacks of these approaches are that: (1) It is time consuming to manually assign the keywords.

(2) The keywords are fixed (i.e., they do not change over the  time or based on a particular user).

(3) As the keywords are manually assigned, they are subject to  discrepancy.

(4) The textual resources are constrained to only those that  have keywords.

Therefore, the work is needed to automate indexing of the  textual document in order to allow the use of association extraction techniques on a large scale. Another research has been focused on constructing techniques to improve the quality of text-mined association rules. Most of these approaches generate a set of rules, and apply ranking techniques such as interestingness as in [8], [10], [15], [22]- [30]. Unlike these approaches, this research is focused on extracted the interesting set of the association rules. That rules are based on semantically richer representations. In mining area, most of previous studies adopt an Apriori for candidate set generation and test approach. However, candidate set generation is still costly, especially when there are a large number of patterns and/or long patterns [18]. Agrawal et al.

had first introduced the problem of association rules mining [2]. Methods for association rules mining from both structured and unstructured documents have been well developed.

Apriori and AprioriTid Algorithms are presented in [1]. These Algorithms, which are used for discovering large itemsets, make multiple passes over the data. This is the main problem of the Apriori algorithm since it reduces the performance of the system by increasing the time and generating tremendously large association rules where most of them are not plausible and useful.



III. D-EART SYSTEM ARCHITECTURE The D-EART system is automatically discovers association  rules from the collection of online structured and unstructured documents as shown in Fig. 1. It is designed to discover three types of relations such as: (1)  The association rules amongst concepts only.

(2)  The association rules amongst the words only that are  remained in the documents after extracted the concepts.

(3)  Get the relations between the concepts and words in the  form of complex rules.

The modifications in the D-EART that overcome the  drawbacks of the previous EART system in [24]-[25] are as follows: (1) On-line documents collecting and it accepts all native  XML documents.

(2) The system designed for concepts representation, and it  takes into account the characteristics of the natural language such as synonymy.

(3) The system automatically indexing documents by using the developed fuzzy weighting schema without using the threshold weight value.

Visualization Phase      (4) The system designed based on a new algorithm for extracting association rules based on concepts (GARC).

The algorithm overcomes the drawbacks of the previous algorithms by employing the power of data structure called hash table. Furthermore the system has the ability to perform different queries on the extracted association rules.

The D-EART system is consists of three main phases beside the online documents collection.  The main phases are Text Preprocessing phase that include transformation, filtration, stemming, synonymy and indexing of documents, Association Rule Mining (ARM) phase that include a new GARC algorithm, and visualization phase.

Text Preprocessing Phase     Fig. 1 The D-EART system architecture  A. Online Documents Collection  The D-EART system works online, so it is considered to be as a web-based text mining system. The D-EART accepts the documents that in XML format (structured) and also the unstructured documents. From the interface of the D-EART system, the user can online access the MEDLINE link and writes the search keywords. The selected documents and their tags are automatically loading into the system and the user selects the specific part of documents that will work on it.

B. Text Preprocessing Phase  The D-EART system has the ability to deal with the native XML documents and the unstructured documents. The process of concept extraction is done and the documents are filtered, stemmed and synonym used. Finally, the XML documents are automatically indexed by using the fuzzy weighting schema.

1) Transformation: Once the online XML documents download into the system, their tags are automatically extracted in a combobox as shown in Fig. 2. The user can determine his specific part of the documents (for example the abstract part, </Abstract Text>) to work on it. Therefore the D-EART system is flexible to work on specific or all parts of documents. In the case of the unstructured documents, the D- EART system transforms them to the XML format.

Fig. 2 Selecting a specific tag of the documents  2) Concept Extraction: The concept is a single word or a group of consecutive words that occurs frequently enough in the entire document collection. It is important to appear the concepts as a unit in the extracted association rules. The process of concept extraction as shown in Fig. 3 can be done as follows: (1) Splitting the documents into sentences by using the End-of-Sentence Detection Algorithm (ESDA) to determine the sentence boundary [36]. (2) Determine each concept candidate using n-grams model [9], [16]-[26]. We collect all the ordered pairs, or 2-grams, (A, B) such that words A and B occur in the same document in this order and the pair is frequent in the document collections. (3) Building a list of all concepts in the D-EART system, and map the concepts from concept list with sentences in documents and then estimate their frequencies. (4) Store all concepts with their frequencies in XML file.

3) Filtration: The documents are filtered by removing the unimportant words from the documents. A list of unimportant words called stopwords is built. The system checks the  Visualize Association Rules in tables or reports format  Native XML documents  Unstructured documents  Online MEDLINE Abstracts  Preprocessed documents         Association Rule Mining Phase  Apply GARC algorithm on the indexed documents to generate all conceptsets whose support is greater than  the user specified minimum support (min_support)  Generate all Association Rules that satisfied a user minimum confidence (min_confidence)  Concepts with frequencies  Stemming  Lexicon  Index documents by using the fuzzy weighting scheme Fuzzy TF-IDF for all  concepts in all documents.

Filtered XML documents  Filtration  Concepts list Stop words  Synonymy Lexicon  Concepts extraction  using n-grams  XML file  Transforme Documents  to XML format    documents content and eliminates these unimportant words (e.g. articles, pronouns, conjunctions, and common adverbs).

Moreover, the system replaces special characters, parentheses, commas, etc., with distance between words and concepts in the documents.

1. For each document in the corpus 2. Sentence boundary End of Sentence Detection  Algorithm 3. Concept List 4. For each concept in the Concepts List 5. Count=0 6.     For each sentence in the documents 7.       N-grams concept in sentence  Concept in  Concepts List 8.        Count +; 9.     End for 10. End for 11. Concept File  Each Concept with its frequencies    Fig. 3 Concepts extraction process  4) Stemming: After the filtration process had done, the D- EART system automatically do word stemming based on the inflectional stemming algorithm which illustrated in [36]. The inflectional stemming algorithm consists of both part of rule- based and dictionary-based.

5) Synonymy: In the synonymy process, the D-EART system matches each concept in the documents with the augment synonymous list.  When the system finds a synonymy for the concept, it replaces the concept in all documents with its synonymy. For example, the phrase hair loss is synonymous with the medical concept alopecia. The actual times occurs number of this concept is the total number of times that hair loss and alopecia occurs in the text. Since a concept representation would unify the expression hair loss with alopecia and thus account for synonymy. In contrast, the systems based on word representation would distribute the count between the three features hair, loss, and alopecia. The word based count would be smaller than the actual number of occurrences of the medical concept alopecia.

6) Indexing: Mathematical formula of weighting schema in D-EART system is developed and used in [24]-[25], and it named fuzzy weighting schema. This formula overcomes the drawbacks of the standard weighting schema. All weighted concepts are store in XML file for using them as input to the mining process.

One of the drawbacks of the previous EART system is that the value of the threshold weight is hard. So we developed the system to automatically compute the weight value for each word and select the actually important concepts without entering the threshold weight value M. We developed the mathematical formula weighting schema and named it fuzzy weighting schema since the threshold weight value is replaced with the fuzzy membership value as shown in equation (1)  )1(10, whereC  Nt ji  j   Where denotes the number of documents in collection C  in which occurs at least once (document frequency of the  term ) and C  denotes the number of the documents in collection C. Therefore, the fuzzy weighting schema is defined as follows:  jNt  jt  jt  )2(  0,0  1,log, ,),(   ji  ji j  ji  tNdif  tNdif Nt  C tNd  jijiwFuzzy  This formula caused developing in the system since the high weighted values were given to the concepts that are more occurrences in the documents. Moreover, new concepts appeared with high fuzzy weighted values although they are disappeared by using the weighing schema. The D-EART system automatically eliminates 10% of all concepts that have low weighted values. After that the system stores all concepts without redundancy with their frequencies in XML file for using them as input to the mining process.

Consider the 6-collection of documents as shown in Fig. 4.

In the indexing process, the fuzzy weighted values are calculated for each concept in the 6 documents. The total number of concepts is equal to 21 concepts in all documents.

We summarized all concepts with their two weighted values in Table I.

Collection of Documents  DID Concepts  D1 D2 D3 D4 D5 D6  C1 C2 C1C3 C6 C4 C3 C4 C5 C3 C5 C5 C4 C2 C3C4 C2 C3 C3 C3 C5 C1C5 C4 C1 C5 C1 C5 C5 C5 C3 C4 C5 C3 C4 C5 C3 C2 C5 C4 C5 C2 C5 C2 C5  Fig. 4 The collection of 6 documents  From Table I, it noticed that a concept C4 has zero weighted values so that the system automatically eliminates it from all documents. The system resorts the concepts based on their weighted values from the highest to the lowest. Table II shows all resorted concepts with their TF-IDF values. By choosing the threshold weight value M=50%, all concepts that in the shaded region had discarded. The system stores all accepted concepts without redundancy which are approximately 4 concepts (C1, C2, C3 and C6) in XML file. Table III shows the same resorted concepts but with their Fuzzy TF-IDF values.

The concepts that appear in the shaded region had discarded, since the less important concepts with fewer frequencies always exist in the bottom of the table. After that the system stores all concepts without redundancy with their frequencies which are approximately 4 concepts (C1, C2, C3 and C5) in XML file for using them as input in the mining process.

It noticed that the descending order of the concepts becomes different from the order in Table II. The main reasons for the difference are:       TABLE I THE TF-IDF AND FUZZY TF-IDF VALUES FOR EACH  CONCEPT IN SIX DOCUMENTS D  -I D  C on  ce pt    Fr eq  ue nc  ie s  N o.

o f  do cu  m en  ts   T F-  ID F  Fu zz  y TF  - ID  F  C1 2 2 0.954 0.318 C2 1 3 0.301 0.151 C3 1 4 0.176 0.117 C6 1 1 0.778 0.129  D1   C4 1 6 0.0 0.0 C3 2 4 0.352 0.235 C4 2 6 0.0 0.0  D2   C5 3 5 0.237 0.197 C2 2 3 0.602 0.301 C3 4 4 0.704 0.469 C4 1 6 0.0 0.0   D3   C5 1 5 0.079 0.066 C1 3 2 1.431 0.477 C4 1 6 0.0 0.0   D4  C5 5 5 0.395 0.329 C3 3 4 0.528 0.352 C4 2 6 0.0 0.0  D5   C5 2 5 0.158 0.132 C2 3 3 0.903 0.452 C4 1 6 0.0 0.0   D6  C5 4 5 0.316 0.263  (1) The first effect of the fuzzy weighting schema, since the high weighted values are given to the concepts that are more occurrences in documents. For example, the concept C6 is in two different orders as shown in Table II and Table

III. The weighing schema considered the concept C6 an important concept although it occurred only one time in all documents.

(2) The second effect of the fuzzy weighting schema is the appearing of new concepts with high fuzzy weighted values in the top of the list. For example, in Table II the concept C5 does not satisfy the threshold weight value although C5 occurred 5 times in D4. In contrast in Table III the concept C5 has a high fuzzy weighted value and exists in the top of the table.

C. Association Rule Mining (ARM) Phase  The D-EART system designed to extract association rules based on concepts by using a new GARC algorithm. The algorithm overcomes the drawbacks of the Apriori algorithm by employing the power of data structure called Hash Table.

The hashing function h(v) and concepts number (N) considered the key factors in hash table building and search performance. The GARC algorithm is utilized with dynamic hash table.

1) Generating Association Rules Algorithm Based on Concepts (GARC): The proposed GARC algorithm as in Fig.

5 employs the following two main steps: (1) based on the  number of concepts (N) in the documents, a dictionary table was constructed as shown in Table IV for N = 6 concepts, and (2) There are also two main processes for a dynamic hash table: the building process, and the scanning process. The mining process for GARC algorithm includes the two processes (building and scanning process) on the given XML file that contains all concepts.

TABLE II THE CONCEPTS WITH THEIR  TF-IDF.

TABLE III    C on  ce pt    D oc  um en  ts  T F-  ID F  C on  ce pt    D oc  um en  ts  Fu zz  y TF  - ID  F  C1 D4 1.431 C1 D4 0.477  C1 D1 0.954 C3 D3 0.469  C2 D6 0.903 C2 D6 0.452  C6 D1 0.778 C3 D5 0.352  C3 D3 0.704 C5 D4 0.329  C2 D3 0.602 C1 D1 0.318  C3 D5 0.528 C2 D3 0.301  C5 D4 0.395 C5 D6 0.263  C3 D2 0.352 C3 D2 0.235  C5 D6 0.316 C5 D2 0.197  C2 D1 0.301 C2 D1 0.151  C5 D2 0.237 C5 D5 0.132  C3 D1 0.176 C6 D1 0.129  C5 D5 0.158 C3 D1 0.117  C5 D3 0.79    C5 D3 0.066  THE CONCEPTS WITH THEIR FUZZY TF-IDF.

TABLE IV  THE DICTIONARY TABLE FOR SIX CONCEPTS IN DOCUMENTS  Dictionary Table Concept' s name Abbreviation Location  Breast Cancer Docetaxel Tamoxifen  Methotrexate Alopecia Fatigue  C1 C2 C3 C4 C5 C6    The hash function h(v) = v mod N, where v is a key  (location of primary concept) is used to build a primary bucket of the hash table. The algorithm scans only the XML file that contained all important concepts not the original documents.

The scanning process is done as follows: (1) Make all possible combinations of concepts then  determine their locations in the dynamic hash table by using the hash function h(v).

(2) Insert all concepts and conceptsets in a hash table and update their frequencies, the process continues until there is no concept in the XML file.

(3) Save the dynamic hash table into secondary storage media.

(4) Scan the dynamic hash table to determine the large  frequent conceptsets that satisfy the threshold support.

GARC_algorithm ( ) 1. Input   minimum support (s), minimum confidence (c ),  the number of concepts (N) 2. Build a primary bucket of hash table 3. IF there is no EOF THEN 4.      FOR each document  D ( d(1) d(2) ... d(n) ) DO 5.           Select each concept c(1), c(2), ..., c(N) 6.         Create all combinations of conceptset with their  occurrences 7.       Insert all conceptsets with their occurrences in hash  table by using h(v) 8.              IF  there is  document D THEN 9.                   Goto line 4 10.              ELSE 11.                  Goto line 17 12.              ENDIF 13.      ENDFOR 14. ELSE 15. Goto line 19 16. ENDIF 17. Determine all large frequent conceptsets that satisfies  the minimum support 18.  Extract all Association Rules that satisfies minimum  confidence 19.  STOP  Fig. 5 The GARC algorithm  2) The Advantages of the GARC Algorithm: The advantages of the GARC algorithm summarize as follows: (1) The algorithm permits the end user to change the threshold  support and confidence factor.

(2)   Small size of dynamic hash table, since with changing the  size of conceptsets the size of dynamic hash table will change.

(3) Less number of conceptsets, since there is no conceptsets with zero occurrences will occupy a size in a dynamic hash table.

3) The GARC Algorithm Case Study: The D-EART system run on a collection of 100 online XML documents selected from MEDLINE by thresholds values: support s=2% and confidence c=50%. The number of concepts N= 30 resulted from the indexing process and used for building a dynamic hash table. Fig. 6 shows the number of all fuzzy weighted concepts that labeling each document. Fig. 7 shows the number of the resultant association rules with c = 50% which is equal to 64 rules.

The D-EART system can do different queries on the extracted association rules. The query supports the medical researchers by a model of important relationships within the concept features. This model might identify relations between  the disease and the suitable treatments, and relations between a treatment and its side effects. Fig. 8 shows the query screen which includes both the categories information and the queries result icons. The user can determine which the categories will get the relations between them. The query results can be saved on the hard disk through the export icon.

Fig.6 The number of fuzzy weighted concepts   Fig. 7 The resultant rules that satisfy c = 50%.

Fig. 8 Query Screen  The advantages of D-EART system are as follows: (1) The user can access XML textual documents online.

(2) The design of the D-EART system is based on concept  representation and considers the synonymy as a characteristic of the natural language characteristics.

(3) It is flexible to work on specific or all parts of the documents with the same structure. Moreover it is not fully domain-independent so we can apply it on other domains.

(4) The proposed GARC algorithm overcomes the drawbacks of the previous algorithms.

(5) It extracts three types of the association rules depending on the analysis of relations between the concepts only, words only and concepts with words. In addition different queries are available on the extracted association rules.



IV. EXPERIMENTAL RESULTS The experiments are performed to compare the  performance of both D-EART system and Apriori-concept system for the number of extracted association rules and the execution time. Finally, evaluate the performance of D-EART system at the three semantic levels: concepts only, words    only, and concepts with words.

The corpus of the PubMed abstracts that used in the  experiments is consists of 10000 biomedical abstracts with keyword search ?breast cancer treatments and side effects? [38]. All experiments are applied on the 10000 documents after divided them into six documentsets 50, 100, 500, 1000, 5000, and all 10000 documents. The systems are implemented by using VS .Net 2005 (C#) and the experiments were performed on Intel Core2 Duo, 1.8 GHz system with Windows XP and 2 Giga of RAM.

A large number of association rules can be extracted by selecting the values of minimum support and confidence in the mining process.  The D-EART system gives the best results by using low support and high confidence values.

Moreover, the number of concepts that entered to the mining process is fewer by using the fuzzy weighting schema. Table V shows the experiments that are applied on various documentsets by different threshold values. It noticed that the number of extracted association rules in D-EART system is useful and always less than that in Apriori-concept system.

The reason returns to the strong effect of using the fuzzy weighting schema in D-EART system.

Fig. 9 and Fig. 10 show that the execution time of Apriori- concept system is increased regularly when the documentsets are increased compared to D-EART system. The mining process in Apriori-word system takes more time for less number of concepts in the documents. The reason is that the mining process in Apriori algorithm depends on the size of documents rather than the number of concepts. The results show that the execution time of Apriori-concept system is about seventh fold of D-EART system. The D-EART system scans the documents only one time as the number of documents increased. Therefore the size of documents does not influence in the mining process. Finally, the results reveal that the execution time for D-EART system is much better than that of the Apriori-concept system in all cases.

TABLE V  THE NUMBER OF ASSOCIATION RULES FOR APRIORI- CONCEPT AND D-EART SYSTEMS  Minimum Support (s), Minimum Confidence (c)  N o.

o f  D oc  um en  ts  Systems s =1%,  c =50%  3%,  50%  7%,  60%  10%,  50%  0 Apriori-concept  D-EART      Apriori-concept  D-EART           Apriori-concept  D-EART           0 Apriori-concept  D-EART            D5000  0,00  5,00  10,00  15,00  20,00  25,00  30,00  Apriori-concept D-EART  Ti m  e in  m in  ut es  s=1, c=50 s=3, c=50 s=7, c=60 s=10, c=50  Fig. 9 Execution time of Apriori-concept and D-EART systems at D=5000  D10000  0,00 5,00  10,00 15,00 20,00 25,00 30,00 35,00 40,00  Apriori-concept D-EART  Ti m  e in  m in  ut es  s=1, c=50 s=3, c=50 s=7, c=60 s=10, c=50  Fig. 10 Execution time of Apriori-concept and D-EART systems at D=10000

V. CONCLUSIONS AND FUTURE WORK This paper presented a new text mining system for  extracting association rules based on concepts representation from online textual documents. This system overcame some of the problems in the previous EART system and the drawbacks of the Apriori algorithm by using the data structure hash table in the mining process. The results of comparing D- EART and Apriori-concept systems reveal that the number of extracted association rules in D-EART system is always less than that in Apriori-concept system. Moreover, the execution time for D-EART system is much better than that of Apriori- concept system in all cases. So concept technique would be suitable to apply to any large corpus of medical text such as portions of the web. The future work will apply D-EART on PDF full text document with figures and images instead of using only the abstract part.

