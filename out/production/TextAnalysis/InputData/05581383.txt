Elimination Algorithm of Redundant Association  Rules Based on Domain Knowledge

Abstract? Many association rule mining algorithms have been developed to extract interesting patterns from large databases.

However, a large amount of knowledge explicitly represented in domain knowledge has not been used to reduce the number of association rules. A significant number of known associations are unnecessarily extracted by association rule mining algorithms.

The result is the generation of hundreds or thousands of non- interesting association rules. This paper presents an algorithm named DKARM, which takes into account not only database itself, but also related domain knowledge, so as to eliminate extraction of known associations in domain knowledge.

Experiments show this algorithm can reasonably eliminate redundant rules, and effectively reduce the number of rules.

Keywords-Data Mining; Association Rules; Domain Knowledge; Redundant Rules

I. INTRODUCTION The purpose of association rule mining technique is that  discovers the association rules which are novel, useful, interesting and hiding in the item set[1]. The algorithm of association rule mining should find the relation from the new and hiding item. However, most classic algorithms [2, 3] only consider the data itself. Whereas the abundant data related domain knowledge is not regarded as the priori knowledge to eliminate the known patterns. The result is that a lot of redundant rules are created within the mined knowledge. The association rule mining algorithms also extract a lot of known associations, which are regarded common knowledge in domain, so as to create a lot of redundancy. For eliminating the redundancy, most researches in the association discovery use the method that brings down the threshold to reduce the number of rules.

In the knowledge discovery system, the Domain Knowledge (DK) is defined as the knowledge that guide and restrict the interesting knowledge on searching, or is called background knowledge. Usually the two definitions are not differentiated. DK refers to the important factors or concepts in a specific domain and the relations between these factors and  concepts. The knowledge discovery algorithm shall reasonably make use of the DK, so as to cut down the searching time. In the paper, the DK is understood as the information that is useful for discovering knowledge. It is not only the professional knowledge in some application domains, but also includes some related knowledge on knowledge discovery and other related background knowledge and so on. The knowledge discovery shall be based on the DK, and make use of the DK to do the targeted knowledge discovery. On the one hand, it may reduce the range of the searching object and improve the efficiency. On the other hand, it may improve the discovery pattern or the interest and reliability of the result. In the meantime, the interest and the reliability also relate to DK itself.

The paper presents an association rule algorithm based on the domain knowledge named DKARM. It not only take into account the DK factor on the data itself, but also improves the classical association rule.



II. THE ASSOCIATION RULES MINING RESTRICTED BY DOMAIN KNOWLEDGE  Because some items owns the mandatory dependence as is known by everyone among them under guided or restricted by the DK, the rules that the association mining get always include plenty of the redundant rules. For examples, in the market selling database, {nail, sinker} owns the mandatory association, and it is a very credible rule. But the user may not be interested to the kind of the rules. What they are really interested in is the rules similarly as {diaper, beer}. The rules are really interesting and can?t be got according to the restriction of the DK. In the traditional association mining process, most mined rules may exist in the intense relation with the known DK. However, the discovery of the useful and new knowledge can?t make use of the rules. That leads to a result which is the interesting and non-interesting association rules mixed together. Therefore, the users have to distinguish and explain them one by one for discovering the interesting pattern.

Obviously, under the DK guiding, those association rules obviously token on shall be put out the association mining.

2010 Seventh Web Information Systems and Applications Conference  DOI 10.1109/WISA.2010.23     This will avoid these rules being seen and extracted by users. In order to diminish the number of the known pattern in the association mining, the paper presents the DKARM algorithm, which use the DK as the priori knowledge to eliminate the redundant association rules and the rules derived from themselves.

A. Instance Analysis Agrawal et al. in 1993[2] firstly proposes the mining  customer transaction database association rules question among itemset. Its main idea is the recursion way based on the frequent item theory. This is a valid association rule mining algorithm. But it may generate lots of candidate itemsets, and need plenty of times to scan the database. In the process, plenty of time is consumed on the exchanging data in the database in the memory.

Agrawal et al. has proved that the association rules own the following properties:  Property 1: The subset of the frequent items is also frequent items.

Property 2: The superset of the non-frequent items is also non-frequent.

To the firmly flaw of the Apriori algorithm, J.Han et al.

proposes a method which doesn?t generate the candidate frequent items-FP tree algorithm [3]. Its main policy is divide and rule. The algorithm main idea is that sets up the structure with no candidate sets based on the FP-tree frequent pattern. It compresses every transaction in the original database to a FP- tree, reduces the mining time of the FP-Growth too much, and improves the mining efficiency.

However, there is also restricting on FP algorithm. When it creates the conditional- pattern base to mine the frequent item, if the branch of the tree is too long, the recursion will still take time.

Discuss the problems of mining the mandatory association rule without removing the explicit DK guiding through analyzing the instance. Considering a set of elements  ={A,B,C,D,E}, all possible combinations of these elements produce the sets: {AB}, {AC}, {AD}, {AE}, {BC}, {BD}, {BE}, {CD}, {CE}, {DE}, {ABC}, {ABD}, {ABE}, {ACD}, {ACE}, {ADE}, {BCD}, {BCE}, {BDE}, {CDE}, {ABCD}, {ABCE}, {ABDE},{ACDE},{BCDE}, {ABCDE} Without considering any threshold, the number of possible subsets is 26, and the maximum number of rules produced with these subsets is 180.

?  If consider that the element C and D have a mandatory association. Notice that there are eight sets where C and D appear together ({CD}, {ACD}, {BCD}, {CDE}, {ABCD}, {ACDE}, {BCDE}, {ABCDE}). The eight sets will produce 92 rules, and in every rule, C and D will appear. The result is that 51.11% of the whole amount of rules is created with the dependence between C and D.

It is important that we can?t directionally remove C and D from in the preprocessing, because C or D may have an association with A B or E. Nevertheless, we can avoid the  combination of C and D in the same set. This eliminates the possibility of generating rules including both C and D.

?  B. Association Rules Mining Algorithm Based on DK (DKARM) In this section, we will use an instance to describe DKARM  on how to eliminate the redundant rules under the DK guiding.

For studying conveniently, the redundant rules are restricted to including two items, which is called as the pairs of the redundancy item. Table 1 is the market database transaction table.

? Describe the DKARM Algorithm  Input: D: the transaction database; min_sup: minimum support; ? : redundant rules under the DK guiding.

Output: P: the frequent pattern set after removing the pairs of the redundancy items  TABLE I. SUPERMARKET TRANSACTION DATABASE  TID ITEMSET T100 I1,I2,I5 T200 I2,I4 T300 I2,I3 T400 I1,I2,I4 T500 I1,I3 T600 I2,I3 T700 I1,I3 T800 I1,I2,I3,I5 T900 I1,I2,I3  TABLE II. 1-ITEMSET COUNT SORT BY DESCENDING AFTER SCAN THE DATABASE A TIME  ITEM Support count I2 7 I1 6 I3 6 I4 2 I5 2  TABLE III. SUPERMARKET TRANSACTION DATABASE (ITEMSET SORTS SUPPORT )  TID ITEMSET T100 I2,I1,,I5 T200 I2,I4 T300 I2,I3 T400 I2,I1,I4 T500 I1,I3 T600 I2,I3 T700 I1,I3 T800 I2,I1,I3,I5 T900 I1,I1,I3  The step 1: Scan the database a time to gain 1-candidate sets, and remove the item whose support is less than min_sup to gain all frequent item that is 1-frequent itemsets.

The item of 1-frequent itemsets sorts support by descending.

The result is the table after the table  sort support by descending. And every transaction in the database rearrange     according to the 1-frequent itemsets order to gain L, as the table  shows. This is ready for combine the repeat path.

The step 2: Scan the database for the second time. Create the FP-tree with the branch number on the base of step 1.

Firstly, create the root node of the tree, which is signed by ?NULL?. And then, scan the database on the second time, and insert the transaction of the database to the root node one by one, according to the order of the L. The FP-tree with the branch number is created through the table 3, as the Figure1 shows. For example, node I3:2(12) indicates the node whose name is I3 and whose support is 2. The branch number is expressed by the string in the brackets. So the ?(12)? is the path from the root node to the first branch and then from the first branch to its second sub-branch.

TID  Support  Node chain NULL  I2:7(1) I1:2(2)  I3:2(12) I1:4(11)  I4:1(13)  I3:2(21)  I3:2(112) I5:1(111) I4:1(113)  I5:1(1221)  I2   7  I1   6  I3   6  I4   2  I5   2  Figure 1. FP-tree with branch number  The step 3: From the root node beginning, mine the each branch in order to get the 2-itemsets which is the combination with the first item of the branch. And then count the support of the item from each branch. Compare the result of counting to the min_sup to get the 2-frequent itemsets. For example in the figure 1, from the ?NULL? node beginning, the node I2 generates the 2-itemset {I2I1:4(11),I2I3:2(12)+2(112),I2I4:1(13)+1(113),I2I5:1(11 1)+1(1221)}, and the node I1 generates the 2- itemset{I1I3:2(112)+2(21),I1I5:1(111)+1(1221)}.

According to the node order of the table , compute the node I3 I4 I5 like that to generate the each 2-itemset of themselves. Suppose the min_sup is 2, and we can get the 2-frequent itemsets{I2I1:4(11),I2I3:2(12)+2(112),I2I4:1(13) +1(113),I2I5:1(111)+1(1221),I1I3:2(112)+2(21),I1I5:1(111 )+1(1221)}  The step 4: Remove the redundant pairs of the items with the DK guiding to get the set ?  from all 2-frequent itemsets.

The step 5: According to the property of the Apriori, generate the K-order candidate item through the K-1-order item. The algorithm is end when there is an order item that is an empty set, and gets the all frequent pattern set P removed the redundant pairs.

As the 2-itemset reserve the support of each transaction in the each branch, we can get the support of the 3-candidate  set to avoid to scanning the database again or searching recursively the FP-tree. For example, owing to the set {I2I1:4(11), I2I3:2(12) +2(112), I1I3:2(112) +2(21)} is the frequent itemsets, the 3-itemset {I2I1I3} is also frequent.

And according to the prefix of sharing the branch number, we can get its support that is 2, and that is {I2I1I3:2(112)}.

? Algorithm illustration  The step 1 and step 2 almost keep the consistence with the FP-tree algorithm. However, we increase the branch number in create the FP-tree for getting the information of the more order itemset in the follow-up steps.

The step 3 is different from the FP-tree in the searching progress. The FP-tree uses the bottom-up method, but we do the combination of the item up-bottom.

In the step 4, after generating the 2-candidate sets, all the redundant element pairs are removed from 2-frequent itemsets in? . Due to eliminate the redundant pairs of the item from DK in ? (as {C,D}), this does not only avoid to generating the rule C->D, but also avoid to generating all the derived rules(as {D->C,C->AD}). These rules include the known mandatory association. According to the property 1 and property 2, the step makes sure that the redundant pairs of the item can?t appear together in the follow-up frequent itemsets, and don?t appear in the association rule. This makes the algorithm be very valid, and be independent any the threshold restricting of the minimum support minimum confidence and so on. The algorithm eliminates all candidate set that includes the redundant pairs of items, and is independent any concept levels. This is the character of the DKRAM algorithm that is different from the other removing the redundant rule algorithm.

In the first two steps, we make use of the FP algorithm to compress every transaction in the original database to a FP- tree, and avoid to scanning the database too many times. In the step 5, we also make use of the core idea of the Apriori algorithm. According to the property of the Apriori, we get the more order itemsets through join and pruning on the base of the 2-itemset. Through increasing the branch number of the FP-tree, we reserve the support of each transaction in the each branch as getting the 2-itemsets.

Therefore, we can get the support of the 3-candidate itemsets, and don?t recursive scan the database again. So the DKARM algorithm avoids the defect of scanning the database too many times, as well as avoids the recursive mining way in the FP-Growth algorithm.



III. EVALUATION Don?t consider the threshold of the minimum support and  the minimum confidence. The figure 2 shows the theoretic computational result of the number of rules after eliminating a different number of the redundant pairs of the item. We use the theory to analyze the transaction database which includes the number of the transaction items from 2 to 12, and then respectively discuss the zero, one and two redundancy pairs of the dependences from the set. When the number of the     redundancy pairs is zero, it is the result without dependence elimination. Notice that in the three cases, as the number of dependences is zero, the number of rules grows exponentially with the number of the element of the set growing, but the elimination of dependences does considerably reduce the number of the rules. Obviously, the higher the number of well known dependences is, the more significant will the rule reduction be.

The percentage reduction of rules produced when zero, one and two pairs of dependences are eliminated is shown in Figure 3. For example, the elimination of one pair generates only 55% of the total number of rules, and eliminates well know rules in 45%. When two pairs are removed, only 30% of the rules are generated, and the reduction increases to 70%. Notice that even if the number of elements increases, these values represent saturation points for these curves.

Meantime, the operation system is windows XP, CPU Interl T2050 1.60GHz and Memory is 1.00GB. We use java to perform the experiment of the DKARM algorithm. The experiment data are from the transaction of the AllElectronics in [1], which includes 9 transactions and 5 items, the RDG1 dataset which includes 100 transactions and 10 items generated by weka, and the contact-lenses dataset which includes 24 transactions and 12 items from weka. We set the support 0.2 and the confidence 0.6 in the experiment. The table  shows the result of the experiment.

Notice the result of the experiment from the actual data, and we can find that the number of rules and time is obviously cut  down. To the user who do the data mining, eliminate the number of the known rules under being restricted by the DK, which is more important than removing the time as generating the association rule. The main purpose of the paper is that fully consider the point of the DK guiding in the mining progress.

However, with eliminating the association pairs of dependence set, it will generate less candidate items. Therefore, the time of the algorithm certainly eliminate.

TABLE IV. THE EXPERIMENT RESULT OF THE DKARM

IV. CONCLUSIONS AND FUTURE WORK The paper presents a method of mining under the DK  guiding, which can be used to eliminate the redundancy association, and make sure the generated rules that is interesting to the user. The threshold restricting of the minimum support etc. doesn?t impact on the elimination. Under the relational domain guiding, how to gain the more complicated items dependences as the priori knowledge to guide our follow-up mining work, and how to evaluate the interest of the rules after mining, both them need us to continue to study.

