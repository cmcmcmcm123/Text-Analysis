Kanthaka: Big Data Caller Detail Record (CDR) Analyzer for Near Real Time  Telecom Promotions

Abstract ? With the competitiveness that is growing in the telecom industry, the operators seek for ways to attract and keep the subscribers in their network. Giving away attractive promotions to their subscribers is a powerful and commonly used approach in that context. With the technical limitations and the scales of the rising subscriber base, these promotions have been limited to a very narrow pattern. In order to gain a competitive advantage, the operators need new technologies and methodologies to support more attractive newer patterns of promotions, despite the challenges involved in complex analyzing procedures. In this research study, we have come up with a system to select eligible users for a particular pre- defined promotion, analyzing the Caller Detail Records, while being scalable for a rapidly escalating subscriber base.

Keywords-CDR; NoSQL; Big Data

I. INTRODUCTION In the present telecom industry, value added services and promotions play a great role in the tightening competition among the service providers in escalating the subscriber base. The promotions such as 4th minute is free after a 3minutes of an IDD call represent a common pattern in currently available promotions. These are handled by the subscribers? central office telecom switch. A switch is responsible for all basic features in placing a call, such as dial tone, signaling other telecommunication equipments about the call, and maintaining a record of the call, SMS or any other event type.

Consider a promotion pattern example defined as; give a promotion notification from ABC snack bar, for all the subscribers who have called the ABC snack bar delivery number more than 5 times within a month. This involves counting the number of calls made by a subscriber to a particular number. The capabilities of telecom switch is not enough to execute this, but executing this can create a big competitive advantage for a telecom operator attracting subscriber with useful and interesting promotions while increasing the income received from commercial parties such as snack bars. The time taken to give away the promotion is a critical factor that a particular promotion may  be effective only in a special season and should be made available on the correct time.

When consider the general pattern of this new promotions, a technology should developed to identify who will be interested in receiving the notification of the promotion or who is eligible to receive the promotion. The CDRs generated by the telecom switch can be used to retrieve the relevant details and do the analysis. Also the dynamic definition and removal of the promotion should be available so that it can serve the need of the market.

This research study is focused on developing a system to fulfill this need, answering the challenges of scale and speed.

More specifically ?Kanthaka? is capable of analyzing 30 million records per day as for the requirement of telecom operators? subscriber base, in near real time (Maximum 30s delay). According to the current settings of telecom operators these records will be fed to ?Kanthaka? in 10 minute intervals as they are generated. This paper presents the considered technologies in developing the system, a brief comparison of them, selected technologies, the implementation details of the system and final performance readings.



II. TECHNOLOGY SELECTION Considering the scale and speed requirements of the  system, a literature survey was conducted on possible technologies to use. The technologies used in current social network sites and search engines were analyzed to identify feasible technologies that are used for near real time analysis of data [1].  ?Big Data? technologies are a category emerged with the high volume data analysis needs raised with the hike of generation of information with the social network sites etc. So the major focus was on the technologies that fall into the category of ?Big Data? as the basic requirements of ?Kanthaka? express similarities with them.  Following is a brief discussion of the findings, in the aspects of ?Kanthaka?.

A.  Complex Event Processing (CEP) CEP is capable of giving a response in real time, after identifying meaningful patterns and relationships among apparently unrelated events. These engines accept queries   DOI 10.1109/ISMS.2013.40       provided by the user and match those queries against continuous event streams. Then it can trigger an event or an execution when the conditions specified in the queries are satisfied. The available CEP engines have proven to be up to the speed requirements of ?Kanthaka? [2].

CEP engines analyze events that arrive within a limited amount of time (window) without keeping any persistent storage of analyzed data. So if a telecom promotion is defined to be given for one month and some failure occurs in the system, there is no way to recover the state. This cannot be tolerated in the context. Therefore CEP is not a good candidate for ?Kanthaka?.

B.   Apache Hadoop ? Map Reduce Map-reduce is widely used in distributed computing in  large clusters where the application is divided into many small fragments of work, each of which may be executed or re-executed on any node [3]. Apache Hadoop is an open source map-reduce implementation which has proven performance at Amozon, Facebook and eBay for search optimizations in PetaByte scales as mentioned in Hadoop site [4]. In scale-wise this is lot more than the needs of ?Kanthaka?. In speed-wise the latency involved in map- reduce is not effective for a data set smaller than few PetaBytes and not recommended for near real time execution needs [5], such as of ?Kanthaka?.

C.   Rules Engines As for the need of current business world several rules  engines exists to support dynamic rules definition [6] which is promotion strategy definition in context of ?Kanthaka?.

From these, Drools was considered for ?Kanthaka?, as it is a widely adapted, production rules engine optimized for pattern matching, using Rete algorithm which is faster in rule execution [7]. In benchmark results, Drools performance is above 100s for time taken to data loading and rule firing which is below the requirements of ?Kanthaka? [8].

D.   Relational Database Management Systems (RDBMS) For data storing needs traditional solution is RDMS which  provides ACID (Atomicity Consistency Isolation Durability) properties. When scaling these relational database systems to high volume data systems the system designer needs to address many aspects since traditional relational database systems are built to run on a single machine.  When scaling these systems sometimes the read and write throughput of the system becomes too much to handle for the system. This is mainly because relational database systems were not designed for distributed or shared systems and the additional overheads incur during these processed would trouble the normal operation of the system. The scaling techniques used on such a system would cost significant complexity and loss of fault tolerance [9]. Also when consider the data to be stored in ?Kanthaka? system; ensuring ACID properties is not essential that this cost become unreasonable [10].

E.   NewSQL This is the solution developed to scale the traditional  RDBMS for the current high volume of data storage. It preserves ACID properties, while providing easy scale up using innovative mechanisms. VoltDB and NimbusDB are widely used implementations of NewSQL technology. When speed is concerned VoltDB has declared to be capable of executing 3.4 million transactions per second (at 10ms latency), which is satisfying the needs of ?Kanthaka? by far [12]. This higher speed has been achieved keeping all the data in-memory and supporting persistency via a commitlog written to the disk. As ?Kanthaka? needs to store data as long as a promotion is active while multiple promotions being active in the same time, it need lot of memory in such scenario. Considering this cost involved in high memory requirements, this technology was set beside.

F.  NoSQL Databases NoSQL standing for ?Not only SQL? which is an umbrella  term to define a class of non-relational structured storage systems that differ from classic relational database management systems. These are focused on providing scalability and fast execution for growing volume of data, trading off in ACID properties. Instead these databases support BASE (Basically Available Soft-state Eventually- consistent) properties, achieving scalability in a limited data model. This is enough performance for ?Kanthaka?.

NoSQL databases were partly influenced by two key research papers: Google BigTable [13] which defines a specific data model focused on storing and querying multi- column data and uses a range-based partitioning scheme which allows data to be stored on multiple distributed nodes and Amazon Dynamo [14] which uses a simpler key-value data model, but the system is more resilient to failures, thanks to a looser consistency model. The concerns of both of these include serving throughput-oriented batch- processing jobs to latency-sensitive serving of data to end users which is more similar to the need of Kanthaka. Lot of open source implementations are available following these researches such as Cassandra, HBase, MongoDB and Hive which are widely adapted by social networking applications.

To select between the options available in NoSQL arena Yahoo Cloud Service Benchmarks (YCSB) [15] was referred. Considering the read latency, update latency and elastic speed up properties Cassandra was selected as the data store for ?Kanthaka?.



III. KANTHAKA SYSTEM ARCHITECTURE With the pre-mentioned technology selection, the most  optimized architecture selected for ?Kanthaka? is as Figure1.? The complete system of ?Kanthaka? is divided into two main tiers as front-end and back-end.

Figure 1. ?Kanthaka? architecture  A.  Front-end A web based UI (User Interface) is provided in the front-  end which is used to define the promotions. When a business administrator adds a promotion via the Web UI, it is stored in the promotion database. These promotions in the database are converted in to Cassandra queries inside the Compiler module. Modular architecture pattern is followed here to ensure loose coupling within the system. Dynamic promotion addition and removal is supported to allow fast adjustments in the rules via the UI.

B.   Back-end At the backend of the system, CDRs are received from the  telecom operator periodically as .CSV (Comma Separated Values) files and they are read by the .CSV file reading Module. The preprocessor takes those CDRs from one side and the promotion queries from the other side. According the information extracted, pre-processing CDRs are stored in Hashmaps in the Memory module. The Hashmaps keeps a counter for each subscriber?s phone number found in CDRs in each interval and increment it according to the defined promotions. Then the increment is sent to Cassandra in a  defined time interval as a batch, reducing the number of database updates. The objective of this is to filter the data according to the defined promotions and only store the useful data, avoiding useless usage of database space. Also as this reduces the amount of data, querying becomes faster.

Selecting the eligible subscribers for a particular promotion is done from the Periodic Eligibility Checking module. This module takes the promotion queries from the compiler and periodically checks whether there are any matches for those queries in the Cassandra database. If there are any matches, the module sends the MSISDN (a number uniquely identifying a subscriber) to the telecom operators? promotion server. At the same time it flags the eligible subscribers' entries of the Cassandra database to avoid reselecting the same subscribers for the same promotion.

With this pattern of architecture, occurrence of concurrency issues is avoided. There is no need of locking the database for reads and writes as no harm is done even if both occurs at the same time. Once the rule is satisfied that entry is flagged and then that entry is not considered for that promotion, no matter it is updated or not after that. Also if the load for the system goes high in seasonal times etc.

?Kanthaka? is flexible to face it adjusting the periods of rule execution and batch loading or elastically adding nodes to the cluster to balance the load. On the other hand, system is open for vast changes if needed, that Cassandra can be replaced any time, backing up its data, with few simple steps. Only the rule compiler and an adapter to load pre- processed data to the cluster are required for the new database.



IV.   KANTHAKA SYSTEM IMPLEMENTATION The web UI of ?Kanthaka? system is implemented to add  dynamic rules in a user friendly manner as a combination of ?AND?s of ?OR?s.

Eg. (Destination No = 011729729 || Destination No = 081729729) && (No of Calls > 30 || No of SMSs > 10) && (Connection Type = ?Prepaid?)  The above promotion basically says if the subscriber has called more than 30 minutes or sent more than 10 SMSes to 0112729729 or 0812729729; he/she is eligible for the promotion. At compiler this rule is analyzed and separates the conditions to be checked at pre-processing module and counters to be kept at database for the query. For the above example ?Destination No? and ?Connection Type? are considered conditions as they decide whether a CDR should increase a counter value in database. These are checked at pre-processor and the counter values are increased in a Hashmap in the memory as shown in Figure 2.

Figure 2.  Pre-processor implementation details ?Kanthaka? is implemented to co-operate with the current settings of telecom operators? network. Java is the language used and provides interfaces to deal with outside. It receives a CDR file with 43 attributes in a row, in 10 minutes intervals. The length of the file depends on the actual events happened in the network. At .CSV file reading module it reads only the attributes needed for the active promotions in the system, checks for the identified conditions and increments Hashmap values accordingly.

The incremented values in Hashmaps are then loaded to Cassandra in batch wise using the special features of database. As this is just an increment in the value, existing in the database, without reading it and updating most of the concurrency issues are avoided. With this database locking is not needed for read and writes and allows maximum use of Cassandra concurrent execution. Also as soon as an entry is selected for a promotion it is flagged and in query running only non-flagged entries are considered. Any chance of offering the same promotion twice for same subscriber is avoided with this.



V. RESULTS AND FUTURE DIRECTIONS Test Environment Processor Model name-Pentium(R) Dual-Core CPU       T4200  @ 2.00GHz Frequency-2000.000 MHz L2cache-1024KB Memory Total -2958 MiB OS Ubuntu - 10.04 Lucid  Kernel Linux  2.6.32.33-generic GNOME 2.30.2 Swap  - 4.7 GB   Figure 3.  Performance in single node  With the results in figure3, a latency increase can be observed, with the number of promotions increase for the same data set. This is because more processing is needed when number of promotions increases. Also latency increases with the number of records increases in the file. It is due to the time required to read the whole file and process it.

Figure 4, shows the result comparison in a cluster of two nodes against single node. Two nodes of the cluster are with the same configuration as for single node, linked via a ?TIA/EIA 568-b.2 category 5e cable? of 90cm length.

For number of records below 1million, performance was worst in two node cluster than in single node due to network latencies introduced. But for larger loads cluster is effective, despite the network latencies, which proves that ?Kanthaka? can deal with seasonal hikes of network usage.

Figure 4.  Performance in two node cluster ?Kanthaka? architecture was developed keeping in mind,  that the system should be highly scalable and elastic to satisfy the future needs of telecom operators? with the growing subscriber base. This is already available for the data storage section, but not in data reading and pre- processing. It is possible to increase the speed of ?Kanthaka? by parallelizing the reading of CDR files and pre processing.

If the CDRs can be fed into the system as several files rather than one, each file can be processed in separate servers. Each server can have its own Hashmaps in memory for counters and aggregation. At updating the Cassandra cluster from the Hashmap values, concurrency issues can occur if each server is allowed to update Cassandra at once.  If this could be       implemented in ?Kanthaka? system with proper locking mechanisms it is doing all operations of reading, updating and querying in parallel, which in return will make the execution more near real time.

Also the Cassandra cluster has its own optimizing techniques with memtable_limit adjustments and having separate disk for commitlogs to enhance the performance in hardware level. At deployment phase those should be considered to have most effective configuration in the cluster.

