Application Research of Cluster Analysis and  Association Analysis

Abstract?For applications of data mining techniques in geosciences, through mining spatial databases which are constructed with geophysical and geochemical data measured in fields, the knowledge, such as the spatial distribution of geological targets, the geophysical and geochemical characteristics of geological targets, the differentiation among the geological targets, and the relationship among geophysical and geochemical data, can be discovered. Due to the complexity of geophysical and geochemical data, traditional mining methods of cluster analysis and association analysis have limitations in processing complex data. In this paper, a clustering algorithm based on density and adaptive density-reachable is presented which has the ability to handle clusters of arbitrary shapes, sizes and densities. For association analysis, mining the continuous attributes may reveal useful and interesting insights about the data objects in geoscientific applications. Quantitative association rules aims to deal with the relationships among continuous attributes of geoscientific data objects. An association analysis algorithm based on the distances among clusters projected on attributes is presented in this paper. Experiments and applications indicate that the algorithms are effective in real world applications.

Keywords- Cluster analysis; Association analysis; Geo-spatial database; Geochemical data; Data processing

I.  INTRODUCTION Cluster analysis has long played an important role in a wide  variety of fields: psychology and other social sciences, biology, pattern recognition, machine learning, image processing, information retrieval, geosciences, and climate research.

Cluster analysis aims to identify groups of objects (clusters) that satisfy some specific criteria, or share some common characteristics. A large number of clustering algorithms have been developed in a variety of domains for different types of applications. None of these algorithms is suitable for all types of data, clusters, and applications. In fact, it seems that there are always rooms for new clustering algorithms that are more efficient or better suited to particular types of data, clusters, or applications [1].

In many application domains, such as geoscientific data processing, clusters of data objects are of arbitrary shapes, sizes and densities, and the number of clusters is unknown. In such scenarios, traditional clustering algorithms, including partitioning methods, hierarchical methods, density-based methods and grid-based methods, cannot identify clusters  efficiently or accurately. Obviously, these are critical limitations or weaknesses of clustering methods. Numerous algorithms have been proposed to solve these problems. For example, Ert?z, Steinbach, and Kumar proposed a clustering technique that addresses the problem of finding clusters in data when clusters are of widely differing sizes, densities and shapes, and when the data contains large amounts of noise and outliers [2]. Ayad and Kamel [3] presented a multiple data clusterings combiner, based on a proposed Weighted Shared nearest neighbors Graph (WSnnG). The problem addressed in the paper is that of generating a reliable clustering to represent the natural cluster structure in a set of patterns, when a number of different clusterings of the data is available or can be generated. A clustering method called CLARANS which aims to identify spatial structures that may be present in the data was proposed in [4]. DBSCAN [5, 6] is a simple and effective density-based clustering algorithm which is relatively resistant to noise and can handle clusters of arbitrary shapes and sizes, but it has trouble when the clusters have widely varying densities. Zhang, Ramakrishnan, and Livny [7] proposed an efficient data clustering method for very large databases, which is based on the notion of clustering feature and effective for incremental clustering. A clustering algorithm based on density and adaptive density-reachable is presented in [8, 9], which aims to handle complex data.

Much work in data mining revolves around the discovery of rules within large quantities of data. The rules over relations are of the form X Y where X and Y are conditions on tuples of the relation. The term association rule has been used to describe a specific form of such rules.

Association rule analysis aims to find frequent patterns in datasets. Frequent patterns are patterns (such as itemsets, subsequences, or substructures) that appear in a data set frequently. Finding such frequent patterns plays an essential role in mining associations, correlations, and many other interesting relationships among data objects. Frequent pattern mining has become an important data mining task and a focused theme in data mining research. Association rule analysis is applicable to many application domains such as bioinformatics, medical diagnosis, Web mining, and scientific data analysis [1].

Algorithms for discovering classical association rules make different assumptions about the type of datasets to be mined. In general, the dataset is a relational table. The domains of the     attributes are restricted to boolean domains. The databases in most business and scientific domains have many types of attributes, including quantitative, categorical or boolean attributes. The classical association analysis algorithms for boolean attributes can not mine quantitative association rules from databases with continuous attributes, and may yield very unintuitive results when applied to continuous attributes. R.

Srikant and R. Agrawal [10] proposed an algorithm for mining quantitative association rules in large relational tables, defined the problem of mining association rules over quantitative and categorical attributes in large relational tables, presented techniques for discovering quantitative rules, and referred to this mining problem as the quantitative association rules problem. For the problem of mining association rules over interval data, R. J. Miller and Y. Yang [11] proposed the definition of interest for association rules that takes into account the semantics of interval data and developed an algorithm for mining association rules under the definition. A definition of quantitative association rules based on statistical inference theory was presented by Y. Aumann and Y. Lindell [12], which reflects the intuition that the goal of association rules is to find extraordinary and therefore interesting phenomena in databases. The definition is not based on a discretization paradigm that ?converts? quantitative data into categorical items, but based on the consideration of the distribution of the continuous data, via standard statistical measures such as mean and variance.

In this paper, on the basis of cluster analysis and the definition proposed by R. J. Miller and Y. Yang [11], a new algorithm for mining distance-based quantitative rules is presented, which is a new approach to handling continuous attributes for association analysis. The purpose is to improve the validity and efficiency of association analysis for handling continuous attributes, and to make it more suitable for real world applications.

The rest of this paper is organized as follows. We present a novel clustering algorithm based on density and adaptive density-reachable in section 2. In section 3, the creation of quantitative association rules based on distance is proposed.

The experiments and applications of the algorithm are presented in section 4. Section 5 concludes with a summary and some directions for future research.



II. CLUSTERING ALGORITHM BASED ON DENSITY AND ADAPTIVE DENSITY-REACHABLE  Clustering aims to find useful groups of objects (clusters).

There are many concepts about clusters. Based on the spatial distribution of data objects, the type of clusters can be described as follows [1]:  Well-separated cluster. A cluster is a set of objects in which each object is closer (or more similar) to every other object in the cluster than to any object not in the cluster.

Center-based clusters. Each object in a cluster is closer to center of the cluster than to centers of any other clusters.

Contiguity-based clusters. Each object in a contiguity-based cluster is closer to some other object in the cluster than to any point in a different cluster.

Density-based clusters. A cluster is a dense region of objects that is surrounded by a region of low density.

In order to handle the different types of clusters and meet the needs of different applications, many clustering algorithms have been developed. These can be categorized into partitioning methods, hierarchical methods, density-based methods, grid-based methods, and model-based methods.

In many application domains, clusters of data objects are of arbitrary shapes, sizes and densities, and the number of clusters is unknown. In such scenarios, traditional clustering algorithms, including partitioning methods, hierarchical methods, density- based methods and grid-based methods, cannot identify clusters effectively or accurately. Obviously, these are critical limitations or weaknesses of clustering methods. In order to solve the problem, we proposed a clustering algorithm which has the abilities to find clusters of arbitrary shapes and sizes, to handle clusters and noise of varying densities, and to identify outliers effectively. The algorithm is called CADD (Clustering Algorithm based on Density and adaptive Density-reachable, CADD) [8, 9].

A. Relevant Definitions of the Clustering Algorithm (1)The density of data points: Given N d-dimensional data  points in dataset D: {Xi} where i = 1, 2,?, N. The density of data points models the overall density of a set of points in dataset D as the sum of influence function associated with each point:  ? ? ? ?  ? ?  ? ?  N  j  XXd  i  ji  eXDensity   ,   ?  ? ? ? ???  Here, Gaussian influence function ? ? ? ?     ,  X, ? ji XXd  jiGuass eXf ?  ? indicates the density influence of each data points to the density of point Xi, and ? is density adjustment parameter which is analogous to the standard deviation, and governs how quickly the influence of a point drops off [15].

(2)Local density attractors: Local density attractors are the data points at which the values of density function Density(X) are local maximum.

(3)Density-reachable distance: Density-reachable distance is used to determine a circular area of data points x, labeled as ?={X| 0<d(Xi, Xj)?R}, and the data points in the circular area are belong a same cluster. The definition formula is:  CoefRDmeanR ? )( 												???  Here, 2/121 1 1  ))(  1()( i N  i i XX  N Dmean ?  ? ? ? ?? ?  is the mean distance  between all data points in dataset D, and CoefR (0<CoefR<1) is named as the initial adjustment coefficient of density- reachable distance.

(4)Density-reachable: Density-reachable means that if there is a chain of objects p1, p2, ,pn=q, q is a local density attractor, and pn-1 is density-reachable from q, then for pi D,(1 i<n-1)     and d(pi,pi+1)?R, we define that object pi,(1 i<n-1) is density- reachable from q.

(5)Adaptive density-reachable distance: In handling the clusters of varying densities, it is important to adjust the density-reachable distance R step by step during clustering. The adaptive adjustment is carried out through multiplying the original density-reachable distance R with an adjustment coefficient ?:  RRAdap ? 																															???  Here, RAdap is adaptive density-reachable distance, and ? is defined as:  )( )( 1  i  i  AttractorDensity AttractorDensity ??  ???  This is because that when the density value of local density attractor of a cluster is greater , the distance between objects in the cluster is smaller, and on the contrary, when the density value of local density attractor of a cluster is smaller, the distance between objects in the cluster is larger. When i=1, let  . Because , so ??1. It is necessary  to point out that adaptive adjustment coefficient ? may also be other value of function which can adjust the density-reachable distance effectively.

)()( 10 AttractorDensityAttractorDensity ?  )()( 1 ii AttractorDensityAttractorDensity ??  B. The Clustering Algorithm The operation of the clustering algorithm consists of the  following steps: ? To calculate the density values of data points in dataset D  using Equation (1), determine the Local density attractors Density(Attractori).

? To compute initial density-reachable distance R using Equation (2) which can be adjusted by the initial adjustment coefficient of density-reachable distance CoefR.

? According to the initial density-reachable distance R and the adaptive density-reachable distance RAdap, search data points which belong to local density attractors respectively. In this step, if the number of data points in the area of density-reachable distance is less than MinPts, such as MinPts ?5 or 7, the data points are identified as noise or outlier and assigned to noise dataset.

The process of the clustering algorithm is illustrated in Figure 1.

Figure 1. The process of CADD  According to the definitions we design and implement the clustering algorithm as below:   Algorithm Clustering Algorithm based on Density and  adaptive Density-reachable, CADD Input: Adjustment coefficient of density- reachable  distance CoefR, density adjustment parameter ?.

Output: Number of clusters, the members of each cluster,  outliers or noise points.

1: Compute the densities of each data points and initial  density-reachable distance.

2:  i?1 3:  Repeat 4: Seek the maximum density attractor ODensityMaxi in the  original dataset of clustering objects as the cluster centroid of Ci.

5: Assign the data objects which are density reachable within adaptive density-reachable distance from ODensityMaxi to cluster Ci, and delete the clustered objects from original dataset.

6:  i?i+1 7:  Until Original dataset is empty.

8:  Make the clusters which have few objects (such as less 5  or 7) into outlier or noise group.



III. DISTANCE-BASED QUANTITATIVE ASSOCIATION RULES Association rules that contain continuous attributes are  commonly known as quantitative rules. There are various methodologies for applying association analysis to continuous data. The types of methods mainly fall into three broad categories [1]: (1)discretization-based methods, (2)statistics- based methods, and (3)non-discretization methods. The quantitative association rules derived using these methods are quite different in nature.

The main concept for creating distance-based association rules is to cluster data objects and mine association rules among these clusters according to the distances between the clusters.

A. Clustering It is necessary to point out that if there are a number of  clusters in a data space, the strong association rules among attributes just exist in each cluster. Therefore, the creation of association rules just focuses in each cluster. The notion proposed in this section can make the process of quantitative association analysis more simple, efficient and effective.

We use the clustering algorithm CADD to group the data points in a dataset. For a specific set of attributes, restrictions are placed on the properties of these data points when projected on X. For this reason, when a cluster is projected on X, the cluster is denoted as CX. Given N d-dimensional data points { iX  ?  }, (i=1,?,N) in CX, the Clustering Feature [7] is defined as:  ?????????????????????????????                								 ),,( 0 RXNCF ? 																							??? Here, N is the number of data points in CX,  ? ?? N  i i X  N X  1 is the centroid of CX, and     2/12  ))(1( XX N  R N i i  ?? ? ?  is the average distance from member points to the centroid or the radius of CX. According the clustering feature CF, the relationships among clusters projected on attributes are determined as presented later.

B. The Association Rules For a cluster, if association rules exist among the attributes:  X={x1,?,xx} and Y={y1,?,yy}, the definition of association rules is x1 Cx1[x1],?,xx Cxx[xx] y1 Cy1[y1], ?,yy Cyy[yy]. The clusters are labeled with the attributes on which they are projected. For abbreviation, the rule is presented as Cx1,?,Cxx Cy1,?,Cyy, where all the attributes X={x1,?,xx} and Y={y1,?,yy} are disjoint. The rule Cx1,?,Cxx  Cy1,?,Cyy holds with confidence c if |( iCxi) ( jCyj)|/| Cxi|?c. The rule holds with support s, if |( iCxi) ( jCyj)|?s.

For distance-based association rules, the confidence c is determined by the distances among the images of clusters Cx1,?,Cxx and Cy1,?,Cyy, and the support s is dominated by the numbers or densities of clusters. In this paper, we use the Euclidean distance metric to measure the distance between data points.

According to the notion proposed by R. J. Miller and Y.

Yang [11], the definitions of distance-based association rules are as follows:  Let x1,x2,?,xx and y be pairwise disjoint sets of attributes. A N:1 distance-based association rule (DAR) is a rule of form R: Cx1,?,Cxx Cy, where each Cxi is a cluster projected on xi and Cy a cluster on Y. R holds with degree of association D0 if:  xiDYCxYCyD i ??? 1])[],[( 0 					???  jidXCxXCxD ixjjii ??? 0])[],[( 												???  Here, we emphasize that Cy[Y] indicates the Cy?Cxi, Cxi[Y] is the image of all data points in the range of Cxi projected on Y, and  is the distance between centroids of Cy[Y] and Cxi[Y]. In real applications, the value of D0 is determined by the radius of the cluster. Because the creation of association rules is in each cluster, it is unnecessary to calculate Equation (7). We define the support S of the association rule as:  ])[],[( YCxYCyD i  ||/|][| DsYCyS ? 																																		???  Here, |Cy[Y]| is the number N of data points in the cluster, and |Ds| indicates the number of all data points in the data space.

The degree of association replaces the traditional notion of confidence and the density threshold (or the number of data points in a cluster) replaces the notion of support.

For 2-dementional data space, as shown in Figure 2, we discuss the creation of distance-based association rules. When there is one cluster in the data space, as shown in Figure 2(a), we can obtain the association rule: Cx  Cy,  0])[],[( ?YCxYCyD  [( XCxD  ])[1],[(  , S=100%, i.e., when the value of X is in the interval of [x1, x2], the value of Y is bound to [y1, y2], and Cy  Cx, , S=100%, i.e., when the value of Y is in the interval of [y1, y2], the value of X is bound to [x1, x2]. The situation is different in Figure 2(b), there are two clusters with same number of data points in the data space, and the rules derived from the data space are C1x Cy,  0])[], ?XCy  0?YxCYCyD 0])[2],[(  , S=50%, and C2x Cy, ?YxCYCyD  CyXCxD ],[(  2],[( yCXCxD  (D [2],[2(  , S=50%. If let the degree of association D0=R (radius of the cluster), we can not derive the rule Cy Cx, because the centroid Ox of Cy[X] is located between C1 and C2, and . As for Figure 2(c), we can obtain the rules: C1y Cx, , S=50%, and C2y Cx, , S=50%, but not Cx Cy. If the distribution of data points in the data space is as shown in Figure 2(d), the association rules derived from the clustering result are C1x C1y, , S=50% and C2x C2y,  RX ?])[ ],[( XCxD  0])[ ?X  [1],[1 XyCXxC 0])  0])[1 ?XyC  0]) ? ?XyCXxCD  0])[1],[1(  , S=50%; C1y C1x, ?XxCXyCD  0])[2],[2(  , S=50% and C2y C2x, ?XxCXyCD , S=50%.

The analysis indicates that the strong association rules only exist among the attributes in a cluster, and other clusters existed in the data space may weaken or break the association among the attributes in the cluster.

(a) One cluster in data space.        (b)Two clusters parallel with X.

(c) Two clusters parallel with Y.      (d)Two clusters oblique crossing axis.

Figure 2. Association rules in 2-dimentinal data space.

In this paper, we focus our research to the N:1 distance- based association rules. N:1 distance-based association rules have a number of important applications. Often in data mining, we are interested in discovering which attributes determine one of a specific set of target attributes. For example, in geochemical applications, there are many chemical elements which are measured in the field, and we have to determine the relationship between a target element and a specific set of elements so as to solve some geochemical problems. Another example is that in medical domain we also have to research the relationships between a specific symptom and other symptoms, and so on.

C. The Association Algorithm  The algorithm presented in this section is a modification of the algorithm proposed in [11], and it includes three phases:  ? In first phase, clusters in the data space are identified using normal clustering algorithms, such as K-means or CADD [8, 9], and the clustering features are calculated.

? In second phase, the distances  are calculated in a cluster which holds support S, here, Y is the target attribute chosen by domain experts interactively and X={x1,?,x  ])[],[( YCxYCyD i  x} are the other attributes of data points in the data space.

? In third phase, the rule is formed by combining the clusters projected on attribute Y which holds  and . 0])[],[( DYCxYCyD i ? ||/|][| DsYCyS ?

IV. APPLICATIONS In the real world application of data mining techniques, we  use cluster analysis and association analysis to research the regional geochemical characteristics in a region of western China. The sampling area is 80*72 Km2, the distance between sampling points and lines is 2 Km, and the number of sampling points is 1517. The lithological distribution pattern in the region is shown in Figure 3: (1) the pale yellow area indicates humus, anemoarenyte and diluvium in Quaternary; (2) the yellow area is sandy and carboniferous dark gray-colored slate and silty mudstone with siltstone in Dyas; (3) the dark yellow area represents clastic rock, dark gray-colored conglomerate, graywacke, and silty slate with silty mudstone in Dyas; (4) yellow and yellowish-brown-colored granite porphyry, hornblende granite porphyry, and masanophyre in Yanshanian magmatism distribute in the pale pink area; (5) the pink area is flesh and red-colored granitello and medium-grain granite in Yanshanian magmatism; (6) the blue area indicates gray and sage green-colored rhyolite, rhyolitic tuff with breccia in Jurassic; (7) the purple area is dimgray-colored medium and coarse-grain quartz diorite and gneissic quartz diorite in Variscan magmatism; (8) the green area is sage green and pale- gray-yellow-colored biotite plagioclase gneiss with amphibole plagioclase gneiss, quartz schist, shallow particle rock, and griotte.

Figure 3. Lithological distribution pattern in the sampling region.

The purposes of clustering analysis and association analysis are to research the characteristics of regional geochemical anomalies, to determine the distribution of anomaly source, and  to analyze the type of mineral resources according to the relationship among the geochemical elements.

A. Cluster Analysis There are 1443 samples actually in the geochemical  measurement region, and for each sample 17 kinds of chemical elements are measured which are Ag, Al, As, Au, Cd, Co, Cu, Fe, Hg, K, Mn, Mo, Pb, Sb, Sn, W, Zn. The sampling points constitute a spatial dataset in which each data point has 17 attributes, i.e., Ag, Al, As, Au, Cd, Co, Cu, Fe, Hg, K, Mn, Mo, Pb, Sb, Sn, W, Zn. The clustering result of CADD is shown in Figure 4. There are four clusters: Cluster1, Cluster2, Cluster3, Cluster4 and Outliers found in the spatial dataset. The distribution pattern of the clusters is consistent with the lithological distribution pattern in the region, which can indicate the characteristics of geochemical anomaly. The distribution area of Cluster1 reflects the geochemical characteristics of the dimgray-colored medium and coarse- grain quartz diorite and gneissic quartz diorite in Variscan magmatism (the purple area).

Figure 4. Clustering result of geochemical samples.

B. Association Analysis Geological survey in the field indicates that mineral  occurrences of W and Pb or Cu exist in the area of Cluster1.

According to the geochemical theory [21], the geochemical characteristics of a geological unit can be evaluated using the association rule of the chemical elements. In this section, we use association analysis to analyze the relationship among the chemical elements so as to determine the characteristics of a geological unit, and provide useful knowledge for mineral resource survey.

Because the chemical elements Cd and Sb are important indicator elements for the evaluation of mineral resources, we research the relationships Cd and Sb with other chemical elements using the distance-based association analysis. For Cluster1, when D0=0.005, the quantitative rules are as follows:  (1)The association rule of Cd with other elements:  As [1.4,45], Ag [0.01,0.29], Au [0.3,1.1], Hg [1.0,120], Mo [0.02,4.7], Pb [5.0,56], Sb [0.03,2.78], Sn [0.1,14], W [0,8.7]  Cd [0.01,0.22], D0=0.005, S=33.6%.

(2)The association rule of Sb with other elements:  Ag [0.01,0.29], As [1.4,45], Au [0.3,1.1],     Hg [1.0,120], Mo [0.02,4.7], Pb [5.0,56], Sn [0.1,14], W [0,8.7.0]  Sb [0.03,2.78], D0=0.005, S=33.6%.

The association rules indicate that Cd and Sb are related with Ag, Au, Hg, Mo, Pb, Sn and W under certain degree of association and support. According to geochemical theory [21] we can conclude that the mineral resources in the region are related with Ag, Au, Hg, Mo, Pb, Sn and W. The conclusion is consistent with the real characteristics of mineral resources in the region.



V. CONCLUSIONS For real world applications of data mining, the key  technique is to handle complex data effectively and efficiently.

The research results indicate that CADD can not only overcome the weaknesses of traditional partitioning and hierarchical methods to handle arbitrary clusters and outliers, but also improve the capability of density-based methods to deal with clusters of varying densities. At the same time, the algorithm can evidently reduce time and space complexity as compared with other density-based algorithms. In association analysis, mining the continuous attributes may reveal useful and interesting insights about the data objects which are of continuous attributes. Quantitative association rules aims to deal with the relationships among continuous attributes of data objects. The association analysis algorithm presented in this paper uses a clustering algorithm to identify the intervals of attributes in clusters and combines the clusters projected on attributes to form distance-based association rules held with degree of association and support. Experiments and applications indicate that the algorithms of clustering and association are effective in real world applications. In future research, we will do more work in the creation of N:N distance-based association rules, and make the algorithm more effective for processing geoscientific data.

The research results may provide new methods and techniques for comprehensive interpretation of geophysical and geochemical data, and minimize the uncertainness and ambiguity of geoscientific data interpretation. Along with the development of geosciences, more and more data are collected, and data mining techniques have evolved important methods to process huge amounts of geoscientific data and to discover useful geoscientific knowledge efficiently and effectively.

