H-PFSP: Efficient Hybrid Parallel PFSP Protected Scheduling for MapReduce System

Abstract?MapReduce provides a data-parallel computing framework, and has emerged as a popular processing model due to the simplicity of operations for big data application developers. Data processing applications from many different domains such as search and data mining are usually developed using open-source Hadoop implementation of MapReduce or self-developed MapReduce-like implementations like Dryad [1] and Ciel [2]. In cloud environments, products like Amazon?s Elastic Compute Cloud (EC2) [3] provide MapReduce services as third-party multi-tenant service. Even within a company, a number of products may share the MapReduce cluster.

Therefore, a fair and efficient scheduler is crucial to improve performance of submitted jobs and guarantee multi-user fair- ness. However, in practice, it is hard to guarantee both fairness and per-job performance, especially when jobs are scheduled without accurate estimation. We show that processor sharing (PS) type of schedulers like Fair Scheduling degrade the per-job performance in a multi-user environment. We present a new scheduling policy, Hybrid Parallel pessimistic Fair Schedule Protocol (H-PFSP), that can finish every job no later than Fair scheduler does. Unlike Fair scheduler, however, it can improve the per-job performance of MapReduce systems with relatively accurate job progress estimation.

Keywords-MapReduce; H-PFSP; fair scheduling; perfor- mance;

I. INTRODUCTION  The advent of big data has increased the prevalence of parallel computing models. Large amount of data is accumulated every day, especially by Internet companies serving a huge number of users. The data needs to be processed in an efficient way to preserve its effectiveness after it is generated. Search engines like Google need to crawl the web pages from a huge number of URLs, parse all of them to extract the feature information, then index the processed data for users to search. All of these jobs handle gigantic databases and need to be done in a timely manner.

In view of these challenges, Google has designed MapRe- duce [4], a parallel processing model. It first decomposes each job into several independent parts with roughly the same size, each of which is processed by a Map task. To aggregate the generated intermediate results, several tasks of another type called Reduce are invoked to collect these results and generate the final results. The data to be pro-  cessed or generated is stored in sequence files in the form of <Key, Value>. The outputs of Map tasks are also in <Key, Value> form. They are partitioned and shuffled to Reduce tasks according to different values of keys. Hadoop [5] is an Apache open-source implementation of MapReduce, where workers of the cluster run on top of JVMs. They can be configured as either Map slots or Reduce slots, which are the independent computing units running concurrently.

The key module of MapReduce is the scheduler. It runs on the master server that performs the centralized control of MapReduce cluster. The scheduler manages all the slot resources on slave workers and allocates them to submitted jobs. For cloud computing service, MapReduce provides a sharing mechanism to multiple users. It is required by the service provider to allocate the resources in a fair way while guaranteeing the overall utilization. Even within a company, many products also need to share a MapReduce service to process their own huge amount of data. The simple FIFO scheduling is so cumbersome for a multi-job scheduler that it starves jobs. Fair scheduler works via polling. It favors the fair allocation of all slots, rigorously trying to guarantee fairness at every moment. Since this fair algorithm actually splits the cluster into slices, the real resources each job occupies are only a portion of the cluster. Therefore, the response time of each job will be much longer than when it monopolizes resources.

Per-job performance is critical to each user or product.

Users and products may be independent, and compete for the resources with each other. In reality, the job submitted by a user may only be a step of its workflow. For example, after the indexing job is completed, the indexed output data will be downloaded to front-end for the search use. The job taking a long time to finish can cause a bottleneck, which slows down the total flow time. For example, the bottlenecks of news search or micro blog search suggest the loss of effectiveness for the outcome.

This paper addresses the problem on how to improve the per-job performance and accelerate the mean flow time of jobs without violating fairness. By showing another semantic of fairness, we can design a better scheduler to accelerate the mean flow time. It is enlightened by Pessimistic Fair   DOI 10.1109/TrustCom.2013.133     Schedule Protocol (PFSP) [6], which can be proven to dominate other online scheduling policies theoretically. It reduces the per-job flow, thereby reducing the mean flow time of the jobs.

H-PFSP uses estimated job execution times as parame- ters by which it makes scheduling decisions. A imprecise estimation may make the whole scheduling invalid, or even degrade the overall performance. Thus, to achieve reasonable accuracy, we update the estimates at intervals during the running progress to capture the latest status of running jobs and clusters. Moreover, the frequent update needs efficient algorithms to reduce the complexity of the update routine.

To achieve this goal, we design an incremental algorithm to make the update complexity acceptable.

We implement H-PFSP scheduler, then embed it into Fair scheduler, so that jobs in each pool can be scheduled by H-PFSP. To evaluate the performance, we test two bench- mark MapReduce programs: Wordcount and Terasort. To validate it in large-scale environment, we also implement a simulation tool. We can generate a large amount of data and simulate the scheduler using this tool. By comparing H- PFSP to Fair scheduler using both experiment and simulation results, it is shown that H-PFSP has better performance than the Fair scheduling in most scenarios.

This paper is organized as follows. Section II illustrates the core idea and the mechanisms adopted by our H-PFSP solution. Section III presents the design and algorithms of H-PFSP. We present the H-PFSP implementation and performance evaluation in Section IV. Section V surveys related work. Finally, we make final conclusion in Section VI.



II. H-PFSP SCHEDULING  In this section, we present the core idea and some key mechanisms adopted by the H-PFSP scheduler. Before we introduce H-PFSP, a simplified version (Parallel PFSP) is presented to illustrate the key idea of H-PFSP.

A. Parallel PFSP  FSP [7] maintains virtual execution states of each job by assuming that jobs are scheduled by PS scheduler. FSP always gives priority to the job that would finish first under PS. It is proven that the flow time of every job is no longer than that under PS, which is defined as the property of protective scheduling in [6]. The priority setting of FSP guarantees the completion order of jobs is the same as under PS. It is a sufficient condition for protective scheduling.

However, it is too strict and unnecessary. Protective scheduling only implies the deadline of each job. Any com- pletion order of jobs is satisfactory provided their deadline is met. Thus, inspired by PFSP [6] which is another protective scheduling algorithm dominating FSP, we present Parallel PFSP. The idea of Parallel PFSP scheduling is to take parallel slots as a processor with the aggregated capacity.

?  ??  ??  ??  ???  ? ?? 	? 	?   ??????????????  ??????????   ??????????????   ??????????????  ??? ??? ?? ???  ??  ??  ??  ???  ??  ??  ??  ???  ? ? ?? 		 ??????????  ? ? ?? 		 ??????????  ????????? ?????  ??  ??  ?????	?  ? ?????????????	?  Figure 1. Comparison of three schedulers. (a) Fair Scheduler; (b) FSP; and (c) Parallel PFSP.

Then, it mimics the shortest remaining processing time (SRPT) scheduling while guaranteeing the protectiveness property, by always scheduling a completable job with the shortest remaining process time. A job is completable [6] at time t if serving it to completion from t without being interrupted will not violate the protectiveness for other jobs, provided that no more jobs will arrive after t in the future.

Suppose we have the relatively accurate estimation of job execution information first, we put an example here to illustrate the advantage of Parallel PFSP in Fig. 1. The algorithm of a modified version is presented in detail in Section III. We roughly show the capacity of resources as continuous without explicitly enumerating the slots and the task count. The real case using discrete slots and tasks can only produce a small deviation and does not have much of an impact on our conclusion. Fig. 1 shows that every job scheduled by Parallel PFSP finishes no later than scheduled by the FSP and the Fair scheduling, which implies the mean flow time jobs is reduced.

We present several challenges and motivations of the Parallel PFSP in MapReduce system before we dig into the H-PFSP scheduling. First, precise estimation of the job size or the execution time is difficult. MapReduce job execution is volatile, especially in a heterogeneous environment. How to make a relatively accurate estimation becomes an issue.

Second, multiple computing slots are used to execute tasks of MapReduce jobs concurrently. Traditional schedulers are mostly designed for scheduling indivisible jobs. However, the key principle of MapReduce is to parallelize job exe- cution using multiple resources by splitting each job into independent tasks. In this context, PS implementation turns into another form - Fair scheduling. So we use Fair and PS interchangeably in the rest of the paper. Finally, the complexity of our H-PFSP scheduling algorithm is critical and needs more consideration.

B. Job profiling and estimation  Similar to FSP and SRPT-like schedulers, H-PFSP needs the estimates of the per-job flow time. The estimated flow time is used to initialize the scheduling algorithm at the start.

During the execution, the job status should be re-estimated since server performance varies due to the background work- loads or failures. The remaining processing time of each job needs to be updated every once in a while. Profiling jobs by the historical job execution information is difficult due to the diversity of jobs and volatility of cluster environment.

For example, the web parsing for a search engine has a computing-intensive Map phase, but does trivial work in the Reduce phase. In contrast, the page crawling has a network- intensive Reduce phase since crawler schedules the crawling tasks according to the hash values of site IPs, which can only be done after the partition and shuffle stages of Reduce phase, while the Map phase of crawling needs little work.

Because of these reasons, we will not estimate the status of the current job by historical job profiling. Instead, the historical execution information of the current job itself will be used. First, we use the information of finished tasks of a job to estimate the average processing time of its tasks, including the average processing time of a Map task Avg(T )Mi and Reduce task Avg(T )  R i . These values  are updated at intervals before we calculate the remaining execution time of the job. Suppose that between two updates, the Map progress for job i has increased from PMi to P?Mi , the average task execution time can be estimated by computing the execution rate during this period.

?PMi = P? M i ? PMi (1)  ?Avg M  i = ? ?AvgMi + (1? ?) ??t/?PMi (2) where ?t/?PMi is the average time of a Map task.

The progress value can be obtained directly in Hadoop implementation. Exponentially weighted moving average (EWMA) method is used here to smoothly estimate the average value. The same estimation method also applies to a Reduce task. Note that we do not update the estimates if ?PMi is too small, e.g., less than 0.02, because it is imprudent to make estimations when the job has little progress. However, we cannot obtain the accurate average time of Reduce task at the Map phase since Reduce phase has not started yet (or a Reduce task has launched but stuck in the shuffle phase because not all Map tasks have finished yet). Thereby, we can only use the average execution times of the Map tasks as estimates instead before the Map phase has completely finished.

We consider the aggregate computing capacity of the parallel slots in MapReduce clusters as a single processor by normalizing the average task processing time as follows:  ?Avg(T )Mi = Avg(T ) M i /N(S)  M (3) ?Avg(T )Ri = Avg(T )  R i /N(S)  R (4)  where ?Avg(T )Mi denotes the normalized average processing time of Map task of job i. N(S)M is the number of Map slots configured in the cluster. The notation is the same for Reduce.

We use the average task execution time and job progress to estimate the remaining execution time of a job.

wi = max { wmin, N(T )Mi ? (1? PMi ) ? ?Avg(T )Mi  +N(T )Ri ? (1? PRi ) ? ?Avg(T )Ri }  (5)  where wi is the remaining execution time of job i. wmin  is a small constant that marks job i as still running even though the value of the estimated remaining execution time reaches 0. N(t)Mi is the number of Map tasks of job i. The notation is the same with Reduce.

C. Slack system update for parallel jobs  To meet the protectiveness constraint, we use a slack vector [6] to indicate how much slackness there is for each job in the current state. We need to construct a virtual PS scheduler, and maintain the job states under the virtual PS scheduling, which is useful to H-PFSP. The slack value of a job is computed as the difference between the remaining flow time of the job under the virtual PS scheduling and the remaining flow time in a schedule serving only this job and the jobs which would finish before it under PS. It can be calculated as follows.

si = ?  1?j<i vj + (m? i+ 1)vi ?  ? ? ?  1?j?i wj  ? ? (6)  where m is the number of running jobs in the virtual PS scheduling queue. vi is the remaining processing time for job i under PS. The initial value of vi equals wi. si denotes the slack value for job i. The virtual PS scheduler sorts the jobs in order of their remaining execution time. If jobs are scheduled using the PS scheduler, the jobs queueing in front of i will finish before job i. As it takes vi to execute the remaining tasks of job i using the slots in the cluster, the jobs following job i in the queue will also have already been processed for time vi when job i finishes under PS. Thus, the remaining flow time of job i amounts to  ? 1?j<i vj +(m?  i+ 1)vi. If only job i and the jobs that would finish before it under PS need to be served, job i will complete after time?  1?j?i wj . Therefore, the slack value of a job indicates how much slackness there is before violating protectiveness property. A schedule of jobs is protective if and only if the slack vector is non-negative at any time. The three variables, i.e., w,v and s constitute a slack system.

Next, we illustrate how to update the slack system if the job information, i.e., w and v, is estimated in the beginning and not re-estimated again during the execution. We present an algorithm to update the slack system with re-estimation periodically during the processing in the next sub-section.

Within a time interval ?t, one or more jobs are running on the slots. In our design, only one job occupies the slot resources in the beginning if the number of tasks in the job is greater than the number of slots. However, the last wave of this job may contain only few tasks and cannot fully utilize all the slots. In this case, the residual slots are assigned to other jobs. If job i has been executed during ?t, assuming that no new jobs arrive in this short period, we update the slack system as follows.

wi = wi ??t?i (7) vj = vj ??t?i/m, for all j (8) sj = sj ??t?i, for all j < i (9)  where m is the number of jobs in the virtual PS queue. Since under PS, the slots are shared all the time, the processing duration should be equally amortized to all the running jobs in the virtual PS queue. Thus, each job should take 1/m time to process under virtual PS. It can be proven that after updating the slack system, eq. (6) still holds. We use ?t?i instead of ?t as follows:  ?t?i = ?t/N(S) (10)  where N(S) is the total number of slots. Here, to consider parallel slots in MapReduce clusters as a processor with the aggregated capacity, we normalize the duration by dividing the time interval by the total number of slots. After updating job i, if we find any job j, for which vj = 0, we remove it from the virtual PS scheduling queue. If job i has been finished but is still in the virtual PS scheduling queue after updating its state, we set si =?.

D. Incremental estimations  The FSP or SRPT scheduler always works statically with a priori job estimation that does not change during the execu- tion. However, the job execution time can be overestimated or underestimated since the system state varies over time.

A relatively long-term estimation makes the job execution suffer from serious performance degradation because the scheduling policy made within a large interval may be totally wrong.

A better alternative is to make incremental estimations at intervals. Before updating the slack system, we re-estimate the job remaining time first. After we re-calculate the remaining execution time of each job, i.e. w?i, we update wi and vi in the slack system as follows.

?wi = w?i ? wi (11) wi = w?i (12) vi = vi +?wi (13)  This modifies the previous estimation. After that, the jobs in the virtual PS scheduling queue need to update their slack values s according to (6). However, computing the slack value for every job is nontrivial. To scale well, we need  to design a method to reduce such computing burden. In fact, we do not need to update all the jobs in virtual PS scheduling queue, but only the jobs affected by the change of estimated remaining time. We now discuss the two possible cases below.

Case 1: After the estimation, Job i moves from position i to i? in the virtual PS scheduling queue, where i < i?.

From eq. (6) we know, the remaining processing time of job i, i.e. wi and vi, only affect the slack values of jobs following it in the PS scheduling queue. From eq. (6) we know that the slack values of jobs between i and i? should involve the terms wi and vi but they do not. So only these jobs need to update their slack values. A simple calculation shows that,  sj = sj + (vj ? vi + wi), for all i < j < i? (14)  si = si +  i??1? ?=i+1  v? + (m? i? + 1)v?i ? (m? i+ 1)vi  ? ? ?  i?1? ?=i+1  w? + w ? i ? wi  ? ? (15)  Case 2: After the estimation, Job i moves from position i to i? in the virtual PS scheduling queue, where i? < i.

Based on the analysis similar to case 1, only jobs between i? and i should have been updated. For these jobs, a simple calculation shows that,  sj = sj + (v ? i ? vj ? w?i), for all i? < j < i (16)  si = si ? i?1?  ?=i?+1  v? + (m? i? + 1)v?i ? (m? i+ 1)vi  +  ? ?  i?1? ?=i+1  w? ? w?i + wi ? ? (17)  After the re-estimation of the job information and modi- fication of the slack system, we can update the slack system as usual by eqs. (7)-(9).

E. Two-step hybrid scheduling  Although we have designed an incremental estimation method to continuously approximate the real flow time of jobs, there is some lag time for the estimates to converge correctly. Thus, it is imprudent to make a scheduling deci- sion based on the estimation at the start of job execution, even though the incremental estimation is implemented.

To address this issue, we design a two-step hybrid scheduling policy to make the Parallel PFSP more effective, before we dig into the details of the Parallel PFSP algorithm.

Two steps are included in both Map and Reduce phases.

When a job just starts Map phase or Reduce phase, a proper threshold K is initialized. Before the job finishes K Map tasks or Reduce tasks, it can preempt resources from other jobs that have finished the first step. By preempt, we actually     mean allocating slots to a job immediately when there are idle slots in the cluster. It does not support real preemption since it wastes resources, thereby decreasing the overall utilization in the system. This is called a fair phase, because for more than one job in the first step, we adopt a Fair scheduling policy, to assign the job with the largest deficit.

After a job has finished K Map tasks in Map phase or K Reduce tasks in Reduce phase, it is scheduled according to the Parallel PFSP. As both the Fair scheduling and the Parallel PFSP are adopted in the scheduling strategy, we call our scheduling scheme Hybrid Parallel pFSP (H-PFSP). We present H-PFSP in detail in Section III instead of Parallel PFSP.

The advantage of the two-step hybrid scheduling is that a job can start right after it is submitted without waiting for a long time. If the job has some unknown faults, it can throw exception in the first place. Programmers can quickly obtain the instructions and find bugs as a result of the early respond.

When a new job arrives, it is put in the running pool as well as the virtual PS scheduling queue with PS related information initialized using default values, e.g. the average execution time of the tasks of the finished jobs. wi and vi are estimated by using these default values.

wi = vi = max { 0, N(T )Mi ? ?Avg(T )MDef +N(T )Ri ? ?Avg(T )RDef  } (18)  where ?Avg(T )MDef is the default average execution time of a Map task. After a job is put in the PS scheduling queue , it will preempt the slot resource until a certain number of tasks have been scheduled. After that, we will obtain a relatively accurate estimate of the job execution time. Suppose that at that time, this job is at position i ordered by PS-remaining time v in the PS scheduling queue, the PS scheduling slack system must be updated following eqs. (14)-(17).



III. DESIGN AND ALGORITHMS  This section presents how we design the H-PFSP schedul- ing algorithms. We illustrate the data structure used in H-PFSP scheduler. Next, we describe the auxiliary steps needed by the scheduling algorithm, including job status re-estimation and slack system update. After that, H-PFSP algorithm is straightforward.

A. Data structure  Virtual PS scheduling. H-PFSP scheduling depends on the virtual PS scheduling to calculate and update the slack system. When scheduling a job according to our H-PFSP design, we need to update the actual job status under H-PFSP and also the status under virtual PS. A PS scheduling priority queue should be maintained. Once a job is submitted, a virtual copy of this job is pushed into the queue. It orders the job copies according to the remaining processing time  under PS. In case of a tie, we place the job submitted earlier in the front.

Slack System Data Structure. We maintain a slack system information structure for every job. It contains the job remaining time under PS, the remaining time under H-PFSP, and the slack value of the job. To facilitate the calculation of the slack system, we also need to store some auxiliary estimates and recalculate them at intervals. These estimates include the current progress of Map and Reduce phases, the average time of Map and Reduce tasks.

B. Re-estimation and slack system update Every once in a while, we need to update the slack system  since all the slack values will change as some jobs have occupied the slots and run during the interval. Moreover, the job status should be re-estimated before the slack value update since the estimation of job remaining time may vary based on the new information. For the sake of simplicity, we update the slack system synchronously with the re- estimation of the execution progress and the remaining time.

Algorithm 1 sketches the algorithm of the updating process.

Algorithm 1 Function: updateJobInfo(job) 1: /* Job shuts re-estimation and slack system update */ 2: while Running do 3: Thread.sleep(UPDATE INTERVAL) 4: for all jobi executed in the interval do 5: Update estimation infos using eqs. (1) - (5) 6: Modify the slack system values according to (11) - (17) 7: Update slack system using eqs. (6) - (10) 8: end for 9: end while  C. H-PFSP algorithm The kernel of our design is an H-PFSP task assignment  algorithm, implemented by H-PFSP comparators. When a slot becomes idle, our scheduler assigns a task to it according to the H-PFSP comparator, which always chooses the job in the fair phase first. If more than one job is in the fair phase, the scheduler chooses one with the largest deficits, i.e., the one with the minimum number of tasks scheduled. If all jobs have finished their fair phase, the scheduler assigns the completable job with the shortest remaining processing time under H-PFSP. The procedure is described in Algorithm 2.

The function isFairPhase determines if a job is in its start phase. A job is in the fair phase under two circumstances.

We discriminate between the two stages of a job. A job is running in the Map stage if the Map progress is less than 0.95; otherwise, it is running in the Reduce stage. A job is in its fair phase if it is in the Map state and the number of scheduled Map tasks is under a certain threshold, or if it is in the Reduce state and the number of Reduce tasks is under the threshold. The procedure is given in Algorithm 3.

By definition, the sufficient and necessary condition for a job i to be completable is that for all jobs j queued in front of i, i.e., vj < vi, sj ? wi, as shown in Algorithm 4.

Algorithm 2 H-PFSP Task Assignment 1: while true do 2: if exist idle slot then 3: assign tasks using H-PFSP comparator 4: end if 5: end while 6:  7: H-PFSP Comparator: 8: for all job for which isFairPhase(job)==true do 9: assign job with the least tasks assigned yet  10: return 11: end for 12: for all job for which isCompletable(job)==true do 13: assign job with shortest remaining processing time 14: return 15: end for  Algorithm 3 Function: isStartPhase(job) 1: /* Decide if a job is in fair phase of Map or Reduce */ 2: if getStage(job) = MAP then 3: return scheduled Map tasks number < FAIR LIMIT 4: end if 5: if getStage(job) = REDUCE then 6: return scheduled Red tasks number < FAIR LIMIT 7: end if 8:  9: Function: getStage(job) 10: if job.getStatus().mapProgress() < 0.95 then 11: return MAP 12: else 13: return REDUCE 14: end if

IV. PERFORMANCE EVALUATION  We have implemented H-PFSP, and We test our algorithm on a cluster of 10 nodes. The OS is Red Hat Enterprise Linux AS release 4. Our implementation can be embedded into the Fair scheduler. We can use Fair scheduler to share the slot resources among different pools, while adopting our H-PFSP scheduler within each pool. The code is an independent plug-in module. Thus the delay scheduling and speculative execution policies are still in use. We have also developed a simulation tool to verify the H-PFSP algorithm in a large- scale environment. Workloads can be generated from the job generator and injected to the H-PFSP simulator.

A. Results for mean flow time  The main motivation of our work is to reduce the mean flow time of jobs in MapReduce like systems without undermining fairness. We test our algorithm and compare it with the Fair scheduling algorithm. In addition, we choose  Algorithm 4 Function: isCompletable(job) 1: /* Decide if a job is completable */ 2: for all jobi for which jobi.v < job.v do 3: if job.w > jobi.s then 4: return false 5: end if 6: end for 7: return true  ?  ??  ??  ??  ??  ? ? ? ?? ?    ?? ? ??? ?? ???  ?? ?? ??  ???????? ???? ?? ?! ? ?? ?! ?  Figure 2. Mean flow time of H-PFSP and Fair scheduling vs. job count.

different values of FAIR LIMIT in the start phase of H-PFSP and observe the effect of this parameter on the performance.

The results show the flow time of each job using H-PFSP is seldom longer than the corresponding flow time using the Fair scheduling policy, which means the fairness is guaranteed. Negative results appear in some cases since the system is volatile and the system status differs between two tests at different times.

We have submitted several WordCount jobs in Hadoop with the same data size simultaneously. Fig. 2 shows the mean flow time under H-PFSP and Fair scheduling. With a small number of jobs, no significant difference exists between them. To explain this, consider an extreme case, there will be no difference at all if only a single job is submitted to the system. With more jobs submitted, the slot resources are shared by more jobs such that per-job flow time increases accordingly. As shown in Fig. 2, the difference between them increases with the number of jobs submitted.

We have tested the performance of H-PFSP by setting two different FAIR LIMIT values. H-PFSP1 sets FAIR LIMIT to 2 and H-PFSP2 sets a higher value 5. The results show that setting a lower value improves the performance. Intuitively, the result shows that the performance H-PFSP2 is between H-PFSP1 and the Fair scheduling policy since H-PFSP2 can be seen as a policy striking an average between H-PFSP1 and the Fair scheduling. Although large FAIR LIMIT may degrade the performance, we suggest a moderately higher value of FAIR LIMIT in practice, especially when each split task is too small or the environment is volatile.

We also test the TeraSort benchmark in Hadoop. In each test, we submit TeraSort jobs with the same size, from 1G to 5G respectively. The data is generated by the TeraGen program using Hadoop. As shown in Fig. 3(a), the mean flow time of jobs under H-PFSP is less than the time under the Fair scheduling policy. With larger job sizes, the improvement is also larger.

Fig. 3(b) shows the mean flow time of five TeraSort jobs with different data sizes in a single test. We generate five data sets ranging from 1G to 5G using TeraGen tool. The result shows that small jobs have more chance to finish     ?  ?  ??  ?  ??  ? ? ? ?  ? ??  ?? ???  ?  ??  ?? ??  ???  ??????????? ???? ??????? ???????  (a) Jobs with same size.

?  ?"#  ?"  "?  #  ? ? ? ?    ?? ? ??? ?? ???  ?? ?? ??  ????!??????? ???? ?? ?! ? ?? ?! ?  (b) Jobs with different sizes.

Figure 3. Mean flow time of H-PFSP and Fair scheduling vs. job size.

earlier by our algorithm, while the flow time of the largest job using H-PFSP is almost the same as using the Fair scheduling policy. It agrees quite well with our design as we let small jobs have more opportunity to obtain the slot resources earlier so as to accelerate their running and reduce the mean flow time of all jobs. The large jobs queue behind, so that they will not block other jobs by occupying slots without releasing them for some time. No matter which scheduling policy is used, the largest job is always finished in the end, so that its completion time exhibits no significant difference.

B. Results for mean slowdown  The slowdown metric [8] is often used to measure how efficient the designed multi-job scheduler is. It is defined as the flow time of a job using a specific scheduling policy divided by the ideal flow time if it runs in the system without sharing resources with other jobs, i.e., it is the only job in the system. This metric can be seen as normalized flow time.

We have done several tests with heavier workloads using our implemented simulation tools. We simulate the work- loads with Poisson arrivals and exponentially distributed job sizes. The average interval is set to 5 mins in this simulation.

We generated 50 jobs using the workload generator. The slot count is 30. Fig. 4 summarizes the results by a slowdown profile plot. It compares the mean slowdown between H- PFSP and Fair. The number of tasks and task execution time of each job are pre-defined and fixed, i.e., each job  ??????? ??????? ??????	 ?????? ??????? ??????? ?????? ??????? ????  ?????????? ? ? ?? ?? 	? ? ?? ??  ?   ?????  ????  ?????  ??  ???$??? ???$??? ???$??? ???$??	 ???$?? ???$??? ???$??? ???$?? $???  ? ??  ?? ?? ??  ?? ??  Figure 4. Mean slowdown profile plot of H-PFSP and Fair scheduling. We compare the mean slowdown by adjusting the FAIR LIMIT parameter. The number of tasks and the task execution time of each job are pre-defined and fixed. The execution time of all tasks for each job is equal.

??????? ??????? ??????? ??????	 ?????? ??????? ???????  ????????? ? 	 ?  ? ?? ?  ?  ???  ????  ?????  ?  ???$??? ???$??? ???$??	 ???$?? ???$??? ???$??? ???$??  ? ?? ?? ?? ??  ?? ??  Figure 5. Mean slowdown profile plot of H-PFSP. The task execution time follows normal distribution N (10, 2.236).

has 200 tasks and each task takes 10 mins to execute. The number of tasks and the execution time of all tasks do not vary with time. We compare the mean slowdown by adjusting the FAIR LIMIT parameter. H-PFSP has smaller mean slowdown. With larger FAIR LIMIT, jobs run in the fair phase for more time which makes the scheduling algorithms inefficient.

Next, we add randomness to the execution time of each task. We assume that it follows normal distribution. Fig.

5 shows that as FAIR LIMIT becomes larger, the mean slowdown time first reduces. This is because the system can make more accurate estimation with longer fair phase.



V. RELATED WORK  MapReduce Scheduling. Google has developed a MapRe- duce programming model [4] to handle big data applications like search engines consisting of crawling and index process- ing [9]. Hadoop is an open-source Java implementation of MapReduce [5]. LATE [10], Mantri [11] and SAMR [12] scheduling algorithms take heterogeneous environment into consideration. Berkeley contributes a plug-in scheduler [13], [14]. A similar idea is used in designing Capacity scheduler [15]. Data locality [16] is considered to further accelerate the execution of MapReduce Job.

Scheduling Optimization. Simple scheduling policies like FIFO and PS are not priority-based policies. They provide     low performance such as mean response time. Priority-based policies have advantages that they optimize mean response time as SRPT does, and provide service differentiation [17]. However, they usually cannot guarantee the fairness property. FSP uses job size information to achieve high performance while guaranteeing fairness [7]. Some other scheduling policies such as PFSP and OFSP [6] have been designed to improve FSP. They both belong to the protective scheduling category. However, none of them is applicable to parallel execution of jobs. Besides, they perform well only if job size information is known a priori [18]. FSP can be modified to improve MapReduce performance in [19].

However, it only uses historical job profile for estimation, which is inaccurate when you have many diverse jobs, workloads and volatility of system status. Some scheduling algorithms [20] consider load balance to eliminate the long tail effect.



VI. CONCLUSION  We have designed H-PFSP, a hybrid protective sched- uler for MapReduce system. It is applicable to multi- user environments either within a company or on a cloud computing platform. H-PFSP improves per-job performance and reduces mean flow time with job execution information estimated during job progress, while guaranteeing that every job finishes no later than under PS as the protectiveness property guarantees. H-PFSP is implemented as a plug-in module of Hadoop and can be used within a scheduling pool. We have evaluated H-PFSP by the implementation using Hadoop and simulations. The results show that H- PFSP reduces the mean flow time of jobs proportionally to the job number and size without sacrificing the fairness.

