Tree Labeled LDA: A Hierarchical Model for Web Summaries

Abstract?We study the applications of hierarchical topic models to represent the content of website summaries. We concentrate on the DMOZ collection of Web extracts and propose a novel Tree Labeled LDA (tLLDA) algorithm to infer topic models using its manually compiled ontology.  The algorithm takes advantage of the ontology structure and infers topic models by jointly modeling word and ontology node assignments for documents.  We evaluate the performance of our topic modeling approach against that of four state-of-the-art algorithms (Labeled LDA, Hierarchically Labeled LDA, Hierarchically Supervised LDA and Supervised LDA) and show improvement in terms of perplexity and accuracy.  Our evaluation shows that topic models produced by tLLDA outperform other algorithms in terms of perplexity for all test sets and all but one test case in terms of accuracy.



I. INTRODUCTION The Open Directory project (also known as DMOZ) aims to  organize web content by compiling a comprehensive directory of public web sites available on the Web.  This Web directory compilation is accomplished through the hard work of thousands of human volunteers who inspect and manually map websites based on their content into a well-defined ontology [1].

Unfortunately, DMOZ ontology elements provide no information beyond their textual labels as to the meaning of associated documents.  Even though much work has been done by the DMOZ project towards organizing and classifying collections of Web documents, the project makes no attempt to extract path-level and statistical views of content.  Such additional information may facilitate tasks such as browsing, searching, and assessing document similarity [2].

Therefore, in this work, we attempted to leverage the manually created ordered ontology structures published by the Open Directory project to model the content of underlying document collections.  The aim was to produce a view of the content that would enable individuals to quickly grasp underlying themes of documents associated with ontology nodes.  Using topic modeling techniques, we developed an approach for constructing statistical views of ontology nodes by associating them with topics, which were probability distributions over a fixed vocabulary.  Our technique estimated distribution parameters as word-multinomials and used these multinomials to produce sorted lists of vocabulary terms for all nodes in the ontology.  Qualitative evaluation of  these lists suggested that top most probable terms, as specified by the corresponding word-multinomial parameters, were indicative of the underlying general theme of Web documents.

We propose a new probabilistic generative model based on the Labeled-LDA [3] approach, which is a supervised variant of the well-known LDA [4] model.  The new Tree Labeled LDA model takes advantage of the hierarchical nature of DMOZ ontology and jointly models word and ontology node assignments as a generative process.

Quantitative evaluation of our approach conducted by comparing predictive power of resulting topic models with that of topic models produced by other state-of-the-art algorithms in terms of perplexity and accuracy for held-out data showed that, for datasets used in the study, the new tLLDA model outperformed Labeled LDA and Hierarchically Labeled LDA topic modeling approaches in terms of perplexity in all cases and Supervised LDA and Hierarchically Supervised LDA in terms of classification accuracy in all but one case where Supervised LDA beats our model by a small margin.  The subsequent qualitative evaluation of the resulting topic models as compared to topic models produced by Labeled LDA and Hierarchically Labeled LDA suggested that topic models produced by the new tLLDA were more semantically indicative of the underlying content.

The rest of the paper is organized as follows.  Section 2 presents the review of notable related works.  In section 3, we introduce the generative Tree Labeled LDA (tLLDA) model in detail.  Section 4 discusses procedures for estimate distribution parameters from data.  In section 5, we discuss experimental setup and evaluate language models produced by the tLLDA approach as compared with other algorithms.  In sections 6 and 7 we discuss conclusions and outline future work.



II. RELATED WORKS The now classical work on Latent Dirichlet Allocation  (LDA) [4] has provided an extensible modular framework for many topic modeling approaches.  In LDA, the basic idea is that documents are represented as random mixtures over latent topics with each topic characterized by a distribution over words [4].  While Latent Dirichlet Allocation has served as basis for many approaches [2], it is fully unsupervised and may not be the best choice when the goal is prediction.  That is, for instance, in a system concerned with movie ratings intuitively good predictive topics could differentiate between       ?excellent?, ?terrible? and ?average? ignoring the genre.

Unsupervised machinery of the basic LDA, however, may estimate topics that correspond to genres if that is the intrinsic structure of the corpus [2].

The Supervised LDA model (sLDA) [2] extends LDA by taking supervision into account and adding a response variable associated with each document. This variable is usually associated with the supervisory labels for documents, such as the film rating adjectives in the above example.  The sLDA model jointly models documents and responses with the goal of finding latent topics that will best predict responses for test data [2].  The response values come from a normal linear model, which covariates in the sLDA model with empirical frequencies of topics in documents [2].

A further refinement on the Supervised LDA model ? the Hierarchically Supervised LDA (HSLDA) [5] ? extends sLDA to take advantage of hierarchical supervision.  The HSLDA model is based on the intuition that hierarchical context of labels provides valuable information about labeling.  As in sLDA, HSLDA jointly models documents and responses by drawing response variable realizations from a Normal distribution, but unlike sLDA it generates label responses using a hierarchy of conditionally dependent probit regressors [5].  In the joint modeling of each document, both empirical topic distribution and whether or not the parent label is applied to the document determine whether or not a label is to be applied.  The HSLDA model views word-multinomials (topics) as global constructs and links them to hierarchy nodes through per-label topic distributions.  This makes HSLDA output difficult to interpret, as the global topics do not directly correspond to nodes.

While Supervised LDA and Hierarchically Supervised LDA have been shown to work well in some applications, they have the limitation of allowing only a single label to be applied to a document and are thus not applicable to document collections where multiple labels can be assigned to texts [3].

Therefore, a radically different way of providing supervisory input to topic modeling was developed.  Named Labeled LDA (L-LDA), this model aimed at joining the multi-label supervision frequently found in modern text databases with word-assignment disambiguation of LDA family of models [3].  In L-LDA, each unique label is viewed as a topic and the goal of the model is to restrict the generative process to operate over a subset of topics, thus allowing for each document to be supervised.

Similarly to L-LDA, Hierarchically Labeled LDA (or hlLDA) [5] constricts the general LDA model to operate over a subset of label-bound topics.  While L-LDA is a general model for corpora where documents may be associated with multiple topics that may or may not be related to each other in any way, hlLDA considers each document to be associated with a set of topics which correspond to the set of nodes on the hierarchical path for each document.  The hlLDA model relies on some of the formalisms described in yet another LDA successor model called Hierarchical LDA (hLDA) [6], which is an unsupervised generative model which infers hidden hierarchical structures from data.  The hlLDA variant  extends the hLDA approach by assuming that the hierarchical structure -- which is hidden from hLDA -- is known and restricts topics accordingly.  Unfortunately, the hlLDA model is limited as it only considers supervision from observed hierarchy paths for each document and does not make use of the hierarchy structure as a whole.

It is important to note that hierarchical topic modeling techniques such as hlLDA and HSLDA place statistical pressure on the posterior to have more general terms in topics towards the root of the hierarchy as there are more paths through nodes at higher levels than there are through lower level nodes [7].  While this is desirable in some cases, the statistical pressures may become overwhelming for sparsely populated hierarchies.  The proposed tLLDA model attempts to combat the tendency to favor higher level nodes by offering a statistical counterbalance derived from the structure of the hierarchy.



III. MODEL In this section we introduce the Tree Labeled LDA  (tLLDA), which is a generative probabilistic model that describes the process of generating a document collection where each document is associated with a distribution over hierarchy nodes.  It improves upon hlLDA by jointly modeling word and node label assignments, which allows it to take the hierarchy structure as a whole into consideration.  Further, the proposed tLLDA model estimates a single word-multinomial for each node of the target ontology, which allows for an easier interpretability when compared with the HSLDA model.

The tLLDA model is intuitively motivated by the nature of manually constructed taxonomies, where conceptual meanings of hierarchy labels often represent more general concepts at levels closer to the root node and increase in specificity towards the leaf nodes [7].  tLLDA attempts to take advantage of hints as to the specific meaning of each document by biasing the generative process towards hierarchy nodes representing narrower conceptual meanings while still allowing words to be generated from all hierarchy levels.

Further, we are motivated by the intuition that hierarchical nodes with edges to large number of children are representative of more general concepts whereas fewer child elements may be indicative of narrower meanings.  The tLLDA model takes the later intuition into account by relaxing the bias towards lower hierarchy levels in those cases where paths are comprised of relatively barren nodes.

The tLLDA model aims to incorporate both the multi-label supervision and supervision derived from the structure of the target hierarchy.  Similar to other topic modeling techniques [5, 8], it adopts the mixed  membership formalism [4] where a document is thought of as a mixture over a set of word- multinomials [8].  The tLLDA approach combines the multi- label supervision of hlLDA with hierarchical supervision by jointly modeling word and label assignment generation.  Also, unlike HSLDA which generates label responses using conditional hierarchy of probit regressors [5] assuming a Normal distribution, tLLDA draws a path  through the hierarchy for each topic directly from a distribution     parameterized by a global vector of multinomial parameters without assuming any underlying distribution shape.

A. Notation Let each document d be represented by a tuple consisting of  list of word indices  where each ,  is the vocabulary and  is the document  length. Let hierarchy  be a tree which is a directed acyclic graph with known structure where  is the set of vertices (or nodes) of size ,  is the set of edges,  is the root node and each node  may have at most one parent.  Let  be the total number of topics equal to the size of the set  .  Note that since the number of topics equals the number of vertices in the hierarchy, topics and vertices (nodes) may be used interchangeably in this context. Let a path to a node  be the list  such that  and each subsequent node in the list  is a child of its predecessor.  For each node  let  be a set such that  .  Let  represent the sub-tree of node  such that and .

B. Theory The tLLDA approach models document creation by  imagining a process that randomly generates two strings of equal length ? a string of words and a string of hierarchy node labels.  The string of words is generated in a way common to other topic modeling approaches [3, 5, 6, 8].  That is, as in other related approaches, tLLDA draws a topic from a distribution parameterized by a topics proportions vector and then generates a word by drawing it from a distribution parameterized by a word-multinomial vector.  At this stage, tLLDA diverges from other topic modeling algorithms [3, 5, 6, 8] and adds a new step to generate a label by randomly selecting a node from the distribution over labels conditioned on the topic drawn earlier.  The probability of drawing node label  from some node (topic)  is    Because probabilities of drawing a word from a topic and drawing a label from a distribution conditioned on the topic are independent, the chance of drawing a word string  and a label string  of length  is    where  is the mixture proportion vector and [8].  The generative process of the algorithm is outlined  in Figure 1:          1. For each topic : 2.     Generate 3. For each topic : 4.     For each topic : 5.         Deterministically set  using Eq. 1 6.     Generate 7.     Generate 8. For each document : 9.     For each topic : 10.        Generate 11.   Generate 12.   Generate 13.   For each word : 14.        Draw 15.        Draw 16.        Draw  Fig. 1. tLLDA generative algorithm  Steps 1 and 2 where the multinomial topic distributions over vocabulary for each topic  conditioned on the Dirichlet prior  are drawn remain identical to the standard LDA, L-LDA and hlLDA.

To use the hierarchy structure as a supervising agent for the generative process we deterministically construct vector  such that  for each topic  in steps 4-5:  (1)  Because the hierarchy structure is known apriori and is fixed for all documents, this deterministic step does not jeopardize the generative nature of the approach.  Having thus generated vector  we define a node-specific matrix  over for each node where ; for each row  and column :    The matrix  is used to project parameter vector of the Dirichlet prior   to lower dimensional vector  in step 6:   In step 7, path assignment proportion vector  is drawn for each topic with parameter vector . Since  is constructed by the use of , the makeup of vector  depends on where the node corresponding to topic  is in the target hierarchy in relation to other nodes. Therefore, the parameter vector  encodes the supervisory input of the hierarchy structure as it is a projection of K-dimensional vector  onto the lower dimensional space defined by ?s sub-tree.

Then, for  number of topics let   be the list of binary topic presence/absence indicators for document  such that  .   As in L-LDA, vector  is generated in steps 9-10 by using a binomial distribution for each topic  with prior  [3].  With that, a document-specific     label projection matrix  over  is defined for each document where ; for each row  and column :  [3]  The matrix  is used to project the parameter vector of the Dirichlet prior  to lower dimensional vector  : [3]  Step 12 draws  by parameterizing a Dirichlet distribution with  computed in step 11.  Then, to generate a word, topic  is sampled from a distribution parameterized by  and a word is sampled from distribution over words parameterized by the word proportion vector .

Unlike other algorithms that repeat the word generation process at this point, tLLDA draws a label assignment from a distribution parameterized by vector  where  is the topic drawn in step 7.  As the vector is projected onto the lower dimensional space defined by ?s subtree (see definition of ), it is in this step that the structure of the hierarchy is included into the supervision -- makeup of  and consequently the values of vector  depend on where the th node is in the target hierarchy in relation to other nodes.

Fig. 2. hLLDA graphical model showing the plate diagram with solid lines  representing probabilistic links and dashed lines representing deterministic relationships.  The shaded circles represent observed nodes whereas un-shaded nodes are latent.



IV. PARAMETER ESTIMATION The model of document generation described in the  previous section outlines a process that is not directly observed.  Instead, an observer is presented with the output of the process, which may be used to infer latent parameters that governed the algorithm responsible for producing the output.

In much of the topic modeling literature, the output is limited to word strings that constitute documents in a given corpus.

The tLLDA model, however, posits that there exists a second output detectible to an outside observer ? namely, the document-length string of label assignments.

And indeed, such additional output can be detected by considering the placement of each document in the target hierarchy.  Since document is associated with a hierarchy node, it is easy to construct a document-length string of label assignments by repeating the node?s label as many times as there are words in the document.  That is, in the tLLDA context, we say that in addition to observing a randomly generated string of words, we observe the eventuality of the randomly generated string of labels being entirely made up of identical elements.

We used Gibbs sampling to estimate topic-word distribution parameter  since compared to other parameter estimation methods, Gibbs sampling yields a relatively simple and computationally efficient algorithm [9].  Sampling equation for a topic for document d and path leading to (notation outlined in Table 1) :      TABLE I.  NOTATION  Dirichlet hyperparameters The number of times word  assigned to topic  not  counting the current instance  Number of times any word is assigned to topic excluding the current instance  The number of times topic  is assigned to document excluding the current instance  Number of times any topic is assigned to document        Observed node assignment for document ;   After a predetermined number of iterations of the sampling process based on distributions estimated using the above equation, parameters can be estimated for any single sample as using the following equations:  , ,

V. EXPERIMENTAL RESULTS  A. Experimental Data We tested the tLLDA on the Web summary dataset  provided by the Open Directory project.  The Open Directory project publishes four distinct data dump pairs (structure file and content file).  These are ?World?, ?Kids and Teens?,     ?Adult? and ?AOL?. The well-structured DMOZ ontology is published in a separate RDF documents devoid of review content.  Review content RDF files are made up of sections corresponding to ontology identifiers (topics).  Each section contains a set of URL references to websites, their titles and description strings that are relatively short with the average of 4.2 and 18.1 words per string respectively.

The ?World? RDF download contains the ontology structure and corresponding reviews for all web sites reviewed by the project excluding those found in ?Kids and Teens?, ?Adult? and ?AOL? repositories. The ?Kids and Teens? download contains reviews of web sites related to children and teenagers, such as reviews of cartoons and primary school activities.  In our experiments, we excluded the ?AOL? repository as the latest file published at [10] contained no review content.  We also excluded the ?Adult? section from our experiments for ethical reasons and tested our approach on the ?World? and the ?Kids and Teens? datasets.  As both the ?World? and ?Kids and Teens? repositories were large (~2.1 million records and ~26,000 respectively) and due to hardware and time constraints we tested our approach using a set of smaller subsets of records extracted from each repository.

For our experiments, we used the English language portions of the ?World? and the ?Kids and Teens? DMOZ datasets.  As preprocessing steps, each raw record, title and description strings were extracted and concatenated into a single document.   Resulting strings where case-folded and tokenized using simple tokenization rules followed by non- letter characters (numbers and punctuation) removal.

Stemming and lemmatization was not applied.  Following the process in related works [8], vocabulary was extracted by a single pass through the data and documents were regenerated by replacing English terms with numerical identifiers of terms in the vocabulary.

Because of the sparse nature of data sets which are made up of relatively small number (average of 5.4 documents per hierarchy node) of short review documents and in order to preserve the underlying relationships within the hierarchy, we extracted six fixed size data windows of various sizes from each of the two data sets. Further, we reserved 10% of the data in each data window for testing by applying simple random sampling without replacement.  The maximum number of records used for testing the approach was limited to 6000 elements.  This number was chosen as it was empirically determined to be the largest number of records that the available hardware was able to process in one twenty-four hour period for each algorithm.

B. Comparison Models We compared the predictive power of the language models  produced by our approach to four related models against the Open Directory (DMOZ) dataset.  The four comparison models included the Hierarchically Labeled LDA [8] and the Labeled LDA [3], Hierarchically Supervised LDA [5] and Supervised LDA [2].  We considered two distinct comparison approaches as the evaluation criteria.  We used perplexity to compare tLLDA performance with that of hlLDA and L-LDA  as perplexity is a common way to compare language models [3].  To compare tLLDA predictive power with that of sLDA and HSLDA we used multilabel classification Exact Match (MR) as the evaluation criteria.  We chose to use multilabel classification as opposed to perplexity for sLDA and HSLDA because sLDA-type approaches associate supervisory topics with distributions over language models rather than with single language model, making evaluation in terms of perplexity not feasible.

We compared language models produced by our approach to those learned by hlLDA and L-LDA as the output of these models lends itself to per-topic language model comparison.

The perplexity measure used for comparison over held-out subset of data  given language model  and the training data [8] is computed via the following formula:    where , ,  is the jth term in the ith string in the held-out collection and  is the probability of term  as per the learned language model .

The tLLDA approach outperformed L-LDA and hlLDA algorithms in terms of perplexity.  Tables 2 and 3 summarize experimental results for ?World? and ?Kids and Teens? data subsets respectively in terms of perplexity values.  We set  and  for all experiments as these values are commonly chosen in the related literature [9].  We set the number of iterations to 1000 as the algorithm appeared to converge past that number of iterations.

TABLE II.  ?WORLD? RESULTS  1000 2000 3000 4000 5000 6000 L-LDA 2642.6 3952.4 4661.7 6016.8 6740.4 8142.8 hlLDA 3159.3 4598 5714.7 7046.4 8264.21 9787.72 tLLDA 690.25 795.99 779.99 877.47 1077.13 1356.14  ?World? dataset perplexity values for L-LDA, hlLDA and tLLDA (rows) and 1000-6000 record data windows (columns)  TABLE III.  ?KIDS AND TEENS? RESULTS  1000 2000 3000 4000 5000 6000 L-LDA 4170.66 4958.17 6232.11 7450.27 8226.14 8976.4 hlLDA 4081.14 6247.47 7767.5 8809.1 9421.62 10035.43 tLLDA 838.26 965.94 1165.07 1297.19 1400.46 1473.13  ?Kids and Teens? dataset perplexity values for L-LDA, hlLDA and tLLDA (rows) and 1000-6000 record data windows (columns)  Since sLDA-type approaches associate supervisory labels with distributions over language models rather than with single language model per label as in the case of hlLDA and L-LDA, it is difficult to meaningfully compare language models of tLLDA with those of sLDA and HSLDA using the perplexity quantifier.  Therefore, in addition to perplexity evaluation we compared performance of tLLDA to that of     other related models by conducting multilabel classification, which is a task of predicting the set of labels appropriate for each document given a training set of documents with multiple labels [3].

In this study, the classification algorithm remained the same for tLLDA, hlLDA, L-LDA, sLDA1 and HSLDA2 with the only variation being the posterior probability approximation procedure, which was specific to each algorithm.  Since the goal of the classification task is to predict the set of labels (a path) for each test document, document-topic approximation routine used during testing must differ from that which was used during model training for topic modeling approaches that take supervision into account.  Therefore, we employed the inference procedure described in [3] to estimate document-topic distribution parameters.   There, global parameters that were estimated during training were used to initialize the state of the unsupervised LDA sampling procedure, which was then used to estimate document-level distributions.

Recalling that our ultimate goal was to help accurately classify web documents and associate them with correct hierarchical paths, we measured the results in terms of example-based classification accuracy which is defined as  where  is the indicator function,  is the number of test documents and  and  are sets of predicted and actual labels for the ith test document respectively [11].

Figures 3 and 4 show accuracy results for tLLDA, hlLDA, L- LDA, sLDA and HSLDA for each of the test data windows in the ?World? and ?Kids and Teens? datasets respectively.

Fig. 3. Accuracy results for each test data window in the ?World? dataset   1  Implementation available at  https://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/chongw/slda/  2  Implementation in Python graciously provided by authors of  HSLDA in personal communication     Fig. 4. Accuracy results for each test data window in the ?Kids and Teens? dataset  C. Topic Visualization Recalling that the goal of this study was to produce a view of the content which would enable individuals to quickly grasp the underlying theme of documents associated with each ontology node and realizing that no clear means existed for quantifying the quality of topic multinomials [9], we evaluated the topics discovered by our model by examining the top words assigned to each topic.  We observed that the top word assignments produced by the tLLDA model (Table 4) appeared semantically more meaningful as compared to those produced by hlLDA and L-LDA, as the language models produced by the later algorithms seemed to favor proper nouns for language models associated with lower-level hierarchy nodes and marginalize other terms that added semantic consistency to evaluation.

TABLE IV.  TOPIC VISUALIZATION  Sample topic visualizations for top performing evaluated algorithms (rows) and several hierarchy levels (columns).  Highlighted terms indicate words that appeared semantically indicative of the content theme during qualitative evaluation by the authors.



VI. DISCUSSION Similar to L-LDA, one of the advantages of tLLDA is the  document-specific topic mixture .  In the context of Web documents, the topic mixture can be inferred for each Web page and since each topic is associated with a node in the target ontology, insight as to the subject matter of each document can be gained by evaluating the topic proportions.

Such insight may be instrumental in helping organizations such as the Open Directory project in their important tasks of digesting the Web content and presenting it to the public in an accessible way.

0.1  0.2  0.3  0.4  1000 2000 3000 4000 5000 6000  A cc  ur ac  y  Number of records  sLDA  HSLDA  LLDA  hLLDA  tLLDA  0.1 0.2 0.3 0.4 0.5 0.6  1000 2000 3000 4000 5000 6000  A cc  ur ac  y  Number of records  sLDA  HSLDA  LLDA  hLLDA  tLLDA  Composers Classical Beethoven,_Ludwig_v an  tLLDA and with includes for a  his samples biography ?  of composer the on work classical grieg edvard in a information ...

biography beethoven van ludwig and brief works of a includes list  L-LDA biography brief composer the of with for key  ?  mozart amadeus life wolfgang work strings mozarts ?  van beethoven  ludwig includes compositions     As node-specific word-multinomials are learned by the tLLDA approach from the underlying collections of documents, it may be possible to consider words in terms of their probability vis-?-vis the corresponding node-level language model and revisit the structure of the target hierarchy.  As target ontologies, such as the one used by DMOZ, are manually compiled, they may contain omissions or overgeneralizations.  Such omissions and overgeneralizations may become prominent when viewed through the prism of their learned language models.

To exemplify, consider the sample cited in Table 4.  There, the set of terms ?biography beethoven van ludwig and brief works of a includes list? are top most probable for the hierarchy node with the path ?Beethoven,_Ludwig_van? as per the node-specific language model learned by the tLLDA approach.  Examination of this list of most probable terms may suggest that the underlying collection of documents related to the famous composer may be further partitioned into two more specific subcategories ? one containing documents related to the biography of the composer and the other related to his works.



VII. CONCLUSIONS AND FUTURE WORK In this paper, we proposed a new topic modeling approach  of Web summaries using a popular Web ontology.  This approach took advantage of the hierarchical structure of the ontology to improve predictive power of resulting topic models.  While we focused on the Web summary data provided by the Open Directory project, the topic modeling approach introduced here can be easily adopted to any hierarchically organized content.  This type of model can be useful in identifying key notions in large collections of text.

In this work, we took advantage of tree-like hierarchical structures exhibited by the Open Directory ontology.  In addition to the hierarchical relationships, however, the ontology also provides lateral links between related nodes that often breach hierarchical boundaries of parent-child relationships.  Understanding how to take advantage of these  links to further improve predictive power of topic models is one area of future research.

Since much of our evaluation was hindered by the computational complexity of parameter estimation algorithms, in future works we will attempt to leverage hierarchical structures to reduce time required to estimate parameters for the proposed model for larger datasets by distributing the parameter estimation process.  We will attempt to build on earlier works on distributed topic modeling algorithms (e.g.: [12]) to improve performance of the tLLDA parameter estimation procedure.

