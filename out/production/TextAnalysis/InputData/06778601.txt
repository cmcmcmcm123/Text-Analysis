Two Layer Cache for RDF Database

Abstract?With the Semantic Web[1]has been paid more and more attention, there are more and more related applications.

For instance, Google has provided semantic retrieval functions, you can get the information of some famous people directly.

RDF[2]database as the data storage tool of Semantic Web also gets many attentions. Now, there are some well-known RDF databases such as Jena[3], Sesame[4], Bigdata[5] and so on.

The role of RDF database is to provide data service for Semantic Web applications. The important metric of database is the query efficiency. Now, most of RDF database?s efficiency has not reached high level and there is a large gap between it and commercial demand[6][7].

In this paper, we present a two layer cache mechanism: RC (RDF database Cache) to speed up the queries and relieve the problem that the runtime of some queries is too long in RDF database. By implementing and test, the query efficiency can increase thirty-one percent or more after using this cache mechanism.

Keywords?RDF(ResourceDescriptionFramework), Database, Cache, RC(RDF database Cache)

I. INTRODUCTION (Heading 1) In 2001, Berners Lee put forward the concept of Semantic  Web. It is the change and extension of the World Wide Web and its goal is to make the computers understand the semantic of the information and be able to provide the dynamic and active service. In the Semantic Web, the info between people and computer or computer and computer can get more fully utilization.

RDF database is an important part of the Semantic Web.

Whatever Semantic Web application you do, you can?t complete it without it. With the Semantic Web gets a hot topic, more and more applications have appeared in our life and the applications have more and more high demand for query efficiency of RDF database in the same time.

To use cache is the most direct way to improve the query efficiency. There is not cache special for RDF database and the cache meets the user's query features. So, RC was designed and realized.

The RC is a two layer cache for RDF database. The first layer uses ARC-Memory algorithm to improve the query efficiency and the second layer uses Key-Value DB to relieve the problem: the runtime is too long when query is too complicated or query result is too large.

The rest of the paper is structured as follows. Section 2 introduces related base info about RC and original intention of designing it. Section 3 describes the framework of RC. Section  4 describes the algorithm in RC. In Section 5 the performance results will be presented.



II. BACKGROUND  A. Cache Replacement Algorithm There are many cache replacement algorithms, and it can be  divided into two kinds. One is the traditional replacement algorithm, the other is self-adaptive algorithm. The traditional algorithm has FIFO, LRU, MRU and so on. They decide to replace by its call frequency and its call time. The method of judgment is very simple and most of them are especially for a fixed access model.

Because of the traditional algorithm is not very efficiency and comprehensive, so the researchers put attention to the self- adaptive algorithm. The most of self-adaptive algorithms use strategy based on the time and frequency balance. There are many famous algorithm, for instance ARC[8], LRFU and so on.

B. RDF Database Now, There are many RDF databases, we can divide them  into two kinds by storage media. One is relational database, the other is non-relational database. The non-relational database is more and more popular with the Semantic Web development and the researchers found low-performance of to store RDF data in relational database.

The loading rate and query efficiency are the metric of the RDF database. From the application point of view, it pays more attention to query efficiency. If the RDF database can?t support fast concurrent query, it will be eliminated by Semantic Web after some time.

There are many open-source RDF database, for example Bigdata, Sesame and so on. AllegroGraph[9]is a famous commercial RDF database, it supports storing a large number triples and provides high-performance data service.

There is not a cache mechanism that meets user?s query features and improves query efficiency significantly.

C. Important Problem RDF database is a very important part of Semantic Web.

Along with the Semantic Web is more and more popular, there is more and more commercial application about Semantic Web. It proposes more stringent requirements about RDF database at the same time.

There are many tests for RDF database. LUBM[10]is a famous test in them. It has fourteen queries. You can get the test set from its data generator, then to use the test queries to test the performance of the RDF database. If you have done it,  2013 10th Web Information System and Application Conference  DOI 10.1109/WISA.2013.10   2013 10th Web Information System and Application Conference  DOI 10.1109/WISA.2013.10   2013 10th Web Information System and Application Conference  DOI 10.1109/WISA.2013.10   2013 10th Web Information System and Application Conference  DOI 10.1109/WISA.2013.10   2013 10th Web Information System and Application Conference  DOI 10.1109/WISA.2013.10     you maybe find something: some queries spend much time to get result. For instance, the second query and the sixth query cost much time to run. The reason of it is the query is too complicated or the query result is too large.

AllegroGraph is high-performance commercial RDF database. Its LUBM8000 test result is shown in Table .

TABLE I. ALLEGROGRAPH LUBM8000 TEST RESULT  INFO\LUBM Query Query 2 Query 6 Result Triples 2,528 83,557,706  Time (s) 278.321 389.062  Query Content  PREFIX rdf: <http://www.w3.org/19 99/02/22-rdf-syntax- ns#> PREFIX ub: <http://www.lehigh.ed u/~zhp2/2004/0401/uni v-bench.owl#> SELECT ?X,?Y, ?Z WHERE {  ?Xrdf:typeub:Gradu ateStudent .

?Yrdf:typeub:Universit y .

?Zrdf:typeub:Departme nt .

?Xub:memberOf ?Z .

?Zub:subOrganization Of ?Y .

?X ub:undergraduateDegre eFrom ?Y }  PREFIX rdf: <http://www.w3.org/19 99/02/22-rdf-syntax- ns#> PREFIX ub: <http://www.lehigh.ed u/~zhp2/2004/0401/uni v-bench.owl#> SELECT ?X WHERE  {?X rdf:typeub:Student}  From the Table , the runtime of Query2 and Query6 is very long, and the result of Query6 is very large. From the query?s content, we can know Query2 is a complicated query and Query6 is a simple query.

Now, we find an important problem in RDF database that the runtime is very long when the query is very complex or the result set is very large. For example, the second query is very complicated and the result set of the sixth query is very big in LUBM Test.



III. RC FRAMEWORK To improve the query efficiency and alleviate the problems  caused by the query result set is too large or query is too complex of RDF database, the RC is designed and realized.

The Cache has two layers. The first layer implemented the  ARC-memory algorithm and realize in memory. The second layer realized in Key-Value database. For more information, see Figure 1.

Fig. 1. RC Framework  The ARC-Memory algorithm of the first layer meets query features of user and can get high level of hit ratio. It can help database getting better query effect. For more information about this algorithm, see Section 4.

To resolve the problem caused by query too complicated or result set too big, the second layer was designed. The data that put in the second layer must meet two conditions:  ? The data must be removed by the first layer.

? The runtime of the query must exceed the limit of time.

The second layer has two parts: Key-pool, KV. Key-pool keeps a fixed-size LRU queue to store the MD5 key of query.

KV is realized in Key/Value database to store the MD5 key and the query result. For more information, see Section 4.

The detail process flow is in Figure 2.

Fig. 2. RC Process Flow  RC RDF database Cache  First Layer:Memory (This layer is using ARC-Algorithm, It meets user's query features and guarantees  high hit rate)  Second Layer:Memory + Key/Value Database (This layer is using LRU-KV algorithm. It is primarily responsible for storing the  timeout query result to improve query efficiency.

When query result is removed by first layer  and its runtime exceeds limit time, it  will be put into second layer.

When hitting in second layer, the  query result will be put into first layer.

Using query string to Generate MD5 Key  In First Floor Cache  YY  In Second Floor Cache  YY  Generate Insert First Floor Cache ThreadYY  First Floor Cache Full  Generate Insert Second Floor Cache Thread  Second Floor Cache Full  Insert data in First Floor Cache  YY  Eliminate data  Eliminate Data  Insert data in Second Floor Cache NN  Insert data in First Floor Cache  Bigdata  Get Result from Bigdata  NN Key/Value database  Get Rusult From Second Floor Cache  Result  NN  Begin  End  YY  NN     Figure 2 shows the process flow of RC. Because of the query string is not a fix-number long, it can be transferred to MD5 code as the key.



IV. ARC-MEMORY ALGORITHM Adaptive Replacement Cache (ARC)is a page replacement  algorithm with high performance developed at the IBM ALmaden Research Center. Its hit ratio is higher than LRU algorithm. This is accomplished by keeping track of both frequently and recently used pages plus a recent eviction history for both.

Because of the design of first layer is realized in memory, so we change the ARC algorithm into ARC-memory algorithm that make the B1 and B2 all in the memory.

The ARC-memory algorithm: INPUT: The MD5 Key x of the Query String, Key-1,Key-2?Key-n INITIALIZATION: Set p=0 and set the LRU lists T1,T2,B1 and B2 to empty.

Case I: Key-n is in T1 or T2 .

Move the element of Key-n to MRU position in T2 .

Case II: Key-n is in B1 .

If |B1| >=|B2| u=1 Else u=|B2|/|B1|. End If Update p = MIN { p + u , c } Call ADJUST (Key-n , p).

Move the element of Key-n from B1 to the MRU position in T2.

Case III: Key-n is in B2 .

If |B2|>=|B1| u=1. Else u=|B1|/|B2|. End If Update p=MAX { p - u , 0 } ADJUST (Key-n , p).

Move the element of Key-n from B1 to the MRU position in T2.

Case IV: Key-n is not in T1 T2 B1 B2.

Case A: |T1|+|B1| >= c .

If (|T1| <c ) Delete LRU page in B1 .

ADJUST( Key-n , p ).

Else Delete LRU page in T1.

End If Case B: |T1+|B1| <c .

s = |T1|+|T2|+|B1|+|B2|.

If(s>=c)  If(s==2c) Delete LRU page in B2.

End If.

ADJUST( Key-n , p ).

End If Move the element of Key-n to MRU position in T1.

Subroutine ADJUST(Key-n , p ) If ( (T1 is not empty) and (( |T1| > p) or (Key-n is in B2 and  |T1|=p)) Move the LRU page in T1 to MRU position in B1.

Else Move the LRU page in T2 to MRU position in B2.

End If  ARC-Memory algorithm has get high level hit ratio, we will test the hit ratio of it and other algorithms in Section 5.



V. PERFORMANCE TEST To show RC performance result, two tests will be done. The  first test will prove the high hit ratio of ARC-memory algorithm in first layer of RC. Then, the second test will put the RC into Bigdata.

A. Cache Hit Ratio Test To compare hit ratio in the test, the LRU algorithm and FIFO  algorithm also be realized in memory.

The OLTP[11]trace will be as the test set. Due to all  algorithms realized in memory, the second field (logical block address) of OLTP trace will be used. The logical block address will be as the query. For more information about this test, see Table  and Figure 4.

TABLE II. THE RESULT OF CACHE HIT RATIO  Cache size\Algorithm  FIFO LRU ARCC  200 0.26265 0.27584 0.29442  400 0.31671 0.33767 0.35618  600 0.35272 0.37641 0.39546  800 0.38054 0.40618 0.4253  1000 0.40443 0.43134 0.44938  1200 0.42459 0.45271 0.46967  1400 0.44204 0.47148 0.48694  1600 0.45714 0.48746 0.5025  1800 0.47056 0.50158 0.5174  2000 0.48298 0.51458 0.5304  2200 0.49416 0.52713 0.54182  2400 0.50445 0.5386 0.55271  2600 0.51453 0.54895 0.56232  2800 0.52428 0.55892 0.57136  3000 0.53362 0.56888 0.58015  3200 0.54152 0.57815 0.58831  3400 0.54901 0.58666 0.59684  3600 0.55681 0.59443 0.60389  3800 0.56363 0.6017 0.61099  4000 0.57139 0.60876 0.61743  In Table , ARCC is the ARC-memory algorithm. The hit ratio of every replacement algorithm grows with the increase of the cache size. The improve ARC algorithm is the best of three replacement algorithm. For detail trends, see Figure 3.

Fig. 3. Hit Ratio Test Result  The test can prove the improve ARC algorithm has ability to maintain high hit ratio and it is better than LRU algorithm and FIFO algorithm.

B. RC Performance Test on Bigdata Cluster To further illustrate the effect of RC, The second test will put  the RC into Bigdata Cluster.

The test set also use the second field (logical block address)  in OLTP trace. The logical block address of OLTP trace will be as SPARQL[12] query. The trend of the test set is equal to the trend of logical block address in OLTP.

Every SPARQL query would be generated by DBpedia Random SRARQL Query Generator. It will be used to generate the query randomly for DBpedia English repository.

There are 241,605,993 triples of DBpedia in BigdataCluster[13][14].

The test will execute 10,000 queries for different cache size.

In last, it will get the runtime (millisecond) for different cache size and query set size. The detail test result is shown in Table  .

TABLE III. THE RESULT OFRC TEST ON BIGDATA  Cache size(1 04)\Qu ery set  1 2 3 4 5 6 7 8 9 10  0 52           100 35           300 32           500 30           700 28           900 28           The situation when cache size is 0 is no-cache cluster. To be seen with the caching mechanism, the query efficiency improved significantly. And with the increase of the cache size, the runtime is less and less. When no-cache in cluster, a query would consume 47.1ms. After to add cache into cluster and set cache size is 100 queries, a query would consume 32.4ms under the same conditions. When setting cache size with 900, the runtime just needs 27.7ms. For detail trend, see Figure 4.

Fig. 4. RC Test Result on Bigdata  It can be seen by the results of the above two tests that the RC can ensure high hit ratio and improve query efficiency significantly.

