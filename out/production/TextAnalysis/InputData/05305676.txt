Hierarchical Clustering Ensemble Algorithm based  Association Rules

Abstract?Present, there is more research on supervised clustering ensemble algorithm, but the research on unsupervised clustering ensemble is studied less. In order to partition data points under fully unsupervised conditions, the hierarchical clustering ensemble algorithm based on association rules (HCEAR) is proposed in this paper. The optimal number of clusters is determined by average degree of clustering using distribution of all clustering memberships and support degree of association rules. Then variation of the hierarchical clustering algorithm was adopted for best partition. Related theories ware proved detail in this paper. Finally, the HCEAR is applied in instance and results show it is effective.

Keywords- clustering; hierarchical clustering; association rules; clustering ensemble

I.  INTRODUCTION Clustering is to group data points into several clusters that  the intra-cluster similarity is maximized while the inter-cluster similarity is minimized [1-3]. With the development of data mining, some ensemble methods integrating some clustering methods were advanced recently, Topchy proofed that the ensemble method can be better than any single clustering algorithm in [4-5].

Clustering ensemble is to integrate some results from some existing clustering partitions for higher quality and better robust. In recent year, ensemble learning technology has been widely used in many areas, such as biology feature recognition, Computer-aided medical diagnosis, text recognition, web information filtering, analysis of seismic waves, and so on. Early research on ensemble learning technology was focus on supervised learning, and the research on unsupervised learning, equals to clustering ensemble, is in the initial stage [6-7]. The clustering ensemble is to search a mutual partition from results of different clustering algorithms and has been proved to be a full NP problem.

In the process of clustering ensemble, above all, we should generate m clustering memberships of dataset X, and then integrate the results of these m clustering memberships according to the mutual function. The research on the design of mutual function is a hot topic in clustering ensemble.

Almost all of existing clustering ensemble methods needs to pre-design thresholds that are difficult to choose, such as co- association matrix advanced by A. L. Fred [8], the method  based on three hyper graphs advanced by A. Strehl and J.

Ghosh [2], the EA (Evidence Accumulation) method advanced by A. Fred [9], WSnnG (Weighed Shared nearest neighbors Graph) advanced by H. Ayad [10], and so on. Although the CEFS (clustering ensembles guided feature selection) advanced by Yi Hong doesn?t need to pre-design the threshold, large complexity and complicated Calculation make the algorithm difficult to use widely.

The HCEAR (Hierarchical clustering ensemble based association rules) algorithm is proposed in this paper. HCEAR makes use of the distribution of clustering memberships, combines m subset of Descartes with support degree of association rules, and then achieves the partition by variation of hierarchical clustering.

This paper is arranged as follows: In section 2, theory of HCEAR algorithm is presented and proved, and then give the steps and analysis the complexity. The instance analysis is given in section 3. The conclusions are deduced in section 4.



II. HCEAR ALGORITHM Assume there are n data points X={x1,x2,?,xn} and m  clustering memberships with ki clusters that are derived from different methods or the same methods with different parameters. These m clustering memberships are R={R1,R2,?,Rm}, which can be denoted as formula (1).

},,{ ,2,1, mkiiii CCCR = , mi ??1  (1)  Here, Ri is the ith clustering membership that denotes results of the ith clustering, ki is number of clusters in the ith clustering membership, Ci,j is the jth cluster of the ith clustering membership.

A. Definitions and Theorems Definition1: m subset of Descartes ),,,( 21 m  m jjjC  is an intersection among all m clusters, which come from m clustering memberships and any two do not come from the same, denotes as formula (2).

??? m  jmjjm m  m CCCjjjC ,,2121 21,),,,( =  (2)  This work was supported by a grant from the Ph.D. Programs Foundation of Ministry of Education of China (No.200801510001)     Here, mikj ii ,,2,1,1, =?? , ijiC ,  is the jith cluster of ith clustering membership.

Definition2: m Descartes mC is the set among all m subset of Descartes, denotes as formula (3).

{ }),,,( 21 mmm jjjCC =  (3) According to the definition2, the number of m subset of  Descartes in m Descartes is up to km.

Definition3: full m Descartes ),,,()( 21 m mm jjjC?  is  the union among all m subset of Descartes, denotes as formula (4).

),,,(),,,()( 21       m m  m  k  j  k  j  k  j m  mm jjjCjjjC m  m  ? ??? ===  =  (4)  Theorem1: the intersection between any two different m subsets of Descartes is empty set.

Proof: Because the intersection between empty set and any one set is empty set, it is only to prove that the intersection between any two sets is empty set.

Now we assume that there be two non-empty m subsets of Descartes ),,( 21 m  m jjjC and )',,','( 21 mm jjjC , and the intersection between them are non-empty set, that is  ??)',,','(),,( 2121 m m  m m jjjCjjjC ? .

Because they are different sets, different clusters were chosen in at least one clustering membership. Assume that the sth cluster and tth cluster were chosen in the ith clustering membership, and they can be revised as  ),,,,,,,( 1121 mii m jjsjjjC +? and  )',,',,',,','( 1121 mii m jjtjjjC +? . From the definition of m  subsets of Descartes, we can see simii  m CjjsjjjC ,1121 ),,,,,,,( ?+? ,  timii m CjjtjjjC ,1121 )',,',,',,','( ?+? .

According to the clustering analysis, we know that the intersection between any two clusters in a clustering is empty set, that is ?=tisi CC ,, ? , so ),,,,,,,( 1121 mii  m jjsjjjC +?  ?=+? ),,,,,,,( 1121 mii m jjsjjjC? . This conflicts with  assumption, so the theorem is proved to be correct.

Theorem2: full m Descartes contains all points to be clustering.

Proof: We assume full m Descartes do not contain all points, that is to say that there are points not in full m Descartes. Let ),,,()( 21 m  mm p jjjCx ?? .

According to clustering analysis, all points have their own clusters, which equals to that if and only if one cluster in any  clustering membership contains xp. Let these clusters are  kqmqq CCC ,,2,1 ,, 21 , and they meet iqip Cx ,? (i=1,2,?,m),  so kqmqqp  CCCx ,,2,1 21 ???? and xp is a element of a m subsets of Descartes. On the base of Definition3,  ),,,()( 21,,2,1 21 m mm  qmqq jjjCCCC k ???? ? , so  ),,,()( 21 m mm  p jjjCx ?? , and it conflicts with assumption. Thereby the assumption is wrong and the theorem is proved to be correct.

Theorem3: The number of non-empty sets in all m subsets of Descartes is greater than or equal to  ii kmax .

Proof: Let i  i kk maxmax = . We assume that the number of  non-empty sets in all m subsets of Descartes is r, and meet r < maxk , and the nun-empty sets are Q1,Q2,?,Qr. In any clustering memberships with the number of clusters greater than r, we can find clusters that contain the r sets. Assume the r sets be Ci,j1,Ci,j2,?, Ci,jr in the ith clustering membership Ri, which equal to  ljil CQ ,? ,( rl ,,2,1= ). So  rjir QQQ ,21 ??? CCC jiji ??? 21 ,,? . According to  Theorem 2, rjijiji CCC ,,, 21 ??? contains all points.

Consequently, each Ci,j( maxkjr ?< ) is a subset of  rjijiji CCC ,,, 21 ??? , which conflicts with the nature of  clustering that the intersection between any two clusters are empty. Thereby the assumption is wrong and the theorem is proved to be correct.

Then we introduce the support degree of association rules.

Definition4: support degree of set A is the number that all clusters of l clustering memberships contain A, which can be denoted as formula (5).

?? = =  ?= l  i  k  j ji  i  CAASup 1 1  ,)( (5)  Here, ? ? ? ?  =? else  CA CA jiji ,0  ,1 , ,  , ml ??1 .

By the Definition3 we can see that the maximum of support degree is m and the only support degree of non-empty m subsets of Descartes and their subsets is m. If Sup (A) is l, then sup (A) can be (l-1) ( ml ??2 ).

According to Theorem2 and Theorem3, we know that nCk mii ?? (max  , and let mCl (= .

Definition5: average degree of clustering Ri is a ratio between the sums of maximum support degree of each cluster multiplied with the number of points in the cluster and the number of clusters multiplied with the number of all points. It can be shown as formula (6).

[ ]? =  ?= i  ii  k  j kikj  i i CSupCnk  RAverage  ),(max,1)(  (6)  Here, ikj ??1 , mi ??1 .

B. Optimal Number of Clusters of HCEAR Let )]([ ii RAverageinversrF = , mi ??1 The optimal number k of clusters of HCEAR is iR  that  meets formula (7).

)]}({inverse[minmin 1 i  m  iii RAverageF  = =  (7)  In order to calculate easily, we can add X as a clustering membership while the number of clustering in all clustering memberships, which ensure the support degree of any set is greater than 1. According to [12], we know that the optimal number k of clusters meets nk ? , so we only consider clustering memberships whose number of clusters be equal or less than n .

C. Processes and Complexity of HCEAR The methods searching for optimal partition can be divided  into two types, one is to learn clustering memberships that meet the number of clusters equals k and the number of them at least equals to 3, Otherwise, it don?t not possess statistical characteristics and cannot achieve the purpose, the other is to learn all clustering memberships, which should prune the X if X is a clustering membership.

The Steps of HCEAR searching for optimal partition can be shown as follows:  Step1: Init the number i of times of operation, go to Step2.

Step2: Search the nun-empty subsets whose support  degrees are (m-i), if there are two sets A and B, and they meet BA ? , then A can be pruned from existing sets and let the number of remaining sets be l. go to Step3.

Step3: If l=k, then go to Step7, else, let i=i+1, if ( 2?? mi ), then go to Step2, else, go to Step4.

Step4: Calculate the number of elements of intersection between any two different set from l non-empty sets, and we can gain a under the half-angle matrix A, which is  qppq QQaA ?== )( , and meets lqp ?<?1 , then go to Step5.

Step5: Search the max pqa , merger Qp and Qq as Q?p, add row and column of Q?p to matrix A, and remove rows and columns of Qp and Qq, and other subsets of Q?p. Then we should re-calculate the value of l, go to Step6.

Step6: if l=k, then go to Step7, else, go to Step5.

Step7: if the intersection between any two different sets is  empty set, the clustering ensemble is end, and we can stop the operation, else, go to Step8.

Step8: Search set A and set B, which meet the number of elements of intersection between them is maximum, then find the set C whose support degree is maximum among sets that contains BA? . If CBCA ?? > , then prune BA? from B, else, prune BA? from A. go to Step7.

The time complexity degree of CEFS in [11] is )( 2mknO , and that of HCEAR in this paper is )( 3mkO . Because the number k of clusters meets k<<n, thereby the time complexity degree of HCEAR is far less than that of CEFS.



III. INSTANCE ANALYSES There are 30 points need to clustering, and their proper  partition are {{1-10},{11-20},{21-30}}, where j denotes the jth points. Table I shows 9 different clustering memberships.

TABLE I.  15 DIFFERENT CLUSTERING MEMBERSHIPS  Serial Number Clustering Memberships  1 {1-30}  2 {{1-17,20,25},{18-19,21-24,26-30}}  3 {{1-6,8-9,13,21},{7,11-12,14,16-18,20,25},{10,15,19,22-24,26-30}} 4 {{1-9,12,},{10-11,13-17,23-24},{18-22,25-30}}  5 {{1-7,9-11},{8,12-18,20,25},{19,21-24,26-30}}  6 {{1-8,10-11},{9,12-18,25},{19-24,26-30}}  7 {{1-7,9-11},{8,12-17,20},{18-19,21-30}}  8 {{1-11},{12-17},{19,21-24,26-30},{18,20,25}}  9 {{1-7,9-11},{12-17},{19,21-24,26-30}{8},{18,20,25}}  A. Optimal Number of Clusters From Table I, we known n=30, m=5, k=3, and we can get  the maximal support degree of clusters of all clustering memberships and the inverse Fi as shown in Table II.

TABLE II.  MAXIMAL SUPPORT DEGREE AND FI  Ri Sup(Rij) Fi j=1 j=2 j=3 j=4 j=5  i=1 1     1 i=2 2 3    0.85 i=3 2 2 2   1.5 i=4 3 2 2   1.29 i=5 5 3 7   0.6 i=6 4 3 2   1.01 i=7 5 4 2   0.85 i=8 3 7 7 3  0.74 i=9 5 7 7 9 6 0.79  From Table II, we know k=3. Then we will search the optimal partition with k=3. In order to present detail, we will learn all clustering memberships and memberships with k=3 respectively.

B. Search for Optimal Partition from Clustering Memberships with k=3 According to the Steps of HCEAR, we can gain the non-  empty sets while support degree changes from m to 2.

TABLE III.  SUPPORT DEGREE AND THEIR CORRESPONDING NON- EMPTY SETS  Support Degree Non-empty Sets  5 {1-6},{7},{8},{9},{10},{11},{12},{13},{14,16-17},{15},{18,25},{19,22,26-30},{20},{21}  4 {1-7},{1-6,9},{13-17},{19,22-24,26-30},{19,21,22,26-30},{7,11},{10,11},{12},{18,25},{20}  3 {1-7,9},{1-7,11},{1-6,8},{8,12},{8,13},{19,21-24,26-30},{7,10,11},{12-17},{18,20,25},{12,14,16-18,20}  2 {1-7,9-11},{18,20,25},{19,21-24,26-30},{1-6,8-9},{8,12-17},{12-18,25} From Table III, we known that the number l of sets meets  l>k while support degree equals to 2, and we can merger some sets for partition points to k clusters. According to Step4, we get the under half-angle matrix A as in shown in Table IV, A1={1-7,9-11},A2={18,20,25},A3={19,21-24,26-30},A4={1- 6,8-9},A5={8,12-17}, A6={12-18,25}.

TABLE IV.  HALF-ANGLE MATRIX A  A1 A2 A3 A4 A5 A6 A1 A2 0 A3 0 0 A4 7 0 0 A5 0 0 0 1 A6 0 1 0 0 6  According to the process of HCEAR, we get {1-11} by uniting {1-7,9-11} and {1-6,8-9}, and then add row and column of {1-11} to matrix A, at the same time, we should remove rows and columns of subsets of {1-11}. We should continue the process of HCEAR until l=k=3, and we get {1- 11},{8,12-18,20,25},{19,21-24,26-30}. According to Step8, we gain A={1-11}, B={8,12-18,20,25}, BA? ={8}, C={1-6,8},  and because of CBCA ?? > , we should remove {8} from B.

Thereby, we get the partition that {1-11},{12- 18,20,25},{19,21-24,26-30}, which meets the condition of clustering, and we can stop the clustering.

C. Search for Optional Partition from All Clustering Memberships Take the clustering with k>2 as memberships, according to  the steps of HCEAR, we can gain the non-empty sets while support degree changes from m to 2, and non-empty sets for support degree being 2 are {1-11},{8,12-18,25},{9,12- 18,25},{19,21-24,26-30}.

Because the number of non-empty sets is greater than k, we can merger {8,12-18,25} and {9,12-18,25} to {8-9,12- 18,25} firstly, then prune {8-9} from {8-9,12-18,25} on the base of Step8, finally, we get the results of clustering, that is {{1-11},{12-18,20,25},{19,21-24,26-30}}.

Either learns clustering memberships whose number of clusters equals to optimal k or all clustering memberships, the results we have gained are both {{1-11},{12- 18,20,25},{19,21-24,26-30}}, and the number of points divided into wrong clusters equals to 3, which is superior to any clustering membership.



IV. CONCLUSIONS At present, the clustering ensemble is relatively mature and  widely used, however, it is applied less under unsupervised situation. Many existing clustering ensemble algorithms need to pre-design initial thresholds, which influence the results of clustering ensemble directly. The HCEAR (hierarchical clustering ensemble algorithm based on association rules) is proposed in this paper. The algorithm implements clustering ensemble under full unsupervised condition. The algorithm adopts hierarchical clustering to partition points by making use of the distribution of results of all clustering memberships and the support degree of association rules. The theorems and definitions were detail proved. Finally, the algorithm was applied in practice data and results show that the HCEAR is superior to anyone of the clustering memberships and it?s effective.

