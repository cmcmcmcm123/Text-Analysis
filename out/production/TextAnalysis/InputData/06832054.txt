Vehicle Tracking with Non-overlapping Views   for Multi-camera Surveillance System

Abstract?With the rapid development of intelligent video surveillance system for transportation, traditional single- camera-based video analysis has become insufficient. Many researches have focused on the non-overlapping multi-camera target tracking. However, the tracking precision and the computing overhead are still big obstacles. This paper proposes a novel vehicle tracking approach for non-overlapping multi- camera targets with data fusion by using minimum cost and maximum flow method. Structured information of moving targets is extracted and associated with other information such as motion time, the topology of camera network to solve targeted vehicle tracks. Besides, to improve the performance of the process, a parallelization algorithm for camera network topology partitioning is presented, which makes it possible for each sub-graph to track target independently in parallel. The experiment results show that the presented approach is able to perform target vehicle tracking analysis with high efficiency and accuracy.

Keywords-vehicle tracking; non-overlapping; camera view; data fusion; minimum cost and maximum flow

I. INTRODUCTION Vehicle has become one of essential part of our daily lives.

Howover, it also has become an impotant cause resulting in transportation accidents, criminality, etc. So traffic video surveillance system has become a crucial measure to monitor the transportation situation while protect our lives and properties. It is not only able to make real-time monitoring, recording for monitored situations, but also able to perform analysis and give warnings for some special or dangerous events, such as law violations or crimes. This would be helpful for security staffs to deal with emergency and ensure the public safety.

The foundation and core technologies of intelligent video surveillance system lie in the detection and tracking of vehicle objectives, because only after the images of targets are extracted we can do further analysis on their characteristics and behaviors. The means of detection and tracking of targets can be divided into two categories: the single-camera way and the multi-camera way. Benefiting from long-term studying, the research on single-camera detection and tracking of targets is relatively mature. The common detection approaches are frame subtraction and background subtraction. The common tracking approaches are the ones based on model, characteristic and mean shift.

However, with the numbers of cameras inside many intelligent video monitoring systems increasing rapidly, the  way of single-camera-based analysis become insufficient to meet the practical requirements. Therefore, the multi-camera- based analysis technologies have attracted more and more attentions of researchers.

Nowadays, the traditional deployment model of surveillance systems for urban roads traffic usually is: monitoring and recording in front-end by cameras, compressing and transmitting videos in middle-end, and storing, processing and analyzing the content of videos in surveillance center in back-end. This kind of model has two disadvantages. First, it is expensive and ineffective to analyze huge amounts of recorded videos manually, especially in multi-camera conjoint analysis; second, huge data of video files causes great pressure on transmission networks and storage devices.

In order to achieve the goal of tracking vehicle targets which pass through multiple cameras, we need to associate the targets captured by different cameras. This kind of association, which leverages the vehicle-related characteristic information and spatial-temporal constraint information, turns out to be a data fusion problem. With the structured information extracted from traffic surveillance videos, combining with the topology structure of camera networks, we build a data fusion model based on minimum cost and maximum flow, and implement a non-overlapping multi- camera target tracking strategy. In order to crack the hard nut of the overhead of processing and storage, this paper puts forward a network division algorithm based on camera network topology. It can improve the stability and the performance of the tracking system with massive data, which implements distributed target tracking, breaks the computational bottleneck of centralized processing and significantly reduces the cost in terms of computation time and network transmission costs.

The rest of the paper is structured as follows. Section II gives the background knowledge related to this work, including an overview of related work about identification and re-identification of targets across multiple cameras and the major methods used in vehicle tracking in multi-camera surveillance systems. Section III presents the features and key technologies used in the approach presented, including the methods of extracting the target associated features, the topology estimation way of camera networks method and the target association algorithm. In section IV, the measurements of the non-overlapping camera in the multi-camera surveillance system both in functionality and performance are  on Embedded and Ubiquitous Computing  DOI 10.1109/HPCC.and.EUC.2013.172   on Embedded and Ubiquitous Computing  DOI 10.1109/HPCC.and.EUC.2013.172     discussed, while a traditional Bayesian fusion method is used as a contrast. Finally, section V draws conclusions and gives some directions for future work.



II. RELATED WORK Tracking targets in a single camera has been widely  studied. Targets are often modeled using various features such as color, motion, size, and time.  However, with the rapid development of intelligent video surveillance system for transportation, traditional single-camera-based video analysis has become insufficient. Many researches have focused on the non-overlapping multi-camera target tracking approaches.

This kind of tracking is often modeled as a global data association issue which is formulated as a maximum a posteriori probability (MAP) problem. Berclaz et al. [1] tracked multiple targets using dynamic programming to solve a global data association problem with overlapping multiple cameras. Wu and Nevatia [2] detected people using an Edgelet-based Adaboost classifier that models target by its histogram, size and time. The trajectory of a target is also estimated by applying a MAP formulation.

Object tracking with partially overlapping camera views has also been researched extensively in the last decade [3, 4].

However, in wide-area tracking and wide-area surveillance applications, it is always unrealistic to assume that all the cameras in the system have overlapping fields of views.

Tracking across disjoint camera views is a more challenging problem than that with overlapping cameras due to lack of spatial continuity, which resulting in blind regions [5].

Identification and re-identification of targets across multiple cameras are challenging because the same target may have significant variations in shape and appearance across cameras. Targets may enter and exit a scene randomly with highly non-linear motions. The process of identification and re-identification is normally performed by finding the similarities of the features of objects, and/or the probabilities of object transitions.

Appearance is an obvious feature to associate objects in different camera views. However, due to different camera angles, illumination variations and different camera parameters, some appearances cannot offer help clearly.

Thus, people prefer to choose features which are influenced by environment less. Color is one of the most commonly used appearance features. Color information is often represented by color histograms in the RGB (red, green, blue) or HSV (hue, saturation, value) color spaces. HSV is more robust to illumination changes due to its inherent properties. Javed et al. [6] proposed a subspace-based brightness transfer function (BTF) by using probabilistic principal component analysis (PCA) to calculate the subspace of BTFs for a set of known correspondences. This method relies on large amount of training data with a good range of clothing colors to give an accurate mean BTF (MBTF). Prosser et al. [7] proposed to use cumulative BTF (CBTF) instead of MBTF, which made use of available color information from a very sparse training set. A comparison of these two different BTFs [8] demonstrated similar behaviors of the two methods when the  simple association problem needed to be solved. Their experiments also showed that appearance matching relying on color exclusively is not reliable when the scenario became more complicated.

Fusing more visual features could help improve the effectiveness of the appearance matching and reduce the influence of environment changes. Wang et al. [9] employed multiple features such as color histogram, height, moving detection, travel time and speed to match objects across non- overlapping views.

Spatial-temporal information is another important evidence to be considered for object re-identification. One way of using spatial-temporal constraints is predicting the objects? positions when they are in the blind region. With the assumption of linear motion model, a Kalman filter or a similar mechanism was employed [10, 11]. The positions of the objects could also be inferred based on a common ground assumption, which allowed the warping between the cameras? views by using a homography matrix [12]. In [13], an expanded triangulation with motion constrains, which assumed linear motion of the objects, was employed to infer the positions of the objects.

Another category of research also used spatial-temporal information, but focused on the recovery of camera network topology, instead of object tracking. To make use of the spatial features, some methods have been introduced to predict the motions of objects by assuming a common ground plane and small gap between two cameras [10, 14]. Kang et al. [14] used a spatial-temporal joint probability data association filter (JPDAF) to formulate a joint probability model for encoding the appearances and motions of objects.

Chilgunde et al. [10] used a Kalman filter to obtain the target?s track in the blind region between cameras. For multi- camera correspondence matching, the Gaussian distributions are applied for computing the tracking parameters across cameras for the target motion and position in the ground plane view. However, this approach is not competent to some complex situations such as obvious crossroads, big turns.

Rahimi et al. [15] obtained the calibration parameters of the cameras and the trajectories of targets by using MAP estimation. Monari et al. [4] intended to track objects in both overlapping and non-overlapping camera networks. They applied 3D positions combined with color space features to perform object association. The 3D positions in the blind region are predicted by a Particle filter, which also was used in [16].

For data fusion algorithm of this kind of global data association issue, Huang and Russell [17] adopted multiple features for vehicle matching in a Bayesian formulation. An association matrix is employed for finding the best assignments for multiple objects. Zhang et al. [18] found an optimal MAP data association in a single camera by finding the max-flow of a network. There are also other fusion algorithms. Song and Roy-Chowdhury [19] proposed an optimized method to combine short-term feature correspondences and long-term feature dependencies across multiple cameras. The LP (linear programming) solution presented in [20] introduced a term that encouraged relative     positions of objects to remain constant, however it assumed that the number of objects was fixed.



III. DESIGN AND ALGORITHM Vehicle tracking in multi-camera surveillance systems has  attracted a lot of attention in recent years because of its widespread applications in many scenarios such as public transportation, campus security, and production process. In order to achieve full network tracking, diversified methods have been proposed in the last decade. One of the most common ones is to associate the same target appeared in different cameras by extracting the target appearance characteristics and spatial-temporal characteristics, while combining with the topology of multiple cameras.

The key to associate the same targets in different cameras is to find the correspondence among various targets.

Considering the scale of surveillance and the cost of installment, most video surveillance systems are non- overlapping which makes the observed targets discrete in space and time. Blind areas make it complex and difficult to track the targets. Now the usual way of doing this is to extract the external features and spatial-temporal features of the targets, and combine with the topology relationship of cameras, then build a spatial-temporal constraint with the target shifting among cameras, finally utilize the proper association algorithm to match the targets captured by different cameras. All of these are done for target tracking within the entire network. The solution for this problem includes three parts: the extraction of target associated features, the topology estimation of camera networks, and target association algorithm.

For large-scale surveillance systems, the number of vehicles that appear in the cameras would be very large even in a short time, which not only leads to a big rise in computational time, but also brings great pressure to the system center. In order to achieve load balance, a sub-graph network division algorithm is designed and implemented. The whole camera network will be divided into multiple sub- graph units, the target correlation algorithm can be executed independently and parallel within each sub-graph units.

A. Extraction of Target Associated Features The extraction of target associated features is the  foundation of the realization of targets association. The approach presented partitions target associated features into two types: descriptive semantic information and characteristic quantity-based numerical data information.

Semantic information refers to the detailed descriptive understandable information including the type, color and size of a target, when and where the target appears, the speed of movement, and its direction and track. The target type means the brand and model of the vehicle. Through exacting and modeling the header information of the target vehicle, the approach presented uses Speeded Up Robust Features (SURF) algorithm to compare the information with the standard vehicle models in database to obtain the type of the target  vehicle. In addition, the motion direction and specific track of the target are gained by continuously tracking and recording the motion trail of the target vehicle within the view of a single camera. The target speed is calculated through the offset of the movement of the target.

Characteristic quantity-based numerical data information refers to the characteristic value information which can be used to identify the level of similarity in the field of image recognition. The characteristic value information applied here includes vehicle type, Histogram of Oriented Gradients (HOG) operators, and Local Binary Pattern (LBP) operators.

All the associated features are used to judge the relevance of two targets, and the semantic information are used for target filtering and choosing.

In order to carry out the associated multi-feature of the target with reasonable fusion, a similarity measure function is presented here. Similarity measure function combines numerical features and set weight for each feature. It is used to describe the level of similarity between the targets and determine the correlation between the targets. Assume target a and target b are two vehicles observed in camera Ci and Cj respectively, the similarity measure function S(Oi,a, Oj,b) is defined in (1).

( ) ( ) ( ) ( ), , , , , , , ,, , , ,i a j b t i a j b l i a j b a i a j bS O O T t t L l l A a a? ? ?= ? ?    (1) The above refers to the contrast ratio of the features  including time denoted by T(ti,a, tj,b), location denoted by L(li,a, lj,b), appearance model denoted by A(ai,a, aj,b). We set a weight ? for each feature to control the reliability. The value of ? depends on the importance of the feature. For time and location, it is relative easy to construct the similarity of the specific objects according to object appearing time and the topology of the monitoring network.  So, we focus on the appearance model, which consists of vehicle type, HOG and LBP.

a) Vehicle type The vehicle type means the brand and model of the target.

Through exacting and modeling the header information of the target vehicle, the system uses SURF algorithm to compare the information with the standard vehicle models in database to obtain the information of the brand and model of the target vehicle. It makes the head area of the vehicle compare with other samples in the feature library and record the matching result.

b) HOG HOG feature descriptor was first proposed by N. Nalal  and B. Triggs [21] in 2005, its core idea is to represent the appearance and outline of the target by gradient or an edge direction of density distribution, without setting gradients and marginal positions. In the approach presented in this paper, two targets? screenshots are used to do the comparison. First, each image is divided into some small connecting areas.

These areas are often referred as cells. Then generate the gradient or edge direction histogram of each pixel within each cell. Finally combine these histograms by the statistical     methods to form the target?s HOG feature descriptor. In order to improve the accuracy of the feature, we use overlapping local contrast normalization technology. Put the local histogram of each cell in a broader interval called block, and calculate the density in the interval. According to the density of the block, the system normalizes other cells within the block. Normalization process makes HOG operator a better robustness with light changes and shadows.

c) LBP A LBP feature descriptor is often used to describe the  local texture features of the image. Since the LBP value is corresponding to pixel window, it means that LBP value is related with the location of the picture. This makes the selection of a suitable comparison window position a critical impact on comparative results. In order to reduce the position dependence of the LBP operator information, the image is divided into several sub-regions. The LBP feature value of each pixel is extracted within sub-region. So the LBP feature value of this sub-region could be described by the statistic histogram of these points? LBP feature. The multiple sub- regional statistic histograms compose the LBP statistic histogram of the whole image.

We use the cosine between feature vectors to represent the similarity. HOG and LBP are treated as two separate features for object matching. If two HOG descriptors are denoted by AH and BH, the cosine similarity score SH of HOG descriptors is calculated by (2). The similarity score SL of the LBP descriptors is calculated by (3).

( )cosH H HH H H  A B s  A B ?  ? = =                              (2)   ( )cosL L LL  L L  A B s  A B ?  ? = =  (3)  B. Topology Estimation of Camera Networks The topology relationship of camera networks refers to  the temporal and spatial patterns of the target movement among cameras. With topology relationship, it is possible to build a spatial-temporal constraint of the target shifting among cameras. According to the spatial-temporal constraint, we can not only properly reduce computing scale, but also further improve the accuracy of target association. The system builds the spatial topology relationship according to installation sites of the cameras and road connection, with the temporal topology relationship obtained by statistical learning. Within the fixed view of a camera, the appearing area and disappearing area of multiple targets can be learned based on the position information of the target appearing and leaving detected by the camera.

As Figure 1 shows, for any pair of nodes (pi , pj), pi Cm , pj Cn , i?j, Cm?Cn, there is a variable Li,j = {0, 1}, where 0 indicates that objects between the two nodes cannot pass directly, without going through other nodes; 1 indicates that objects can directly pass the two nodes without going through other nodes. Since we are not concerned on the connections  inside the camera, the transfer inside the camera is marked by dotted line. In real scenarios, traffic lights and traffic conditions may have some influence on the time it takes for the target to pass the distance between two cameras. Gaussian model is adopted here to indicate the temporal relationship between two cameras. The relationship is marked by solid edge in the figure. Its parameters (?i,j, ?i,j2) are obtained by training and learning.

Figure 1. Topology relationship of camera network  C. Target Association Algorithm The target association algorithm in our approach is a  minimum cost and maximum flow algorithm based on the relationship of the camera network topology. Figure 2 shows an example of the cost-flow network built in this paper. The following steps are taken to construct a cost-flow network: a) Select observed targets in each camera, based on the  semantic information within a certain period of time, mark each of these selected targets as Oi,a;  b) For each Oi,a, camera Ci has recorded its track information, thus we simply add the start point ui,a and end point vi,a in the track to graph G as independent nodes. At the same time, we also need to add a directed edge e(ui,a, vi,a) which has infinite capacity and zero cost to graph G;  c) For any pair Oi,a and Oj,b, if the similar function between them is S(Oi,a, Oj,b) > 0, then add a directed edge e(vi,a, uj,b) which has the capacity of Cap(Oi,a, Oj,b) and the cost of S(Oi,a, Oj,b) to graph G. What?s more, if vi,a falls into disappearing area of the camera Ci, uj,b falls into the appearing area of the camera Cj and Lm,n=1, make Cap(Oi,a, Oj,b)=2, otherwise make Cap(Oi,a , Oj,b) = 1;  d) Append virtual nodes source s and sink t for the graph G. For each region node pa in the camera subnet, if there are other directed edges from other camera subnet point to pa, for each node ui,a that falls into pa, add a directed edge e(s, ui,a) which has the capacity of 1 and the cost of 0 to graph G. On the contrary, if pa has a directed edge point to other camera subnet, for each node vi,a that falls into pa , add a directed edge e(vi,a, t) that has the capacity of 1 and the cost of 0 to graph G.

After building cost-flow network, the minimum cost flows of the whole network can be solved by minimum cost and     maximum flow solution algorithm, which is actually to obtain the flow with maximum cost by the above algorithm.

The obtained result is also a series of path of the maximum similarity values. The system controls the display of the predicted numbers of tracks according to the requirements of users, while gives more priority to the track with higher prediction probability for display. Besides, the system has a modification module, which is capable of self- revising according to the correction command submitted by users, including modifying the structure of cost-network G, recalculating and rebuilding new prediction track.

Modification module can fix some errors produced by the procedure of videos analysis and process, thus improve the robustness of the system.

Figure 2. The example of a cost-flow network  D. Camera Network Sub-graph Partitioning A large surveillance system sometimes contains more  than thousands of cameras which forms a huge camera network. If we want to track targets within the whole network, the amount of calculation is extremely enormous.

To overcome this obstacle, in our approach, the camera network is divided into several independent sub-network units according to the characteristics of its topology relationship.

The target association algorithm then can be carried on independently at the same time.

The sub-graph partition should not destroy the structure of the camera network, so that the trajectory calculated in each sub-graph unit can be easily linked to form the total vehicle trajectory. In our system, the sub-graph network division algorithm should follow several principles. For each sub- graph A, A = {pi, qj | i, j = 1, 2, ... , m}, satisfies: a) Completeness Principles: For each target, if its  appeared node qj A, then the last disappeared node pi satisfied pi A. In reverse, if the target?s disappeared node pj  A and the target appears again, then the next appeared node qk satisfied qk A.

b) Minimization Principles: The amount of nodes in each unit should be as small as possible on condition of the completeness principle. It means that the camera network should be divided elaborately to get sub-graph units as many as possible.

The completeness principle makes sure the complete independence of each unit, so that all nodes involved are within the sub-graph unit when calculating the associate relationship of the targets, and do not need exchange information with other units. Only each sub-graph remains independent, can the trajectory connect into the whole track.

The minimization principle makes the structure of each sub- graph unit as small as possible, cannot be re-split, which guarantees the minimum calculation overhead of the associated algorithm.

The method of devising the camera network into sub- graph units is as follow: in a field of camera view, if an appeared node qi only connected with a disappeared node pj, then remove the dotted connection between them and put qi and pj into two different sub-graph units. Eventually the entire camera network turns into an unconnected graph. Take C3 in Figure 1 as an example, the appeared node q1 is only connected with the disappeared node p4, then remove the edge e'(q1, p4) between them, makes q1 and p4 assigned into two different sub-graph unit. The method of dividing the whole network into sub-networks provides the distributed intelligent video surveillance system with strong support. It is possible to build some local process units to process video information, and the local process units only transmit key information to the central process unit for deeper analysis.

Our system applies the framework of MapReduce based on Hadoop to realize the parallel computing of all sub-graphs.

First, different tasks from different sub-graphs are mapped to different MapReduce nodes for track analysis independently.

Then, all information will be returned to central server to be reduced by the presented approach mentioned in previous sub-sections.



IV. EXPERIENCES AND PERFORMANCE To verify the effectiveness of the approach presented in  this paper, a batch of vehicle monitoring videos from 23 cameras in the real campus security monitoring system of Huazhong University of Science and Technology (HUST) are applied and analyzed. All videos are in the resolution of 1920?1080 with 25 frames per second. 8 non-overlapping cameras with 5-minute durations are selected for evaluation.

These dataset are challenging because: i) the roads monitored are two-way with lots of intersections, that makes the tracking environment more complicated; ii) due to the shadows of many trees along the roads in the campus, the colors and illuminations of the vehicle objects tracked change obviously across cameras; iii) the traffic are relative heavy. For example, in the selected cameras, more than 500 vehicles passed through the cameras; iv) high resolution video takes much time to process. The map view is shown in Figure 3.

For evaluation of tracking, the tracking metric in [18] is used. Evaluation results across multiple cameras are shown in Table I. In the table, real targets indicate the number of real vehicles captured by the specified cameras. Track recall is the successful tracking rate within a camera. It is hard to get all tracks of all objects precisely. If the degree of accuracy of a target tracked is more than 90% compared to the     corresponding real track, it is considered as mostly tracked (see Table I). Since the time span of the group video data is limited, some vehicle tracks are broken off, which are marked as partially tracked. Since some conditions of camera views are not perfect enough, when a car disappears in the view which goes beyond the detection area, it is marked as mostly lost. Mostly lost also includes those cars that disappear in blind regions.

Figure 3. Cameras for evaluation on the map  TABLE I. EVALUATION OF MULTI-CAMERA TRACKING  Camera number  Real targets  Track recall  Mostly tracked  Partially tracked  Mostly lost  1 15 100% 15 0 0 2 46 87.0% 37 3 6 3 43 90.7% 36 3 4 4 39 97.4% 36 2 1 5 34 100% 30 4 0 6 38 92.1% 31 4 3 7 9 77.8% 5 2 2 8 9 77.8% 5 2 2  Average 30.38 90.35% 24.38 2.5 2.25  A. Location, Time and Appearance Measures For the location measure, enter/exit area is defined for  each camera and their connections across cameras are used as spatial features. For the time measure, the mean and variance between two corresponding enter/exit areas is learned during the training stage. A single Gaussian distribution is used to model the object travel time. In this practical traffic surveillance system, since all cameras are deployed in a campus, the distance between two neighbor cameras is not far.

Most of them are 50-200 meters, and the speeds of most vehicles in the campus are between 10-40km/h. Table II shows the parameters for enter/exit areas pairs during the training stage.

TABLE II. LEARNED PARAMETERS OF TIME MEASUREMENT FOR ASSOCIATED ENTER/EXIT AREA PAIRS  Pair of Enter/Exit Area  Arrival time (second) Mean Variation  Camera 1-2 8.5 7.5  Camera 2-3 6 9  Camera 3-4 11.5 3.5  Camera 4-5 7 7  Camera 5-6 3 3  Camera 6-7 14 1  Camera 7-8 5 2  Additionally, since there are two intersections in the blind regions and there is a traffic light in one of the eight camera views, the variation of the link between them is set slightly looser.

Location, time, SURF, HOG, and LBP are used for appearance measure. Here, the weights for the location, time, SURF, HOG and LBP are set to: 1, 1, 0.85, 0.22, and 0.55, respectively, based on experiments.

B. Overall Performance of Multi-camera Tracking Since the classification method we used concerns  primarily with the front area of the vehicle, the identifying accuracy is poor when a target shows only the back part in the camera views. However, it is a good opportunity to test how robustly the system can run in such a serious situation.

Some vehicle objects of correct tracking across 6 major cameras are shown in Figure 4, while some of incorrect tracking are shown in Figure 5.

Camera 1 Camera 2 Camera 3 Camera 4 Camera 5 Camera 6  Figure 4. Objects of correct tracking across six major cameras  Camera 1 Camera 2 Camera 2 Camera 3 Camera 2 Camera 4  Figure 5. Objects of incorrect tracking (marked by red crosses)  C. Computational Time Eight serials of monitoring videos with duration of 5  minutes are processed by the server with 2.26 GHz 2xIntel Xeon E5520 CPU. The amount of objects we detected is 233.

We use two ways to search for the tracks. One is the traditional Bayesian method, the other is our minimum cost and maximum flow (MCMF) method.

In accuracy, the contrast ratio between our method and the Bayesian method is 1:1.11; in the computation time, our method increases about 27.5%. Table III shows the comparison of detail computational time taken by two methods and Table IV shows the accuracy comparison.

TABLE III. COMPARATION ON COMPUTATIONAL TIME  Record amount  Computational time Bayesian  ms MCMF  ms Performance improvement  3 1534 1013 34% 6 4112 2763 33%  10 5873 4279 27% 12 5627 4628 18% 15 18811 12461 34% 17 13195 10567 20%  Average 8192 5951.83 27.5%  TABLE IV. COMPARATION ON ACCURACY  Record amount  Accuracy Bayesian MCMF Comparison  3 100% 100% 1:1 6 100% 100% 1:1  10 100% 100% 1:1 12 50% 57.14% 1:1.14 15 58.33% 83.33% 1:1.42 17 53.33% 70% 1:1.31  Average 76.94% 85.08% 1:1.11

V. CONCLUSIONS In this paper, we present a new vehicle tracking  surveillance approach with non-overlapping views in multi- camera. A framework to perform robust multiple targets tracking across multiple cameras is discussed. The multi- objects in multi-camera data association problem is formed with a minimum cost and maximum flow mode. For robust multi-camera tracking, time, location, classification type, and appearance of targets are effectively applied as a similarity measure. The approach is tested with real surveillance videos from the campus of HUST. The experimental results validate the robustness and effectiveness of the approach.

Experimental results also indicate that the method of tracking across multiple cameras is feasible after a high quality of feature extraction and target detection. For future work, more efficient and effective appearance models and feature extraction are desired to increase the accuracy of the approach.

