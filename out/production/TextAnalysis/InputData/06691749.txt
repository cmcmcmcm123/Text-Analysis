Scalable Approximation of Kernel Fuzzy c-Means Zijian Zhang

Abstract?Virtually every sector of business and industry that use computing, including financial analysis, search engines, and electronic commerce, incorporate Big Data analysis into their business model. Sophisticated clustering algorithms are highly desired to deduce the nature of data by assigning labels to unlabeled data. We address two main challenges in Big Data.

First, by definition, the volume of Big Data is too large to be loaded into a computer?s memory (this volume changes based on the computer used or available). Second, in real-time applications, the velocity of new incoming data prevents historical data from being stored and future data from being accessed. Therefore, we propose our Streaming Kernel Fuzzy c-Means (stKFCM) algo- rithm, which reduces both computational complexity and space complexity significantly. The proposed stKFCM only requires O(n2) memory where n is the (predetermined) size of a data subset (or data chunk) at each time step, which makes this algorithm truly scalable (as n can be chosen based on the available memory). Furthermore, only 2n2 elements of the full N ? N (where N >> n) kernel matrix need to be calculated at each time-step, thus reducing both the computation time in producing the kernel elements and the complexity of the FCM algorithm. Empirical results show that stKFCM, even with very small n, can provide clustering performance as accurately as kernel fuzzy c-means run on the entire data set while achieving a significant speedup.

Index Terms?kernel clustering; fuzzy c-means; scalable algo- rithms; streaming data; projection

I. INTRODUCTION  The ubiquity of personal computing technology has pro- duced an abundance of staggeringly large data sets which may exceed the memory capacity of a computer (whether that computer is a cell phone or a high-performance cluster).

Large data sets come from streaming data as well those that are presented to a system sequentially such that future data cannot be accessed. These challenges stimulate a great need for sophisticated algorithms by which one can elucidate the similarity and dissimilarity among and between groups in these gigantic data sets.

Clustering is an exploratory tool in which data are separated into groups, such that the objects in each group are more similar to each other than to those in different groups. The ap- plications of clustering algorithms are innumerable. Clustering is a common technique widely used in pattern recognition, data mining, and data compression for deducing the nature of a data set by assigning labels to unlabeled data. Clustering itself is a general task to be solved and plenty of algorithms have been  TABLE I IMPORTANT ACRONYMS AND NOTATION  Acronym Definition FCM fuzzy c-means  KFCM kernel FCM stKFCM streaming KFCM  KPC kernel patch clustering [1] RKHS reproducing kernel Hilbert space  Notation Definition c or k number of clusters  N number of objects n number of objects in data chunk x feature vector ? Rd X set of x X set of X , {X0, . . . , Xt, . . .} U partition matrix ui ith column of U v cluster center w weight vector ?i kernel representation of xi, i.e., ?(xi) ? set of ?  ?(xi,xj) kernel function, ?(xi,xj) = ?i ? ?j K kernel matrix, K = [?(xi,xj)], ?i, j HK reproducing kernel Hilbert space imposed by K  d?(xi,xj) kernel distance, ||?i ? ?j ||2 [i] set of integers, {1, 2, . . . , i}  proposed for solving this problem such as k-means, single- linkage clustering, and Gaussian-mixture-model clustering.

The k-means algorithm is one of the most popular clustering algorithms due to its simplicity. For a set of N feature vectors (or objects), the program will choose k cluster centers randomly at the beginning. Each data point will be assigned to its nearest cluster center, then the cluster centers will be recomputed. These steps are repeated until the algorithm con- verges (and there are many ways by which convergence can be defined). The output will be a partition matrix U ? {0, 1}N?k, a matrix of Nk values. Each element uik is the membership of vector xi in cluster k; and the partition matrix element uik = 1 if xi belongs to cluster k and is 0 otherwise.

Fuzzy c-means (FCM) is analogous to the k-means algo- rithm with fuzzy partitions, which gives more flexibility in that each object can have membership in more than one cluster.

The constraint on fuzzy partitions is that all the memberships of an object must sum to 1, thus ensuring that every object has unit total membership in a partition: (  ? k uik = 1). Hence, the  partition matrix U ? [0, 1]N?c, where the partition elements are now on the interval [0, 1].

The FCM (as well as the k-means) model is based on the assumption that the feature vectors are grouped in similarly- sized hyperspheres. Kernel methods can overcome this lim- itation by projecting the vectors into a higher dimensional feature space where the patterns can be discovered as linear relations [2]. Consider some non-linear mapping function ? : x ? ?(x) ? RD? where D? is the dimensionality of the higher-dimensional feature space created by the function ?.

For (most) kernel algorithms, including kernel FCM, explicitly transforming x is not necessary. Instead, a kernel matrix K is used, which consists of the pairwise dot products of the feature vectors in a transformed high dimensional space HK ; this space is called the Reproducing Kernel Hilbert Space (RKHS). Given a set of N objects, literal kernel FCM (KFCM) requires to store an N ? N kernel matrix K = [?(xi,xj)] = [?(xi) ? ?(xj)], i, j ? [N ], which poses challenges for clustering Big Data. Although an abundance of research has been conducted on fuzzy clustering algorithms for Big Data [1, 3?9], only a select few of these algorithms are appropriate for kernel methods.

In this paper, we devise an approximation of the KFCM algorithm for streaming data, i.e., big data which could be viewed as data streams. We assess the performance of our algorithm by comparing the partition matrix to that of the literal KFCM. Empirical results demonstrate that our algorithm could provide similar results to the literal KFCM while only requiring access to small data chunks in the current and previous time step. The memory requirement is reduced from O(N2) to O(n2), where N and n are the size of the full data set and each subset data chunk, respectively.



II. RELATED WORK  To date, a substantial number of algorithms have been developed for clustering big data. Roughly, these algorithms can be categorized into three classes: sampling, distributed clustering, and data transformation algorithms.

A. Sampling and non-iterative extension  Sampling the data set is the most basic and obvious way to address big data. In sampling methods, algorithms are run on a reduced representative sample, and then the sample partition is non-iteratively extended to approximate the clustering solution for the remaining data in the full data set. If the data are sufficiently sampled, there is only a small difference between the result of the approximate partition and the result of clustering the entire data set. These algorithms are also called extensible algorithms. The notion of extensibility for FCM was introduced in [10]. Algorithms that produce an approximate result of the full data set by first solving the problem using a sample set and then non-iteratively extending the result on the full data set are referred to as extensible algorithms.

Sampling approaches can be further divided into two cat- egories: random sampling and progressive sampling. Pro- gressive sampling schemes for FCM were well studied in [11]. The authors showed that progressive sampling is widely used in many clustering algorithms. Sampling schedule and  termination criteria are the most central components of any of these approaches. The most well-known progressive sampling method is generalized extensible fast FCM [12] which is the extension of [10]. In [12], the algorithm starts with statistics- based progressive sampling and terminates with a representa- tive sample that is appropriate to capture the overall nature of the data set. Reference [12] extends this algorithm so that it could be applied to more general cases. Instead of clustering numerical object data, [13] and [14] extend the algorithm to attack the problem of clustering numerical relational data. In kernel clustering, cluster centers are linear combinations of all the data points to be clustered; hence, the sampling ap- proaches mentioned previously are inappropriate. The authors of [15] and [8] tackled the problem of kernel clustering by proposing a novel sampling of the kernel matrix which results in significant memory savings and computational complexity reduction while maintaining a bounded-error solution to the kernel k-means and KFCM problem; although, the solutions in [8, 15] are not truly scalable to big data as they require the loading of an N?n rectangular of the kernel matrix; hence, as N grows, eventually there is a point at which the only loadable n becomes less than 1, thus invalidating the scalability of the algorithm.

B. Distributed clustering  Distributed clustering algorithms could be classified into two types: incremental loading of data subsets that can be fit into current memory capacity and divide-and-conquer ap- proaches.

1) Incremental clustering: Algorithms in this category se- quentially load small chunks or samples of the data, clustering each chunk in a single pass, and then combining the results from each chunk. Representative algorithms are [16] and [17].

In these approaches, the clusters are updated periodically using information from both the incoming data and obsolete data.

Single pass FCM (spFCM) was proposed in [3]. This algorithm performs weighted FCM (wFCM) on sequential chunks of data, passing clustering centers from each chunk onto the next. At each time step, the algorithm clusters a union set consisting of data in the current step and the cluster centers passed from previous step. The authors of [4] extend [3] by incorporating more results from multiple preceding time steps and in [5], they propose an algorithm that passes c weight values which are sums of membership values of the points in the current subset onto next iteration. Another incremental algorithm called bit-reduced FCM (brFCM) was proposed in [6]. This algorithm first bins the data and then clusters the bin centers. While the performance of brFCM highly depends on the binning strategy; brFCM has been showed to provide very efficient and accurate result on image data. Havens et al.

extended spFCM and other incremental FCM algorithms to kernel clustering in [7], although the results of these kernel extensions were disappointing.

Bradley introduced a data compression technique in [18].

This algorithm uses a buffer to contain the current subset of data. The data are then compressed twice. In the first     compression, objects that are unlikely ever to move to other clusters are discarded. Those objects are found by calculating the Mahalanobis distance between the object and the cluster center. Those that fall within a chosen radius are then chosen.

In the second compression period, more cluster centers are introduced into the data set in order to find more stable points.

After these two compressions, the space will be filled with new data. The algorithm will keep running until all the data have been processed. Farnstrom introduced a special case of this algorithm [19] in which all the points in the buffer are discarded each time.

Gupta uses evolutionary techniques to search for the global optimal solution to the sum of the squares (SSQ) problem which is required to find cluster centers [20]. Each chunk is viewed as a generation. The fittest cluster centers survive to the next generation; bad centers are killed off with new ones selected. In kernel methods, no actual cluster centers exist in RKHS. Passing cluster centers to next time step is unpractical.

The author of [1] presented Kernel Patching Clustering(KPC) algorithm. This algorithm selects approximate pseudo-centers at each time step, merging them repeatedly until the entire data set has been processed. Since the space complexity depends on only the size of chunks, algorithms of this type are (usually) truly scalable.

2) Divide and conquer: Algorithms in this category cluster each chunk in sequence as well. But rather than passing the clustering solution from one chunk onto the next, these algorithms aggregate the solutions from each chunk in one final run. Due to this final run, most of the algorithms in this type are not truly scalable. Online FCM (oFCM) was proposed in [9]. oFCM aggregates the solutions from each data chunk by performing wFCM on all the resultant cluster centers. Again, it was shown in [7], that the kernel extension of oFCM performed poorly. Reference [21] views the problem of merging different results from disjoint data sets as the problem of reaching a global consensus, while [22] assigns a weight to the cluster centers in each chunk, and then performs LSEARCH on all the weighted centers retained. LSEARCH is a local search algorithm that starts with an initial solution and then refines it by making local improvements. Other algorithms like those proposed in [23] and [24] use special initialization techniques to improve the accuracy of the k- means algorithm.

C. Data transformation methods  Algorithms of this sort transform data into other structures with the intention of making the clustering process more efficient. Perhaps one of the earliest well-known clustering algorithms for data streams is BIRCH [25], which transforms the entire data set into a tree-like structure called a clustering feature (CF) tree. The leaves of the CF tree are then clustered.

PAM [26] transforms the data set into a graph structure, and then searches for a minimum on the graph. CLARANS [27] is a variation of CLARA [26] and draws a sample of the data set and applies PAM on the sample. The key difference between CLARA and CLARANS is that CLARA draws the sample at  the beginning of the search while CLARANS draws it at each step of the search. The benefit of the CLARANS approach over CLARA is that the search is not confined to a localized area.

While many of the algorithms mentioned in Section II produce high-quality partitions for big data sets, unless noted, they are not appropriate for kernel clustering. The only (that we know of) truly scalable approach to kernel fuzzy clustering is the spKFCM algorithm proposed in [7, 28] and, as shown in [7], the spKFCM approach produces less-than-desirable results for some data sets.



III. KERNEL FCM Kernel FCM (KFCM) can be generally defined as the  constrained minimization of  Jm(U ;?) = c?  j=1  n? i=1  umij ||?i ? vj ||2, (1a)  = c?  j=1  ( n?  i=1  n? k=1  ( umiju  m kjd?(xi,xk)  ) /2  n? l=1  umlj  ) ,  (1b)  where U is a fuzzy partition, m > 1 is the fuzzification pa- rameter, and d?(xi,xk) = ?(xi,xi)+?(xk,xk)? 2?(xi,xk) is the kernel-based distance between the ith and kth feature vectors. The function ?(xi,xk) = ?(xi) ? ?(xk) is the kernel function, which can take many forms, including the popular radial basis function (RBF), polynomial, and linear forms.

KFCM solves the optimization problem min{Jm(U ;?)} by computing iterated updates of  uij =  [ c?  k=1  ( d?(xi,vj)  d?(xi,vk)  ) 1 m?1  ]?1 , ?i, j, (2)  where the kernel distance between input datum xi and cluster center vj (in the RKHS) is  d?(xi,vj) = ||?(xi)? vj ||2. (3) The cluster centers v are linear combinations of the feature vectors,  vj =  ?n l=1 u  m lj?(xl)?n  l=1 u m lj  . (4)  Equation (3) cannot by computed directly, but by using the identity Kij = ?(xi,xj) = ?(xi) ? ?(xj), denoting u?j = u  m j / ?  i |umij | where umj = (um1j , um2j , . . . , umnj)T , and substituting (4) into (3) we get  d?(xi,vj) =  ?n l=1  ?n s=1 u  m lj u  m sj?(xl) ? ?(xs)?n  l=1 u 2m lj  + ?(xi) ? ?(xi)? 2 ?n  l=1 u m lj?(xl) ? ?(xi)?n l=1 u  m lj  =u?Tj Ku?j + e T i Kei ? 2u?Tj Kei  =u?Tj Ku?j +Kii ? 2(u?Tj K)i, (5) where ei is the n-length unit vector with the ith element equal to 1. This formulation of KFCM is equivalent to that proposed     in [29] and, furthermore, is identical to relational FCM [30] if the kernel ?(xi,xk) = ?xi,xj? is used [31].

Equation (5) shows the obvious problem which arises when using kernel clustering with big data: the distance equation?s complexity is quadratic with the number of objects, i.e. O(N2) (assuming the kernel matrix is precomputed). Furthermore, the memory requirement to store K is also quadratic with the number of objects.

A. Weighted KFCM  Assume that each data point in X has a different weight, wi, which represents its influence on the clustering solution.

These weights can be applied to the KFCM objective at (1b) by  Jm(U ;?) = c?  j=1  n? i=1  wiu m ij ||?i ? vj ||2, (6)  where it is now obvious how w ? Rn, wi ? 0, affects the solution of KFCM. Hence, the only difference between KFCM and wKFCM is that the distance at (5) in the iterated updates of (2) are computed with the weights included, i.e.,  dw? (xi,vj) =  ?w ? umj ? (w ? umj )TK(w ? umj ) +Kii  ? 2?w ? umj ? ( (w ? umj )TK  ) i , (7)  where w is the vector of weights and ? indicates the Hadamard product. The idea behind wKFCM will be instrumental in our design of the proposed stKFCM algorithm.



IV. STREAMING KFCM ALGORITHM  Consider a streaming data set X = {X1, X2, . . . , Xt, . . .}, and its projection onto a set of RKHSs HKt , such that ? = {?1, ?2, . . . , ?t, . . .} are the kernel representations of X , where ?t = {?t1, ?t2, . . . , ?tn}, and ?ti = ? (xti) ? HKt . The goal of the proposed stKFCM algorithm is to approximate the clustering solution of KFCM on X , while only having access to a limited number of chunks of X up to some time t. The naive solution is to store all history of X and run KFCM on all the samples. However, the memory requirement for storing the kernel matrix K is O(N2), where N is the number of samples in the history; hence, KFCM is not appropriate for big data, which all streaming data sets become at some point.

The proposed stKFCM algorithm only requires access to Xt and Xt?1 at time step t and only requires O(n2) storage requirement, where n is the size of the data chunk Xt. Figure 1 illustrates the sub-matrices required by the stKFCM algorithm.

In similar spirit to stKFCM, Havens proposed the stream- ing kernel k-means (stKKM) algorithm in [32]. It provides accurate results for using kernel k-means with streaming or incremental data. The main idea of this algorithm is to take the cluster centers V t?1 = {vt?11 , . . . ,vt?1c }, where vt?1i ? HKt?1 , and project them into HKt (the RKHS produced by Kt) as meta-vectors. We will explain meta- vectors in the following section, which proposes stKFCM.

Using these meta-vectors, information is passed from previous  n  n  Kt-1 K(t-1, t)  Kt  N  N  stKFCM only requires storage of 2 (n ? n) matrices at each time step  Fig. 1. The stKFCM algorithm only requires storage of two (n?n) portions of the full (N ?N) kernel matrix at each time-step. The sub-matrix Kt,t?1 is used to project the cluster centers from time (t?1) into the RKHS imposed by the kernel matrix Kt.

time steps into the current time step. Then, at each time step, the data chunk Xt is clustered together with the (appropriately weighted) meta-vectors. We now propose the use of meta- vectors to approximate KFCM with the stKFCM algorithm.

A. Meta-vectors  Assume we are clustering not only the current data chunk Xt, but also a set of meta-vectors A = {a1,a2, ...,an}, ai ? HKt . The meta-vectors ai are linear combinations of all ?ti ? ?t; i.e., aj =  ?n i=1 ?ij?i, ?ij ? R.

Proposition 1. Let the partitions U t and U? denote fuzzy partition values of Xt and A, respectively. Let wXt and w?  be the weights of Xt and A. Since the cluster centers are linear combinations of the feature vectors, the cluster center vtc in the kernel feature space can be written as  vtk = n?  i=1  q?tik, (8)  where  qtk =  ? ???  wXt1 (u t 1k)  m + ?c  j=1 ? t 1jw  ? j (u  ? jk)  m  wXt2 (u t 2k)  m + ?c  j=1 ? t 2jw  ? j (u  ? jk)  m  . . .

wXtn (u  t nk)  m + ?c  j=1 ? t njw  ? j (u  ? jk)  m  ? ??? ; (9a)  q?tk = qtk  |wXt ? (utk)m|+ |w? ? (u?k )m| . (9b)  Proof: The meta-partition q?t at (9) is formed by repre- senting the cluster center vtk as the weighted linear sum of the data chunk ?t (i.e., Xt) and the meta-vectors A, and using the fact that A is a linear sum of ?t itself. See [32] for a more detailed proof of a similar proposition.

Now we show how to project the cluster centers produced at time t?1 (i.e., in HKt?1 ) to the current time t. Note that this     proposition is similar to that proposed in [32] for the use with stkKM; however, it is important to note that the formulation of q?k for KFCM clustering is different from that of kernel k-means.

Proposition 2. A projection of vt?1k ? HKt?1 into HKt can be computed by the optimization over meta-vectors ak,  argmin ak  ||vt?1k ? ak||2 = n?  i=1  ?tik? t i. (10)  Proof: The optimization has the closed form solution of  ?tk = ( Kt )?1  K(t,t?1)q?t?1k , (11)  K(t,t?1) = ?(xti,x t?1 j ), i, j = 1, ..., n. (12)  Remark 1. The closed form solution for the weights ?tk at (11) is the solution to the optimization under the squared Euclidean norm. The inverse operation in (11) is often best replaced, in practice, with a pseudo-inverse, which we denote by (?)?. One could also use a simple gradient descent to minimize the quadratic. Furthermore, this is only one way to project the cluster center vt?1k into the current RKHSHKt . We imagine that an L1-norm optimization could also find utility when a sparser solution for ?tk is desired.

The distances between the cluster center vTk and each of ai, ?ti, and an arbitrary feature vector ?(x) are computed as  ||ai ? vtk||2 = (?ti)TKt?ti + (q?tk)TKtq?tk ? 2(?ti)TKtq?tk; (13a)  ||?ti ? vtk||2 = Ktii + (q?tk)TKtq?tk ? 2(Ktq?tk)i; (13b) ||?(x)? vtk||2 = ?(x,x) + (q?tk)TKtq?tk ? 2?(x, Xt)q?tk.

(13c)  These distances at (13) allow us to propose the stKFCM algorithm at Algorithm 1. The algorithm has the following basic steps:  1) The KFCM solution is computed for the first data chunk; 2) The weights of each cluster center are computed as  the sum of the partition elements associated with each center;  3) The cluster centers are projected into the next time step; 4) The meta-partition q?t is computed; 5) The partition of the meta-vectors A is updated; 6) The partition of the feature vectors Xt is updated; 7) Optionally, the partition of the full data set X can be  computed in one single-pass at the end.

The stKFCM algorithm is essentially a single-pass algorithm that computes the KFCM cluster solution of each Xt together with the weighted meta-vectors A, which are the projected cluster centers from step t ? 1. Hence, the (compressed) information from all previous time-chunks is passed down through the meta-vectors A.

Remark 2. The important projection step at Line 3 of the stKFCM algorithm is equivalent to taking the vectors vt?1  (appended by n zeroes) represented in the RKHS of the kernel matrix  Kt?1,t = [  Kt?1 K(t?1,t)  K(t,t?1) Kt  ] ,  and using the Nystrom approximation to represent them in the low-rank approximation of the kernel matrix computed as (and reordered such that the time t columns are first)  K?t,t?1 = [ Kt|K(t,t?1)  ]T ( Kt )?1 [  Kt|K(t,t?1) ] .

Furthermore, it is known that the error ?Kt,t?1? K?t,t?1?2 ? ?n+1 + O(N/  ? n), where ?n+1 is the (n + 1)th eigenvalue  of Kt,t?1 [33]. Jin et al. [34] also showed that this error is further bounded if there is a large eigengap, which is often the case for data sets that can be partitioned into high-quality clusters. What this shows for the stKFCM algorithm is that the projection error at each step is bounded; hence, the total error is bounded by the size and number of data chunks used to complete the stKFCM process.



V. EXPERIMENTS  We evaluated the performance of our algorithm on data sets for which ground truth exist. In these experiments, we compared the hardened partition from the proposed stKFCM to the the recently proposed Kernel Patch Clustering (KPC) [1], as well as the KFCM partition run on the whole data set.

We present results for different chunk sizes, from 0.0001N to 0.5N . The value of the fuzzifier m was fixed at 1.7. The experiments on the 2D15 and 2D50 were run on a Intel Core 2 Duo core processor with 4 GB of memory. Results of MNIST and Forest data set were generated by a quad-core CPU with 32 GB of memory. The results are expressed as the mean and standard deviation over 100 independent experiments, with random initializations and random data sample ordering.

A. Data sets  1) 2D15: This synthetic data set is composed of 5,000 2- dimensional vectors. As shown in Fig. 2(a), it is obvious that 15 clusters are preferred in this data set. We used an RBF kernel with width of 1 on this data set.

2) 2D50: This data set consists of 7,500 2-dimensional vectors with 50 clusters preferred. An RBF kernel with a width of 1 was used.

3) MNIST: These data were collected from 70,000 28?28 images of handwriting digits from 0 to 9 by the National Insti- tute of Standards and Technology (NIST). We normalized the value of each pixel to the unit interval and organized the pixels column-wise into a single 784-dimensional vector. Therefore, this data set is composed by 70,000 784-dimensional vectors with 10 clusters preferred. An inhomogeneous polynomial kernel with degree of 5 was used in our experiment.

Algorithm 1: Streaming Kernel Fuzzy c-Means (stKFCM) Input: number of clusters ? c; fuzzifier ? m;  X = {X0, X1, X2, . . .}; kernel ? ? Compute K0 = ?(X0, X0)  1 U0 =KFCM(c,m,K0) q?0c = u  c/|u0c |, c = 1, . . . , k  for t = 1, 2, . . . do Kt = ?(Xt, Xt), K(t,t?1) = ?(Xt, Xt?1) for k = 1 to c do  2 wk = |qt?1k | 3 ?tk = (K  t)?K(t,t?1)q?(t?1)k u?ik = 1, if i = k, else u  ? ik = 0, U  t = [0]n?c  while any u?ij or utij changes do 4 Compute q?tk with (9)  for i, j = 1, . . . , c do  5 u?ij =  ? ? c? k=1  ( d?(ai,v  t j)  d?(ai,vtk)  ) 1 m?1  ? ? ?1  where d?(ai,vtj) is computed with (13a)  for i = 1, . . . , n, j = 1, . . . , c do  6 utij =  ? ? c? k=1  ( d?(x  t i,v  t j)  d?(xti,v t k)  ) 1 m?1  ? ? ?1  where d?(xti,v t j) is computed with (13b)  Compute q?tk with (9)  7 Optional extension: The partition of the full data set X is computed by the following steps.

for i = 1, . . . , N , j = 1, . . . , c do  uij =  ? ? c? k=1  ( d?(xi,v  t j)  d?(xi,vtk)  ) 1 m?1  ? ? ?1  where d?(xi,vtj) is computed by (13c).

4) Forest: This data set is composed of 581,012 carto- graphic variables that were collected by the United States Ge- ological Survey and United State Forest Service (USFS) data.

There are 10 quantitative variables and 44 binary variables.

These features were collected from a total of 581,012 30?30 meter cells of the forest, which were then determined to be one of 7 forest cover types by the USFS. We normalized the features to the unit interval by subtracting the minimum and then dividing by the subsequent maximum. We used the RBF kernel with a width of 1.

B. Evaluation criteria  1) Adjusted Rand Index: Rand index is one of the most popular comparison indices of measuring agreement between two crisp partitions of a data set. It is the ratio of pairs of agreement to the number of pairs. The Adjusted Rand Index (ARI) we are using here is a bias-adjusted formulation  0 0.2 0.4 0.6 0.8 1  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9   Feature 1  F ea  tu re   (a) 2D15  0 0.2 0.4 0.6 0.8 1  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9   Feature 1  F ea  tu re   (b) 2D50  Fig. 2. Synthetic data sets  developed by Hubert and Arabie [35]. The result is a number between 0 and 1 where 1 indicates perfect match. In order to compute ARI, we first harden the fuzzy partition and then compare it with the ground-truth partition.

2) Purity: Purity, also called clustering accuracy, is an external validation measure to evaluate the quality of the clustering solution. The purity of each cluster is given by the ratio between the amount of right assignments in this cluster and the size of the cluster. The purity of the clustering solution is then expressed as a weighted sum of the individual purities.

Thus, the purity is a real number between 0 and 1. The larger the purity, the better the performance.

3) Run time: Our algorithm could be used either as an approximation for unloadable data or acceleration for load- able data. Thus, time consumption is a crucial criteria. We compared times to compute the partition matrix with different chunk size as well as to KFCM run on the entire data set. All the times are recorded in seconds.

Table II contains the results of our experiments. On the 2D15 and 2D50 data sets, both KPC and stKFCM are success- ful at finding the preferred partitions. However, the stKFCM shows better results than KPC on the 2D50 data set, equaling the performance of KFCM down to the 2% chunk size.

Furthermore, stKFCM is much faster than KPC at small data chunk sizes for 2D15 and 2D50. This is because the KFCM iterations at each chunk converge faster with the stKFCM     algorithm. Note that KPC is faster than stKFCM for larger chunks; this is because of the inverse calculation at Line 3 of stKFCM. Both KPC and stKFCM produce very good speedup over KFCM at small data chunk sizes, while still producing partitions nearly equivalent to the literal KFCM.

All three fuzzy clustering algorithms produce partitions that do not match well to the ground-truth for the MNIST and Forest data sets (which is also the case of k-means and other similar crisp partitioning algorithms). This does not alarm us as there is a big difference between classification and clustering results; i.e., classification uses user-supplied labels (a.k.a. ground truth), while clustering aims to find natural groupings. Hence, these results simply tell us that the natural groupings in these two data sets (as produced by c- means partitioning) do not match well to the ground truth labels. The aim of our proposed algorithm is to approximate the partitions of the KFCM for large data sets. And both KPC and stKFCM succeed at that for the MNIST data. The stKFCM algorithm exceeds the performance of KFCM for all chunk sizes, except for 0.02%, while KPC meets or exceeds the KFCM performance for chunk sizes > 0.5%; clearly, the stKFCM outperforms the KPC algorithm for the MNIST data. We have seen this behavior, i.e., the sampled algorithm exceeding the performance of the literal algorithm, in other studies [8, 15] and attribute it to the influence of outliers or noise on the literal algorithm. This hypothesis has not been proved.

On the Forest data, both KPC and stKFCM struggle to match the performance of KFCM (run on the entire data set) in terms of the ARI criteria; however, in terms of Purity both KPC and stKFCM perform fairly, with KPC having a slight edge here. We believe that this is caused by the fact that the classes in the Forest data have very unbalanced numbers of samples; two of the seven classes, Spruce-Fir and Lodgepole Pine, comprise greater than 85% of the data set. Hence, the KPC and stKFCM algorithms, which sequentially operate on small samples of the data set, can be presented with samples that are comprised mostly of these two classes.

Overall, these results are very pleasing as they show that stKFCM, even with very small data chunk sizes, achieves clustering performance near to that of KFCM run on the entire data set. Furthermore, the projection method used in stKFCM shows better performance than the medoid method used by KPC, even showing better run-time for some data sets.



VI. CONCLUSION  Big data analysis has become very pertinent. It creates many wonderful opportunities accompanied by numerous big chal- lenges. Both memory space and computational resources are important considerations in real life applications. In this paper, we proposed the stKFCM algorithm that significantly reduces both the memory requirement and computational complexity for performing KFCM clustering. Empirical results show that stKFCM achieves accurate results while only requiring access to very small portion of the kernel matrix.

In the future, we will examine how we can better represent and pass on the information from previous data chunks, including using hybrids of random sampling and projection methods. We will also look at other methods of projection, with a focus on overall clustering performance.

