An innovative algorithm for clustering retail items

Abstract? Dendrogram for clustering retail items is a tool used  in customer relationship management (CRM) for  implementing homogeneous schemes for all the items in one  cluster. In agglomerative hierarchical clustering, dendrograms  are developed based on the concept of ?distance? between the  entities or, groups of entities. These entities may be the  customers, retail items, business units etc. as per the business  problem. The present paper focuses on agglomerative  hierarchical clustering of retail items based on the concept of  ?similarity? of the items which is regarded as the inverse of  ?distance? (or ?dissimilarity?). Similarity is measured based on  the strength of association rule/s containing the candidate  items to be merged. An algorithm has been proposed for  developing dendrogram and it has been explained with an  example in retail sale. The algorithm overcomes the negative  impact of negative association between the items on the quality  of clustering.

Keywords- association rule; clustering; CRM; dendrogram;  data mining

I. INTRODUCTION  In the recent years, the primary task in business analytics of customer relationship management (CRM) has been to integrate ?relationship technology (i.e. data consolidating and data mining)? with ?loyalty schemes?. CRM is expected to enhance value to customers through raising satisfaction levels on transactions. If customers appreciate the value provided by a scheme in CRM, they are expected to continuously enhance the relationship with the firm involved through loyalty to the products/brands, purchasing more, advocating the firm to others, etc. With the advent of data mining technology, cluster analysis of items is frequently done in supermarkets and in other large-scale retail sectors.

Clustering of items has been a popular tool for identification of different groups of items where appropriate programs in CRM are designed for each group separately with maximum effectiveness and return. For example, items frequently purchased together are placed in one place in the shelf of a retail store. A discount-oriented promotional offer is designed for a group of items with incidence of high association in purchase. Such clustering of items is useful in operations other than CRM also. One such illustration is seen in clustering of retail items for the purpose of inventory control or, placing joint replenishment order.

Cluster analysis or clustering assigns a set of entities or  observations or objects into subsets so that observations in  the same cluster are similar in some sense. These subsets are  called clusters. Clustering is a method of unsupervised  learning, and a common technique for statistical data  analysis used in many fields, including machine learning,  data mining, pattern recognition, image analysis and  bioinformatics. There are various algorithms used for  clustering. Hierarchical algorithms find successive clusters  using previously established clusters. These algorithms  usually are either agglomerative ("bottom-up") or divisive  ("top-down"). Agglomerative algorithms begin with each  element as a separate cluster and merge them into  successively larger clusters. Agglomerative hierarchical  clustering creates a hierarchy of clusters which may be  represented in a tree structure called a dendrogram. The root  of the tree consists of a single cluster containing all  observations, and the leaves correspond to individual  observations. Any valid metric may be used as a measure of  similarity between pairs of observations. The choice of  which clusters to merge or split is determined by a linkage  criterion, which is a function of the pair-wise distances  between observations. Different clusters are obtained at  different levels of the tree diagram of dendrogram. This  gives an opportunity to compare the performance of various  clustering in different levels with respect to a selected  performance criterion. In CRM, the performance of  clustering may be evaluated based on total profit, customer  retention, total sale etc. Inventory managers may choose  total relevant inventory cost as a parameter for analyzing the  performance of clustering at various levels.

An important step in any clustering is to select a distance  measure, which will determine how the similarity of two elements is calculated. This will influence the shape of the  clusters, as some elements may be close to one another  according to one distance metric and farther away according  to another distance metric. For example, in a 2-dimensional  space, the distance between the point (x = 1, y = 0) and the  origin (x = 0, y = 0) is always 1 according to the usual norms, but the distance between the point (x = 1, y = 1) and  the origin can be 2, 2 or 1 if you take respectively the 1-  norm (Manhattan distance), 2-norm (Euclidean distance) or  infinity-norm (Maximum norm) distance. The Mahalanobis  distance corrects data for different scales and correlations in  the variables. Another approach is based on inner product or  scalar product where the angle between two vectors can be  used as a distance measure when clustering high     dimensional data. The Hamming distance measures the  minimum number of substitutions required to change one  member into another. Another important distinction is  whether the clustering uses symmetric or asymmetric  distances. Many of the distance functions discussed here  have the property that distances are symmetric (the distance  from object A to B is the same as the distance from B to A).

In other applications, e.g., sequence-alignment methods in  [1], this is not the case. A matrix populated with both upper  and lower triangles gives complete measures of distance.

The concept of ?distance? for the purpose of clustering  has been extended to various other aspects for measuring  similarity or dissimilarity between the items. The work in  the current research paper gives a novel algorithm for  agglomerative hierarchical clustering based on the strength  of association rule within the items.



II. AGGLOMERATIVE HIERARCHICAL CLUSTERING AND  DEDNDROGRAM  Agglomerative hierarchical clustering method builds the  hierarchy from the individual elements by progressively  merging clusters that operates in a bottom-up fashion [2]. In  the example given in fig. 1, we have six elements {a}, {b},  {c}, {d}, {e} and {f} in the Euclidean field. Clustering is to  be done on the basis of Euclidean distance of similarity. The  first step is to determine which elements to merge in a  cluster. Usually, we want to take the two closest elements,  according to the chosen distance metric. Based on Euclidean  distance metric, the dendrogram is given in fig. 2 which has  been developed in four steps. In step-1, b and c form one  cluster and similarly, d and e form one cluster and hence,  there are four clusters obtained, viz. {a} {b, c} {d, e} and  {f}. Similarly in the subsequent steps, merger happens till  all fall in one cluster.

Fig.1: Objects for clustering in Euclidean Field  Fig.2: Hierarchical clustering dendrogram

III. STRENGTH OF ASSOCIATION RULE AS A BASIS OF  CLUSTERING  Association rule is a type of data mining rule or pattern  that correlates one set of items or events with another set of  items or events. An association rule must satisfy the  minimum values of support, confidence and lift which are  measures of strength of a rule. Use of data mining algorithm  in [3] helps in finding frequent itemsets efficiently and  hence in mining association rules efficiently from sale  transaction data or, order list data. There are other data  mining algorithms also to find frequent itemsets for mining  association rules [4]. With given minimum threshold values  of support, confidence and lift, all possible association rules  can be mined. An association rule is represented in the form,  X => Y (read as ?X associates Y?) where X and Y are two  mutually exclusive sets of items or events. X is known as  antecedent and Y is known as consequent.  ?Support? of an  itemset in a transaction database is defined as the  percetntage of occurrence of the itemset, out of all the  transactions.  X=>Y holds with support s%, if s% of the  transactions in the database contain ?X? and ?Y? both.

?Confidence? of an association rule X=>Y is defined as the  percentage of transactions containing X and Y both, out of  all the transactions containing X. X=>Y holds with  confidence c%,   if c% of the transaction in the database that  supports ?X? also supports ?Y?. ?Lift? of an association rule  X=>Y is computed as the confidence of the rule divided by  the confidence assuming independence of consequent from  antecedent, i.e., lift = (percentage confidence of the  rule)/(percentage support of Y).  A lift ratio greater than 1.0     suggests that the rule is positive and hence, X gives lift to Y.

Lift value less than 1.0 implies a negative association rule  and hence, implication is that  purchase of X reduces  purchase of Y. For most of the uses, association rules can  not be directly applied in business decision-making.

The work in [5] gives an association clustering method for  grouping of inventory items for designing an optimum can-  order inventory policy. The method can be used to develop  dendrogram for agglomerative hierarchical clustering with  ?support? of an itemset as the measure of ?similarity (seen as  inverse of ?distance?)? between or amongst the items.

Support of an itemset, as defined earlier, is the percetntage  of occurrence of the itemset, out of all the transactions of  order list or in sale. The paper in [5] considers a list of items  from order list for clustering and does not consider  confidence and lift in the similarity measure. Lift gives a  meaningful interpretation in terms of positive and negative  association. If we do not address the issue of lift, there may  come many items in one cluster with negative association  effect which is not desirable for the clusters. Moreover,  confidence is an important measure of strength of  association between two itemsets. Hence, it is expected that  confidence as a measure of strength of association must find  place while clustering the items.



IV. ASSOCIATION RULE-BASED CLUSTERING  As we understand, strength of association is described with three relevant measures, viz., rule support, confidence and lift, due regard must be given to all three while clustering of items based on their association. Moreover, negative association rule suggests that item associated are not purchased together. In view of this, items correlated by negative association rule are not to be clustered in one group if we are clustering items based on the coincidence of purchase in the same market basket. In the following sub- section, implication of negative association rule has been explained.

A. Implication of Negative Association Rule  Positive association rules are generally mined with the threshold values of support and confidence . However, this may be misleading sometimes which is explained below with the example given in table 1. Table 1 shows the number of transactions with respect to presence and absence of two items, X and Y in a transaction.  For the association rule, ?X => Y?, where, X is called antecedent and Y is called consequent, support and confidence are computed as given below.

Support of the rule = (8,000/20,000)*100 % = 40%  Confidence of the rule = (8,000/12,000)*100% =  66.67%  Support value of 40% and confidence value of 66.67% are  reasonably good for the rule, ?X = Y?, to qualify for being  called a valid association rule. An association rule of the  form, ?X => Y?, implies that purchase of X causes purchase  of Y in many transactions and it happens in certain  percentage of transactions which is given by the confidence  value of the rule.

Table 1. Illustration of Negative Association Rule  X is  present  X is  absent row  Y is  present 8,000 7,000 15,000  Y is  absent 4,000 1,000 5,000  column 12,000 8,000 20,000  However, if we analyze further, it is observed that X in  fact inhibits purchase of Y. The reality is that item, Y, is  purchased on its own in most of the transactions with a very  high support of 75% (in 15,000 transactions out of total of  20,000 transactions). However, when X is purchased, Y is  purchased only in 66.67% of those transactions. As 66.67%  is less than 75%, it is interpreted that X inhibits the purchase  of Y. This fact is captured by ?lift?. Lift of the association  rule, ?X => Y? is given by the expression, ?(Confidence of  the rule)/(Support of the consequent)?. For the example  under discussion, lift is 0.89, i.e., (66.67%)/(75%). It is  evident that if the confidence of the rule is less than the  support of Y, the lift value is less than?1? and it implies that  X inhibits purchase of Y and it is a negative association  rule.

Positive association rules are with lift value of more than  ?1? and a lift value of less than ?1? implies a negative  association rule [4]. According to [5], when lift is less than  1, negating the rule produces a better rule. If the rule, ?IF B  an C THEN A?, has a lift value less than ?1?, then the rule,  ?IF B an C THEN not A? is a better rule.

B. Details of the proposed Clustering Tehniques  The present work in this paper complements the existing literature on the technique of association clustering. The proposed technique in this work considers only the positive associations for clustering. There are two steps in this technique. First step is mining association rules with threshold value of ?1.00? for lift. Suitable threshold values for support and confidence are also chosen. In the second step, these association rules along with the support, confidence and lift are taken as input for clustering. Hence, this step gives an algorithm for developing the dendrogram.

As the proposed algorithm for clustering makes use of ?all strength measures of association rule/s? in stead of only     ?support of itemset?, the proposed algorithm has been named as ?association rule based clustering?.

As discussed, there are two phases involved in the  proposed clustering technique as mentioned below.

Phase-I: Mine association rules from the transaction data  with some threshold values of rule support and confidence  with lift more than 1.00. The threshold values of rule  support and confidence are chosen with low values so that  all the items under consideration find place at least in one  rule. Amongst these rules, we will select only the positive  rules, i.e., the rules with lift value of greater than 1.00.

The input and output in this step are as given below.

Input:  I = {I1, I2,??,Im}      //set o items  D = {t1, t2,???tn}  //a transaction database with ti as one transaction  Threshold rule support = s  Threshold rule confidence = c  Threshold lift = 1.00  Output:  A = { {r1, s1, c1, l1}, {r2, s2, c2, l2},??,{ rn, sn, cn, ln}}  // set of positive association rules (ri)  with rule support (si) , confidence (ci) and lift (li)  Phase-II: Make use of the output in phase-1 for  developing the dendrogram. The algorithm is given in fig.3.

Key steps involved in this phase are described below.

Step 1: Input to this phase is the output of phase-I, i.e., the  set of association rules with the values of rule support,  confidence and lift, given by set A.

Step 2: Obtain the set of individual items present in at least  one of the association rules in A by I = {I1, I2,??,Im}.

Step 3: Start with tree level s initiated as 1, the itemset  similarity is defined as very high value (tending to infinity),  and number of clusters (itemsets) is m (the total number of  itemsin I). Hence, the set of clusters at level 1, L {1}  , contains  all 1-item clusters in I. That is, L {1}  = {{I1},  {I2},???.,{Im}}.

Step 4: To generate a set of candidate itemsets for next level  (C (s+1)  ) each pair of itemsets in the previous level are joined.

Step 5: To evaluate itemset similarity, i.e., similarity  amongst the items in a cluster, each of the association rules  is checked if all the items in the candidate set exist in the  rule (either in the antecedent or in the consequent). If all the  items exist in a rule and no other item is present in the rule,  then sum up rule support, confidence and lift for the rule.

Similarly, sums are obtained for all other rules where all the  items are present in a rule. Sum of all such sums is taken as  the measure of similarity.

Step 6: To generate L (s+1)  (i.e.,  the set of itemsets in level  (s+1)), the two itemsets are merged if their similarity is the  highest value among all itemsets in C (s+1)  .  Hence, L {s+1}  =  {L {s}  - La {s}  - Lb {s}  }U{La {s}  U Lb {s}  }.

Input: A = { {r1, s1, c1, l1}, {r2, s2, c2, l2},??,{ rn, sn,  cn, ln}}// set of  positive association rules (ri)  with rule support (si) , confidence (ci) and lift (li)  Output: DE  / Dendrogram: a set of ordered tuples  Association rule based clustering algorithm:  I = {I1, I2,??,Im}      //set of individual items present in  at least one of the association rules in A  s =1;              // the tree level  sim = Infinity;            // the itemset similarity (inversely proportional to distance) is very high  k = m;         // the total number of itemsets (clusters) at  tree level L  L{s} = {{I1}, {I2},???.,{Im}};    // the set of itemsets  (clusters) at level s  DE =  <s, sim, k, L{s}>; for (s=1; k=1; s++)   //generate all candidate itemsets  (join step)  for each itemset Li {s} ? L{s}  // ? is for ?belongs to?  for each itemset Lj {s} ? {L{s} - Li  {s} }  for each association rule ri where all items in Li  {s} and Lj {s} exist either in  antecedent or in consequent of a rule  and no other item is present in the rule  {  [sim]i = [rule support]i + [confidence]i + [lift]i // the  itemset similarity equals the sum of rule support, confidence and lift  [sim]i++;  sim{Li {s}, Lj  {s}} =  [sim]i++;  // the similarity for Li {s}  and Lj {s}  { La {s}, Lb  {s}} = arg max sim{Li {s}, Lj  {s}};  Li {s}? {L{s}, Lj  {s}? L{s} // the two clusters with the highest similarity  L{s+1} = {L{s} - La {s}- Lb  {s}}U{La {s} U Lb  {s}}; // the set of  itemsets (clusters) in the next level  sim = sim{La {s}, Lb  {s}; k = k-1; L{s} = L{ s+1};  //update  the parameters DE = DE U <s, sim, k, L{s}>;  }  return DE;  Fig.3: The pseudo-code of the association rule based  clustering algorithm  Step 7: The steps 4-6 are iterated with updating the  dendrogram (DE) by adding the tuple <s, sim, k, L {s}  > into  DE where s =s+1, sim = sim{La {s}  , Lb {s}  ,  k = k-1,  L {s}  = L {  s+1} .  Itereation stops when there is no association rule with  all items of any pair of combined clusters/itemsets in a level  and this level is the last level of clustering. Hence, all items  may not be merged in one cluster as per the proposed  algotithm in most of the cases.



V. APPLYING THE PROPOSED CLUSTERING TECHNIQUE  The proposed technique of clustering was applied on the sale transaction data of nine grocery items with 10,000 sales transactions (at least one of the selected nine items is present in each transaction). Moreover, the customers have been chosen who are loyal to a store and hence, not moving from one store to another. This is guaranteed to a great extent by the location of the stores chosen where no other store is located nearby. Association rules were mined from the transaction data with different threshold values of rule support and confidence. It was found that for maximum threshold values of 0.20 for rule support and 0.65 for confidence, all nine items found place. With threshold values above this, it was found that at least one item was missing.

For the chosen threshold values of rule support and confidence, only 15 association rules were found to qualify as given in table 2.

Table 2. Association rules mined covering all the items  Rule  # Antecedent  Consequent  Rule  Support Confidence Lift Sum   Noodles ? Maggi  (200 g). and Tomato  Sauce (Maggi) (200  g).

MDH Chicken  Masala.

0.23 0.93 2.76 3.92  MDH Chicken  Masala.

Noodles ? Maggi  (200 g). and  Tomato Sau  0.23 0.68 2.76 3.67   MDH Chicken  Masala. and  Noodles ? Maggi  (200 g).

Tomato Sauce  (Maggi) (200 g).

0.23 0.87 2.57 3.67  Tomato Sauce  (Maggi) (200 g).

MDH Chicken  Masala. and  Noodles ? Maggi  0.23 0.68 2.57 3.48  5 Kismis (100 g). Basmati (1 kg). 0.21 0.71 1.25 2.17  Tomato Sauce  (Maggi) (200 g).

MDH Chicken  Masala.

0.25 0.74 2.19 3.18  MDH Chicken  Masala.

Tomato Sauce  (Maggi) (200 g).

0.25 0.75 2.19 3.19  Coffee ? Nescafe  (50 g).

Bournvita -  Cadbury (500 g).

0.23 0.67 1.32 2.22   MDH Chicken  Masala. and Tomato  Sauce (Maggi) (200  g).

Noodles ? Maggi  (200 g).

0.23 0.92 1.57 2.72   Poha (Chura) (1  kg). and Sooji (1  kg).

Noodles ? Maggi  (200 g).

0.20 0.89 1.52 2.61  Poha (Chura) (1  kg).

Noodles ? Maggi  (200 g).

0.34 0.88 1.5 2.72  12 Sooji (1 kg).

Noodles ? Maggi  (200 g).

0.35 0.85 1.46 2.66  MDH Chicken  Masala.

Noodles ? Maggi  (200 g).

0.26 0.78 1.35 2.40  Tomato Sauce  (Maggi) (200 g).

Noodles ? Maggi  (200 g).

0.25 0.73 1.25 2.23  15 Sooji (1 kg). Basmati (1 kg). 0.37 0.72 0.85 1.94  All nine items find place in one or more rules, either in the antecedent or in the consequent. The rules are given with the value of rule support, confidence and lift whereas the last  column is the sum of these three measures of strength of association. Amongst these 15 rules, 14 rules were positive, i.e., with lift of more than 1.00 and only one rule was negative, i.e., with lift value less than 1.00.

For applying the proposed technique for clustering of the  items, we consider only the positive association rules (i.e.,  with lift > 1.00). In 14 positive association rules, nine items  (as selected for the purpose of clustering) are as given  below.

Basmati (1 kg), Bournvita - Cadbury (500 g), Coffee ?  Nescafe (50 g),  Kismis (100 g),  MDH Chicken Masala,  Noodles ? Maggi (200 g), Poha (Chura) (1 kg),  Sooji (1  kg), Tomato Sauce (Maggi) (200 g).

As per the proposed technique, in level 1 of the  dendrogram, all individual items will be differen clusters as  given below.

Level 1 (9 itemsets/clusters): {Basmati (1 kg)},  {Bournvita - Cadbury (500 g)}, {Coffee ? Nescafe (50 g)},  {Kismis (100 g)}, {MDH Chicken Masala}, {Noodles ?  Maggi (200 g)}, {Poha (Chura) (1 kg)}, {Sooji (1 kg)},  {Tomato Sauce (Maggi) (200 g)}  {MDH Chicken Masala} and {Tomato Sauce (Maggi)  (200 g)}are merged in level 2 as the similarity is highest for  these two itemsets which is obtained from rules # 6 and #7.

Each of these two rules contains only the two items, i.e.,  {MDH Chicken Masala} and {Tomato Sauce (Maggi) (200  g)}. For rule#6, sum is 3.18 and for rule#7, the sum is 3.19  and total is (3.18 +3.19), i.e., 6.37. This similarity of 6.37,  as given by the sum of sums is higher than the similarity  between any two itemsets. Hence, these two items are  merged in level 2 as given below.

Level 2 (8 itemsets/clusters):  {Basmati (1 kg)},  {Bournvita - Cadbury (500 g)}, {Coffee ? Nescafe (50 g)},  {Kismis (100 g)}, {MDH Chicken Masala, Tomato Sauce  (Maggi) (200 g)}, {Noodles ? Maggi (200 g)}, {Poha  (Chura) (1 kg)}, {Sooji (1 kg)}  Adopting the similar logic, we obtain the clusters in  different levels as given below.

Level 3 (7 itemsets/clusters):  {Basmati (1 kg)},  {Bournvita - Cadbury (500 g)}, {Coffee ? Nescafe (50 g)},  {Kismis (100 g)}, {MDH Chicken Masala, Tomato Sauce  (Maggi) (200 g), Noodles ? Maggi (200 g)}, {Poha (Chura)  (1 kg)}, {Sooji (1 kg)}  Level 4 (6 itemsets/clusters):  {Basmati (1 kg)},  {Bournvita - Cadbury (500 g), Coffee ? Nescafe (50 g)},  {Kismis (100 g)}, {MDH Chicken Masala, Tomato Sauce  (Maggi) (200 g), Noodles ? Maggi (200 g)}, {Poha (Chura)  (1 kg)}, {Sooji (1 kg)}  Level 5 (5 itemsets/clusters):  {Basmati (1 kg), Kismis  (100 g)}, {Bournvita - Cadbury (500 g), Coffee ? Nescafe  (50 g)}, {MDH Chicken Masala, Tomato Sauce (Maggi)  (200 g), Noodles ? Maggi (200 g)}, {Poha (Chura) (1 kg)},  {Sooji (1 kg)}     Level 5 of the dendrogram with 5 clusters is the last level  as there is not possibility of further clustering as no  association rule exists in the list for further clustering.



VI. CONCLUSION  If we apply the algorithm of [5] for clustering which is  based on only the strength of the rule support, in level 2,  Basmati (1 kg) and Sooji (1 kg) are merged in one cluster as  the distance (= 1- rule support) between them is minimum if  we compare it amongst the distances between all the pairs of  items. However, these two items are correlated by a  negative association rule with lift value of 0.85 (which is  less than 1.00). As per the interpretation of negative  association rule, the antecedent item inhibits the purchase of  the consequent item. With the objective of clustering items  which are purchased together, this pair should not find place  in any level of clustering in the dendrogram. Moreover, if  Basmati (1 kg) and Sooji (1 kg) are put in one cluster, the  former item (i.e., Basmati (1 kg)) no more gets an  opportunity to be in one cluster with Kismis (100 g),  whereas Kismis (100 g) and Basmati (1 kg) are positively  associated as given in rule#5. The proposed algorithm in  this paper is designed to avoid such undesirable clustering  in the dendrogram. As CRM makes an attempt to design  homogeneous strategy for all the items in a cluster, items  associated by negative rule in the same cluster will have a  negative effect on the objectives of a program.

Further work on devising more appropriate utility based  clustering may be done as an extension of this work. The  dendrogram can be used in designing promotional offer of  itemsets/clusters obtained in various levels where based on a  performance indicator, the best level of clustering can be  identified for implementation. The same can be used for  designing joint inventory replenishment policies with  minimum total relevant cost.

