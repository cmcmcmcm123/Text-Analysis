Large Scale Predictive Analytics for Real-Time  Energy Management

Abstract ? As demand for cost-effective energy and resource management continues to grow, intelligent automated building solutions are necessary to reduce energy consumption, increase alternative energy sources, reduce operational costs and find interoperable solutions that integrate with legacy equipment without massive investments in new equipment and tools. The ability to analyze, understand and predict building behavior offer tremendous opportunities to demonstrate and validate increased energy efficiencies, which may ease many particular exorbitant pressures taxing the grid. In this paper, we describe a research platform driven by an existing campus microgrid for developing large scale, predictive analytics for real-time energy management.

Keywords?data mining, smart grid, big data, time series.



I. INTRODUCTION Advances in technology have led to an unprecedented  ability to collect and store data. Harnessing the real-time streaming sensor data from modern electric microgrids and smart grids via advanced processing, modeling, optimization, real-time forecasting and analytics is a major challenge due to the sheer volume, complexity, and rate of acquisition. Managing and controlling actual energy delivery is an optimization and prediction problem that depends on many factors, some of which are present in monitoring within the grid itself, and some of which are only available outside the system, such as weather, residents? behavior and economics.

University campuses offer a comprehensive setting to deploy a microgrid and to maximize its operational benefits.

In addition, it offers a unique assemblage of both intellectual and physical resources, including cutting-edge analytics and high performance computing resource needed to successfully analyze this vast amount of continually produced heterogeneous stream data. We present a forward thinking, innovation engine employing novel time series prediction algorithms to improve operational efficiency; lower operating costs, and reduces the overall carbon footprint of the microgrid.



II. SMART GRID  A. Smart Grid A smart grid is a modernized electrical grid that uses  information and communications technology to gather and act on information about the behaviors of suppliers and consumers, in an automated fashion.  The goal of the smart grid is to improve the efficiency, reliability, economics, and sustainability of the production and distribution of electricity [1]. The present-day electric grid was originally built in the 1890s and improved upon as technology advanced through each decade. Today, it consists of more than 9,200 electric generating units with more than one million megawatts of generating capacity connected to more than 300,000 miles of transmission lines [2].  The digital technology enabling two-way communication between the utility and its customers, and the sensing along the transmission lines is what makes the grid smart.

Similarly to the Internet, the Smart Grid consists of controls, computers, automation, new technologies and equipment working together.  In the case of the smart grid, these technologies need to work with the electrical grid to respond digitally to quickly changing electric demand.  The predictive power of the time series methods is of critical importance for enabling an efficient smart grid performance.

B. UCSD Microgrid Environment The University of California, San Diego (UCSD) is the  owner-operator of a 45 MW peak load Smart Grid and one of the first adopters of many new technologies, including multiple renewable and non-renewable energy generation resources, significant energy storage, and sophisticated monitoring for controlling flex-demand loads for a community of 54,000 energy consumers with an effectually, insatiable demand. With an aim of nurturing consumer adoption of energy-efficient vehicles, UCSD has made considerable progress in a adopting a campus-wide, electric-vehicle (EV) fleet that includes providing direct-to- solar plant connections and storage for solar plant power production [3]. UCSD?s 45 MW Microgrid includes a master controller and optimization system that self      generates 92% of its own annual electricity load and 95% of its heating and cooling load. It also owns and maintains a 69 kV substation, ninety-six 12kV underground feeder circuits and four 12kV distribution substations throughout the 1200-acre campus.

C. System Architecture Current smart grid integration has been conducted  through a public-private partnership consisting of UCSD researchers and staff, and the resources of Power Analytics? and OSISoft?s hardware, software and expertise combined to create a smart grid master controller. In collaboration with our industry partners, UCSD has developed a microgrid master controller that can monitor and control real time operations of the microgrid, and conduct power system analysis to verify reliability constraints for the planning and operation of the microgrid. Integration of predictive analytics with the smart grid master controller?s optimization and scheduling capabilities is dramatically improving the energy management team?s ability to optimize indigenous resources and import energy, as well as export surpluses and shed loads when lucrative capacity, energy and ancillary price signals are received.  UCSD now saves more than $800,000 per month through use of its own system microgrid generation when compared to the alternative of being a direct access customer importing from the grid.

The UCSD microgrid power system and building facilities are highly instrumented, and currently monitor approximately 84,000 data streams per second and is designed for expeditious integration of distributed energy resources (DER). The microgrid controller is integrated with OSI Soft?s PI data server on campus. Data collection and data analysis techniques are centrally managed by the on campus PI servers, which are interfaced with the Power Analytics? microgrid controller. The UCSD microgrid also has one of the nation?s largest academia installations of synchrophasors (phasor measurement units [PMUs]) for data collection and data processing, which includes Gordon?s data intensive capabilities. Ultimately, the microgrid controller is also expected to utilize this data to provide the capability to operate the UCSD microgrid in an islanded condition. Local and national government as well as utility entities are collaboratively engaged to utilize the UCSD microgrid to improve management and efficiencies of the utilities and statewide grid operations such as demand response, excess generation, renewable supply, load balancing and power outages.

D. Microgrid Data Collection Data collection and analysis software is centrally  managed through eight servers interfaced with the Power Analytics? master controlle. Two servers are dedicated to OSISoft interfaces; two are web servers for Power Anralytics master controller and OSISoft web services; one is an OSISoft PI server, and one is hosting Viridity?s pro Viridity?s software. See Figure 1.

Figure 1. UCSD Microgrid Platform  Since February 2011, the UCSD campus data acquisition system has collected continuous measurements from the heating, ventilation and air conditioning (HVAC) system at one-minute intervals. They include more than 17,000 measurements from the network simulation model, nearly 10,000 additional measurements from electric power meters, and a large number of dependent variables computed in real time. The UCSD microgrid power system and building facilities are highly instrumented, and currently monitor approximately 84,000 data streams per second and is designed for expeditious integration of distributed energy resources (DER).  The steady state archive writes exceed 500,000 events per second. The simultaneous read rate is over 20,000,000 events per second and growing as more assets and more powerful sensors are added to the system daily. This very large, complex data holds enormous potential for discovering patterns and forecasts that can create significant energy savings.  Advanced forecasting and ?big data? analytics techniques are employed in order to realize real improvements in energy efficiency and reductions in the energy cost of the campus microgrid, as presented below.



III. BIG DATA PROCESSING POWER  A. Computing Architecture The SDSC?s Gordon system was specifically designed  for data-intensive workloads and careful attention was paid to data movement and capacity at all levels of the memory hierarchy. Each dual socket node contains two eight-core Intel Sandy Bridge processors and 64 GB of DDR3-1333 memory. With four channels per socket, the Gordon compute nodes have a theoretical peak memory bandwidth of 85 GB/s. Groups of sixteen compute nodes are connected to pairs of 36 port switches using QDR InfiniBand (40 Gb/s) and these switches are then networked into a 4x4x4 dual rail 3D torus. Each switch is also connected to an I/O node, which serves both as a router to the Lustre parallel file system and houses sixteen 300 GB Intel 710 Series (Lyndonville) SSDs. Pairs of bonded 10 GbE connections from the I/O nodes to the Lustre Object Storage Servers provide an aggregate bandwidth of up to 100 GB/s into Gordon.

The decision to place the SSDs in the I/O nodes and export using the iSER network protocol rather than physically installing on the compute nodes allows for significant flexibility in their deployment. For the majority of jobs using Gordon?s flash-based memory, one SSD is exported to each compute node and made available to user jobs as a 300 GB scratch file system. In some instances though, all sixteen SSDs are exported to a single ?big flash? compute node and combined into a single RAID 0 device with a raw capacity of 4.8 TB [4].

The other novel data intensive feature of Gordon is the use of ScaleMP?s vSMP foundation software to aggregate multiple compute nodes and the corresponding I/O nodes into large shared memory nodes. Although data exchange between physical nodes still takes place via the underlying IB network, the user is presented with a logical shared address space. The basic building block for the vSMP nodes is 16 compute nodes and one I/O node, with each unit contributing nearly 1 TB of DRAM and 4.8 TB of flash. Gordon currently has four of these 16+1 vSMP nodes in production, but has the flexibility to deploy more as needed, including 32+2 or larger configurations.

B. Hadoop Cluster Path SDSC has deployed a persistent 16-node Hadoop  cluster with the Hadoop file system (HDFS) stored on the flash drives of the Gordon supercomputer [5].

All Gordon users have temporary access to flash storage. Typically, at the start of a job, the Torque prologue creates a temporary directory on the flash file system that is available for the duration of the job and deleted once the job finishes. For projects requiring Hadoop clusters or frequently accessed databases, persistent access is preferred. To accommodate these workloads, a portion of the Gordon system is reserved for dedicated I/O node projects.

In this case, the I/O node serves as the access point to the Hadoop cluster and as the management node. The I/O node hosts all metadata and resource management services, whereas the compute nodes host the Hadoop Distributed File System (HDFS) data and execute map tasks and reduce tasks. HDFS is created on the flash drives that are exported via iSER and are presented to HDFS as local devices.  This platform is used for data exploration, preparation and cleaning for the purpose of this study.  A number of big data and especially smart grid analytics projects are consuming tools like R[6], Mahout[7], Hive[8] and other Hadoop based technologies employed  on this platform due to the 4V?s [9] nature of the Smart grid data.

C. Smart Grid Analytics Test Bed Figure 2 provides an overview of the analytics test bed.

The key steps in the process are described below. Campus building and energy data are collected from 86,000 distinct data streams, of raw data deposited to the PI server. The PI server performs cursory analysis, provides dashboard capability for visualization, and offers other management functions such as alerts and basic reporting.

Figure 2. Analytics Test Bed Data Flow  From the PI server, the data is transferred to the Data Oasis high performance file system over the SDSC LAN where it is staged for the real-time analytics. Numerically intensive statistical analysis is performed on the dedicated set of the Gordon compute cluster (I/O nodes) and results are staged on the Data Oasis file system. The final analysis results will be transferred from Data Oasis to the PI server where models will be integrated into the existing architecture for real-time scoring and predictions. From the perspective of the PI system, the data generated on Gordon is simply another data stream.

From Data Oasis, the original data and some subset of the derived data products can also be sent to the SDSC Cloud Storage environment for archive, reporting and future analysis.



IV. DATA MINING  Predictive data analytics have the potential to greatly enhance the smart grid and amplify its impact by enabling the understanding of an increasing wealth of data about energy usage and the kinds of demands placed on the grid.

UCSD is committed to acquiring alternative forms of energy generation, with the goal of self-sufficiency and main grid independence by 2016. The campus utilizes smart grid data with advanced forecasting and ?big data? analytics in order to realize real improvements in energy efficiency and reductions in energy cost.

Buildings consume significant energy, and according to the DOE are responsible for 70% of the total electricity consumption of the US. UCSD, has over 450 buildings supported by a campus scale micro-grid.  In order to predict and manage energy use holistically across the buildings, we first need to create powerful predictive models for each building and then discover and consider their inter- dependencies by employing the vast amounts of heterogeneous data collected at the second level frequency from the smart grid.   Traditional time series models performance and scalability are innsuficient  for this large and compelx data.  We have employed scalable mutli- variate predictive analytics techniques to identify suspect and unforeseen performance behaviour.  This approach will enable the establishment of the baseline models for future measurement comparisons and creation of temporal and generalized building behavior consumption models.

Applying such approaches to the available data in the microgrid environment enables informed, real-time decisions and enhancements, resulting in a truly intelligent and optimized micro grid.

A. Data Description  Campus building and energy data are collected for the UCSD Microgrid.  Data is currently collected from 86,000 distinct data streams, from approximately 30 different buildings on the UCSD Campus.  Data Streams, include measures and set points, are collected from the Building Management system, Johnson Control systems, which includes HVAC systems, the central utility plant, Schneider Electric power meters, as well as from photovoltaic panels, network model output data and weather data stations at various location on campus.  The data collected includes measurements of airflow, carbon dioxide level, current, damper position, dew point, humidity, power, pressure, real power, speed, temperature, valve, voltage.

Initial research focused on modeling three buildings, which are representative of the majority of campus buildings.  The three buildings selected are newer buildings representing the leading edge of campus energy efficiency and technology and more importantly contain the most instrumentation for data collection.   The buildings include the San Diego Supercomputer Center, which includes computer rooms, conference rooms and office space.

EBU3B building is the home for the computer science department and it includes classrooms, conference rooms and offices.  The third building is called Pacific Hall and it consists of two large laboratory spaces.  In order to create highly accurate models, input attributes, algorithm parameters and environmental condition variables were chosen while exploring issues of seasonal variability and timing.

The buildings sensor data ranges from February 2011 and May 2013.  All three datasets contained the time stamp, the year, month, day, outside temperature and the power usage (in kW) variables.  We added the time of the week (Monday-Sunday) variable as well as a workday flag (=1 if yes, =0 if no).

B. Predicting Energy Consumption using Time Series Approach Modeling the energy usage behavior of campus  buildings is an important step to improving the efficiency of the system. An accurate model of future energy usage compared to actual energy usage enables anomalies discovery in the actual data that may signify wasteful usage of energy. This predictive information is used to combat the high future demand or optimize pre-heating or pre-cooling scheduling to maximize the energy efficiency.  The short- term predicted energy usage, can determine how much renewable energy should be used, help guide decisions regarding demand response, excess generation, renewable supply and load balancing and manage power outages.

Ultimately the microgrid controller will utilize these predictions to provide the capability to enable the operation of the UCSD microgrid in an islanded condition, if necessary.

The energy use in commercial buildings varies depending on many factors including weather, building insulation, equipment efficiencies, hours of equipment operation, system controls, building size, etc, overt time.

There are various modeling techniques that could be  employed to analyze and predict building energy consumption.  Most of the traditional predictive methods overlook, discretize or require oversimplification of the temporal characteristic of the data in order to produce a model [10][13].  In order to caputre the temporal nature of the Smart Grid data we used a time series approach was chosen for energy demand forecasting and building behavior modeling due to the temporal nature of the Smart Grid data.  The strength of this model is that it can work on any building with existing historical or real-time energy consumption data.  The goal of this study is to create highly accurate models predicting energy demand for the campus micro grid at multiple levels of granularity.  The study presented below focuses on modeling building level energy behaviors.

Time series data containing multiple variables (i.e., multivariate time series data) commonly occurs in a wide variety of fields including biology, finance, science and engineering. A time series (or more generally temporal data) is a sequence of measurements that follow non- random orders and can be generated either from a fixed- point measurement at several time intervals or a convolved spatial-temporal variation as measured from a moving detector. Multivariate time series analysis is used when one wants to model and explain the interactions among a group of time series variables. Much of the scientific data is in the form of multivariate time series. Examples include ECG measurements, energy consumption data, temperature, and sign language hand movements, among others.

Multivariate time series classification attempts classification of a new time series based on past observations of time series examples, rather than providing an analysis of a single variate time series. A data mining technique called MineTool-TS is introduced which captures the time-lapse information in multivariate time series data through extraction of global features and metafeatures [14].

Unlike traditional approaches such as Hidden Markov Models, recurrent Artificial Neural Networks, Dynamic Time Warping [15][16][17] and most recently Tclass[18], MineTool can handle real-world, continuous, noisy data really well, and does not suffer from the large number of parameters, non-linear search and black-box model issues, as well as the assumptions difficult to obtain in energy data.

If one could replace the time series by a static data consisting of variables that capture the relevant and interesting features (e.g., number of zero crossings, slope, extreme or average values) of the time series, then the standard MineTool technique could be used. Even though this approach is rather successful for a small number of simple datasets and problems, neither of these two approaches yields high accuracy results in modeling real life, complex time series data. Instead, we use a more sophisticated approach to extract features from multivariate time series data that yields much higher accuracy [14].

C. MineTool for Static Data The core data-mining algorithm that underlies  MineTool-TS is MineTool [14]. The advantages of MineTool over traditional algorithms such as support vector machine and artificial neural net (ANN) are its automated steps that make it more accessible and     applicable in a variety of domains, its accuracy and robustness and the analytical form of the model at the output.

An important algorithmic issue in data mining is how to find the optimal complexity of the model or the fitting function. Too much complexity in the model can result in model overfit, whereas not enough complexity can result in an underfit. The mathematical foundations of MineTool are based on considerations to balance the competing dangers of underfit and overfit to identify the level of model complexity that guarantees the best out-of-sample prediction performance without ad hoc modifications to the fitting algorithms themselves[19][20]. MineTool creates a predictive model architecture that is linear in the parameters. The algorithm searches for a model M that best relates rows of the input variable values Xij to the appropriate target value yi (yi = M(Xij)), where i = 1,?,N and j = 1,?,K. The model parameters are either linear combinations of the input (Xi? , where prime indicates transpose of the vector, index i refers to the ith observation), linear transformations of the input variables ( (Xi)), or highly non-linear transformations of the input ( (Xi, )).

Equation (1) describes the general form of a MineTool model, and the specifics of the method in given in [14]:  1 1 ( ) ' ( , ) '  QP  i p q  y ? = =  = + +i i p i q qX' X X   1. MineTool Model Equation.

In its simplest form, the model would be a linear  combination of the input parameters (i.e., a linear regression model). MineTool goes beyond a simple linear model by introducing the linear (such as level-1 and level-2 transformations producing cross-products, ratios, squares, cubes etc.) and non-linear transformation of the input variables, if their addition increases the model accuracy.

The non-linear transforms  are single hidden layer feed forward ANN-like transforms, just like the ANNs of the same architecture, with the difference that the non-linear transformed inputs are combined into a linear model.

Metafeature and Global Feature Detection: To be able to process a (time) series dataset (represented with multiple rows of data describing one instance or observation) using MineTool, the data needs to be ?flattened,? or made static.

This needs to be accomplished without losing the important information incorporated in sequential measurements varying with time. Historically, this has been done either by summarizing the data and writing only the mean of the different row values of one observation, or recording the difference between the pairs of rows and then treating them as single instance entries. These techniques work somewhat well on just a limited set of time series problems. For real life, complex scientific datasets, these approaches are most often too weak to incorporate the important time changes in the data. The MineTool-TS solution to this problem is to collect the important time-changing information that can occur in one of the time series variables. While a value varies with time, it most often increases, decreases or stagnates. There are other, more complex features one can record, that consist of the three basic changes, such as  bipolar signature (relevant in case of flux transfer events), where a value goes up, then goes down crossing the axis, and goes up again (the sinusoid function demonstrates the bipolar behavior, for example). Global features, just like the metafeatures, are used to extract the information from all the rows representing one observation. Global features describe one instance of rows using one measurement, such as: the maximum value, minimum value, mean, mode or the number of zero crossings. The metafeatures and global features included in the MineTool-TS algorithm are listed here (all used in the analys except for the bipolar feature):  Increasing Metafeature: An increasing metafeature is recorded for all the consecutive rising time-series measurements. For each increasing event, we record its start point, duration, gradient and average value, so that the increasing events can be used for analysis and comparison.

Decreasing Metafeature: A decreasing metafeature is recorded for all the consecutive reducing time-series measurements. For each decreasing event, similar to the increasing events, we record starting point, duration, gradient (which is negative in this case) and average value.

Plateau Metafeature: A plateau metafeature is recorded for all the consecutive non-changing time-series measurements. MineTool-TS allows for a small amount of noise to be ignored, so that the true plateaus are captured.

Bipolar Signature Metafeature: A bipolar signature metafeature is recorded for all the consecutive time-series measurements that increase, decrease and cross the zero, and increase again.

Global Minimum: For each single variable, the global minimum feature extracts the minimum value of all of the time observations belonging to one time series instance for the variable, and records it as the global minimum feature for that input channel.

Global Maximum: The maximum value of all of the time observations belonging to one time series instance for the variable is recorded as the global maximum feature for that variable.

Mean: The average value of all of the time observations belonging to one time series instance for the variable is recorded as the global mean feature for that specific variable.

Mode: The mode value of all of the time observations belonging to one time series instance for the variable is recorded as the global mode feature for that specific input variable.

Number of Zero Crossings: Lastly, the number of zero crossings occurring during the time observation recorded measurements is written down as the number of zero crossings global feature.

Next, once all the requested features are collected, the MineTool-TS algorithm performs the feature space segmentation to group similar features and makes them have a higher predictive value for data mining. More details on the algorithm can be found in [14].

D. Modelling Results We provided a three-level predictive analysis of power  usage: static analysis (where all the sensor information     entries are thought of as independent instances) and time series (where the entries are considered a part of a time series, i.e. time dependent measurements), for all three datasets (CogSci, PAC Hall and SDSC Building data).  The three buildings have different usage profiles, different schedules, occupancy and energy needs.

In the static case, we compared the results of static MineTool to a multilayer perceptron method (an artificial neural network (ANN) method)[13][22].  In the case of time series analysis, we used the MineTool-TS method described above.

To prepare the data for time series analysis, the dataset was treated as a set of one-hour long streams of data.

Below, we demonstrate the results of the static vs. time series analysis and show the accuracy achieved in predicting the energy usage for each of the different buildings.

Dataset 1 ? CogSci Building Data: Table 1 demonstrates the results of the static vs. time series analysis for the first dataset, the CogSci building data.  We used the 10-fold cross validation evaluation and the table lists the standard numerical prediction evaluation measures: CC (correlation coefficient), MAE (mean absolute error) and RMSE (root mean squared error).

From the multivariate time series classification analysis point of view, out of all the standard metafeatures (such as increment, decrement and plateau features) and the global features (such as mean, number of zero crossings), the best performing feature chosen by the algorithm was the mean as well as the plateau feature with a relaxed noise level.

This is due to the fact that the fluctuations within an hour are minimal, and there are no distinctive increases and decreases within the one hour long streams that have a high predictive capability.

Figure 3 demonstrates a part of the time series prediction model represented in a tree form. We can see that workday, hour of the day, month of the year and the temperature all seem to have a high predictive ability in forecasting the usage, and that the workday seems to be the highest determining factor of the levels of power usage.

Table 1 demonstrates that treating the data as time series indeed leads to stronger model accuracy. MineToo- TS achieved the correlation coefficient of 0.9, as tested using 10-cross validation, whereas the static analysis only yielding accuracy at the random guessing level of below (0.49 and 0.44).

TABLE 1. COMPARATIVE ANALYSIS OF POWER USAGE MODELS FOR THE COGSCI DATASET  Analysis Method CC MAE RMSE  Static  MineTool 0.4939 3.4812 7.3371  Static ANN 0.4436 6.8216 8.5553  Time Series  MineTool 0.9013 2.6971 3.8941     Figure 3. Part of the CogSci Power Usage Prediction Model  We believe that adding more measurements, such as the  occupancy, class schedule and similar information to the current data would increase the accuracy of the models.

Furthermore, campus events, cloud coverage and similar variables would, we believe would increase the predictive accuracy even further.

Dataset 2 ? PAC Hall Data: We provided a slightly different two-level analysis of the PAC Hall dataset. To be able to compare the time series analysis to the standard data mining, we preprocessed the second building data by averaging the values within each hour stream of the data.

Table 2 illustrates the results of the static averaged data vs.

time series analysis for this dataset.  We also used the 10- fold cross validation evaluation and the table lists the standard numerical prediction evaluation measures: CC (correlation coefficient), MAE (mean absolute error) and RMSE (root mean squared error).

Table 2 above confirms that treating the data as time series leads to stronger model accuracy.  MineTool-TS achieved the accuracy of 0.95 correlation coefficient, tested using 10-cross validation, whereas the static analysis performed on the hour averages data yielded 0.64 CC accuracy.

Figure 4 demonstrates a part of the time series prediction model represented in a tree form. We can see that in the case of PAC Hall data, the power usage has changed from year 2011 to 2012, and year 2013 as well, as the predictive analysis output shows a distinct submodel for 2011. In the case of this building, particular day of the week seems to have a higher predictive ability as well, not just a workday as in the case of the previous dataset.  Hour of the day, month of the year and the temperature all still seem to have a high predictive ability in assessing the usage as in the CogSci dataset case.

TABLE 2. COMPARATIVE ANALYSIS OF POWER USAGE MODELS FOR THE PAC HALL DATASET  Analysis Method CC MAE RMSE Static ANN 0.6354 3.8081 5.1177 Time series  MineTool 0.9508 1.3368 2.0348       TABLE 3. COMPARATIVE ANALYSIS OF POWER USAGE MODELS FOR THE SDSC DATASET  Analysis Method CC MAE RMSE  Static ANN 0.7589 63.927 95.2282  Time series  MineTool 0.9736 8.989 9.2737  Dataset 3 ? SDSC Building Data: Using the same two- level analysis as in the case of previous building, we obtained the SDSC power usage data analysis results shown in Table 3.

Table 3 above reiterates that multi variate time series analysis is indeed over-performing static analysis of the data. The time series model achieved 0.97 correlation coefficient accuracy, while the static models achieved 0.76 CC accuracy.

Figure 5 demonstrates a part of the time series prediction model represented in a tree form. Power usage patterns changed from year 2011 to 2012 and 2013 in this building as well, and the day of the week, not just workday, as well as the hour are important predictors of power usage.

In summary, predictive analytics of energy big historical usage data obtained excellent predictive results of power usage in kW, in three building of the UCSD micro grid system.  The multivariate time series classification produced excellent models (accuracy yielding 0.9-0.97 CC) as compared to the standard data mining analysis where the data is treated as static, independent rows, and instances are processed as independent examples, or averaged over an hour of streaming data.  The models demonstrate that the time of the day, workday, day of the week, month, hour and outside temperature are all excellent predictors of the power usage per building on the micro grid.  Put in the global perspective, this initial analysis of big data will enable us to be closer to predicting power usage and lower the cost of energy of the entire campus microgrid.

FIGURE 4. PART OF THE PAC HALL POWER USAGE MODEL   Figure 5. Part of the SDSC Power Usage Model

V. CONCLUSION UCSD is committed to acquiring alternative forms of  energy generation, with the goal of self-sufficiency and main grid independence by 2016. We are utilizing smart grid data with advanced forecasting and a ?big data? analytics platform leveraging available large scale data and HP system in order realize tangible improvements in energy efficiency and cost.  We are further exploring clustering methods, real time and active learning models employed for predicting peaks and potential outages.   As data is rapidly growing, the stream mining and predictions become futher challenging due to the vast volume and speed of data.   Future work will involve scalling of the ongoing efforts to efficiently model the campus in its entirety and evalutae the system in real-time.  As model complexity and data granularity increase the need for large memory nodes and HDFS rise.  Investigating the parallelization of the worksflows and Minetool on a number of different platforms is be the focus of the future work.  This initial real-life case study has shown clear value through the big data analytics in both efficiencies and cost savings.    Applying such approaches to the available large scale sensor data in the microgrid environment enables informed, real-time decisions and enhancements, resulting in a truly intelligent and optimized microgrid.

ACKNOWLEDGEMENT  The authors would like to personally thank Byron Washom, director of Strategic Energy Initiatives; John Dilliott, manager of Energy and Utilities; and Robert Austin, administrator of Energy Management Systems at UCSD; Bob Caldwell, President of Centaurus Prime and a UCSD microgrid consultant who were instrumental in providing access to campus energy data; as well as our funding agencies, i.e., the National Science Foundation and the California Energy Commission.

