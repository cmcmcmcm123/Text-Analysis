Optimal synaptic learning in non-linear associative memory

Abstract?Neural associative memories are single layer per- ceptrons with fast synaptic learning typically storing discrete associations between pairs of neural activity patterns. For linear learning such as employed in Hopfield-type networks it is well known that the so-called covariance rule is optimal resulting in minimal output noise and maximal storage capacity.

On the other hand, numerical simulations suggest that non- linear rules such as clipped Hebbian learning in Willshaw-type networks perform better, at least for sparse neural activity and finite network size. Here I show that the Willshaw and Hopfield models are only limit cases of a general optimal model where synaptic learning is determined by probabilistic Bayesian  considerations. Asymptotically, for large networks and very sparse neuron activity the Bayesian model becomes identical to an inhibitory implementation of the Willshaw model. Similarly, for less sparse patterns, the Bayesian model becomes identical to the Hopfield network employing the covariance rule. For intermediate sparseness or finite networks the optimal Bayesian rule differs from both the Willshaw and Hopfield models and can significantly improve memory performance.



I. INTRODUCTION  A neural associative memory is a single layer perceptron  with fast, typically ?one-shot? learning corresponding to the  storage of M associations between pairs of binary memory vectors {(u? ? v?) : ? = 1, ..., M}. They exist both in a hetero-associative feed-forward form storing the mapping  between the address memories u? and the associated content  memories v?, but also in an auto-associative recurrent form  where address and content memories are identical, where  typical tasks include denoising and pattern completion.

Neural associative memories have close relationships to  Hebbian cell assemblies [1] and are widely used in neuro-  science as models of neural computation for various brain  structures, for example, neocortex [2], [3], [4], [5], [6],  hippocampus [7], [8], perirhinal cortex [9], cerebellum [10],  [11], [12], and mushroom body [13]. In addition, neural asso-  ciative memories are potentially useful in technical applica-  tions such as cluster analysis, speech and object recognition,  or information retrieval in large databases [14], [15], [16],  [17], [18], [19], [20], [21].

Learning in neural associative memories is strongly con-  strained by the ?one-shot? property. For example, gradient  descent methods (such as error-backpropagation) are not vi-  able because they require repeated training of the whole pat-  tern set. Instead it is straight-forward to use simple Hebbian-  like learning rules: If, during presentation of a single pattern  pair, both the presynaptic and postsynaptic neurons are active  then the synaptic weight must be increased. There have been  numerous previous attempts to develop optimized learning  Andreas Knoblauch is with the Honda Research Institute Europe, Carl- Legien-Strasse 30, D-63073 Offenbach/Main, Germany (phone: +49 69 89011 750; email: andreas.knoblauch@honda-ri.de).

models that maximize ?storage capacity? as defined, for  example, by the number of stored memories or the stored  Shannon information per synapse (e.g., see [22]).

One of the earliest and simplest one-shot learning model  is the so-called Steinbuch or Willshaw model with binary  synapses and clipped Hebbian learning [23], [24], [25], [26],  [27], [28], [29]. Here a single coincidence of presynaptic  and postsynaptic activity increases the synaptic weight from  0 to 1, while additional coincidences do not cause further  changes. In contrast, linear learning models of the Hopfield-  type [30], [31], [32], [33], [34], [35], [36], [37] add the  contributions of each pattern pair. For binary memory pat-  terns the general linear learning rule can be described by  four values ?, ?, ?, and ? specifying the weight increments for the pre-/postsynaptic activations u?i /v  ? j = 0/0, 0/1, 1/0,  and 1/1. A third model class is based on the ?Bayesian  Confidence Propagation Neural Network? or BCPNN rule  of Lansner and Ekeberg [38], [39], [40], [41], [42], [43],  [6] and employs Bayesian maximum-likelihood heuristics for  synaptic learning and retrieval.

The first two model classes are theoretically well in-  vestigated, whereas the BCPNN-type models still lack a  comparable consideration. Surprisingly, the maximal network  storage capacity C in bits per synapse is almost identical for the binary Willshaw and linear Hopfield type models:  The Willshaw model can achieve up to C = 0.69 bits per binary synapse, whereas the linear learning model can  achieve only a slightly higher capacity of C = 0.72 bits per real-valued synapse. However, closer investigations reveal  that the Willshaw model can achieve non-zero capacity only  for very sparse patterns where the number of active units  per pattern vector scales logarithmic with the vector size. In  contrast, the linear model achieves the maximum C = 0.72 for almost arbitrary sparseness. Only for linearly or non-  sparse patterns performance drops to the capacity of the  original Hopfield network (e.g., C = 0.14bps [30], [35] or rather C = 0.33bps for ?hetero-associative? feed-forward networks considered here [44]). In any case, the linear  learning model achieves maximal storage capacity only for  the optimal covariance learning rule (e.g., [33], [35]) which  becomes equal to the Hebb rule for very sparse patterns, and  equal to the Hopfield rule for non-sparse patterns. Moreover,  simulation experiments show that the capacity of the optimal  linear model remains well below the capacity of the Willshaw  model for any reasonable finite network size (e.g., C=0.2bps  vs C=0.5bps for n = 105 neurons [44]). This suggests that the linear covariance rule is not always optimal, in particular  not for finite networks and sparse memory representations  as found in real brains [45]. Similarly, the BCPNN model  has recently been shown to have a suboptimal performance     [46], [47], basically equivalent to the linear homosynaptic  rule which is well known to have a factor 1 ? p lower storage capacity than the optimal covariance rule (here p is the fraction of active units in an address memory; see rule  R3 in [33]p259).

This paper develops the generally optimal associative  memory that minimizes output noise and maximizes storage  capacity by activating neurons based on Bayesian maximum-  likelihood decisions. The corresponding neural interpretation  of the Bayesian associative memory corresponds, in general,  to a novel non-linear learning rule. A signal-to-noise analysis  allows to compute the storage capacity and to compare  optimal Bayesian learning to the previous model types.

Specifically, it turns out that the previous models are only  special limit cases of the general optimal Bayesian model.

Asymptotically, for large networks and very sparse memory  patterns, the Bayesian model becomes essentially identical to  the binary Willshaw model (but implemented with inhibitory  rather than excitatory synapses [48]). Similarly, for less  sparse patterns, the Bayesian model becomes identical to  the linear model employing the optimal covariance rule.

For intermediate sparseness and finite networks the optimal  Bayesian learning rule performs significantly better than the  previous learning models.



II. MEMORY STORAGE  The task is to store M associations between address patterns u? and content patterns v? where ? = 1 . . .M .

We assume that all memory patterns are binary vectors.

Address patterns u? have dimension m and content patterns v  ? dimension n. Then we assume that each address neuron i and each content neuron j can memorize its unit usage  M1(j) := #{? : v?j = 1} (1) M0(j) := #{? : v?j = 0} = M ? M1(j) (2) M ?1(i) := #{? : u?i = 1} (3) M ?0(i) := #{? : u?i = 0} = M ? M ?1(i) . (4)  Similarly, each synapse ij can memorize its synapse usage  M11(ij) := #{? : u?i = 1, v ? j = 1} (5)  M01(ij) := #{? : u?i = 0, v ? j = 1} = M1 ? M11 (6)  M00(ij) := #{? : u?i = 0, v ? j = 0} = M ?0 ? M01 (7)  M10(ij) := #{? : u?i = 1, v ? j = 0} = M0 ? M00 (8)  for i = 1, . . . , m and j = 1, . . . , n. Note that it is sufficient to memorize M , M1, M  ? 1, and M11. Thus, an implementation  on a digital computer requires only about (mn + m + n + 1)ldM memory bits. Note also that the synaptic weights of the Willshaw network and the linear models can be expressed  in terms of the synapse usages. The weights of the Willshaw  network are wij = 1 if M11 ? 1 (and wij = 0 otherwise).

The weights of the linear models are wij = ?M00 +?M01+ ?M10 + ?M11.



III. OPTIMAL RETRIEVAL  Given a query pattern u? the memory task is to find the  ?most similar? address pattern u? and return a reconstruction  v? of the associated content v?. In general the query u?  is a noisy version of one of the address patterns u?. For  clarity the following analysis considers only zero query noise  assuming u? = u?. For the general case see a technical report [44].

Now the content neurons j have to decide independently of each other whether to be activated or to remain silent. Given  the query u?, the optimal maximum-likelihood decision  v?j =  {  1 , pr[v?  j =1|u?]  pr[v? j =0|u?]  ? 1 0 , otherwise  (9)  minimizes the expected Hamming distance dH(v ?, v?) :=  ?n  j=1 |v ? j ? v?j | between original and reconstructed content.

If the query pattern components are conditional independent  given the activity of content neuron j, e.g., assuming inde- pendently generated address pattern components, we have for  a ? {0, 1}  pr[u?|v?j = a] = m ?  i=1  pr[u?i|v?j = a] = m ?  i=1  Mu?ia(ij)  Ma(j) . (10)  With pr[v?j = a|u?] = pr[u?|v ? j = a]pr[v  ? j = a]/pr[u?] (the  Bayes formula) we obtain  pr[v?j = 1|u?] pr[v?j = 0|u?]  =  (  M0(j)  M1(j)  )m?1 m ?  i=1  Mu?i1(ij)  Mu?i0(ij) . (11)  For a neural formulation we can take logarithms of the  probabilities and obtain synaptic weights wij and dendritic potentials xj := log(pr[v  ? j = 1|u?]/pr[v  ? j = 0|u?]),  wij = log M11M00 M10M01  (12)  xj = (m ? 1) log M0 M1  +  m ?  i=1  log M01 M00  +  m ?  i=0  wij u?i (13)  such that pr[v?j = 1|u?] = 1/(1 + e?xj ) writes as a sigmoid function of xj , and a content neuron fires, v?j = 1, iff the dendritic potential is non-negative, xj ? 0. Note that indices of M0(j), M1(j), M00(ij), M01(ij), M10(ij), and M11(ij) are skipped for brevity. Thus, the generally optimal Bayesian  learning rule is non-linear and differs from both the Willshaw  and linear models.

Evaluating eq. 13 is much cheaper than eq. 11, in particular  for sparse queries having only a small number of active  components with u?i = 1. However, the synaptic weights eq. 12 may not yet satisfy Dale?s law that a neuron is either  excitatory or inhibitory. To be more consistent with biology  we may add a sufficiently large constant c := ?minij wij to each weight. Then all synapses have non-negative weights  w?ij := wij +c and the dendritic potentials remain unchanged if we replace the last sum in eq. 13 by  m ?  i=0  wij u?i =  m ?  i=0  w?ij u?i ? c m  ?  i=0  u?i . (14)    Here the negative sum could be realized, for example, by  feedforward inhibition with a strength proportional to the  query pattern activity (e.g., [49]).



IV. ANALYSIS OF SIGNAL-TO-NOISE RATIO AND  STORAGE CAPACITY  To build a memory system with high retrieval quality we  have to minimize the expected Hamming distance dH(v ?, v?).

The expected Hamming distance E(dH) = nqp10 + n(1 ? q)p01 can be computed from the component output error probabilities  p01 := pr[v?j = 1|v?j = 0] = pr[xj ? ?j |v ? j = 0] (15)  p10 := pr[v?j = 0|v?j = 1] = pr[xj < ?j |v ? j = 1] (16)  where nq is the mean activity of a content pattern and ?j is the firing threshold, e.g., ?j = 0 for dendritic potentials xj as in eq. 13. Intuitively, retrieval quality will be high if the ?high potential distribution? pr[xj |v?j = 1] and the ?low potential distribution? pr[xj |v?j = 0] are well separated, i.e., if the signal-to-noise ratio (SNR)  r := SNR(?lo, ?lo, ?hi, ?hi) := ?hi ? ?lo  max(?lo, ?hi) (17)  is large (see [31], [33], [35], [37]). Here ?lo := E(xj |v?j = 0) and ?2lo := Var(xj |v  ? j = 0) are expectation and variance  of the low-potential distribution, and ?hi = E(xj |v?j = 1) and ?2hi := Var(xj |v  ? j = 1) are expectation and variance of  the high-potential distribution.

In the following we compute the SNR assuming that each  address unit is one with the same probability p := pr[u?i = 1] independently of other units. For the content patterns it is  sufficient to assume q := pr[v?j = 1]. Then let u? = u ? be a  noise-free query pattern having k one-entries. Without loss of generality we assume further that u?i is one for i = 1, . . . , k and zero for i = k + 1, . . . , m. Equivalently to eq. 13 (but replacingm?1 by m for brevity), a content neuron j will be activated if the dendritic potential xj exceeds the threshold ?j := log(M0/M1) (instead of ?j = 0), where  xj := m log M0 M1  +  k ?  i=1  log M11 M10  +  m ?  i=k+1  log M01 M00  (18)  = m log M0 M1  +  k ?  i=1  log M11  M0 ? M00  +  m ?  i=k+1  log M1 ? M11  M00 . (19)  For given M1, M0 the remaining variables are binomially distributed, M00 ? BM0,1?p and M11 ? BM1,p, where pr[BN,P = z] =  (  N z  )  P z(1 ? P )N?z . For large NP (1 ? P ) the binomial BN,P can be approximated by a Gaussian G?,? with mean ? = NP and variance ?2 = NP (1 ? P ). Given  u?i and v ? j we have then  M11 ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  G M1p,  ? M1p(1?p)  ,u?i /v ? j = 0/0  G M1p,  ? M1p(1?p)  ,u?i /v ? j = 1/0  G (M1?1)p,  ? (M1?1)p(1?p)  ,u?i /v ? j = 0/1  G 1+(M1?1)p,  ? (M1?1)p(1?p)  ,u?i /v ? j = 1/1  (20)  M00 ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  G 1+(M0?1)(1?p),  ? (M0?1)p(1?p)  ,u?i /v ? j = 0/0  G (M0?1)(1?p),  ? (M0?1)p(1?p)  ,u?i /v ? j = 1/0  G M0(1?p),  ? M0p(1?p)  ,u?i /v ? j = 0/1  G M0(1?p),  ? M0p(1?p)  ,u?i /v ? j = 1/1  (21)  From this we can approximate the distribution of the potential  xj for low units and high units, respectively. For large k and m ? k the sums of logarithms are approximately Gaussian distributed.

For Gaussian random variables X ? G?,? it is  E(log X) = log ? ? ? ?  i=1  1 ? 3 ? ? ? (2i ? 1) 2i  (?/?)2i  ? log ? (22) Var(log X) ? (?  ? )2 (23)  which can be proved using the Newton-Mercator series (for  a detailed proof see [44], app. C). Here approximations are  valid for ? ? ? and tight for ?/? ? 0 if ? 6? 1.

From this we can compute the exact mean dendritic  potentials ?lo, ?hi and variances ? lo, ?  hi for low-units and  high-units. Fortunately, it turns out that the mean potential  difference ?? := ?hi ? ?lo can be well approximated by using only the first order term in eq. 22 (while all higher-  order terms become virtually identical for ?hi and ?lo; for more details see [44], apps. D,F). Then the first-order  approximations ??lo, ? ? hi of ?lo, ?hi are  ??lo = m log M0 M1  + k log M1p  M0 ? (M0 ? 1)(1 ? p)  +(m ? k) log M1 ? M1p 1 + (M0 ? 1)(1 ? p)  = ?k log(1 + 1 ? p M0p  )  ?(m ? k) log(1 + p M0(1 ? p)  )  ? ?k(1 ? p) M0p  ? (m ? k)p M0(1 ? p)  (24)    ??hi = m log M0 M1  + k log 1 + (M1 ? 1)p  M0 ? M0(1 ? p)  +(m ? k) log M1 ? (M1 ? 1)p M0(1 ? p)  = log  [(  (M1 ? 1)p + 1 M1p  )k  ? (  M1(1 ? p) + p M1(1 ? p)  )m?k]  ? k(1 ? p) M1p  + (m ? k)p M1(1 ? p)  (25)  where the approximations are valid for large M0p, M1p ? ?. Therefore the mean difference?? between the high- and low-distributions is  ?? := ?hi ? ?lo ? ??hi ? ??lo ?  (  k(1 ? p) p  + (m ? k)p  1 ? p  ) (   M1 +   M0  )  (26)  In order to get the signal-to-noise ratio eq. 17 we still have  to compute the variances ?2lo and ? hi for xj in eq. 19.

Given the unit usages M1, the random variables M00(i, j) and M11(i, j) are independent and thus the variances sim- ply add. Because each variance summand is positive, for  large M1p, M0p ? ? we can simply assume M11 ? G  M1p, ?  M1p(1?p) and M00 ? GM0(1?p),  ? M0p(1?p)  in all  cases (cf., eqs. 20,21). With eq. 23 we get  Var(log M11) ? 1 ? p M1p  (27)  Var(log(M0 ? M00)) ? 1 ? p M0p  (28)  Var(log(M1 ? M11)) ? p  M1(1 ? p) (29)  Var(log M00) ? p  M0(1 ? p) . (30)  Thus, the variances ?2 := Var(xj) ? ?2lo ? ?2hi for the potentials of both low-units and high units are approximately  ?2 ? k(Var(log M11) + Var(log(M0 ? M00))) +(m ? k)(Var(log(M1 ? M11)) + Var(log M00))  ? k(1 ? p) p  (   M1 +   M0  )  + (m ? k)p  1 ? p  (   M1 +   M0  )  =  (  k(1 ? p) p  + (m ? k)p  1 ? p  ) (   M1 +   M0  )  ? ??  (31)  Therefore, the signal-to-noise-ratio r := ??/? (eq. 17) is given by  r2 ? ?? ? ?2  ? (  k(1 ? p) p  + (m ? k)p  1 ? p  ) (   M1 +   M0  )  ? m Mq(1 ? q) (32)  where the last approximation is true because for large  M, M1, M0, k ? ? the unit usage and pattern activity will be close to the mean values, M1 ? Mq, M0 ? M(1 ? q), and k ? pm.

In summary, forMpq ? ? the SNR r ? m/(Mq(1?q)) of optimal Bayesian learning is identical to the asymptotic  SNR of linear learning with the optimal covariance rule  (e.g., see ?Covariance3 in [33] p259; or eq. 3.28 in [35] p95).

Since the network storage capacity C can be written as a function of the SNR r (e.g., see [35], [44]), for Mpq ? ? the Bayesian learning model has also the same asymptotic  network storage capacity as the linear covariance rule, C = 0.72 bps.



V. RELATION TO LINEAR LEARNING MODELS AND THE  COVARIANCE RULE  In general, the synaptic weights of the Bayesian associative  network (eq. 12) are a non-linear function of presynaptic  and postsynaptic activity. In the following we show that  under some conditions the optimal Bayesian rule can be  approximated by a linear learning rule. For large networks  the synapse usages will be close to its expectations, i.e.,  M00 ? M0(1 ? p), M01 ? M1(1 ? p), M10 ? M0p, and M11 ? M1p. In fact these approximations will make only a negligible relative error if the standard deviations  are small compared to the expectations. The most critical  variable is M11 having expectation M1p and standard de- viation M1p(1 ? p). Thus, the approximations are valid for M1p ? ?, or Mpq ? ?, if we assume M1 ? Mq. Then the argument of the logarithm in eq. 12 will be close to one.

A linear approximation of the logarithm around one yields  wij ? f(M00, M01, M10, M11) := M11M00 M10M01  ? 1 . (33)  Similarly, the resulting function f can be linearized around the expectations of the synapse usages. The partial derivatives  are  ?f  ?M00 |Mxy=E(Mxy) =  M11 M10M01  |Mxy=E(Mxy)  =  M0(1 ? p) (34)  ?f  ?M01 |Mxy=E(Mxy) = ?  M11M00 M10M201  |Mxy=E(Mxy)  = ? 1 M1(1 ? p)  (35)  ?f  ?M10 |Mxy=E(Mxy) = ?  M11M00 M210M01  |Mxy=E(Mxy)  = ? 1 M0p  (36)  ?f  ?M11 |Mxy=E(Mxy) =  M00 M10M01  |Mxy=E(Mxy)  =  M1p (37)    4 7 10 22 25 32 50  ?5  ?4  ?3  ?2  ?1   pattern activity k (=l)  o u tp  u t n o is  e ?  m=n=100  4 12 71 100 292 595 1250 2500  ?7  ?6  ?5  ?4  ?3  ?2  ?1   m=n=5000  pattern activity k (=l)  o u tp  u t n o is  e ?  L?Hebb  L?Cov  Willshaw  Bayes  L?Hebb  L?Cov  Willshaw  Bayes  Fig. 1. Experimental retrieval output noise ? := dH(v ?, v?)/l as function of pattern activity k := mp. for the optimal Bayesian rule (cyan solid), Willshaw  rule (black dashed), linear covariance rule (red solid), and the linear Hebb rule (blue dashed). Here ? is defined as the expected Hamming distance between retrieval output v? and original content v? normalized by the mean content pattern activity l := nq. Queries u? contained half the one-entries of the original address patterns u?. For each data point the number of stored memories, M , has been chosen maximal such that the Willshaw model does not exceed ? = 0.01 (data taken from [29]). Left panel: Small networks with m = n = 100 and M = 7, 26, 20, 11, 10, 7, 4 for k = l = 4, 7, 10, 22, 25, 32, 50.

Right panel: Larger networks with m = n = 5000 and M = 3985, 31481, 7082, 736, 202, 49, 12 for k = l = 4, 12, 71, 292, 595, 1250, 2500. Each data point averages over 10000 retrievals in 100 different networks.

For q := M1/M the linear approximation writes finally  wij ? M00  M(1 ? p)(1 ? q) ? M01  M(1 ? p)q  ? M10 Mp(1 ? q) +  M11 Mpq  (38)  or for ? := 1/(Mpq(1 ? p)(1 ? q)) wij ?  = pqM00 ? p(1 ? q)M01 ?(1 ? p)qM10 + (1 ? p)(1 ? q)M11 . (39)  This is essentially (up to factor ?) the covariance rule as discussed in many previous works (e.g., see [50], [51],  [32], [52], [33], [53], [34], [35], [36], [37]). Thus, in the  asymptotic limit Mpq ? ? optimal Bayesian learning becomes equivalent to the covariance rule of linear learning  models (see Fig. 1).



VI. RELATION TO THE WILLSHAW MODEL AND  INHIBITORY ASSOCIATIVE MEMORY  The Willshaw or Steinbuch model is one of the sim-  plest models for distributed associative memory employing  synapses with binary weights  wij = min(1, M11(ij)) ? {0, 1} . (40)  The dendritic potentials of the content neurons are simply  xj = ?m  i=0 wij u?i. The Willshaw model works particular good for ?pattern part retrieval?, i.e., if the query pattern u? contains a subset of the one-entries of an address pattern  u?. Then the optimal threshold is maximal, i.e., equal to the query pattern activity ?j =  ?m  i=0 u?i. This implies that a  single ?missing? input, i.e., u?i = 1 but wij = 0, excludes activation of content neuron j.

This observation suggests that the Willshaw model should  be interpreted as an essentially inhibitory network where  zero weights become negative, active weights zero, and the  optimal threshold zero. In particular for a diluted network  with low connectivity the inhibitory interpretation of the  Willshaw model with optimal threshold control seems to be  much more realistic than an excitatory interpretation (e.g.,  compare [48] vs. [54]).

Under some conditions the optimal Bayesian model be-  haves quite similar: Obviously, the synaptic weight eq. 12  becomes plus or minus infinity if one of the synapse usages  Mxy is zero. Since memory patterns are typically sparse it is most likely that only the synapse usage M11 ? BM,pq remains zero. Then the optimal synaptic weight is wij = ?? such that, similar as for the Willshaw model with maximal  threshold, a single inhibitory input, u?i = 1 and wij = ??, can silence the postsynaptic content neuron j.

In fact, for sufficiently sparse memory patterns with  Mpq ? ? but still Mp ? ? and Mq ? ? the synapse usages M00 ? M(1 ? p)(1 ? q), M01 ? M(1 ? p)q, and M10 ? Mp(1 ? q) will be close to their large mean values, whereas the synapse usage M11 remains small and, occasionally, even may assume zero. Thus, the synaptic  weights, up to an essentially constant offset, are dominated  by the infinitely negative weights due to the M11 = 0 events. Therefore, in such a regime, the Bayesian model  becomes equivalent to the Willshaw model (Fig. 1). Note  also that this regime offers novel functional interpretations  for strongly inhibitory circuits, for example, involving basket  or chandelier cells [55], [48].



VII. CONCLUSIONS  This paper develops the optimal neural associative mem-  ory based on Bayesian maximum-likelihood considerations  assuming local synaptic learning (section II) and indepen-  dent address attributes [56], [57]. In general, the resulting  optimal synaptic learning rule is non-linear and differs from  previously investigated linear learning models of the Hopfield  type [30], [58], [32], [33], [53], [34], [35], [36], [37], simple  non-linear learning models of the Willshaw type [24], [23],  [25], [59], [26], [28], [18], [29], and BCPNN-type models  employing suboptimal Bayesian heuristics [38], [39], [40],  [41], [42], [43], [6].

The previous models become optimal only in the asymp-  totic limit of many stored memories,M ? ?, depending on the pattern activity parameters p and q: In the limit Mpq ? ? of moderately sparse and non-sparse memory patterns, the optimal Bayesian model becomes equivalent to the linear  network employing the covariance rule and thus achieves  a maximal network storage capacity of C = 0.72 bits per synapse. In the limit Mpq ? ? of very sparse patterns, the optimal Bayesian model becomes equivalent to the Willshaw  network and thus achieves C = 0.69 bits per synapse. For a large range of intermediate sparseness and finite network size  the optimal Bayesian model can perform significantly better  than the previous models. These theoretical results have been  verified by numerical experiments illustrated by Fig. 1 (see  also a related technical report [44], [47]).

Note that this paper shows that even the best method of  Hebbian-type synaptic plasticity (modifying synaptic weights  based on presynaptic and postsynaptic activity) can store  significantly less than one bit per real-valued synapse. By  contrast, we have shown elsewhere that storing memories in  similar network models by structural plasticity (meaning the  elimination and generation of synapses [60]) yields a much  higher diverging synaptic capacity where the information that  a binary synapse can store scales with the logarithm of the  network size n [61], [62], [63], [64], [29], [65].

