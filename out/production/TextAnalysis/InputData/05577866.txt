Classification of Closed Frequent Patterns Improved by Feature Space  Transformation

Abstract?In some real-world applications, the predefined features are not discriminative enough to represent well the distinctiveness of different classes. Therefore, building a more well-defined feature space becomes an urgent task. The main goal of feature space transformation is to map a set of features defined in a space into a new more powerful feature space so that the classification based on the transformed data can achieve performance gain compared to the performance in the original space. In this paper, we introduce a feature transformation method in which the feature transformation is conducted using the closed frequent patterns. Experiments on real-world datasets show that the transformed features obtained from combining the closed frequent patterns and the original features are superior in terms of classification accuracy than the approach based solely on closed frequent patterns.

Keywords- Closed Frequent Pattern, Feature Space Transformation, Classification Accuracy

I.  INTRODUCTION Classification such as decision trees [1, 17], Support  Vector Machine [2], neural network [3], Bayesian networks[30], k-nearest neighbor[31], case-based reasoning[32], etc. has been received considerable attention in various communities. Classification is a supervised learning task that can predict the class labels of unseen instances. If the predefined features represent well the distinctiveness of different classes, the classification could achieve high classification performance. However, in some cases, original single features could not satisfactorily capture the intrinsic structure of each class and thus cannot provide useful information to the classifier. Thus, in order to improve the classification performance, there is an emerging need for constructing a good feature space for the complex structural data. Feature space transformation, which is used to transform the original single feature space to another new more powerful feature space, can solve this problem.

One of attempts to improve classification accuracy is to apply frequent patterns in the classification task. In this research area, frequent patterns are used as features which   Corresponding author  make classification models more accurate and easier to understand. Frequent pattern mining (also association mining) was first proposed by Agrawal, Imielinski and Swami [4] and widely studied in the past decades. A frequent pattern is generated from the given database with its frequency no less than user-specified minimum support count. Frequent patterns contain much more underlying semantics than single features since a frequent pattern is the combination of single features. However, usually a large number of frequent patterns can be generated at a given minimum threshold so that it is impracticable to use the whole frequent patterns as features and becomes difficult to interpret. This led the researchers to study closed patterns, which can extract the whole frequent patterns even their exact frequencies without any information loss. Also the number of closed frequent patterns is much smaller than that of frequent patterns. Thus, it is a good choice to use closed frequent patterns as features instead of the whole frequent patterns.

The contributions of this paper can be summarized as follows: 1) We introduce closed frequent pattern-based feature space transformation, 2) the extensive experimental results on real-world datasets show that the transformed feature space obtained from combing the closed frequent patterns and single features could achieve better classification accuracy than that solely on closed frequent patterns.

The rest of the paper is organized as follows. In Section II, we review related work. The basic terminology and the problem definition are presented in Section III. The extensive experimental evaluations on real-world datasets are reported in Section IV and our conclusion and future work are given in Section V.



II. RELATED WORK In this paper, we use closed frequent patterns as features  to represent the data and then classification method is applied on this transformed feature space. Thus, our research belongs to pattern-based classification task. Associative classification [5, 6, 7, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 28], which integrates classification and association mining, is related to our research topic. During last decades,  DOI 10.1109/CIT.2010.235    DOI 10.1109/CIT.2010.235    DOI 10.1109/CIT.2010.235     there are a large number of publications about associative classification, however, here we list a few of them.

CBA(Classification based on Associations) proposed by Liu, Hsu and Ma is the first frequent pattern-based classification method [5]. At first it generates all the class association rules and ranks them in the descending order of confidence and support. Rule selection is performed on the ranked rule list with a sequential coverage paradigm and prediction is based on whether the rule condition satisfies the test instance. If there is no rule to match the test instance, the takes the default class value like C4.5.

CMAR(Classification based on Multiple Association Rules) another pattern-based classification method proposed by Li, Han and Pei [6]. To avoid classification bias and overfitting caused by the single rule in classification, CMAR predicts the class label with multiple rules and the classification task uses a weighted 2 on multiple high confidence rules. To predict a test instance, all rules matching the test instance are collected. If all of them have the same class label, assigns this class label to test instance.

However, if they have different class labels, CMAR divides the rules into groups according to class labels and compare the effects of the groups and then assign the class label of the strongest group.

CPAR(Classification based on Predictive Association Rules) proposed by Yin and Han [7] uses the basic idea of FOIL[18] in rule generation. CPAR uses a greedy algorithm to generate rules directly from the training data and selects multiple literals and builds multiple rules simultaneously.

To avoid overfitting, it uses expected accuracy to evaluate rules and choose the best k rules in prediction.

Top-k rule mining[13] discovers top-k covering rule groups for each row of gene expression profiles for pattern- based classification. It uses several pruning steps, such as top-k pruning based on support and confidence upper bounds and backward pruning, to filter the uninteresting rules. Then RCBT classifier is build from these top-k covering rule groups and prediction is performed based on the classification score which combines the support and confidence measures of the rules.

Almost all of associative classification methods have a drawback that should manipulate a large number of association rules. Thus, it is impracticable to use frequent patterns as features in classification tasks. To overcome this problem, we can use closed frequent patterns in pattern- based classification instead of the whole frequent patterns, because the number of closed frequent patterns is much smaller than that of frequent patterns.

The more recent works that most relate to our research are [8, 9, 19]. All of them use closed frequent patterns as features for choosing the most discriminative patterns, however, any of them do not mention how the transformed feature space affects the classification accuracy with respect to different minimum support thresholds.



III. BASIC TERMINOLOGY AND PROBLEM DEFINITION Given a database D and a user-specified minimum  support threshold  (0   1), F = {f1, f2, ?, fm} is the set  of all categorical features and C = {c1, c2, ..., ck} is the set of class labels. Database D contains n transactions D = {xi, yi} ni 1  , where xi  F is a set of features and yi  C is a class label. A subset of F is called an itemset. The support of an itemset X is defined as the number of instances that contain the itemset X and denoted as support(X). For numeric features, the continuous values are discretized first.

Definition 1  Frequent Pattern For a database D, an itemset X is frequent if support(X)  n * , where n is the number of database transactions.

Example: Given a transaction database TDB in Fig. 1 (a) and  = 0.4, then n = 5 and minimum support count = 5 * 0.4 = 2. The generated frequent patterns are shown in Fig 1 (b).

Figure 1. (a) Transaction database and (b) frequent patterns  Definition 2  Closed Frequent Pattern A frequent itemset X is closed if there does not exist an  itemset Y such that X  Y and support(X) = support(Y).

Example: Still take Fig. 1 (a) as an example, where n = 5 and minimum support count = 2. According to the property of closed frequent pattern, the generated closed frequent patterns are shown in Fig. 2 (b). From Fig. 2 (b), we can see that the number of closed frequent patterns is smaller than that of frequent patterns in Fig. 1 (b).

Figure 2. (a) Transaction database and (b) closed frequent patterns     Definition 3  Feature Space Transformation Assume P = {p1, p2, ?, pp} be the set of closed frequent  patterns generated from the given database D at a given .

Therefore, the feature space transformation can be expressed as follows: D = {xi, yi} ni 1   D? = {zi, yi}  n i 1   where zi   P is  a set of generated closed frequent patterns.

Example: Still take Fig. 1 (a) as an example and the generated closed frequent patterns are illustrated in Fig. 2 (b).

Then the transformed feature space is shown in Fig. 3 (b).

Figure 3. (a) original transaction database and (b) transformed feature space

IV. EXPERIMENTAL RESULTS We use several datasets from UCI Machine Learning  Repository [10] to test the effect of closed frequent pattern- based feature space transformation for the classification accuracy. In these experiments, we compare classification results of two transformed feature space. One feature space is from solely on closed frequent patterns and the other is from combining closed frequent patterns and single features. We show the results at a wide range of s. A summary of these datasets is shown in Table I.

TABLE I.  DATASET USED IN EXPERIMENTS  Dataset Instances Features Classes Diabetes 768 8 2  Glass 214 9 7 Heart-statlog 270 13 2  Iris 150 4 3 Waveform 5000 40 3  Wine 178 13 3 Zoo 101 16 7   First, all the continuous features in these datasets are  discretized into binary ones with entropy-based method [11] and discretizations are performed with Weka version 3.6.0 [12]. The discretized datasets are shown in Table II.

TABLE II.  DISCRETIZED DATASET  Dataset Instances(n) Features(F) Classes(C) Diabetes 768 15 2  Glass 214 20 7 Heart-statlog 270 18 2  Iris 150 12 3 Waveform 5000 109 3  Wine 178 37 3 Zoo 101 34 7   The state-of-the-art C4.5 [1] and SMO in Weka and  C5.0 in Clementine are chosen as classification models.

Notice that, classification accuracy is the primary evaluation criterion for the experiments and 10-fold cross validation is used. The results of classification accuracy from the transformed feature space are shown in Table III, IV, V, VI VII, VIII and IX. In these tables, P is the set of closed frequent patterns and F  P is the set of features union of original single features and closed frequent features. From different maximum value of   listed in each table, we can see that the stop point of generating closed frequent is different in each dataset respectively. These tables also give average classification accuracy. From these tables, we conclude that at most cases the transformed feature space from F  P achieves much higher classification accuracy than features only from P.

TABLE III.  CLASSIFICATION RESULTS ON DIABETS  (%) C4.5 SMO C5.0  P F P P F P P F P 5 77.99 77.99 76.30 76.30 79.95 79.95  10 77.86 77.86 77.08 77.08 79.30 79.30 15 76.56 76.56 74.09 74.09 78.65 78.65 20 76.17 75.78 75.39 75.00 79.30 79.56 25 75.91 75.91 75.65 74.09 77.73 79.56 30 71.22 76.04 70.70 75.13 70.96 78.65 35 71.22 75.91 67.84 76.69 70.96 78.39 40 69.40 76.04 68.49 75.78 70.31 78.39 45 69.40 76.17 68.10 75.91 70.44 78.52 50 69.40 75.91 68.62 74.74 70.31 78.39 55 69.14 75.78 68.23 77.08 70.05 78.65 60 69.14 75.78 68.23 77.08 70.05 78.65 65 69.14 75.78 68.23 77.08 70.05 78.65 70 70.05 75.78 69.14 77.08 70.05 78.65 75 67.84 75.78 67.84 76.95 67.84 78.65  Average 72.03 76.21 70.93 76.01 73.06 78.84 Single Feature 75.78 76.82 78.65  TABLE IV.  CLASSIFICATION RESULTS ON GLASS  (%) C4.5 SMO C5.0  P F P P F P P F P 5 77.10 77.10 80.37 80.37 79.91 79.91  10 76.17 76.17 78.97 78.97 82.24 82.24     15 76.64 76.64 77.10 77.10 80.37 80.37 20 72.90 76.64 77.10 78.04 77.57 80.37 25 72.43 76.17 77.57 78.50 78.04 80.37 30 72.90 76.64 77.57 78.97 78.50 80.37 35 71.03 74.77 76.17 78.50 77.57 79.44 40 69.63 74.77 71.03 79.44 75.23 79.44 45 68.69 74.77 69.63 74.30 71.96 79.44 50 68.69 72.90 70.09 75.70 71.96 79.44 55 65.42 73.83 62.62 76.17 66.82 79.44 60 64.49 74.77 63.08 73.83 66.82 79.44 65 50.47 74.77 48.60 73.83 50.00 79.44 70 50.47 74.77 48.60 75.23 50.00 79.44 75 49.07 74.77 49.07 76.17 47.20 79.44 80 49.07 74.77 45.79 74.77 47.20 79.44 85 47.20 74.30 47.20 75.70 47.20 79.44  Average 64.84 75.21 65.92 76.80 65.76 79.85 Single Feature 74.77 73.36 79.44  TABLE V.  CLASSIFICATION RESULTS ON HEART-STATLOG  (%) C4.5 SMO C5.0  P F P P F P P F P 5 82.96 82.96 80.37 80.37 90.00 90.00  10 85.93 85.93 80.37 80.37 90.37 90.37 15 83.33 83.33 81.48 81.48 88.89 88.89 20 81.11 81.11 81.11 81.11 88.89 88.89 25 79.63 79.63 83.33 83.33 87.78 87.78 30 83.33 83.33 78.89 78.89 88.89 88.89 35 81.85 81.85 80.74 80.74 87.41 87.41 40 80.74 80.74 83.33 82.96 87.04 87.04 45 82.96 82.96 83.70 81.48 87.04 87.04 50 82.59 82.59 83.70 82.59 86.67 86.67 55 81.11 80.37 82.22 84.07 83.70 87.04 60 72.96 81.48 72.96 83.33 74.07 87.41 65 72.96 81.48 72.96 83.33 74.07 87.41 70 70.00 81.48 70.00 84.44 70.00 87.41 75 70.00 81.48 70.00 84.44 70.00 87.41  Average 79.43 82.05 79.01 82.20 83.65 87.98 Single Feature 81.85 84.07 87.41  TABLE VI.  CLASSIFICATION RESULTS ON IRIS  (%) C4.5 SMO C5.0  P F P P F P P F P 5 95.33 95.33 94.67 94.67 96.00 96.00  10 95.33 95.33 94.67 94.67 96.00 96.00 15 94.00 94.00 94.67 94.67 96.00 96.00 20 94.00 94.00 94.67 94.67 96.00 96.00 25 94.00 94.00 94.00 94.00 96.00 96.00 30 94.00 94.00 94.67 94.67 96.00 96.00 35 94.67 94.67 95.33 94.67 96.00 96.00  Average 94.48 94.48 94.67 94.57 96.00 96.00 Single Feature 94.00 94.00 96.00  TABLE VII.  CLASSIFICATION RESULTS ON WAVEFORM  (%) C4.5 SMO C5.0  P F P P F P P F P 5 73.06 73.04 78.68 78.66 84.38 84.38  10 74.66 75.58 85.36 85.58 86.26 86.16 15 74.24 76.16 82.86 85.42 82.06 85.80 20 73.06 75.66 77.78 85.84 78.30 87.20 25 62.94 76.06 64.30 85.92 66.44 87.06 30 52.44 76.10 51.18 85.96 52.90 87.18 35 40.50 76.24 41.10 85.92 41.10 87.18 40 40.50 76.24 41.10 85.92 41.10 87.18  Average 61.43 75.64 65.30 94.90 66.57 86.82 Single Feature 76.44 85.96 87.32  TABLE VIII.  CLASSIFICATION RESULTS ON WINE  (%) C4.5 SMO C5.0  P F P P F P P F P 5 94.94 94.94 97.75 97.75 98.88 98.88  10 94.38 94.38 98.88 98.88 98.88 98.88 15 95.51 95.51 98.31 98.31 98.88 98.88 20 96.07 96.07 97.75 97.19 99.44 99.44 25 95.51 94.94 96.63 97.75 98.88 98.88 30 95.51 94.38 94.38 98.31 99.44 98.31 35 92.13 93.26 94.38 98.88 97.19 98.31 40 94.94 96.63 94.38 98.88 98.88 97.75 45 95.51 96.63 93.82 99.44 97.19 98.88 50 95.51 96.63 95.51 98.88 97.19 98.88 55 96.07 96.07 94.94 98.88 96.63 98.88 60 82.02 95.51 81.46 98.88 86.52 100 65 73.60 95.51 71.91 98.31 76.97 100 70 74.72 95.51 72.47 98.31 76.40 98.88 75 44.38 95.51 44.38 98.31 44.38 98.88 80 44.38 95.51 44.38 98.31 44.38 98.88 85 44.38 95.51 44.38 98.31 44.38 98.88  Average 82.91 95.44 83.28 98.45 85.56 98.91 Single Feature 95.51 98.31 98.88  TABLE IX.  CLASSIFICATION RESULTS ON ZOO  (%) C4.5 SMO C5.0  P F P P F P P F P 5 93.07 93.07 96.04 96.04 100 100  10 88.12 88.12 96.04 96.04 100 100 15 91.09 91.09 96.04 96.04 100 100 20 90.10 90.10 96.04 96.04 100 100 25 89.11 89.11 92.08 96.04 98.02 100 30 90.10 90.10 92.08 96.04 98.02 100 35 89.11 89.11 93.07 97.03 97.03 99.01 40 91.09 91.09 93.07 96.04 97.03 99.01 45 91.09 91.09 93.07 96.04 97.03 99.01 50 90.10 90.10 93.07 96.04 97.03 99.01 55 91.09 91.09 93.07 96.04 97.03 99.01     60 86.14 91.09 87.13 95.05 89.11 99.01 65 87.13 93.07 85.15 96.04 89.11 99.01 70 87.13 93.07 86.14 96.04 89.11 99.01 75 89.11 93.07 87.13 97.03 89.11 99.01 80 81.19 93.07 79.21 97.03 79.21 99.01 85 40.59 93.07 40.59 96.04 40.59 99.01  Average 86.20 91.21 88.18 96.16 91.61 99.36 Single Feature 92.08 97.03 99.01

V. CONCLUSION  AND FUTURE WORK In this paper, we introduce closed frequent pattern-based  feature space transformation and the experimental results on real-world datasets show that the transformed feature space from union of closed frequent patterns and original features achieves much higher classification accuracy than that from only the closed frequent patterns with respect to various s.

In the future, I plan to extend my research to mining the most discriminative closed frequent patterns efficiently from a large number of frequent patterns, to address the real-world application challenges and further improve the learning performance.

