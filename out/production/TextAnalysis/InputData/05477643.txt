On Developing an Effectual Progressive Sampling-Based Approach for Association  Rule Discovery

Abstract? Association rule discovery from large databases is one of the most challenging tasks in data mining. The process of frequent itemset mining, the first step in the mining of association rules, is a computational and I/O intensive process necessitating repeated passes over the entire database.

Sampling has often been suggested as an effectual tool to reduce the size of the dataset operated at some cost to accuracy. Data mining literature presents with numerous sampling based approaches to speed up the process of Association Rule Mining (ARM). In our earlier research [29], we presented a proficient progressive sampling-based approach for mining association rules from massive databases.

In this article, we validate our earlier approach with different empirical variations and also present an analysis on the validations using synthetic datasets. The approach starts with an initial sample selection process based on the temporal characteristics and size of the database. Subsequently, the frequent itemsets and the negative border are mined from the initial sample using Apriori algorithm. The patterns in the negative border are then sorted based on their support and the midpoint itemset in the sorted negative border is scanned in different variations (sizes) of the database to check its frequency. If the support of the midpoint itemset is greater than the support threshold, the sample size is progressively increased to a larger size. The aforesaid process is repeated until an optimal sample size is met and then association rules are mined from the optimal sample determined. The empirical validation also results the appropriate database size for conducting the midpoint itemset scan.

Keywords- Data mining; Frequent Patterns; Association Rule Mining (ARM); Sampling; Progressive sampling; Temporal characteristics; Apriori; Negative border; Midpoint itemset.



I. INTRODUCTION Data Mining is an area of contemporary research and  development in Computer Science. Of late, data mining has emerged as a field of extensive research from a wide range of diverse groups of people. Formally, data mining is defined as "The non-trivial extraction of inherent, previously unknown, and potentially valuable information from data" [2, 3, 4]. Commonly, data mining includes an assortment of algorithms: Clustering, Classification, ARM [15] and more.

Of all data mining tasks [22], ARM is considered as the most significant and well researched data mining techniques.

Ever since its introduction by Agrawal et al. [14] in 1993,  the task of ARM has received an immense deal of attention.

ARM techniques are being extensively applied in marketing and retail communities besides many other diverse fields [20]  ARM algorithms aim at extracting interesting correlations, frequent patterns, associations or casual structures that satisfy a predefined minimum support and confidence, among items present in transaction databases or other data repositories [6]. The association rules illustrate a relational association between selected attributes in the database [5]. An association rule will be of the form YX ? , that means for every instance of X  that is true, Y  is also true, where X  and Y  are itemsets in the transaction databases.

Generally, the significance of an association rule is defined by two indicators, support and confidence.

Starting from Agrawal?s [14], a number of researchers have made significant contributions to improve association rule mining. Some noteworthy accomplishments in the field of ARM include, approaches that improve the effectiveness of computing the frequent patterns from massive datasets [21, 24], approaches that apply constraints to identify interesting patterns [24,25,26], and approaches that purges irrelevant association rules by making use of some interestingness measures [23]. The ARM process generally consists of two steps:  ? Frequent itemset generation and ? Discovery of association rules using the frequent  itemsets [17].

The first step of ARM, frequent itemset mining, has been  shown to dictate the computational and I/O requirements requiring repeated passes over the whole database [17]. In spite of the numerous algorithms available for ARM, Apriori has been the most renowned and widely used owing to its effectiveness in knowledge discovery [21]. But, the Apriori algorithm itself suffers from two bottlenecks,  ? Complex frequent itemset generation that requires intensive time, space and memory requirements and  ? Multiple scans of the database [5].

Executing traditional ARM algorithms on massive real-  time databases can take hours or even days, and in the years to come, the problem would only turn out to be even worse.

Lately, researchers have attempted to devise proficient approaches that minimize the I/O and computational requirements of the ARM techniques. Among the various research efforts undertaken to improve the effectiveness of ARM, sampling has proved to be capable of minimizing the  _____________________________________    I/O traffic involved in such data-intensive applications [8].

Sampling in ARM can be defined as the process of reducing the volume of the data to be analyzed [1, 9, 10]. Sampling is a popular data reduction technique that has been effectively applied to a variety of data mining algorithms so as to reduce intensive computational overhead incurred. Sampling speeds up the ARM process by more than an order of magnitude via reducing the I/O costs and radically shrinking the number of transactions to be considered [10]. Generally, the validity of a sample is determined by two significant characteristics namely,  ? Sample size and ? Quality of the sample.

In the context of statistical sampling techniques, the term  quality refers to appropriateness of the sample to capture the definite characteristics of the original database [18].

In the "standard" association-rule mining perspective, the mining studies that were formerly infeasible due to the enormous time requirements were achievable by the use of sampling. Sampling facilitates rapid discovery of preliminary association rules that would help the user in directing the data mining process by refining the criterion for ?interesting? rules [8] ,[17]. Especially, when the data to be processed instigates as a stream flowing at a faster rate, the only possible choice is sampling [27]. In addition to the primary advantages of sampling, considering factors like transaction length and transaction frequency [13] in sample selection can appreciably improve the quality of the sample chosen for ARM.

Suppose if the rules mined from a sample were unsatisfactory, the sample size is increased in most general cases and the mining process is repeated again. The process of increasing the size of the sample continues as an iterative procedure until interesting rules are found [7].The chief limiting factor in devising sampling-based algorithms stems from the fact that the support of an itemset in a sample usually deviates from the support in the whole database.

These luck-of-the draw fluctuations can either leave out itemsets that were frequent in the database but not in the sample and false itemsets that are frequent in the sample but not in the database [8].  Considering the above scenario, it becomes obvious that a fitting sample size is the chief factor influencing the success of the sampling technique [11]. In order to determine the optimal sample size speedily, researchers have recently turned to progressive sampling.

Progressive sampling aims at starting with a smaller sample and then increasing it progressively given that model accuracy improves considerably [1].

? As with the other ARM algorithms, progressive sampling for association rule mining also suffers from two problems.

? The model accuracy of each sample size can be measured either by executing ARM algorithms on the samples [1] or on the complete database [12], which is, nevertheless, very costly.

? To meet efficiency considerations, previous algorithms usually determine the model accuracy of a sample size by executing association rules only on  a sample with this size, and thereby neglecting the phenomenon of randomness [28] that is to be considered for sample selection [11].

We have employed progressive sampling in the proposed approach to decide on an optimal sample size for achieve desirable number and quality association rules. A number of researches have been proposed for ARM based on sampling and progressive sampling [1, 10-13, 19]. The motivation behind this research is that, incorporating the temporal characteristics of data and progressing sampling based on negative border for sample selection will end up with better results.

In our earlier research [29], we presented an efficient progressive sampling-based approach for effectual discovery of association rules from large databases. In this article, we perform an extensive validation of our former approach with different empirical variations and also present an analysis on the validations using synthetic datasets. The approach discovers an optimal sample size that will result in all possible association rules that hold in the actual database.

First of all, an initial sample of certain size is selected relevant to the actual size of the database. The individual transactions in the sample are chosen by considering the temporal characteristics of the data items in the database.

Then, a renowned association rule mining algorithm, Apriori, is applied to the initial sample to determine the frequent itemsets with the aid of a minimum support level. Based on the frequent itemsets generated, we calculate the negative border of the initial sample. Then, the calculated negative border is sorted based on their support levels. The itemset corresponding to the midpoint of the sorted negative border is scanned in different variations of the concrete database to check if it satisfies the minimum support level. If it satisfies, the sample size chosen initially is progressively increased and the above process is repeated until the support of the midpoint itemset in the sorted negative border is less than the minimum support level. Else, the sample chosen is opted as an optimal choice and association rules are mined from the sample. Lastly, we determine a fitting database size for effectual midpoint itemset scan, so as to reduce computational overhead further.



II. NOVEL Progressive Sampling-Based Approach for Effective Association Rule Mining  The innovative progressive sampling-based approach and the different empirical variations proposed for determining a fitting sample of a massive database has been presented in this section. The proposed approach aims to fasten and produce acceptable accuracy in association rule mining. The concept of progressive sampling has been made use of in the proposed approach for identifying a fitting sample of a large database. The task of ARM comprises of two steps:  Frequent itemset generation using support threshold and Association rules generation using confidence threshold.

It has long been identified that the first step of  association rule mining, frequent itemset generation, dominates the computational and I/O requirements.

Sampling can be thought of as an obvious source for    considerably reducing the computational and I/O requirements incurred in frequent itemset generation [24]. In data mining, sampling has been suggested as a powerful data reduction tool operated at some cost to accuracy. Sampling- based approaches can reduce the computational cost and I/O overhead of mining remarkably, as the mining algorithms has to deal with only a small dataset compared to the original database. Time and again, samples proffer good enough accuracy with far less computational cost. However, the choice of the right sample size for ARM is hardly ever known [16]. So as to determine an appropriate sample size, researchers have moved onto progressive sampling.

Progressive sampling works as follows, it starts with an initial sample of data and then progressively increases the size of the sample until an acceptable accuracy (or other performance measure) is achieved [14].

One another important concern in sampling large databases is the random selection of samples without considering any of the characteristics of the database. As so ever, it has also been difficult to determine an optimal sample size for effectively mining association rules. The solution proposed for the above stated problems in the presented approach are, 1) Sample Selection based on the temporal  characteristics of the original database.

2) Progressive sampling based on an estimated negative  border.

The proposed approach is likely to yield considerable  reduction in computational time with some cost to accuracy (Optimality between accuracy and time). It would be worth- mentioning if we could further reduce the computational overhead involved in the proposed approach. The possible improvement that can be effected to the proposed approach is, 3) Determining a suitable database size (m) for  scanning the midpoint itemset in the negative border, rather than performing the entire database scan.

The primary motivation behind the proposed research is that, progressive sampling based on negative border will speed-up and facilitates the process of discovering an optimal sample size for effective ARM. The steps involved in the proposed innovative progressive sampling-based approach are as follows: 1) Selection of initial sample iS  (systematic sampling)  of size ? n ?, based on the temporal characteristics of Database D .

2) Generation of frequent itemsets and negative border  using Apriori algorithm.

3) Sorting of the negative border based on the support  level of itemsets.

4) Selection of the midpoint itemset in the sorted  negative border.

5) Single scan on the left behind records of the database 1D  of size ?( nd ? )? to determine the support of the  midpoint itemset, where iSDD ??1 6) If the support computed for the midpoint itemset is  less than the specified support, the selected sample is  optimal and association rules are mined from it. Else, the sample size ? n ? is increased and steps 2-5 are performed progressively until an optimal sample size is achieved.

7) After determining the optimal sample, we analyze  with different percents of database 1D  to identify a suitable database size ? m ? for performing midpoint itemset scan.

Based on the analysis results, we determine ?m ? such that,  )( ndm ?? .



III. EXPERIMENTAL RESULTS This section describes the results obtained from the  experimentation on the proposed progressive sampling- based approach for ARM. The experiments are conducted in order to assess the practical feasibility of using samples for finding frequent itemsets. The proposed progressive sampling-based approach is implemented in Java (jdk 1.6).

The experimentation is performed using synthetic dataset consists of 30K transactions with 13 items in each transaction. Apriori, the most renowned association rule mining algorithm, has been chosen as the benchmark to evaluate the performance i.e. accuracy and time complexity of the proposed sampling approach. The notion of model accuracy for a particular dataset varies sensitive to relevant interaction parameters (e.g. support, confidence, important items to the user) as well as the inherent properties of the dataset in question. Here, we describe accuracy as the number of association rules discovered for different support thresholds by the proposed approach in comparison to Apriori algorithm rather than the similarity among the individual rules. The number of association rules discovered by the Apriori algorithm and the corresponding results are plotted in Figure 1. The results show that there has not been so many rules left mined by the proposed approach when compared to Apriori and obviously with increased thresholds (more significant patterns), the proposed approach almost achieves the model accuracy of the classical Apriori algorithm.

Figure1.Accuracy graph  The need for sampling in ARM was aggravated to the fact that association rule algorithms require multiple passes over the whole database, and subsequently the database size is by far the most influential factor of the execution time for very large databases.  So, one important measure that illustrates the effectiveness of the sampling-based    approaches is the timing incurred to complete ARM. We have measured the time incurred by the Apriori algorithm and the proposed approach to mine association rules from the same database for different support thresholds. The results obtained are plotted in Figure 2. Clearly, we could see an appreciable reduction in the timing required for ARM using Apriori and the proposed approach.

Figure 2. Time graph    The above two results clearly demonstrate the effectiveness of the proposed approach to mine a good number of association rules with appreciable reduction in the processing time.

? Analysis on empirical variations: The results obtained on the application of empirical  variations to the former approach are presented in this sub- section. Here, we attempt to reduce the computational overhead incurred for performing a midpoint itemset scan on the concrete database D . Clearly, we could eliminate the records in the sample iS  during the midpoint itemset scan because it is does not reflect the true state of the database.

So, in the proposed empirical variation, we perform database scan over iSDD ??1 of size nd ?  .  The results shown in Figure 3 clearly show that the proposed approach achieves considerable reduction in the processing time merely by reducing the database size for the midpoint itemset scan.

Figure 3. Processing time reduced by Empirical variation (d-n)  The empirical variation achieves significant reduction in the processing time incurred for the database scan and ultimately the overall processing time.

The next empirical variation is aimed at reducing the computational overhead further. After determining the optimal sample, we analyze with different percents of database 1D  to identify a suitable database size ? m ? for performing midpoint itemset scan. Based on the analysis results, we determine ?m ? such that, )( ndm ?? . The results obtained for different ? %m ? of the databases for a fixed support of 40% is presented in figure (Figure 4) below.

0.05  0.1 0.15  0.2 0.25  30 60 90 Percentage of Remaining database  P ro  ce ss  in g  tim e  in s  ec   Figure 4. Processing time recorded for different database sizes  Also, to identify a suitable database size ? m ? for performing midpoint itemset scan, we evaluate the support of the midpoint itemset on different ? %m ? of the databases.

The figure 5 shows the graphical depiction of the different supports obtained for the midpoint itemset on different ? %m ? of the databases.

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8  30 60 90 Percentage of Remaining database  M id  i te  m 's  s u  p p  o rt   Figure 5. Support thresholds determined for different database sizes  From the results, it is obvious that there is considerable variation in the support values computed for say 60%, 30%.

Here, we identify a suitable database size ? %60?m ? for performing midpoint itemset scan.



IV. CONCLUSION In this paper, we present an innovative progressive  sampling-based approach that determines an optimal sample for effectual mining of association rules from a massive database. Firstly, we have selected a preliminary sample of some size proportional to the size of the database and dependant on the temporal characteristics of the data items in the database. Secondly, the negative border of the selected    sample is generated using the Apriori algorithm. Thirdly, based on the support of the midpoint itemset in the sorted negative border, the sample size was either progressively increased or association rules are mined by regarding it as an optimal sample. Fourthly, we have determined an optimal database of some fitting size for effectual midpoint itemset scan, so as to reduce computational overhead further.

