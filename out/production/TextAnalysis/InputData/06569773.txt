GUIdiff ? A Regression Testing Tool for Graphical User Interfaces

Abstract?Due to the rise of tablets and smart phones and their impact on everyday life, robust and high-quality Graphical User Interfaces (GUIs) are becoming more and more important.

Unfortunately, testing these GUIs still remains a big challenge with the current industrial tools, which only cater to manual testing practices and provide limited oracle functionalities such as screenshot comparison. These tools often result in large amounts of manual labor and thus increase cost. We propose a new GUI regression testing tool called GUIdiff, which works similar to diff tools for text data. It executes two different versions of a System Under Test (SUT) side by side, compares the GUI states against each other and presents the list of the detected deviations to the tester. The tool is semi-automatic in the sense that it finds the differences completely automatic and that the tester labels them as faults or false positives.



I. INTRODUCTION Graphical User Interfaces (GUIs) represent the main con-  nection point between a software?s components and its end users and can be found in almost all modern applications. This makes them attractive for testers, since testing at the GUI level means testing from the user?s perspective and is thus the ulti- mate way of verifying a program?s correct behavior. However, GUIs are often large and difficult to access programmatically, which poses great challenges for an application?s testability.

Capture and Replay (CR) tools promise to automate testing at the GUI level. However, they have always been controversial, since they require a great amount of manual effort on the part of the testers [1], who need to record and maintain input sequences. Especially in the context of frequently changing GUIs, CR tools generate high maintenance costs, due to fragile input sequences.

In an earlier work [2] we presented GUITest, a Java library which allows to write robustness tests for complex graphical applications. Although these tests were fully automatic, they lacked a fine-grained oracle and consequently only detected severe faults, such as crashes. In this work we build upon our library, but set the focus on regression testing. This gives us more powerful oracles, since we can compare two SUT versions against each other in order to detect anomalous behavior. The idea is to run the two versions in parallel and report the differences in the observed GUI states back to the tester. As we described in [2], even complex GUIs can be executed fully automatic. The general idea is to capture GUI state information (obtained from the operating system?s accessibility API) in widget trees (Figure 1) and automatically derive possible actions for each GUI state. By iteratively selecting and executing particular actions, it is possible to automatically walk through the GUI (Figure 2). The idea  type: TButton  ...

rect: [15, 25, 65, 55]  hasFocus: true enabled: false  yp title: "Button"  Desktop  Window  Button Text SliderMenu  MI MI MI MI  type: TMenuItem  ...

title: "File"  ABC  Derive Actions  Fig. 1. GUIdiff uses the operating system?s accessibility API and represents the current state of a GUI as a widget tree whose nodes refer to the visible controls and are annotated with their property values. It uses the tree to derive and execute possible actions (green).

Desktop Desktop  Different...

Button SliderMenu  File Edit View  Diff.

Text  ?

Example  Button Some Text Menu  File Edit View Help  Slider  1 2  Fig. 3. Detecting differences in the GUI states of two SUT versions.

now, is to run two versions of the same application side by side, execute the same actions (if possible) and observe the differences in the widget trees of their states. This way of comparing GUI states is more robust than the comparison between two screenshots, since, for example, the position of controls does not matter. It will also allow us to only compare the properties of the same controls against each other and ignore newly introduced controls in the more recent version of the SUT. We expect that this will reduce false positives.

DOI 10.1109/ICST.2013.84     a) b) c) d) e)  Fig. 2. Automatic sequence generation: In each state we derive possible actions, select one, execute it and get to the next state.



II. CHALLENGES In order to execute the same actions on the two SUT  versions and in order to detect differences, one needs to first relate identical controls in the different states to each other. A similar problem is found in diff tools, which are often used by programmers to detect changes in source files. The diff tool needs to align the text such that equivalent lines correspond to each other, a problem which is known as sequence alignment.

However, in our case we need to align trees in order to find which controls are identical. There has been done extensive research on tree alignment algorithms [7] which we will make use of. Once we are able to align the widget trees of two GUI states, we can a) execute the same actions on identical control pairs (which allows us to execute the two SUT versions in parallel) and b) compare the properties of the controls (title, color, numeric values, enabled / disabled, ...) against each other in order to detect and report differences. Figure 3 shows the initial states of two versions of the same SUT and their widget trees. The alignment shows which controls correspond with each other. By comparing their property values, we can find and mark the differences, so that the tester can spot them easily.

Tree alignment algorithms can be computationally expen- sive, since the ones which respect hierarchy often exhibit exponential runtime complexity [7]. Despite the fact that even widget trees for complex applications usually do not contain much more than a few hundred controls, this could slow down the execution of the test. Therefore, we will have to find a trade-off between alignment accuracy and performance.

Another challenge might be the amount of detected differ- ences. As shown in Figure 3, we will present the differences in a graphical way, so that they are easy to spot. How- ever, excessive amounts of false positives (purposely removed controls / deliberately changed properties) might cause the tester to get tired of labeling. Therefore, we will strive to list the most ?suspicious? differences first. This could be done such that, whenever the tester labels a difference as false positive, GUIdiff re-sorts the list of differences to penalize similar occurrences. Another option would be to provide a query language that allows to express interests (?Show only differences in dialog X!?). Query languages such as SQL or Prolog might be suitable for this task.



III. RELATED WORK A large part of the available GUI testing tools falls into  the capture and replay (CR) or scripting categories. Popular commercial and open source tools are for example eggPlant,  monkeyrunner for Android and Selenium. As mentioned ear- lier, these tools often induce a lot of manual labor, since broken test cases require frequent maintenance.

There have been interesting approaches to the automatic generation of test cases for GUIs, among which the GUITAR framework [3]. Their idea is to walk through the GUI and to automatically generate an event flow graph, from which they derive test cases by applying several coverage criteria. Other automatic test generation approaches have been developed by Amalfitano et al. [4] who perform crash-testing on Android mobile apps, Marchetto and Tonella [6] who generate test suites for AJAX applications using metaheuristic algorithms and Artzi et al. [5] who use feedback-directed random test case generation for JavaScript web applications.



IV. CONCLUSION We presented an approach for regression testing of appli-  cations with GUIs, which uses side by side testing of two SUT versions to spot differences in the GUI. Our approach is orthogonal to existing techniques, which focus on test case generation. We are interested in the power of the oracle that we obtain by comparing two SUT versions. Currently we walk through the GUI at random, but our approach might benefit from using one of the abovementioned techniques. We are currently implementing the GUIdiff tool for MacOSX and Windows and are looking forward to present first results soon.

