The Incremental Risk Functional: Basics of a Novel Incremental Learning Approach

Abstract?Incremental learning gets increasing attention in research and practice as it has the advantages of continuous adaptation and handling big data with a low computation and memory demand at the same time. Several approaches have been proposed recently for online learning, but only few work has been done to regard the influence of the approximation structure. Hence, we introduce the incremental risk functional which directly incorporates knowledge about the approximation structure into its parameter update. Exemplary, we apply this approach to regression estimation through linear-in-parameter approximators. We show that the resulting learning algorithm converges and changes the global functional behavior only as little as necessary with every learning step, thus resulting in a stable incremental learning approach.



I. INTRODUCTION  Machine learning is used in many complex tasks to auto- matically acquire details about a system of interest based on data from that system. A special case is the online learning task, which means that learning stimuli, i.e., the individual data points, are obtained sequentially and the performance of the function approximator is evaluated after each learning stimu- lus. Hence, classical batch learning is not applicable. Online learning is, on the one hand, important for handling big data which cannot be processed at once. On the other hand, it can be used for online generation of a system or environmental model as well as for online time series prediction or adaptive control.

In these cases, a big or continuously growing batch of data is available. But often timeliness and computational costs are crucial demands. Hence it is not possible to build a hypothesis from all acquired data. Instead, it becomes necessary to revise the current hypothesis incrementally depending only on the most recent learning stimuli or in the extreme case a single stimulus.

Besides the lower computing demand, online learning allows for fast and continuous adaptation to nonstationary systems. Consequently, an online learning algorithm is more widely applicable than batch learning, as it serves as well for an online scenario as for offline learning of data sets which are too large for batch learning.

Formally, the online learning task is characterized by learn- ing on a sequence of data which can be described in rounds.

In round t the learning algorithm is presented an instance xt ? Rd which is transferred from input space to parameter space by the approximation structure through a vector of basis  functions ?(xt) ? Rn. This input is then used to predict its label y?t ? R through an approximator y?t = f(?(xt), ?t) with the parameter vector ?t, which is the hypothesis maintained by the learning algorithm. Afterwards, the correct label yt is given and the learning algorithm suffers an instantaneous loss L(y?t, yt) ? 0 reflecting how wrong the prediction was.

With the new pair of an instance and its corresponding label, henceforth called an example (xt, yt), the learning algorithm updates its hypothesis to ?t+1 with the aim to minimize the cumulative loss  Lc =  ND? t=0  L(y?t, yt) (1)  where ND is the length of the data sequence provided.

But usually, for each learning step many of such revisions with zero error on the new example are possible. Hence, the problem of finding a hypothesis that accounts for the new information is potentially under-determined. So other con- straints are needed to define a unique solution. Furthermore, the resulting hypothesis of online learning depends not only on the data but also on their sequence of presentation, in contrast to batch learning. Here the dilemma arises to integrate a single example such that prior learned knowledge is only affected where appropriate.



II. RELATED WORK  In the batch-case, the goal is a minimal risk of loss, which means to find the hypothesis that minimizes the risk functional [11]  R(?) =  ? L(y, f(?(x), ?)) dF (x, y) . (2)  It measures the risk of any chosen hypothesis ? to incur some loss, given that the input and respective output values are distributed according to the joined probability distribu- tion F (x, y). But this distribution is usually unknown for a learning problem and hence only some value-pairs of (xi, yi) from training data are available. According to [11], the risk functional can be approximated by  Rb(?) =  t  t? i=1  L(yi, f(?(xi), ?)) , (3)  for batch learning with a sufficiently large set, i.e., t?? in the ideal case, of identically, independently drawn data. This method can be applied to an online scenario with an ever   DOI    DOI 10.1109/SMC.2013.259     increasing data set but the computational demand increases also with every new example.

For an online setup the only available information is the most recent example (xt, yt) at a time step t, which fixes the memory and computation demand at design time to the minimum. As the minimization of the resulting loss generally is under-determined, a unique solution can only be found if a regularization-term is added. A widely applied technique is some form of Tikhonov regularization which incorporates the norm ???, e.g., the Euclidean norm, of the parameter vector or a norm ?f(?(x), ?)? of the function to be minimized as well (see e.g., [4], [8]). The impact of regularization is parametrized by a weighting factor. This way the resulting functional behavior can be shown to be as flat as possible, thus preventing over-fitting [9]. But as this regularization does not include any knowledge about prior learning data, it is usually only applicable for learning with a set of training data and not for online learning.

Based on the classical perceptron algorithm [10], a unique solution for a parameter update is to minimize the loss along its gradient through a gradient descent (GD) on an error potential that is given by the example [13], also known as Passive-Agressive [1]. This is analogous to a regularization term penalizing the amount of change ???? of the parameter vector and is parameterizable as well through the step size of the GD, also referred to as the learning rate. The learning rate can be chosen in every learning step so as to directly minimize the loss, which is called normalized gradient descent.

This learning algorithm allows for a continuous and flexible adaptation but is prone to noise.

Besides these first order algorithms, in the approach of second order learning the way an example is used to update the parameter vector is changed through previous data as well.

This way a higher robustness to noise is achieved at the cost of less flexibility to adapt to changes in the data. In the idea of the Relaxed Online Maximum Margin Algorithm [7] and the Ellipsoid Method [12] a set of possible parameter vectors is kept that is consistent with prior examples as well as the current example. This idea is further extended to a Gaussian distribution of parameter vectors in the work of Confidence Weighted Learning [5], Adaptive Regularization Of Weights [2] and Gaussian Herding [3]. These approaches are similar to the well known recursive least squares (RLS) [6]. Here, an estimation of the covariance matrix W of the parameter vector ? is incrementally updated together with a forgetting factor and used to increase or decrease the adaptation of single parameters in a learning step. The covariance matrix and the parameter vector together define a Gaussian distribution of parameter vectors. If the covariance matrix is the identity W = I , RLS and GD yield the same result for learning on one example.

Hence, GD as a flexible first order and RLS as a noise- robust second order algorithm represent the state of the art of online learning algorithms. Yet, interpreting equation (3) as the sum over all examples in a sequence up to a time step t, online learning directly regards the last element of the sum.

The remaining N ? 1 examples are only indirectly available through the current parameter vector ?t. All mentioned meth- ods incorporate this parameter vector into the optimization task, but none of them with respect to the actual functional behavior of the approximator f(?(x), ?t), i.e. the influence of  the approximation structure ?(x). Thus the presented learning approaches do not minimize the risk in every learning step as the amount of influence of a parameter is not regarded in the additional constraints. The aim of this paper is hence to define a learning paradigm based on the optimization goal in an online setup with respect to the functional behavior, hence the incremental risk functional.



III. APPROACH TO ONLINE LEARNING  A. Incremental Risk Functional  In online learning, the knowledge of prior examples {1, . . . , t ? 1} is condensed into the functional behavior f(?(x), ?t) of the current hypothesis. Accordingly, the global functional behavior should change as little as possible in order to not destroy prior information. At the same time, the example should be incorporated as good as possible which might be a conflicting demand.

Following this idea, we propose an incremental formulation of the risk functional containing the most recent example and, similar to a regularization, the overall change of the output, with a factor ?t > 0  Rinc(?) = ?t ? ? ?  L(f(?(x), ?t), f(?(x), ?))dx  +  L(yt, f(?(xt), ?)) . (4)  A new parameter vector is chosen according to the mini- mization of Rinc(?). As the probability density F (x, y) is unknown, the change of the functional behavior is accounted for by an equally weighted integral for the input x over the considered bounded input space ?.

The factor ?t can be seen as to steer the stiffness of the approximator. The bigger its value is, the more a change of the approximator is punished. If a lot of data is condensed in the learning process and hence the contribution of the current example is comparably small, a big value of ?t accounts for this evidence. In contrast, in the initial learning phase with no or only low data background, this weighing factor should be low, thus putting more weight on the present example. Con- sequently it should be chosen as a monotonically increasing value (?t ? ?t?1) as the learning process progresses.

B. Application to LIP Approximators  To investigate the properties of the incremental risk func- tional, we consider the case of regression estimation for Linear- In-Parameter (LIP) approximators. The problem of regression estimation can be expressed by minimizing the risk functional with squared loss L(a, b) = (a? b)2 [11]. A general N -node LIP-approximator is given by  f(?(x), ?) =  N? i=1  ?i?i(x) = ? T?(x) (5)  defining a linear subspace of functions from Rd to R1 with the basis {?i(x)}ni=1 [6]. In this case, the parameter vector ?t is updated incrementally with respect to the minimization of  Rinc(?) = ?t ? ? ?  ((?t ? ?)T?(x))2dx  +  (yt ? ?T?(xt))2 . (6)     As Rinc(?) ? 0 ? ?t > 0, if a unique solution of a zero partial derivative exists, it is the minimum of the incremental risk functional. For LIP-approximators the resulting equation  ?Rinc ??i  = 0 ? i ? [1;N ] (7)  has the form of a linear system of equations which can be written as  (A+  ?t B(xt))?t+1 = A?t +   ?t ?(xt)yt (8)  with symmetric matrices  (A)i,j =  ? ?  ?i(x)?j(x)dx and (B(xt))i,j = ?i(xt)?j(xt) .

With the basis {?i(x)}ni=1 in the function space L2(?), A is the Gramian matrix given by the standard inner product on functions. Hence the matrix A is positive definite for linearly independent ?i(x) and has an inverse A?1. Choosing the substitution u = v = 1??t?(xt), the second part of (8) can be expressed as 1?tB(xt) = uv  T and thus the Sherman?Morrison formula yields the entire inverse  (A+ uvT )?1 = A?1 ? A ?1B(xt)A?1  ?t + ?(xt)TA?1?(xt) . (9)  So the minimization has a unique solution and as the matrix A is constant for any given approximation structure and can be inverted offline, equation (9) poses a numerically cheap way to compute the inverse for a new example [9].

C. Local Convergence  If the parameters ?t are not changed, the incremental risk functional has the value  Rinc(?t+1 = ?t) = (yt ? ?Tt ?(xt))2 , (10) which is equivalent to the local error of the approximator. The partial derivative for this case  ?Rinc(?t+1 = ?t)  ??t+1,i = 2(yt ? ?Tt ?(xt))?i(xt) (11)  shows that the gradient is only zero, if the target value yt is already met by the approximator or the influence of all ?i(xt) vanishes. Otherwise, a change of parameters ? = ?t+1??t ?= 0 minimizes the risk functional and we can conclude  0 < ?t ? ? ?  (?T?(x))2dx  ? ((?t +?)  T?(xt)? yt)2 < ?t ? ? ?  (?T?(x))2dx  +((?t +?) T?(xt)? yt)2  < (?Tt ?(xt)? yt)2 . (12) Consequently, the local error of the new parameter vector after learning is less than before. Thus learning converges locally. The global convergence properties are investigated by an example in the next chapter, as it cannot be expected that the global error is monotonically decreasing because of the insufficient information of a single example.

0 50 100 150  ?2     Number of Learning Stimuli (N L )  C um  ul at  iv e  Lo ss  ( L C  )      GD RLS IRMA  Fig. 1. Cumulative loss of online learning algorithms.

Looking at the influence of the stiffness ?t on equation (8) in one incremental learning step, obviously for ?t ? ? the old parameter vector ?t is kept, i.e., ?t+1 = ?t. On the other hand, for ?t ? 0 equation (8) takes the form  B(xt)?t+1 = ?(xt)yt (13) ?  N? i=1  ?j(xt)?i(xt)?t+1,i = ?j(xt)yt ? j ? [1;N ] .

?  N? i=1  ?i(xt)?t+1,i = yt .

So, in one extreme case, the result is a parameter vector ?t+1 that does not change the approximation at all. In the other extreme case, it is forced to reproduce the example exactly.

The magnitude of change in parameters ? increases for a decreasing ?t and the adjustment of ?t hence allows to choose how much the functional behavior might change.



IV. EXEMPLARY APPLICATION  The investigation scenario is chosen to satisfy two require- ments. On the one hand the setup is as simple as possible, as it is restricted to a one-dimensional function, to allow a focus on the learning algorithm and its methodological behavior. On the other hand the used approximator is chosen to constitute nonlocal basis functions {?(x)}i with each having a different amount of influence on the functional behavior. Thus the demand on the stability of the learning algorithm is strong. The approach is compared to the two basic state-of-the-art methods of parameter adaptation, gradient descent and recursive least squares. So to demonstrate the resulting learning algorithm IRMA (Incremental Risk Minimization Algorithm) for a LIP approximator, a polynomial fp : R? R of order N  fp(x, ?) = N? i=0  ?ix i (14)  is used, starting with the initial parameter vector ?0 = 0.

The stiffness is initialized to a small value ?0 = 0.1 to     0 1 2 3   0.2  0.4  Input: x  O ut  pu t:  y  0 1 2 3   0.2  0.4  Input: x  O ut  pu t:  y  0 1 2 3   0.2  0.4  Input: x  O ut  pu t:  y  0 1 2 3   0.2  0.4  Input: x  O ut  pu t:  y  0 1 2 3   0.2  0.4  Input: x  O ut  pu t:  y  0 1 2 3   0.2  0.4  Input: x  O ut  pu t:  y 0 1 2 3   0.2  0.4  Input: x  O ut  pu t:  y  0 1 2 3   0.2  0.4  Input: x  O ut  pu t:  y  0 1 2 3   0.2  0.4  Input: x O  ut pu  t: y  Polynomial Order: 4 Polynomial Order: 6 Polynomial Order: 10  Incremental Learning  N o  . L ea  rn in  g   S ti  m u  li:  N  o . L  ea rn  in g   S  ti m  u li:   N o  . L ea  rn in  g   S ti  m u  li:   Reference: Complete Batch  N o  . L ea  rn in  g   S ti  m u  li:       Target Function Learning Stimuli Approximation  0 1 2 3   0.2  0.4  Input: x  O ut  pu t:  y  0 1 2 3   0.2  0.4  Input: x  O ut  pu t:  y  0 1 2 3   0.2  0.4  Input: x  O ut  pu t:  y  Fig. 2. Results of online learning by minimizing the incremental risk functional in comparison to standard batch regression.

represent that the starting point supplies only low information.

As the information increases with every example, the stiffness increases as well stepwise by ?t = ?t?1 ? 1.05. As a target function ft we use the nonlinear nonmonotonic function  ft(x) = x ? e?x2 , x ? [0, 3] (15) with output values in the interval [0, 0.43]. The online learning algorithm is provided with random examples whose target  values are disturbed by equally distributed noise in the range ?0.05, i.e. about ?10% of the codomain range. In a first scenario we compare IRMA to GD and RLS with respect to the cumulative loss (1) after NL examples (xi, yi), i.e., the prediction error along the online learning. 500 trial runs with a sequence of 150 examples each are randomly generated and used to train a polynomial of 6th order incrementally. GD is set up with a normalized learning rate. RLS is initialized     with the identity as a covariance matrix and a forgetting factor of 0.9. Both are combined with a Tikhonov regularization of ? ? ?T? with ? = 0.05. These parameters are optimized by systematically scanning the range of possible values. The mean cumulative loss as well as its upper and lower standard deviation are shown in Figure 1 on a logarithmic scale. The results show that IRMA leads to the lowest cumulative loss and shows nearly no variance of the results for different learning sequences. Especially GD is an unsuitable method for online learning of polynomials as the different amounts of influence of the parameters and their global effect destabilize the gradient learning. Contrariwise RLS and IRMA are stable and grow sub-linearly, hence guaranteeing convergence. Yet, the cumulative loss of IRMA is lower, as it minimizes the worst case in every step.

Two additional aspects of this scenario are investigated to show details of IRMA. On the one hand, we take polynomials of 4th, 6th and 10th order for the approximation. The expres- siveness with 4th and 6th order is appropriate for the target function whereas the 10th order tends to over-fitting because it is too expressive. On the other hand, we take snapshots of the resulting functional behavior after 10, 80 and 150 examples have been presented in order to demonstrate the convergence properties. This way we see how the approximation changes with increasing information and expressiveness. For compari- son, additionally a batch regression is made as a benchmark with standard polynomial fitting of Matlab (through the Van- dermonde matrix) using the total amount of 150 examples altogether. Thus, we can compare the result to that of an algorithm with complete knowledge of all data which would be unfeasible for big sets of data.

The results of these investigations are shown qualitatively in Figure 2. At first with only 10 examples, the approximation is still bad and especially the 10th order polynomial is not fully set up. The provided information with these sparse samples is too low to represent the target function appropriately. But yet at an input of about x = 1.5, every learned output nearly meets the target function as here some data cumulate. The 10th order polynomial shows that in regions of low data density the initial value of zero is preferred. Hence, a localized impact of examples is achieved even for an approximator with globally active basis-functions.

As expected, with more examples, the approximation gets better. With 80 presented examples, the regression has con- verged fairly well for any order of the polynomial as the total target function is covered. Even though the target values are subject to noise, presenting 70 more examples does not affect the approximation much. A notable feature is that even the polynomial of higher order is stable in the online scenario with sparse and noisy data. And the more data are presented, the more similar get the results for different polynomial orders.

In comparison to the benchmark of batch regression over the total data set, nearly no difference can be seen. This is supported by a numerical investigation. To account for the influence of the randomly chosen learning sequence, here the above investigation is repeated 1000 times and for every sequence the mean squared error (MSE) of the resulting approximation with respect to the ground truth is calculated on 1000 test points. The average and the standard deviation of the MSE are shown in Table I. The results show that  the MSE as well as the standard deviation decreases with an increasing amount of data. As expected, with a low data density, the simple batch learning tends to over-fitting which results in a big difference to the ground truth, whereas IRMA without any additional regularization is stable. With more data, batch learning achieves nearly no error and the comparison shows a similar error for IRMA even though it has much less information available at a time. Thus IRMA is more stable than its batch-counterpart and it provides a reasonable method to inhibit over-fitting. Accordingly, online learning is no drawback in the quality of the result, but provides the possibility to incorporate new knowledge into the approximator at every time step and still has a low demand on computation and memory, even for big data sets.



V. DISCUSSION  In a nutshell, along with the minimal local error every approach optimizes different optimization criteria. While GD learning minimizes the change of the parameter vector, RLS learning minimizes the parameters variance. The newly intro- duced IRMA minimizes the change of the global functional behavior. Consequently, the advantage of the incremental risk functional is that the resulting learning algorithm thus enforces a localized learning which does not depend on whether the basis {?(x)}i consists of localized or global functions. This behavior is also reflected in the exemplary investigation as the stability of the resulting learning algorithm can be seen even for overly expressive and nonlocal approximators and a stable low cumulative loss is achieved contrary to other online methods. If the basis {?(x)}i consists of functions with the same amount of influence on the global functional behavior, the result of IRMA is similar to that of GD. In comparison to the batch algorithm a good result is achieved that is just as stable even though the data are only available one at a time.

This way, a low computational demand is achieved even for big data.

In this work we focused on regression estimation and LIP approximators as an exemplary online learning scenario. Both restrictions can be relaxed, keeping the fundamental principles.

In an online setup, one example should have only localized effects as its expressiveness is limited. Hence the idea of minimal change of the global functional behavior and maximal local incorporation of an example at the same time is the same for other tasks and for approximators with nonlinear parameter influence.

With the nonlinearity of parameter influence, the mini- mization of the incremental risk functional gets more com- plex. Especially local minima can exist for both parts of the minimization (the functional behavior and the local er- ror) and accordingly an analytical minimization will be hard to get and an approximate solution is confronted with the well known problems of nonlinear minimization. Furthermore, pattern recognition or density estimation as well as other learning tasks can be expressed within the incremental risk functional through the choice of another loss function. Its basic idea is hence not unique to regression estimation so that the resulting behavior of a learning algorithm for other tasks can be expected to be comparable.

With the approach of the incremental risk functional it is possible to get more stable online learning algorithms for     TABLE I. THE TABLE SHOWS THE AVERAGE MSE TO THE GROUND TRUTH AND ITS STANDARD DEVIATION OVER 1000 RANDOMLY DRAWN LEARNING SEQUENCES. THE MSE IS COMPARED FOR ONLINE AND BATCH LEARNING WITH DIFFERENT AMOUNTS OF EXAMPLES.

No. of examples Polynomial Order: 4 Polynomial Order: 6 Polynomial Order: 10 10 (incr.) 3.1 ? 10?3 ? 2.9 ? 10?5 5.5 ? 10?3 ? 6.0 ? 10?5 1.2 ? 10?2 ? 1.2 ? 10?4 10 (batch) 4.4 ? 10+2 ? 1.9 ? 10+8 7.2 ? 10+7 ? 5.1 ? 10+18 1.8 ? 10+14 ? 1.3 ? 10+31 80 (incr.) 3.0 ? 10?4 ? 1.2 ? 10?8 2.2 ? 10?4 ? 1.5 ? 10?8 3.1 ? 10?4 ? 2.3 ? 10?8 80 (batch) 2.0 ? 10?4 ? 5.3 ? 10?9 1.0 ? 10?4 ? 1.2 ? 10?8 4.0 ? 10?4 ? 5.3 ? 10?6 150 (incr.) 2.0 ? 10?4 ? 2.1 ? 10?9 9.4 ? 10?5 ? 3.0 ? 10?9 1.2 ? 10?4 ? 3.1 ? 10?9 150 (batch) 1.6 ? 10?4 ? 6.4 ? 10?10 5.0 ? 10?5 ? 6.7 ? 10?10 8.4 ? 10?5 ? 8.7 ? 10?9  different tasks and approximators than previous adaptation methods provided. But still this is just a starting point for a new learning paradigm. More theoretical investigations of our proposed algorithm need to be done to ensure not only local but global convergence on the long run and to further clarify the advantages over GD and RLS. Additionally, approximators with nonlinear influence of parameters are of interest for further investigations as well as different online learning tasks like pattern recognition.



VI. CONCLUSION  In this work we introduced the incremental risk functional as a novel approach for online learning, i.e. on big data or continuously at runtime. We showed that for the case of regression estimation by approximators with linear parameter influence, the minimization can be solved analytically and the resulting learning algorithm is locally contracting while maintaining a minimal change of the global functional be- havior in every online learning step. Thus, the worst case of an increasing total loss is minimized. As an example, the resulting online learning algorithm was applied to a polynomial approximator and demonstrated that even with sparse data the approximation is stable and with dense data it achieves similar results as a regression over the complete batch of data, but with much lower computational demands. At the same time localized learning is achieved with incrementally presented data although a basis of nonlocalized functions is used. And the approach of online learning allows to flexibly adapt to changes in the underlying learning data.

Concerning the different optimization criteria of online learning approaches, it can be seen that the risk minimization learning leads to a similar behavior as gradient descent learning if the amount of influence of all parameters is the same. But it is a more general approach as it incorporates knowledge about the approximation structure and the effects of its parameters on the output into the parameter update. The incremental risk functional is thus very reasonable for online learning problems and for dealing with big data.

Regardless of the task and approximator it provides a method to derive a stable online learning algorithm as it allows to incorporate new examples incrementally only locally and thus ensures that prior learned knowledge is kept as far as possible. Hence the worst case risk is minimal in every single step.

