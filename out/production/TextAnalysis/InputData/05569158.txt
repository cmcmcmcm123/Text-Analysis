

Abstract?Classification is one of the most popular data mining techniques applied to many scientific and industrial problems.

Recently,fuzzy association rule has been extensively studied in classification. In this paper,a new classification model is pro- posed,which is based on interpretable fuzzy association rules and automatic generating membership functions. In addition,a modified algorithm for classification is presented. The results on five data sets indicate that the proposed classifier can be regarded as an accurate and effective classification technique. Compared with other classification approaches and fuzzy logics, a relative error rate is lower in our results.

1. INTRODUCTION  Today is an era of data collection and storage. However, knowledge representation and information acquisition from huge data are a bottleneck for many engineering applications.

Currently many researchers are working hard to extract useful information via designing effective classifiers. One way to create a classifier is to use a learning algorithm to construct a classifier from a training set of objects whose classification(s) is known.

In real-world applications, many data may be fuzzy, varied data types or conflict but useful, discovering quantitative association rules has been regarded meaningful and important.

Since Agrawal et al.[1] introduced the notion of association rules, association rules mining has become a focal point of research in knowledge discovery. The algorithm proposed by Srikant and Agrawal [8] proceeded by partitioning attribute domains into several intervals and transforming quantitative values into binary ones in order to apply the classical mining algorithm. Generally, this method always leads to the so-called ?sharp boundary? problem because it may under-emphasize or over-emphasize the elements near the boundaries of intervals in the mining process. A natural way to deal with this problem is that an element around the boundary could partly belong to two adjacent intervals, which is fuzzy in nature. The use of fuzzy sets in connection with association rules has been motivated by numerous authors ([9]?[14]). Compared with traditional association rules, fuzzy association rules provide linguistic explanation and overcome the sharp boundary prob- lems in continuous (numerical) attributes, and also can be used to analysis the mixture relations both in discrete (nominal) attributes and continuous attributes.

One of the earliest automated methods for finding fuzzy sets in association rule mining has been proposed in [15].

Apart from common clustering techniques such as k-means, approaches based on genetic algorithms have recently been developed [16]. An extension of the equi-depth partitioning algorithm, which allows for combining crisp values, intervals and fuzzy sets in the antecedent and consequent part of association rules, has been proposed in [17]. As a disadvantage of purely data-driven approaches to partitioning, they cannot guarantee the linguistic interpretability of the resulting fuzzy sets. In principle, this problem can be avoided by using predefined fuzzy partitions [18]. However, it is impossible for the user to specify all fuzzy partitions by hand, if the data set comprises a large number of attributes.

In order to cope with the above problems, we propose a new algorithm based on the AFS (Axiomatic Fuzzy Set) theory ([19]?[26]). In order to verify the quality and efficiency of AFS fuzzy associative classification(AFSFAC), we apply the proposed algorithm to find the classification rules for five benchmark data sets?the Iris data, the Wisconsin breast cancer data, the Wine data, the Sonar data and the Pima classification data and compare its results with other rule- based or associative classifiers. The illustrative results show that AFSFAC outperforms the other classifiers on almost each data set presented.

The rest of the paper is organized as follows: Section 2 gives an overview of AFS theory. Section 3 introduces fuzzy association rule and the modified degree of support.

Section 4 presents an algorithm based on AFS to design a fuzzy classifier. The algorithm is applied to five data sets and the experiment results are showed in section 5. Finally, conclusions are presented in Section 6.

2. OVERVIEW OF THE AFS THEORY  In this section, we recall some notations and properties of AFS fuzzy logic that will be of immediate use in the study. For more details the reader is referred to [4], [5], [7]. In essence, the AFS framework supports the studies on how to convert the information in databases into the membership functions and their fuzzy logic operations.

Definition 1: ([21]) Let M be a non-empty set. Then the set EM? is defined by     EM? = { ?  i?I( ?  m?Ai m)|Ai ? 2 M , i ? I, I is any  non-empty indexing set }.

A binary relation R on EM? is defined as follows:for any?  i?I( ?  m?Ai m), ?  j?J( ?  m?Bj m) ? EM ?,[?  i?I( ?  m?Ai m) ] R  [? j?J(  ? m?Bj m)  ] ?? (i)  ?Ai (i ? I), ?Bh (h ? J) such that Ai ? Bh; (ii) ?Bj (j ? J), ? Ak (k ? I), such that Bj ? Ak.

It?s clear that R is an equivalence relation. EM?/R is denoted by EM . The notation  ? i?I(  ? m?Ai m) =?  j?J( ?  m?Bj m) means that ?  i?I( ?  m?Ai m) and? j?J(  ? m?Bj m) are equivalent under equivalence relation  R. In what follows, each ?  i?I( ?  m?Ai m) ? EM is called a fuzzy concept.

Theorem 1: ([23]) Let M be a non-empty sets. Then (EM,?,?) forms a completely distributive lattice under the binary compositions ? and ? defined as follows. For any?  i?I( ?  m?Ai m), ?  j?J( ?  m?Bj m) ? EM,? i?I  ( ?  m?Ai  m) ? ? j?J  ( ?  m?Bj  m) = ?  k?ItJ  ( ?  m?Ck  m) (2.1)  ? i?I  ( ?  m?Ai  m) ? ? j?J  ( ?  m?Bj  m) = ?  i?I,j?J (  ? m?Ai?Bj  m) (2.2)  where for any k ? I t J (the disjoint union of I and J , i.

e. , every element in I and every element in J are always regarded as different elements in I t J), Ck = Ak if k ? I , and Ck = Bk if k ? J .

(EM,?,?) is called the EI (expanding one set M ) algebra over M?one type of AFS algebra. For ? =?  i?I( ?  m?Ai m), ? = ?  j?J( ?  m?Bj m) ? EM , ? ? ? ?? ? ? ? = ? ? ?Ai (i ? I), ?Bh (h ? J) such that Ai ? Bh.

Definition 2: ([5]) Let ? be any concept defined on the universe of discourse X . R? is called a binary relation (i. e. , R? ? X ?X) of ? if R? satisfies: x, y ? X , (x, y) ? R? ? x belongs to concept ? at some degree and the degree of x belonging to ? is larger than or equal to that of y, or x belongs to concept at some degree and y does not at all.

Definition 3: ([5]) Let X be a set and R be a binary relation on X . R is called a sub-preference relation on X if for x, y, z ? X , x 6= y, R satisfies the following conditions: 1. If (x, y) ? R, then (x, x) ? R; 2. If (x, x) ? R and (y, y) /? R, then (x, y) ? R; 3. If (x, y), (y, z) ? R, then (x, z) ? R; 4. If (x, x) ? R and (y, y) ? R, then either (x, y) ? R or (y, x) ? R.

A concept ? is called a simple concept on X if R? (defined by Definition 2 ) is a sub-preference relation on X . Otherwise ? is called a complex concept on X .

Definition 4: [23] Let X, M be sets, 2M be the power set of M, ? : X?X ? 2M . (M, ?,X) is called an AFS structure if ? satisfies the followings  AX1: ?(x1, x2) ? X ?X, ?(x1, x2) ? ?(x1, x1); AX2: ?(x1, x2), (x2, x3) ? X?X, ?(x1, x2)??(x2, x3) ?  ?(x1, x3).

Then X is called the universe of discourse, M is called the attribute set and ? is called the structure.

Let X be a set of objects and M be a set of simple concepts on X . If ? : X ? X ? 2M is defined as follows: for any (x, y) ? X ?X ,  ?(x, y) = {m | m ?M, (x, y) ? Rm} ? 2M (2.3)  where Rm is the binary relation of simple concept m ? M (refer to Definition 2). Then (M, ?,X) is an AFS structure.

we define the membership functions of fuzzy concepts in EM via the AFS structure of data and EI algebra EM . Let (M, ?,X) be an AFS structure and S ? 2X be an ?-algebra over X . We assume that for each simple concept ? ?M , there exists m? a measure on S with 0 ? m?(U) ? 1 for any U ? S. For the fuzzy concept ? =  ? i?I(  ? m?Ai m) ? EM ,  if ?x ? X,?i ? I , A?i (x) ? S, then the membership function of fuzzy concept ? is defined as follows. For any x ? X ,  ??(x) = sup i?I  ?? ? ??Ai  m?(A?i (x))  ?? (2.4) where  A? (x) = { y ? X | ?(x, y) ? A } (2.5)  A? (x) is the set of all elements in X whose degrees of belonging to concept  ? m?Am are less than or equal to that  of x.

In general, the measure m? for a simple concept ? is  constructed to weight the completeness of the set A ? S as a extent of concept ?. In practice, the measure m? can be induced by a function ? : X ? [0,?) as the following Definition 5.

Definition 5: ([23]) Let X be a set, S is a ?-algebra over X . ? : X ? R+ = [0,?). 0 <  ? x?X ?(x) < ?. For any  A ? S, we define a measure m over ?-algebra S on X ,  m(A) = ?  x?A ?(x)? x?X ?(x)  . (2.6)  3. FUZZY ASSOCIATION RULE AND CLASSIFICATION In this section, we will introduce the definitions and nota-  tions for fuzzy association rule and fuzzy associative classifi- cation.

Let X and Y are fuzzy sets, the two commonly used T -norm are product: T (x, y) = xy and min: T (x, y) = min(x, y). For fuzzy sets X and Y , ?X(t) is the degree that a tuple t belongs to a fuzzy set X , and then, the support and confidence of the fuzzy association rule are:  Dsupp(X ? Y ) = ?  t?D T (?X(t), ?Y (t)) |D|  ,  Dconf(X ? Y ) = ?  t?D T (?X(t), ?Y (t))? t?D ?X(t)  .

There are many generalizations of confidence measure pre- sented in the literature [2], [3].

We apply fuzzy classification rules that each describe one of the C classes in the data set. The rule antecedent is a fuzzy description in feature space and the rule consequent is a crisp (non-fuzzy) class label from the set {1, 2, . . . , c}.

Let C = {C1, C2, . . . , Cc} be a finite set of class labels. No- tably, in associative classification, only those rules each with one single class label as its consequent need to be considered.

Since Cc is a crisp set with value {0, 1}: ?Cc(tp) = 1 if p ? classCc otherwise ?Cc(tp) = 0, the support of the fuzzy association rule Ac ? Cc is calculated as follows:  Dsupp(Ac ? Cc) = ?  t?D T (?Ac(t), ?Cc(t)) |D|  =  ? t?Cc T (?Ac(t), ?Cc(t))  |D|  +  ? t/?Cc T (?Ac(t), ?Cc(t))  |D|  =  ? t?Cc ?Ac(t) |D|  =  ? t?Cc ?Ac(t) |Cc|  ? |Cc| |D|  .

Let  Supp(Ac ? Cc) = ?  t?Cc(?Ac(t)) |Cc|  (3.7)  it only depends on the number of tuples in class c regardless of the tuples of other classes. Compared with the Dsupp, it can avoid the unbalance of the number of each class, which may ignore the interesting rules of the class with a few samples while the other classes have much more samples.

In this paper, we will employ it as the support. And the confidence is calculated as follows:  Conf(X ? Y ) = Dconf(X ? Y )  = ?  t?D T (?Ac(t), ?Cc(t))? t?D ?Ac(t)  =  ? t?Ac T (?Ac(t), ?Cc(t))?  t?D ?Ac(t)  +  ? t/?Ac T (?Ac(t), ?Cc(t))?  t?D ?Ac(t)  =  ? t?Ac ?Ac(t)? t?D ?Ac(t)  .

We will employ this two measures Supp and Conf to our proposed classifier. The Supp is used to filter the concepts that have litter effect on each class and Conf is used to measure the strength of the fuzzy association rules of the class.

4. A NEW ASSOCIATIVE CLASSIFICATION ALGORITHM BASED ON AFS  In this section, we will present a new associative classifica- tion algorithm based on AFS theory. The pseudocode of the main idea showed as following:  AFSFAC{class, attributes, ex., min Supp, ?1, ?2}  ? Learned rules? {} ? Rule? Learned one rule{ex., ?1, ?2} ? while condition{ex.} > ?2  ? Learned rules? Learned rules+Rule ? ex.? ex.? {ex. correctly classified by rule} ? Rule? Learned one rule{ex., ?1, ?2}  ? Return Learned rules  A. Preprocess  Let F = {f1, f2, . . . , fs} is the feature set of the data set, all tuples of the data set are D = {t1, t2, . . . , tn}. And the data are labeled by c classes, which are C1, C2, . . . , Cc, i.

e. , D =  ? 1?i?c Ci, Ci  ? Cj = ?, i 6= j. For any tuple  t = (?1, ?2, . . . , ?r), ?i = fi(t)(i = 1, . . . , r) is the value of t on feature fi. Let M = {mij | 1 ? i ? s, 1 ? j ? si} be the set of simple concepts associating to the features. For example, if mj1,mj2,mj3 and mj4 are the fuzzy concepts ?small?, ?medium?, ?no-medium?, ?large? associating to the feature fj respectively, the weight function of them can be defined according to the training data and their semantics as follows :  ?mj1(ti) = hj1 ? fj(ti) hj1 ? hj2  (4.8)  ?mj2(ti) = hj4 ? |fj(ti)? hj3|  hj4 ? hj5 (4.9)  ?mj3(ti) = |fj(ti)? hj3| ? hj5  hj4 ? hj5 (4.10)  ?mj4(ti) = fj(ti)? hj2 hj1 ? hj2  (4.11)  where j = 1, 2, . . . , s,  hj1 = max 1?i?n  {fj(ti)}  hj2 = min 1?i?n  {fj(ti)}  hj3 = ?i=n  i=1 fj(ti) n  hj4 = max 1?i?n  {|fj(ti)? hj3|}  hj4 = min 1?i?n  {|fj(ti)? hj3|}  B. The algorithm of proposed classifier  Since each class of the data set has the same method to generate the association rules, we only need to give the method of one class. Now, we have the following steps to design the classifier for one class: Step1: For each feature fi(i = 1, . . . , s), choose the corre- sponding simple concepts(fuzzy partition) mij , j = 1, . . . , si, where si is number of fuzzy set(fuzzy partition) of the feature fi, and so the total of the fuzzy sets is  ?r i=1 si.

Step2: Let the form of association rule is Ac ? Cc, where Ac represents the antecedent of the rule of class c and Cc represents class c. Employing the Supp and Conf defined in previous section, suppose inirule is the fuzzy sets pruned     by Supp, i. e. , inirulec = {mij | Supp(mij ? Cc) ? min Supp, i = 1, . . . , r, j = 1, . . . , si, c = 1, . . . , C}, where min Supp is the minimum threshold of Supp.

Step3: Generate the description of the class c from the fuzzy sets produced in step 2. The rule selection is similar with sequential forward selection. Let Dec is the description of class c, ?1 and ?2 are parameters. Let Dec = ? and delk = ?.

3.1: Generate the first rule. First, choose the maximum  confidence of the fuzzy sets mi1j1 from inirule, i. e. ,  mi1j1 = arg max mij?inirule  Conf(mij ? Cc)  Let A1 = mi1j1 , if max(DA1 ? Dc) ? ?1, then Dec=Dec  ? A1. Otherwise, rule = A1  ? (inirulec ?  mi1j1), where  A1 = arg max Ai?rule  Conf(Ai ? Cc)  denotes the rule with maximum implication in rule. Go on until max(DA1 ?Dc) ? ?1.

3.2: After obtaining the first rule, let allk = {k | Dc(k) ? DDec(k) > ?2, k ? D ? delk},  k? = arg max k?allk  Dc(k)?DDec(k)  Mk? = {mij | Dc(k?) ? Dmij (k?) ? ?2, mij ? inirulec}. Then, let inirule = Mk? , following the strategy of step 3.1 to find a rule Ai with simple concepts in Mk? .

3.2.1 choose the maximum implication of the fuzzy sets mi1j1 from Mk? (inirule), i. e. ,  mi1j1 = arg max mij?Mk?  Conf(mij ? Cc)  3.2.2 Let A1 = mi1j1 while max(DA1 ? Dc) ? ?1 or Dc(k?) ? DA1(k  ?) ? ?2 if Dc(k?)?DA1(k?) ? ?2  break delk = delk  ? k?  elseif max(DA1 ?Dc) ? ?1 rule = A1  ? (inirulec ?mi1j1)  Msk? = {Ai | DAi(k?)?Dc(k?) ? ?2} A1 = arg max  Ai?rule Conf(Ai ? Cc)  endif endif  endwhile Dec=Dec  ? A1.

3.3: Go back to 3.2 until allk is ?. Finally, we obtain the final description Dec of class c.

Step4: The classifier has been built, then we can test the classifier. For each testing sample u, the following steps is proceeded: 4. 1 Compute its AFS structure.

4. 2 Compute its membership degree of each class, i. e.

?Dec(u), c = 1, . . . , C.

4. 3 Estimate the class of the sample. That is, the class label of the sample is arg  C max c=1  ?Dec(u).

Determine the class label of each testing sample or new sample by its membership degree belonging to each class. Suppose y is a testing sample or new sample, by (4.8), (4.9), (4.10), (4.11) one can compute the weigh ??(y) for all simple concept ? ? M . Let the characterization of class i, ?Xi =  ? ???Xi  ? =? i?I(  ? m?Ai m) ? EM , i = 1, 2, ..., l. By (2.5), one has  A?k(y) = {x | ?(y, x) ? Ak}, k ? I . By formula (2.4), we have the membership degree of y belonging to ?Xi as follows:  ??Xi (y) = sup i?I  ?? ? ??Ai  m?(A?i (y))  ?? = sup  i?I  ?? ? ??Ai  ? x?A?i (y)  ??(x)? x?X?{y} ??(x)  ?? the class label of y is arg max  1?k?l {??Xk (y)}.

We traverse all the parameter ?1 and ?2 through 0.1 to 0.9 with step length 0.1 of training samples. Then, compute the average accuracy average?1 with respect to ?1. Find the first ?1 that satisfies train?1+0.1 ? train?1 and train?1?0.1 ? train?1 , and then ?1=?1.

The following method is to determine a suitable ?2.

Method 1 Choose ?2 with minimum error rate, that is  ?2 = arg min ?2?[0.1,0.9]  train?1,?2 . If there are more than one ?2 satisfied the condition, choose the maximum. So it can be wrote as ?2 = max arg min  ?2?[0.1,0.9] train?1,?2 .

Method 2 From 0.9 to 0.1, find the first ?2 that satisfies train?1,?2+0.1 ? train?1,?2 and train?1,?2?0.1 ? train?1,?2 , and ?2 = ?2.

5. EXPERIMENT RESULTS  In this section, first, we give the experiment result with our algorithm on five UCI data sets?-Iris, Wine, Breast- Wisconsin, Sonar and Pima. Second, we evaluated our method against other associative classifiers on the data sets. For all of the data sets, ten-folds cross validation were applied. The experiment results have showed that the good performance of the AFS logic.

The experiment result with method 1 in table I and the result with method 2 in table II.

To evaluate the performance of our proposed method, in table III, we compared the accuracy of AFSFAC with the well-known associative classifiers CBA, CMAR and CPAR (in some literature is called hybrid between rule-based methods and associative classifier), and with nonassociative classifiers such as rule-based classifiers (Ripper) and decision trees classifiers(C4. 5). The published results of their experiments were collected from the literature.

6. CONCLUSION  In this paper, an associative classification algorithm based on AFS is proposed, which can automatically generate the     TABLE I THE RESULT OF EACH DATA SET WITH METHOD 1  Data set ? ?1 ?2 Train Test Num. R  Iris  0.2 0.2 0.8 0.0319 0.0333 4.9 0.3 0.2 0.8 0.0319 0.0333 4.9 0.4 0.2 0.8 0.0319 0.0333 4.4 0.5 0.2 0.9 0.0326 0.0333 4.0  Wine  0.2 0.4 0.6 0.0131 0.0393 23.6 0.3 0.4 0.6 0.0131 0.0393 23.6 0.4 0.4 0.6 0.0144 0.0393 23.3 0.5 0.4 0.6 0.0150 0.0281 18.7  Breast  0.2 0.3 0.4 0.0275 0.0315 28.6 0.3 0.3 0.4 0.0265 0.0315 27.6 0.4 0.3 0.4 0.0265 0.0315 27.1 0.5 0.3 0.4 0.0292 0.0315 16.8  Sonar 0.2 0.5 0.5 0.1106 0.2404 40.0 0.3 0.5 0.3 0.1122 0.2500 45.3 0.4 0.6 0.6 0.1368 0.2308 31.9  Pima 0.2 0.5 0.7 0.2227 0.2565 41.8 0.3 0.5 0.9 0.2271 0.2370 26.5 0.4 0.8 0.7 0.2571 0.2708 18.0  TABLE II THE RESULT OF EACH DATA SET WITH METHOD 2  Data set ? ?1 ?2 Train Test Num. R  Iris  0.2 0.2 0.8 0.0319 0.0333 4.9 0.3 0.2 0.8 0.0319 0.0333 4.9 0.4 0.2 0.8 0.0319 0.0333 4.4 0.5 0.2 0.9 0.0326 0.0333 4.0  Wine  0.2 0.4 0.8 0.0225 0.0169 16.8 0.3 0.4 0.8 0.0225 0.0169 15.8 0.4 0.4 0.8 0.0194 0.0169 16.0 0.5 0.4 0.8 0.0218 0.0225 15.0  Breast  0.2 0.3 0.9 0.0320 0.0386 15.3 0.3 0.3 0.9 0.0302 0.0329 13.0 0.4 0.3 0.9 0.0304 0.0329 12.8 0.5 0.3 0.9 0.0310 0.0300 10.7  Sonar 0.2 0.5 0.5 0.1106 0.2404 40.0 0.3 0.5 0.5 0.1138 0.2404 38.9 0.4 0.6 0.6 0.1368 0.2308 31.9  Pima 0.2 0.5 0.7 0.2227 0.2565 41.8 0.3 0.5 0.9 0.2271 0.2370 26.5 0.4 0.8 0.9 0.2594 0.2656 16.8  TABLE III THE COMPARISON OF DIFFERENT DATA SETS  Data set Iris Wine Breast-w sonar Pima #attr 4 13 10 60 8 #cls 3 3 2 2 2 #rec 150 178 699 208 768  C4. 5 94.00 92.12 94.71 - 73.70 Ripper 94.00 92.12 95.28 - 73.19 CBA 94.67 94.96 96.28 77.50 72.90  CMAR 94.00 95.00 96.40 79.40 75.10 CPAR 94.70 95.50 96.00 79.30 73.80  AFSFAC(1) 96.67 97.19 96.85 76.92 76.30 AFSFAC(2) 96.67 98.31 97.00 76.92 76.30  membership degree. The results generated by this classifier is interpretable and similar to human intuition. Performance studies on the five data sets indicated that the proposed method is highly competitive, compared with other classification ap- proaches and fuzzy logics in term of accuracy.

