Mining Product Features from Online Reviews

Abstract?With the advance of the Internet, e-commerce systems have become extremely important and convenient to human being. More and more products are sold on the web, and more and more people are purchasing products online. As a result, an increasing number of customers post product reviews at merchant websites and express their opinions and experiences in any network space such as Internet forums, discussion groups, and blogs. So there is a large amount of data records related to products on the Web, which are useful for both manufacturers and customers. Mining product reviews becomes a hot research topic, and prior researches mostly base on product features to analyze the opinions. So mining product features is the first step to further reviews processing. In this paper, we present how to mine product features. The proposed extraction approach is different from the previous methods because we only mine the features of the product in opinion sentences which the customers have expressed their positive or negative experiences on. In order to find opinion sentence, a SentiWordNet-based algorithm is proposed. There are three steps to perform our task: (1) identifying opinion sentences in each review which is positive or negative via SentiWordNet; (2) mining product features that have been commented on by customers from opinion sentences; (3) pruning feature to remove those incorrect features. Compared to previous work, our experimental result achieves higher precision and recall.

Keywords-text mining; sentiment classification

I.  INTRODUCTION As internet has become one part of people?s daily lives,  e-commerce has been developed into one of the most import methods in business deal. More and more products are sold on the Web, and more and more people are shopping on the Web. In order to share their shopping experiences and feedbacks, an increasing number of customers post product reviews at merchant websites and express their opinions and experiences in any network space such as internet forums, discussion groups and blogs, which is a great wealth of opinions about products. As a consequence, mining opinion has become a perspective research topic since [20, 6, 14]. It is quite different from traditional text summarization [7, 21] in many ways: (1) Opinion mining is a structured summarization rather than a free document which is generated by most text summarization systems. (2) Users are mainly interested in the product features that customers have  positive or negative opinions on. A systematic procedure of opinion mining is formed in many researchers? effort, and Hu and Liu in [14] propose one useful method which is feature-based opinion summarization of reviews. It is performed in three steps:  1. Mining the features of the product that customers have expressed opinions on, and then ranking the features according to their frequencies that they appear in the reviews.

2. For each feature, identifying opinion sentences in each review and determining each opinion sentence?s semantic orientation. The specific reviews that express opinions are attached to the feature, which facilitates browsing the reviews by potential customers.

3. Summarizing the results using the discovered information.

A feature-based opinion summary is helpful for a potential customer to find out the feeling of the existing customers. If potential customers are very interested in a particular feature, they can click the link to see what existing customers like or why they make a complaint.

Extracting product features is the foundational step of opinion mining. So this paper focuses on mining product features. We propose a different method to extract the product features from online reviews. In this paper, we aim to mine product features that the reviewers have commented on. This task mainly involves three subtasks: (1) Identifying opinion sentences in each review which are positive or negative via SentiWordNet [3]; (2) Mining product features that have been commented on by customers from opinion sentences; (3) Pruning feature to remove those incorrect features.

This paper proposes using SentiWordNet to distinguish the opinion sentences which is a new attempt and utilizes a number of techniques based on data mining and natural language processing methods to mine product features. Our experimental results show that these techniques are highly effective.

The remainder of this paper is organized as follows: Section 2 describes related work, Section 3 gives an overview of our system, describes and evaluates the main components, Section 4 shows the experiment results and Section 5 makes a conclusion.

DOI 10.1109/ICEBE.2010.51

II. RELATED WORK Many researchers have proposed different methods to  solve the problem of mining product features. Our work is closely related to Hu and Liu?s work in [4, 14, 15] on mining opinion features in customer reviews. In [14], they design a system to perform the summarization in two main steps: feature extraction and opinion direction identification. The inputs to the system are a product name and an entry page for all the reviews of the product. The output is the summary of the reviews.

The system performs five tasks: (1) POS tagging [13], which parses each sentence and yields the part-of speech tag of each word (whether the word is a noun, verb, adjective, etc) and identifies simple noun and verb groups (syntactic chunking). (2) Frequent feature generation. Hu and Liu use association rule mining [1] to find all frequent itemsets, which is a set of words or a phrase that occurs together. The association rule miner CBA [12] based on the Apriori algorithm in [1], finds all frequent itemsets in the transaction set with the user-specified minimum support 1%. (3) Feature Pruning aims to remove those incorrect features. Two types of pruning are presented: (a) Compactness pruning checks features that contain at least two words, which are named feature phrases, and removes those that are likely to be meaningless. (b) Redundancy pruning removes redundant features that contain single words. (4) Opinion words extraction with all the remaining frequent features after pruning. (5) Infrequent feature identification. Hu and Liu suppose people like to use the same opinion word to describe different features. So they can use the opinion words to look for features that cannot be found in (2). If one sentence contains no frequent feature but one or more opinion words, find the nearest noun or noun phrase of the opinion word as an infrequent feature.

The proposal in [14] can produce a number of features, but only explicit features could be found and implicit feature does not be extracted. The irrelevant sentences may be thought as opinion sentences, and the nouns in irrelevant sentence would be extracted as features. In addition, the premise (people use the same pinion word to describe different features) of finding infrequent features is not so reasonable. Different features trend to be described by different opinion words.

In [2], Popescu and Etzioni introduce an unsupervised information-extraction system (OPINE), which mines reviews in order to build a model of important product features, the evaluation by reviewers and the relative quality across products. OPINE performed four main tasks: (1) Identify product features. (2) Identify opinions regarding product features. (3) Determine the polarity of opinions. (4) Rank opinions based on their strength.

OPINE is built on top of KnowItAll, a Web-based, domain-independent information extraction system [16], which instantiates relation-specific generic extraction patterns into extraction rules to find candidate facts.

KnowItAll?s Assessor assigns a form of Point-wise Mutual Information (PMI) between phrases that is estimated from Web search engine hit counts [19]. It computes the PMI  between each fact and automatically generated discriminator phrases. Given fact f and discriminator d, the computed PMI score as (1):  )()( )(),(  fHitsdHits fdHitsdfPMI  + +=  (1)  The PMI scores are converted to binary features for a Naive Bayes Classifier, which outputs a probability associated with each fact [16].

OPINE extracts explicit feature for the given product class from parsed review data. (1) The system recursively identifies both the parts and the properties of the given product class and their parts and properties, in turn, continuing until no candidates are found. (2) The system finds related concepts and extracts their parts and properties.

Opine achieves 22% higher precision than Hu?s, but has 3% lower recall.

Scaffidi presents a probability-based algorithm and compares it to an existing support-based approach in [5].

Specially, he uses each algorithm to extract features from 7 Amazon.com product categories, and then asks end users to rate the features in terms of helpfulness for choosing products. The end users prefer the features identified by the probability-based algorithm. This probability-based algorithm can identify features that comprise a single noun or two successive nouns (which end users rated as more helpful than features comprising only one noun), yet even for collection of tens of thousands of reviews, it still executes fast enough (at around 1ms per review) for practical use.

OpinionFinder [18] is a system that processes documents and automatically identifies subjective sentences as well as various aspects of subjectivity within sentences, including agents who are sources of opinion, direct subjective expressions and speech events, and sentiment expressions. It outputs files using inline Standard Generalized Markup Language (SGML) markup. It?s not suitable for indentifying the opinion sentences in reviews, because identifying opinion sentence does not care the sources of the opinion and the sentiment expressions are free styles to process in reviews.

So we propose a new method to find opinion sentences using SentiWordNet.



III. THE PROPOSED TECHNIQUES In this section, we give an architectural overview for our  product feature extraction system in Figure 1. The system performs the extraction in two main steps: opinion sentence identification using SentiWordNet-based algorithm and product feature extraction. The inputs to the system are all the reviews of the product in the database. The output is the feature set of the reviews as Figure 2.

We download (or crawl) all the reviews, and put them in the review database. The feature extraction function, which is the focus in this paper, firstly identify the opinion sentence that a lot of people have expressed their positive or negative opinions on, and then extract the product feature including explicit feature and implicit feature. Finally we filter the result via pruning irrelevant feature. Below, we discuss each of the functions in feature extraction in turn.

Review Database  Further Processing  Explicit Feature  Implicit Feature  Feature Pruning  POS Tagging  Feature Generation  Infrequent Feature  Opinion Sentence Identification Feature Set  Frequent Feature  Digital_camera_1: Photo quality:  Frequency: 5 Ratio: 0.013454  Weight: Frequency: 7 Ratio: 0.034127  ?  Figure 1.  The product feature extraction system.

Figure 2.  An example of output.

A. Part-of-Speech Tagging (POS) POS tagging is the part-of-speech tagging [13] from  natural language processing. Firstly, we give some sample sentences from some reviews to describe what kinds of opinions are handled, and the application of part-of-speech tagging from natural language processing will be discussed later.

For a given product, the aim of our system is to find what people like and dislike. Therefore it is an important step to find out the product features that people talk about. However, it is difficult to understand natural language and deal with some types of sentences [14]. The following are some easy and hard sentences from the reviews of a digital camera:  ?The images are very vague.? ?Totally an undoubted very smart camera.? In the first sentence, the user is satisfied with the picture  quality of the camera, image is the feature that the user talks about. Similarly, the second sentence shows that camera is the feature that the user expresses one?s opinion on. While the features of these two sentences are explicitly mentioned in the sentences, some features are implicit and hard to find.

For example,  ?While light, it will not easily fit in pockets.? This customer is talking about the size of the camera, but  the word ?size? is not explicitly mentioned in the sentence.

To find such implicit features, semantic understanding is needed, which requires more sophisticated techniques. Thus in this paper, we propose a simple method to find the implicit feature, called Feature Mapping using associative spread. For example, it is too heavy to carry. And we can imply the  weight of this subject from the adjective word. The detail will be shown in Feature Generation section.

However, implicit features occur much less frequent than explicit ones. So we focus on finding features that appear explicitly as nouns or noun phrases in the reviews. To identify nouns/noun phrases from the reviews, we use the part-of-speech tagging.

In this work, we use the openNLP [17] linguistic parser, which to parse each sentence and yield the part-of-speech tag of each word (whether the word is a noun, verb, adjective, etc) and identifies simple noun and verb groups (syntactic chunking). The following shows a sentence with the POS tags.

I/PRP am/VBP absolutely/RB in/IN awe/NN of/IN this/DT camera/NN ./.

The openNLP system generates XML output. For instance, camera/NN indicates a noun. Each sentence is saved in the review database along with the POS tag information of each word in the sentence.

A transaction file is then created for the identification of frequent features in the next step. In this file, each line contains words from a sentence, which includes only preprocessed noun, verb, adjective, adverb phrases of the sentence. The reason is that other components of a sentence are unlikely to be used in expressing opinions. Here, pre- processing includes the deletion of stopwords, stemming (Stanford POS Tagging [11] and JWI [10] for WordNet [22]) and fuzzy matching. Fuzzy matching [9] is used to deal with word variants or misspellings. For example, ?redeye? and ?red-eye? actually refer to the same feature. All the occurrences of ?redeye? are replaced with ?red-eye?.

B. Opinion Sentences Identification In opinion mining, users primarily care about what the  customers like and dislike. So we only need to extract these sentences called Opinion sentences that people use to express a positive or negative opinion. Observing that people often express their opinions of a product feature in opinion sentences, we can extract opinion sentences from the review database using all the opinion words. In a review, there are many sentences. Some are opinion sentences, others are irrelevant sentences. For instance, let us look at the following two sentences:  ?The performance of this camera is perfect.? ?I bought this camera yesterday.? In the first sentence, the feature, performance, is in the  opinion sentence. But in the second sentence, it?s not an opinion sentence, and no feature exists.

In order to find the opinion sentences, we use opinion words which are sentiment words that people used to express their positive or negative attitudes. Only four kinds of words can express the sentiment, they are nouns, adjectives, adverbs and verbs. Because we use nouns as product features, and few opinion sentences use nouns to express the sentiment. So in this paper, we only think about adjectives, adverbs and verbs as opinion words. Sentiment includes three types: positivity, negativity and neutrality. Neutrality is usually used to describe a fact, and users pay much attention to the positivity and negativity. In review extraction, we take     more care of the positivity and negativity of the opinion words. For evaluating the sentiment of each sentence, we make the quantization of opinion words to calculate the sentiment score of each sentence. The sentiment score of each opinion word is acquired from SentiWordNet.

SentiWordNet is a lexical resource for opinion mining [3]. SentiWordNet assigns to each synset of WordNet three sentiment scores: positivity, negativity, objectivity. The assumption that underlies SentiWordNet?s switch from terms to synsets is that different senses of the same term may have different opinion-related properties. Each of the three scores ranges from 0.0 to 1.0, and their sum is 1.0 for each synset.

This means that a synset may have nonzero scores for all the three categories, which would indicate that the corresponding terms have, in the sense indicated by the synset, each of the three opinion-related properties only to a certain degree. For example, the synset [estimable(3)], corresponding to the sense ?may be computed or estimated? of the adjective estimable, has an Obj score of 1.0 (and Pos and Neg scores of 0.0), while the synset [estimable(1)] corresponding to the sense ?deserving of respect or high regard? has a Pos score of 0.75, a Neg score of 0.0 and an Obj score of 0.25.

From this observation, we can extract opinion sentences in the following way: for each sentence in the review database, if it satisfies that the positive or negative score is greater than certain score, to extract this sentence as opinion sentence. The positive and negative score can be calculated in below formulas.

Firstly, calculate the opinion word?s sentiment. In SentiWordNet, each opinion word has different sentiment scores in different scenes. It?s difficult to classify the opinion word into the right scene. Here, we use the sentiment average of the opinion word in each scene as its final sentiment score in review.

? =  = n  i iScore  n WordScore posadvadjvpos  )(1)( .).,.,(    (2)  ? =  = n  i iScore  n WordScore negadvadjvneg  )(1)( .).,.,(  (3)  After acquiring the sentiment of each opinion word, we need to get the adjectives, adverbs and verbs? sentiment score in the sentence respectively. The opinion word in the same part of speech has the same sentiment weight in the sentence, so we think the sentiment average in each part of speech as the sentiment score of adjectives, adverbs and verbs.

? =  = n  i advadjvWordScore  n advadjvScore ipospos  .)).,.,((1.).,.,(  ? =  = n  i advadjvWordScore  n advadjvScore inegneg  .)).,.,((1.).,.,(  And then we calculate the positivity and negativity of each sentence as follow:  .)(.)(.)( advScoreadjScorevScoreScore pospospospos ++=  .)(.)(.)( advScoreadjScorevScoreScore negnegnegneg ++=  It?s an opinion sentence, if Max(Scorepos, Scoreneg) ? ?; It?s not an opinion sentence, if  Max(Scorepos, Scoreneg) <  ?. In our experiment, we find ? equals 0.08.

As shown in the previous example, perfect is an adjective  that can be considered as the opinion word. So we can use opinion sentence identification to decide the sentence is an opinion or not. We explain the process in the follow example:  The/DT pictures/NNS are/VBP razor-sharp/JJ and clear/JJ ./.

In this sentence, we have three opinion words, including are, razor-sharp and clear. First we use stemmer to get each opinion word?s base form, and then calculate its sentiment score. Verb be is a stopword, so we omit it. Firstly, using formula (2) and (3), razor-sharp is 0.313 in positivity and 0.0 in negativity, and clear is 0.410 in positivity and 0.049 in negativity. Secondly, using formula (4) and (5), we get that the sentiment score of adjective is 0.362 in positivity and 0.025 in negativity. At last, under formula (6) and (7), the sentence sentiment score is 0.362 in positivity and 0.025 in negativity. Max(0.362, 0.025) > 0.08, so it is a opinion sentence.

C. Features Generation This step is to find features that people are most  interested in. The features are classified into explicit and implicit.

1) Explicit feature.

Explicit feature is the product features that appear in  noun form in the reviews. Most of the features are expressed explicitly in the reviews. So we focus on extracting the majority of the explicit features which means finding most of the feature in the reviews. Instances below are the explicit features in the opinion sentence. In order to extract the explicit features, we do not use association rule mining [1].

We use probability-based algorithm to extract the product feature. Explicit features are divided into frequent and infrequent features. In the previous step, we have filtered the irrelevant sentences, so the infrequent feature also can have a high probability to appear in the result of probability-based algorithm. We do not need to extract the infrequent features independently, because the amount of the infrequent features which cannot be found in our algorithm is very small.

In [14], Hu and Liu use Association rule mining to find all frequent itemsets, which are a set of words or a phrase that occur in one sentence. However, its computation is too large, and the result still needs compact pruning. We use a much rigorous method to finish this step. Each noun in the same sentence should appear nearby, and then the noun phrases can be considered as the candidates of product features.

The picture is great, but the flash is terrible.

The battery life is not so good.

The quality of the picture is perfect.

Here ?picture? and ?flash? will be misunderstood as one  feature in [14]. We will distinguish the two nouns into two features, because they are not nearby. In sentence 2, ?battery?  (4)  (5)  (6)  (7)     and ?life? is appearing nearby, so they are considered to organize one feature. In sentence 3, ?quality? and ?picture? separated by the preposition are also considered nearby and describe one feature.

For acquiring the better nouns used for product feature, we employ the nearby principle to extract the nouns are described by the adjective or verb. One adjective or verb only can be used for one object.

The picture is beautiful in this camera.

The lens and flash are very good.

In sentence 1, picture has the ?beautiful? to describe, but  camera does not have any description. So we only extract picture as candidate feature. In sentence 2, it is a special case (conjunction sentence), so lens and flash can be described by one adjective or verb. Lens and flash are two features.

2) Implicit feature.

The amount of implicit feature is few in the reviews, and  most of them use some specific adjectives to express implicit opinions. The adjective almost has clear meaning, which is one adjective mapped to one noun. So we can create a mapping database to obtain the implicit feature.

It is too heavy to carry out.

It is too expensive.

The feature ?weight? can be understood in Sentence 1,  and the price also can be gotten in Sentence 2. So we map heavy to weight, and expensive to price. In order to understand more adjectives, we use WordNet (WordNet 3.0) to extend the adjective set. The synonymset and antonymset of one adjective are used to describe the same feature. So if one adjective belongs to one synonymset or antonymset, the mapping noun is the feature expressed implicitly. If there are no explicit features in the opinion sentence, we use this method to find the implicit feature. In order to get implicit feature, we first stem the adjective, and then use JAWS [8] to get the the synonymset and antonymset from WordNet. If the adjective belongs to one synonyset or antonymset, the corresponding feature will be extract.

After getting the candidate features, we can calculate their probability of occurring in the reviews.

Sentence Occurrenceobability =Pr   (8)  Occurrence: the occurrence number of the candidate feature in the reviews;  Sentence: the number of opinion sentences.

The experiment shows the occurrence probability of the  feature is bigger than 0.6%, which is much smaller than 1% in [14].

3) Feature Pruning.

The probability-based algorithm still can generate useless  or fake features, which are some uninteresting and redundant ones. So feature pruning aims to remove these incorrect features. We use the pruning method proposed in [14].

Redundancy pruning focuses on removing redundant features that contain single words. Hu and Liu in [14] define that p-support (pure support) of feature ftr is the number of sentences that ftr appears in as a noun or noun phrase, and these sentences must contain no feature phrase that is a superset of ftr. For example, feature manual has the support  of 10 sentences, which is a subset of feature phrases manual mode and manual setting in the review database. The support of the two feature phrases are 4 and 3 respectively, and the two phrases do not appear together in any sentence. Then the p-support of manual would be 3.

If a feature has a p-support lower than the minimum p- support and the feature is a subset of another feature phrase, it is pruned. Here, the minimum p-support is 3. In the previous example of manual with a p-support of 3, it is not pruned. Thus all the three features, manual, manual mode, manual setting, could be meaningful.



IV. EXPERIMENTS In our experiments we use sets of reviews for 5 product  classes from Hu and Liu [21, 22]. There are five electronics products: 2 digital cameras, 1 DVD player, 1 mp3 player, and 1 cellular phone. Hu?s system and OPINE are the review mining system most relevant to our work. Hu?s system uses association rule mining to extract frequent review noun phrases as features. Frequent features are used to find potential opinion words (only adjectives) and the system uses WordNet synonyms/antonyms in conjunction with a set of seed words in order to find actual opinion words. Finally, opinion words are used to extract associated infrequent features. The system in [21] only extracts explicit features.

OPINE is built on top of KnowItAll, a Web-based, domain- independent information extraction system. OPINE?s Feature Assessor uses PMI assessment to evaluate each candidate feature and incorporates Web PMI statistics in addition to review data in its assessment. Our system uses pre- processing in identifying the opinion sentences which can reduce the interference of irrelevant sentence. By addition, we use Linguistic pattern and probability-based algorithm to extract the feature instead of Associated Rule and PMI, this simple method gains a good result. In the following, we analyse the experiment?s result.

TABLE I.  RESULT OF OPINION SENTENCE IDENTIFICATION  Product Name No. of manual Opinion Sentences SentiWordNet Approach  Recall Precision  Digital cameral 1 466 0.903 0.931  Digital cameral 2 274 0.923 0.966  Cellular phone 398 0.915 0.922  Mp3 player 1219 0.882 0.904  DVD player 488 0.891 0.912  Average 569 0.908 0.927  Table I show the recall and precision of SentiWordNet Approach are very high, the average of recall is 0.908 and the average of precision is 0.927, which means this approach is effective to identify the opinion sentences. The errors happen in the following cases:  This camera keeps on autofocussing in auto mode with a buzzing sound which can?t be stopped.

Get a ?system error? problem 30 days after purchase.

Awesome!

In sentence 1, the opinion words (buzzing sound) which are nouns cannot be marked as an opinion sentence in our method, but these situations are very limited. So we can neglect using nouns as opinion words. In sentence 2, the sentence does not have any opinion words, but it still expresses the opinion implicitly, and the feature also can be gotten from this sentence. This expression style is seldom in reviews. In sentence 3, it is considered as an opinion sentence, but there are no features. These situations do not interfere with the feature extraction.

TABLE II.  COMPARISON OF HU?S, OPINE?S AND OUR RESULT  Product Name  No. of manual Features  Hu?s OPINE Opinion Sentence Re Pr Re Pr Re Pr  Digital cameral 1 79 0.822 0.747 -0.02 +0.19 +0.10 +0.05  Digital cameral 2 96 0.792 0.710 -0.06 +0.22 +0.11 +0.07  Cellular phone 67 0.761 0.718 -0.03 +0.23 +0.11 +0.10  Mp3 player 57 0.818 0.692 -0.03 +0.25 +0.09 +0.06  DVD player 49 0.797 0.743 -0.02 +0.21 +0.12 +0.09  Average 69 0.80 0.72 -0.03 +0.22 +0.10 +0.07  a. Re: recall Pr: precision  From table II, we know that our method is better than Hu?s either in precision or in recall. Compared with OPINE, the precision of our system is not so good, but the recall is much better. Overall our proposal will have better performance than OPINE.

In order to show that our system?s performance is robust across multiple product classes, we used two sets of reviews downloaded from Epinions.com for Movie and Laptop. In Movie?s reviews, we get 0.78 in precision and 0.89 in recall.

In Laptop, we get 0.83 in precision and 0.91 in recall. So the average of the two products is 0.805 in precision and 0.90 in recall.

In summary, with the average of recall of 90% and the average precision of 79%, we believe that our techniques are quite promising, and can be used in practical settings.



V. CONCLUSION In this paper, we propose a different method to extract  the product feature, which employs data mining and natural language processing methods. Compare to previous works, we propose a new pre-process to identify the opinion sentence using SentiWordNet, which can get 0.927 in precision and 0.908 in recall. The simple extraction based on probability also can get a better result than Hu?s. Although OPINE has the best precision, our method improves the precision and recall at the same time. So our proposed techniques are effective in performing the feature extraction.

This paper leaves many avenues for future work, we plan to further improve these techniques and classify features according to the opinions? strength that have been expressed, e.g., to sort the features from the strongest to the weakest.

Feature extraction is the first step, and the summarization will be done in the future work.

