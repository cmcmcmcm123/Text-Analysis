Enhanced Semantic Automatic Ontology Enrichment

Abstract?With the fast growing development of the Web, the adoption of ontologies to improve the exploitation of infor- mation resources, is already heralded as a promising model of representation. However, the relevance of information that they contain requires regular updating, and specifically, the addition of new knowledge. Recently, new research approaches were defined in order to automatically enrich ontology. Usually they extensively use either statistical models or experts to provide the relevance and placement of new concepts. Unfortunately these approaches suffer the following drawback: The detection of new elements and position in ontology are not automatically established, and lack of use of semantic or syntactic information to extract relations between concepts.

In this paper, we present an approach for ontology enrich- ment based on two steps where a novel effective filtering step is utilized. First we extract the correlation between terms of a learning dataset using the generation of association rules.

Second we retain the relevant new concepts using an extracted semantic information. The suggested approach was tested on an ontology of mechanical industry competencies. Experiments were performed on real data, which show the usefulness of our approach.

Keywords-Text mining; Ontology; Association Rules; Mutual information; Syntactic analysis.



I. INTRODUCTION  Ontologies are a knowledge and reasoning representation.

They are the result of an organization process for the whole  concepts in a specific field as well as the relations between  these concepts. Ontology transmits an exhaustive and  rigorous conceptualization of domain or field into a mode  able to be analyzed by various information technology  methods. To build an ontology for a specific field, we need  an expert in order to transfer the domain knowledge into an  ontological form [1], [2], [3]. However, this manual process  based on an expert is a fastidious and time consuming.

Thus, the relevance of ontology information needs to be  up to date. In addition, concept enrichment by the addition  of new knowledge is essential. In order to accelerate this  endeavor and to remove any subjectivity, recent research  has been interested in semi-automatic and automatic  techniques for ontology enrichment. In literature, most  of work often used statistical or syntactical approaches  to enrich their ontology[4], [5], [6], [7], [8]. Moreover,  many approaches integrated data mining techniques. These  approaches use neither semantic nor syntactic information  to extract relations between concepts. Also the detection of  new elements and position in ontology are not automatically  established.

In this article, we suggest an automatic enrichment  methodology for ontology based on combining two types  of knowledge: present in the initial ontological structure  suitable for a field and that extracted from a textual corpora.

The process of enrichment is based on the generation of  association rules between terms that represent correlation  which are validated by semantic information and statistical  measurements.

This article is organized as follows: in section 2, an outline  of the techniques of enrichment of ontology is presented.

Third section describes our approach ontology enrichment  by a generic base of associative rules. Lastly, this paper  culminates with a conclusion as well as a description of  our works in progress.



II. STATE OF THE ARTS  The ontology enrichment process is composed of two  steps: the search for both new concepts and their relevant  relations and secondly their placement in the ontology.

A. Discovering potential conceptual term  We distinguish two methods for the discovery candidates  concepts: the first is based on statistical calculations using  several measurements to select potential terms according to  their distribution in corpora [9], [7], [10], such as complex  measurements such as mutual information, tf-idf, etc, or the  use of statistical laws of term distributions [5]. These various  proposals enable identifying new enrichment terms candi-  date, but do not allow placing them in ontology, without a  tiresome human intervention [6]. The second is a syntactic  method which determines at determining the grammatical  function of a word or a word group within a sentence. It is  based on the following assumption: the grammatic structure  reflect semantic dependences [11]. It uses the grammatical  functions of a word or a word group to present new concepts.

They present the drawbacks of identifying only the relations     labelized by verbs. Other approaches use also syntactic  patterns [12]. The extracted terms illustrate the new potential  enrichment concepts.

B. Concepts placement in ontology  To place the provisional terms, it is essential to detect  the relations between these new terms and initial concept  ontology. [7] propose a statistical approach based on the  frequent co-occurrence of candidates terms with the con-  cepts of initial ontology. A lack of precision is noticed  among the new concepts and the ontological structure.

Other approaches are based on data mining techniques [6],  [13]. Several approaches suggest using frequent correlations  which exist among the corpora terms while using an ex-  tracted association rules [14] among potential concepts [11],  [8]. Each rule expresses the relation between two or more  concepts of the field. This enrichment process requires  filtering considering the large number of rules generated.

Consequently human intervention is necessary to define  the semantic relations discovered. Other work [15], [5]  is based on the classification methods in order to bring  closer the candidates terms contained in the texts to the  concepts present in ontology. The principle is to gather by  a method of clustering terms according to their number of  occurrences within corpora [10]. The disadvantage of these  approaches is that they do not detect the relations among  the candidates terms, i.e., which unfortunately then requires  a human intervention for the addition of these new terms.



III. APPROACH  Figure 1. The general process of our approach  The purpose of this section is to present our approach.

The overall process is described in 1. It consists of three  phases. In the following sub-sections these three phases are  presented in detail.

A. Corpora preprocessing  To extract and to preserve the context of sentences con-  tained in the corpora [16] initially obtained from web sites,  we manually built a list of stop words to be filtered (e.g.

&nbsp, mailto, next, previous, GIF, jpg . . . ). These words  are always frequent in the corpus in spite of the extraction  text phase (passage of the format HTML with the textual  format. This list is used to filter and clean the corpora.

According to the expert analysis of the field for the  initial ontology concepts and the texts of the companies  describing the field of ontology, the choice of the words  to be kept is according to three grammatical categories  (Verb, Noun and Adjectif ). In order to do that, we treated  morpho-syntactically the corpora using TreeTagger [17].

This application enables us to obtain the category and lemma  of each word. A filtering step according to these three  categories was made to build a new corpora based on words  considered grammatically relevant.

B. Window size creation and concept extraction  The objective is to search within the corpora the words  which are correlated to the concepts of the ontology. Per-  haps, a brief explanation of our use of the term window  size is in order. A window size is a set of words that  surround concepts. For this, from the processed copora, we  seek correlations between concepts of ontology and words  of documents to enrich the ontology with more relevant and  useful words. The question that arises at this point is: How  to search for words correlated to the concepts of ontology?

The answer consists in two steps:  1) construction of Window size;  2) generation of association rules.

Nevertheless, in order to obtain more relevant concepts,  consider the following hypothesis: the more a word is  close to a initial concept, the more likely this word has  strong semantic correlation. Thus, sentences are defined  by considering window sizes (WS). A window size is a  set of words that include one or more initial concept of  ontology. In other words, as aforementioned a set of words  that surround concepts. Our goal is to identify in sentences  how to represent these WS. For instance, if WS is set to 1  that means that a sentence is composed by one word before  and one after the pivot concept.

Figure 2. Example of Window Size (WS)  In figure 2 the pivot is stampings. Using WS whose size  1, we get the following transaction ?stampings, cuttings     and industrial?, and by specifying a WS= 2 : ?stampings,  cuttings, expertise, industrial and process. These windows  are the transactions for the next step.

The second step is the generation of the association  rules. In order to detect the semantic correlation between  the terms in documents and the ontology?s concepts, an  association rule algorithm as adapted [18] to our concern.

More formally, let I = {i1, ....in} a set of terms, and D a set of sentences, where each sentence corresponds to a  subset of elements of I . An association rule is thus defined  as X?Y, where X?I , Y?I , and X ? Y = ?. The support of a rule corresponds to the percentage of sentences in D  containing X?Y. The rule X?Y has a confidence ratio c, if c% of sentences from D containing X also contain Y .

C. Filtering and Enrichment  In order to minimize the number of false concepts learned,  the words found in documents that are in correlation with  several different concepts are removed. Indeed, as a dictio-  nary is not used, by excluding words correlated to more than  one concept, only those words that are highly correlated with  unique concepts should be retained.

After this phase, we obtain a list of potential concepts  learned. However a problem persists: what is the quality of  words in this list? In other words, do the concepts learned  correspond to real features or just noise? The various  experiments conducted have demonstrate that effectively it  was necessary to follow the learning by a filtering step.

One of the most commonly measures used for finding  the correlation between two words is the Cubic Mutual  Information (MI)[19]. This measure depends on a context  c. Given this context, it is then based on three frequencies:  nb(x, c) the number of co-occurrences of x and c, nb(y, c) co-occurrences of y and c, nb(x, y, c) co-occurrences of x,y and c. The measure then is computed with AcroDefMI3  formula [20]:  AcroDefMI3(x, y, c) = nb((x, y) ? c)3  nb(x ? c) ? nb(y ? c) (1)  In our work, we wish to compute the dependency between  two concepts x and y. In this case, nb (x, y) represents the  number of web pages where x and y are present together.

The problem that persists is: How to identify context? To  do this, we will study the different situations of concepts  and words learned. We found that the major situations  in which the concepts and words learned appear in are:  Subject, Object, Verb and adjectives. The normal parsing  of natural language does not detect these variants of  categories. For this, a was used parser1 that can generate  the grammatical dependencies from which context can be  1http://www.latl.unige.ch/  extracted.

Example Consider the concept stampings which is a  concept of the original ontology. After the generation of  the previous steps, the learned words which are correlated  according to the association rules are: cuttings, machining,  Development and Client.

Firstly, we verify that there is no learned word as a con-  cept of the original ontology. For this, the term machining  is eliminated in this first step. By doing so, we need to  identify the context of each word so that we can generate  the AcroDefMI3 values. Concerning the concept learned  cuttings, its context in relation to the term stampings is  sought. When the collection of documents is mined where  both words appear, the following sentences are found:  ? Our expertise is in : cuttings, stampings and industrial  plastic process.

? Transformation of metals by cutting, stamping and  welding.

? We make tools for cutting and stamping.

? Our competency is based on three areas of expertise:  cutting, stamping and welding.

Based on the logical chaining of the grammatical  dependencies rules, we get the following information:  Competency, expertise, Metal, tools, transformation. These  terms forms the context of stamping and the word learned  cuttings.

To implement the correlation formula AcroDefMI3 , and to  evaluate the three frequencies queries were posted to Google.

The formula is applied between the concept stampings and  all words learned where it is correlated. The values obtained  are shown below.

? AcroDefMI3 (stampings, cuttings, C1) = 0.13462  ? AcroDefMI3 (stampings, Development, C2) = 0.03029  ? AcroDefMI3 (stampings, client, C3) = 4.9693E-005  Because of the numbers of pages indexed by google that  changes from one word to another, that prevent us from find-  ing a threshold from which we can filter candidates. Hence,  the first potential concept listed by the filter AcroDefMI3 is just accepted. So the word cuttings is validated as a new  concept and the others are removed.



IV. EXPERIMENTATIONS  In this section, the results of the different experiments we  conducted to validate our methodology are presented.

A. Corpora  To test our approach we used a corpora constituted by the  texts of twenty companies in the mechanical industry (our  ontology describes competences of these companies). This  corpora was built by collecting documents from the web     sites. Concepts found on these pages were cross-references  with their corresponding codes according to NAF2. The  collection contains 11926 documents is our test corpora.

B. Building an ?Ontology for competence traces?  1) Normalisation: The first normalisation step is to ren-  der explicit the concepts of the knowledge domain. These  concepts should be expressed in the form of ?cognitive  constructs?, which have a non-contextual meaning. This al-  lows the arrangement of various constructs into semantically  interpretable formulations. [21] Proposed to identify such  cognitive constructs by applying differentiation mechanisms  among concepts. These authors suggested applying four  semantic differentiation principles to clarify the difference  between each ?cognitive constructs? and its ?parent? con-  structs .

Our objective is to engineer an ontology that deals with  ?competence traces?. The normalisation procedure has been  divided into three phases. The first automatically identifies  candidate terms for the ontology. A corpus was extracted  from mechanical industrial company web sites, with the  indexation tool SMART for this application. Using this first  identification of terms, the second uses informal domain  expertise, asking several experts to provide an initial version  of the potential organisation of the domain concepts (con-  ceptual classes and their relationships). Here the experts?  thoughts have ?converged? to two approaches. First, a top-  down method offers a generic conceptualisation of the notion  of ?competence traces?. We show in Figure 2 that the results  are a so called ?generic ontology?. Second, a bottom up  approach which entails a pragmatic notion of competence  traces, closely linked to the activity filed analysed (mechan-  ical industry). This 2nd approach results in a enumerated part  of the final ontology called the ?domain ontology?. Finally, a  normalisation phase reduces ambiguities among the concepts  identified and improves the formalisation of relationships  among terms and their definitions. The differentiation prin-  ciples proposed by Bachimont [2] were applied.

2) Formalisation: The formalisation structures the var-  ious levels of the ontology, notably by defining additional  properties to the ?cognitive constructs? identified previously.

The three main types are differentiated: metaphysic, struc-  ture, and parataxic concepts. These three types correspond  also to distinct generality levels, from the conceptual level  of the ontology, to the more pragmatic.

The first level of the ?competence traces? ontology is  composed of very generic commonly called ?metaphysical  concepts?. For this level, the generic competence model  must be created. Two generic classes are defined which  correspond to the types of company capabilities of interest:  2It is one of the codes of INSEE, it allows the coding of the principal ac- tivity carried on in the company or association. We have code APE of 1973 to 1992 and code NAF since January 1st, 1993. (http://www.insee.fr/fr/nom def met/nomenclatures/naf/pages/naf.pdf)  the classes ?technological competence traces? and ?method-  ological competence traces?. The structuring level covers a  set of interrelated concepts which should make possible to  provide an enumerative and conceptual description of the  knowledge domain. These concepts have the advantage of  being independent of the specific activity field addressed:  ?Competence Traces Ontology? CTO can be re-used for  various company activity domains.

The third, parataxic level of the ontology is constituted  by what we called in 3.2.1 the ?domain ontology?. The  ?competence trace? concepts at that level are directly linked  to the application field (mechanical industry sector). When  applying our approach to another industrial sector, it is  interesting to underline that the structuring level of the on-  tology remains stable and only the parataxic level has to be  modified. Furthermore, the concept classes of this ?domain  ontology? are characterised by sets of class instances. These  instances correspond to specific terms of the domains, used  as ?competence trace identifiers?. They are directly used to  extract the pertinent information which is composed of the  company competence traces. Figure 3 partially illustrates  this domain?s ontology.

Figure 3. Parataxic level of the ontology (mechanical industry)  3) Operationalisation: This does implement a computer  version of the ontology for the application of inference  mechanisms. We have employed the competence trace on-  tology using OWL (Ontologie Web Language).

C. List of initial ontology concepts  The initial domain ontology contains a large number of  concepts, and the enrichment is a tedious task that requires  a lot of time. Consequently, in a first time we chose to test  on a single conceptual domain of the initial ontology to  measure the performance of our approach. The initial list  of concepts described in Table IV-C.

In order to study what might be the best distance between  words and concepts to retain. Different experiments were     Table I LIST OF INITIAL ONTOLOGY CONCEPT.

Initial Concepts Assembly Production  Boiler Grinding Turning Surface  Stamping Treatment Manufacturing Thermal  Forging Process Milling Usage Laser Machining Carry  conducted by varying the parameter of windows size from 1  to 5. Then, to extract the correlation relationships between  words, we use a version of the Apriori algorithm 3. In  experiments, the support was varied from 1 to 10%. After  analyzing the results, we have assumed a support of 1%,  and a window size of 4.

Table II describes the generated association rules. As  could be constated, the terms of the corpora correlated to  the concept of the ontology are discriminant for the domain  concerned. Thus, a provisional list of concept learned is  obtained.

Table II EXEMPLE OF GENERATED ASSOCIATION RULES  Generated Association Rules  assembly ? injection (3.2, 100.0) stampings ? cutting (4.0, 90) forging ? Threading (1.7, 100.0) forging ? simulation (1.9, 100.0) area ? Metrology (2.1, 100.0) area ? Welding (1.6, 100.0) machining ? expertise (2.3, 100.0) machining ? accuracy (1.8, 100.0) machining ? installation (2.9, 100.0)  As we explained in the previous section, we eliminate  from this list the common terms (those correlated with  several initial concept for the same support value). Figure  IV-C shows a set of concepts learned. Due to the association  rules generated, we have not only identified new candidate  terms as a concept of the ontology, but also have determined  the relations between the initial and the new concepts to be  automatically positioned in the ontology.

D. Discussion  As seen from, the fact of using windows where the  pivots words are the initial concepts of the ontology, and  the using nouns, verbs and adjectives enables to improve  considerably the detection of correlation in texts. In the  same way, the use of filters significantly retained correlated  relevant concepts. It should be noted that our approach was  3http://fimi.cs.helsinki.fi/fimi03/  Table III EXEMPLE OF CONCEPTS LEARNED  Concepts Learned Adjusting Cutting  Metal Injection Metrology Assembly  Tools precision expertise Welding Molding Simulation Turning Threading  sufficiently automatic to be applied in various fields and thus  extract the significant concepts elsewhere.

The first results obtained are promising: we discovered  and placed suitably new concepts. The analysis of the  ontology obtained showed that the whole of the concepts  discovered is coherent since most of them could be attached  to ontology via the rules obtained. These results were also  validated by the expert of the field.



V. PERSPECTIVES  Future work may entail in a broad series of projects and  initiatives. Firstly, our learning method depends on the qual-  ity of documents constituents the training corpora. Currently  it is created manually from a collection of sites. We would  like to extend our method of learning corpora creating by  generating a specific queries on the web according to a  specific criteria for the domain treated.

Secondly, our approach can naturally be extended by  enhancing learning. In this case, we have just to reconsider  the new concepts learned as initial concepts.

Thirdly, the work on this project will continue enrichment  on all the domain of the ontology and at the same time  validate obtained results either by a expert or by statistical  methods.

Finally, the extraction method is based on the generation  of association rules between concepts and words of texts.

We want to investigate whether the order between words can  affect the correlation. Traditionally, text mining approaches  normally rather uses the n-grams to consider the order  between words or characters. The advantage of n-grams is to  find words that are very close (i.e. based on the value of n).

The failure of these approaches in the context of data mining  is that the words most be consecutive to be validate. But this  does not always reflect natural language structure. Our idea  is to extend the approach using the concept of sequential  patterns which extracts pertinent concepts that are close but  not consecutive.



VI. CONCLUSION  Many studies and applications have addressed the problem  of ontology enrichment, in developing approaches and tools  for building and updating of such resources. Often, the  operations of identification and placement of new elements     in these semantic resources such as ontology, are made  manually.

In this paper, we present a new approach of ontology  enrichment based on data mining technics specifically  the association rules and the use of extracted semantic  information to retain relevant new concepts. Our approach  is based on three steps:  1) Preprocessing of a textual corpora (cleaning and lem-  matising: treetagger).

2) Creating window size to promote the correlation be-  tween corpora words and concepts of the ontology,  then the application of the APRIORI algorithm to  extract association rules according to validated param-  eters (support, confidence).

3) Automatic analysing and semantic filtering of  generated rules to keep those that are relevant to the  initial domain ontology.

The obtained enrichment results seem auspicious for the  chosen domain ontology. This approach is sensitive to the  domain studied: it is important to have a corpora that  describes the subject domain accurately, so there is a very  rich language with nouns and adjectives available. Although  the company websites do not represent the ideal source for  building a such corpora for the domain studied (companies  competency in the mechanical industry), but we have had  promising results.

