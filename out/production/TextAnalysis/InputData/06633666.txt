Replay-based Strategy Prediction and Build Order Adaptation for StarCraft AI Bots

Abstract-StarCraft is a real-time strategy (RTS) game and the choice of strategy has big impact on the final results of the game.

For human players, the most important thing in the game is to select the strategy in the early stage of the game. Also, it is important to recognize the opponent's strategy as quickly as possible. Because of the "fog-of-war" in the game, the player should send a scouting unit to opponent's hidden territory and the player predicts the types of strategy from the partially observed information. Usually, expert players are familiar with the relationships between two build orders and they can change the current build order if his choice is not strong to the opponent's strategy. However, players in AI competitions show quite different behaviors compared to the human leagues. For example, they usually have a pre-selected build order and rarely change their order during the game. In fact, the computer players have little interest in recognizing opponent's strategy and scouting units are used in a limited manner. The reason is that the implementation of scouting behavior and the change of build order from the scouting vision is not a trivial problem. In this paper, we propose to use replays to predict the strategy of players and make decision on the change of build orders.

Experimental results on the public replay files show that the proposed method predicts opponent's strategy accurately and increases the chance of winning in the game.

Keywords-Strategy; Prediction; StarCraft; Build Order; Adaptation; Decision Tree; Feature Expansion

I. INTRODUCTION  In StarCraft, each player comes with a strategy (usually represented as a build order) given to the game map and opponents. When the game starts, each player follows the prepared build orders. At the same time, they plan to send a scouting unit to the opponent's area. Although the operation of the scouting is optional, it is nearly mandatory in human games. If the unit arrived in opponent's area successfully, it can give limited vision (around him) to the player.

There are a lot of difficulties to recognize opponent's strategy: 1) The amount of information from the scouting unit is proportional to the survival time and active movement in the enemy's territory. However, it requires a careful control  This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (2013-016589, 2010-0018950).

* corresponding author   of units to avoid attacks from enemy's force. 2) In the fog-of-war conditions, the vision allows the player to see only the limited area around the scouting unit. Because other areas are invisible to the player, it has high uncertainty to infer the current states of place visited or non-visited.

In case that the prediction of strategy is successful, there is risk to make decision on the change of build orders. For expert players, they have common sense on the choice of build orders when they recognize the opponent's strategy with uncertainty. Usually, the knowledge is not formalized to be used in the AI bots. The decision should be related to the accuracy of the prediction and the winning ratio of specific build orders.

Although there are some works on the design of the scouting unit control mechanism, it is still under developed to be used in the competition [1]. The scouting unit should survive in the enemy's area for long time and navigate the area continuously to update information. In addition, they need to be defensive to the attack of enemy. For human players, the controlling of scouting unit is one of the key factors to win the game.

The design of the strategy recognizer and the change of build order is not a trivial problem. It is necessary to automate the design process with the help of huge amount of data from the web. The replays on the web are important resource to learn the recognizer and the build order change. For example, Weber et ale run machine learning algorithms on the data extracted from replays ofhigh-level players [2]. Although the results are promising, their extractor ignores the "fog-of-war" and the data file contains all of the opponent's information (usually not visible to the player).

In this work, we propose to use the replays to design "strategy predictor" and change "build orders." In the strategy prediction, we found that the best learning algorithms for the prediction is different on the stage of the game. Based on the observation, we combine machine learning algorithms with feature-expanded decision trees (usually perform well in the later parts of the game). From the replays, it is possible to get statistical data on the relationships among build orders. If build order A is effective on the build order B, the winning ratio from the replays could be saved to make decisions.

Finally, we extract the information from the replays         considering the ?fog-of-war.? It allows the extracted information should be limited to the visible to players. The realistic settings could give practical insight on the use of replays for the design of AI in the competition.



II. BACKGROUNDS  A. StarCraft Strategy StarCraft is a popular real-time strategy game where players  collect resources (minerals and gas), construct buildings and produce units to attack other players. The goal of the game is to eliminate all the buildings of opponents. However, when the player gives up the game, it stops although there are buildings remaining for the player. There are three races: Protoss, Terran, and Zerg. Each race has different units, buildings, and upgrade options.

For each race, there are a lot of different ways to determine building orders (a sequence of building construction) and unit production schedule. Because players have limited resource and time for the construction and production, it is desirable to select one strategy and optimize their actions to the choice. In fact, there is no golden strategy that beats all other strategies.

Each strategy has strong and weak points to other strategies and experienced players have knowledge on the relationships among them.

It is possible to prepare strong build orders but its value is highly dependent on the opponent?s strategy in the game. It is important to collect information on the enemy?s territory and guess the strategy. If the prepared choice is risky in the context of opponent?s choice, the player should change the current build order. The decision making on the build order is not easy because the information on the opponent is imperfect.

Each player has limited vision around alliance units and the scouting unit is not guaranteed to survive for long time.

B. StarCraft AI Competition StarCraft AI competition has been held as special events in  game AI conferences (IEEE CIG and AIIDE). In the competition, each participant submits an AI program and each program plays multiple games against other entries.

Because each entry has different styles of playing, it is difficult to get general goodness over the diversity of strategy.

Unlike human leagues, the bots usually not change their build orders and pay small attention on the scouting.

In human games, players usually change their strategy in the next game with the same opponent. However, bots usually have no mechanism to change their build order when they lose game to the opponent. If the build order programmed is not working to the opponent, there chance to lose all the games with the player. It is not yet common to prepare multiple build orders and adaptively change (not randomly) its choice during the game. It requires ?strategy prediction,? ?scouting? and ?build order adaptation? mechanisms.

From the list of entries of the competitions, we investigate sixteen bots from their source code and replays to check the existence of scouting. Table 1 summarizes the entries show  the scouting behavior (31% of the Bots). However, their main function is not observation but disturbance to the construction and production of opponents.

Table 1. Scouting in StarCraft AI Competitions  AI Players Race Scouting Behavior Nova Terran Disturbance  Skynet Protoss Disturbance UalbertaBot Protoss Disturbance  ItalyUndermind Zerg Disturbance SPAR Protoss Observation   We have developed Xelnaga for the StarCraft AI  competition since 2011. Like other entries in the early days, our submission has only one ?build order? specialized in the use of ?Dark Templar.? As expected, the strategy is successful to beat players but fail to beat all entries. In the version, we have no mechanism to scout the opponent?s region. The focus is to follow the predefined build order until we have enough attack units. This is a kind of all-or-nothing strategy. It is weak to the very early attack or ?observer? production strategy.

In 2012, we add a function to scout the opponent?s are using probe (a resource collecting unit in PROTOSS). From the observation, we count the assimilator and the gateway in the enemy?s territory. If the number of gateway (attack unit production building) is more than three and no assimilator (gas extractor), we recognize it as ?very fast attack?. If the number of gateway is two, the strategy is ?fast attack?.

Although we use very simple rules, they?re helpful to recognize the early attack and change our strategy. However, it is a still big problem to recognize a variety of strategies and handle them properly by changing our build orders. However, it is a still big problem to recognize a variety of strategies and handle them properly by changing our build orders.

C. StarCraft Replay Mining Game mining is an interdisciplinary research to extract  useful knowledge from game-related data sources with data mining techniques [3]. The knowledge can be used to build better games and artificial intelligence for the games [4][5].

There are several sources of information originated from games: transcripts, logs, text chats, social networks, and so on.

It can be used to mine the behavior of gamers to improve the design of the games [6][7][8].

Since gamers have played the StarCraft for more than ten years, their replays have been archived in game-related portals. AI researchers analyze the replays to build a ?strategy? prediction model using CBR [9], J48, k-NN, NNge [2] and Bayesian network [10]. Also, it helps to find the goal of players [11] and relationships among build-orders (strong or weak) [12].

Because the previous works are dependent on some replay programs without ?fog-of-war? options, they pose unrealistic assumption that players have full vision to the opponent.

Recently Park et al. tried to predict that opponent?s strategy with fog-of-war [1]. In the experiments, they use their agents         (bots) to records realistic (with fog-of-war) observation during the game. Although it is successful to collect realistic logs but it should play the games against other agents to get the data. So, the approach is not useful to analyze the replays.

In our 2013 version, we develop a technique to collect realistic logs from replays with an ?observer? role agent. The software simulates each game from replay in a fastest mode and the observer records all the relevant game events considering the ?fog-of-war.? Hostetler et al. infer the strategy in fog-of-war [13]. Their focus is to infer the current opponent?s unobserved information from observed data.



III. PROPOSED METHODS In this paper, we propose a framework to exploit the  replays to predict strategy of opponent and making decision on the change of build orders. Fig. 1 shows the overview of the proposed framework.

Fig. 1. Overview of the proposed method (FOW = Fog of War)  The replays are available from famous internet game portals.

However, there is no information on the strategy used in the replay. The strategy ?labeling? can be done by human experts.

The labeled replays can be used as a training data for the supervised learning in the next step. It is necessary to automate the ?labeling? by modeling the human experts. The replays record all the gaming events (mouse click, the production of units, the construction of buildings and upgrades) in a binary format.

The Extractor converts the raw replay files into human readable text files. Because the Blizzard, the creator of the StarCraft, does not support the conversion, the extraction is dependent on some software personally developed by experts.

They?re Lord Martin Replay Browser and BWChart. Because the extraction software is built without support from the game creator, it has several limitations. For example, it has limited support on the multiplayer games and no option to reflect the ?fog-of-war? in the games.

The next step is to build a feature vector from the raw text files. Because the replays store all the events necessary to recover the games again, it has large amount of useless information to predict the ?strategy.?  In this step, expert  knowledge is required to the choice of features. Although it records all the information on the units, buildings and user commands but most of them are not useful. Simply, it is possible to set filters to delete useless information. Because the raw data is too coarse, additional preprocessing techniques (averaging, counting and so on) can be applied.

In previous works, machine learning algorithms are successful to predict the strategy of opponents in the early stage of the games [1][2]. However, they?re not interpretable to human experts who design the build orders of the bots.

Also, it is not easy to maintain high accuracy throughout the games (early, middle and end stages). As a solution, we propose to use a decision tree to predict the strategy. Because the model is interpretable to human experts, it is straightforward to convert them into a build order. To enhance the performance of decision tree, we expand a feature set for the model by incorporating new features based on time comparison. Because single machine learning algorithm fails to cover all the stage of the games, we propose to assign different machine learning models for each stage.

Based on the prediction, the player should make decision on the change of the build order. There are big uncertainties in the decision. In this architecture, we automatically get the statistics on the winning ratio of strategy against others. For example, it stores the winning ratio if strategy A plays against strategy B. The final decision is based on the prediction accuracy and the winning ratio for the predicted strategy and the player?s current strategy.

A. Replay Preprocessing Although replays are stored in a binary format, it is possible  to convert them to game logs using Lord Martin Replay Browser2 or BWAPI3. They extract types of buildings, units, upgrade and their making time from replays. The extracted raw data is encoded as a feature vector, containing temporal features. If units or buildings are produced or constructed multiple times during the game, each feature describes the time when they?re made first. For example, the PROTOSS player constructs multiple Gateways (buildings for the attack unit production) during the game. But in the feature vector, the feature ?Gateway? stores only the time that the first Gateway is constructed.

? ? ?  = Px  Pxt xf P by  produced (yet)not   was0  by  producedfirst  is  when time )(   , where x is a unit type, build type or unit upgrade. A subset of an example feature vector for a Terran player is shown in Table 2. In the game, second gas was not yet produced by the player.

Table 2. A subset of an example feature vector (from a Terran player?s feature vector for a Protoss vs. Terran match)  Attribute Game Time Pylon 1:20   2 http://lmrb.net 3 http://code.google.com/p/bwapi/         Gateway 2:05 Gas 2:40  Expansion 11:00 Second Expansion 15:11 Third Expansion 18:45 Fourth Expansion 0:00  Second Gas 0:00   B. Strategy Prediction In the strategy prediction, we propose to use feature  expanded decision tree. The only difference with the standard decision tree is that it incorporates a lot of new features into the original vector. In StarCraft, the order of game events (action A prior to action B) is one of important factors to identify the strategy. Fig. 2 shows an example of feature-expanded decision tree for the StarCraft.

(a) Standard decision tree      (b) Feature-expanded decision tree   Fig. 2. An example of standard and feature-expanded decision tree (C, S and F stand for ?Citadel,? ?Stargate,? and ?First Expansion,? respectively)   The number of features in the vector is N. There are )1(5.0 ??? NN  comparisons among the features (only, ?>?  operation is considered). The new feature has one of ?true? and ?false? value. Because the number of new features is large, feature selection is adopted. The percentage of ?true? value for each strategy (class) is calculated. For example, x1>x2 is true for all the replays labeled as ?Fast DT? strategy.

The comparison is worth to be considered. If a feature shows 100% for at least one strategy (class), it is selected.

(a) Feature expansion   (b) Feature selection   Fig. 3. Feature expansion and the selection for the decision tree   Also, we propose to use an ensemble approach where different machine learning models take charge of different stage of games. The separation of game stage is done by experts. Usually, the game can be divided into early, middle and end stages. In case that the expert knowledge is not available, it is possible to assign machine learning models that perform the best on the training samples at the given time.

For example, we can use ?Random Committee? models from the game start to 9 minutes and ?Feature-Expanded DT? after the time.

C. Build Order Change From the replays, it is possible to get statistics on the  relatinoships among strategies. In sum, the winning ratio when strategy A plays against strategy B. From the training samples, it is possible to get prediction accuracy (0~1) on the trained models. ? is the maximum winning ratio if the player changes the current build order into new one (from the statistics). ? is 0.5.

?? ??+?= )1( E[Win] Approach Ensemble  Approach Ensemble AccuracyAccuracy

IV. EXPERIMENTAL RESULTS  A. Experimental Setup In this paper, we collect StarCraft replays from  YGOSU.com. The number of replays is 570 and all the games are PROTOSS vs. PROTOSS. Because we can extract text logs in the perspective of each player of the game, the number of samples is 1140. Also, we repeat the extraction two times by controlling the ?fog-of-war? options. As a result, we have two sets of data samples (?with fog-of-war? and ?without fog-of-war?). The number of features in the vector is 56.

Also, we use the data4 from Weber et al. [2]. Because they already preprocessed the raw replay files, it is easy to use for the experiments. Also, they have data for games among all races (PvP, PvT, PvZ, TvT, TvZ, and ZvZ). For example, PvP represents PROTOSS versus PROTOSS. The number of samples for each type of games is ranging from 542 to 1150.

In total, they have 5493 samples. However, they do not consider the ?fog-of-war? in the log extraction. Also, the raw replay files are not available for the data. It makes difficult to know the player who wins the game for the replay. The number of features for PROTOSS, TERRAN, and ZERG is 56, 51, and 48, respectively.

Table 3. The number of samples (FOW = Fog-of-War) (P = PROTOSS, T = TERRAN, Z = ZERG)  Types FOW Raw Replays # Samples  YGOSU.com P vs. P O O 1140 P vs. P - O 1140  Weber et al.

P vs. P - - 542 P vs. T - - 1139 P vs. Z - - 1024 T vs. T - - 628 T vs. Z - - 1150 Z vs. Z - - 1010   Table 3 summarizes the details of data used. The number of  strategies for each race is seven. For example, the PROTOSS has ?Fast Dark Templar,? ?Fast Observer,? ?Fast Expansion,? ?Fast Legs,? ?Reaver Drop,? ?Carrier,? and ?Unknown.? Ten-fold cross validation is used for all experiments. The machine learning algorithms are implemented with the WEKA API [14]. The machine learning algorithms are evaluated at different time steps throughout the game. The overall performance is defined as the average accuracy during the game. N is the number of sampling points during the game (in this paper, N=31, Game Time = 15 min).

? =  = GameTime  t Classifier tAccuracyN  Acc Avg  )(1 _  B. Feature-Expanded Decision Tree  Table 4. Comparison of standard DT and the feature-expanded DT in terms of accuracy and the size of model (the number of leaves and the size of the tree) (W =Weber dataset, Y=YGOSU.com)  Race  (Source) Standard DT Feature-Expanded DT  Accuracy (%) Size Accuracy  (%) Size  P (Y) 89.49 (157, 313) 99.73 (15, 29) P (W) 89.68 (125, 249) 99.77 (14, 27) T (W) 91.05 (122, 243) 99.96 (11, 21) Z (W) 95.76 (72 , 143) 100.0 (10, 19)  Average 91.50 (119, 237) 99.87 (13, 24)   4 http://eis.ucsc.edu/StarCraft_Data_Mining  The purpose of the ?feature-expanded? decision tree (FBDT) is to build machine learning models interpretable to humans and easily converted into programming codes (as a build order categorization). The algorithm is applied to the replays from each race. For comparison, the standard DT (without feature expansion) is used. It shows that the FBDT is accurate compared to the standard DT and the size of the model is relatively small. Also, the result from our YGOSU.com data is similar to the Weber?s dataset. Fig. 4 shows an example of conversion from the FBDT into a programming code.

IF (FirstExpansion <= Stargate){ IF(RoboBay <= FirstExpansion){ IF(Citadel <= RoboBay){ IF(Legs <= Archives){ IF(FourthExpansion <= Legs) ?Unknown? ELSE ?Fast Legs? } ELSE ?Fast DT? } ELSE{ IF(RoboSupport <= Observory){ IF(SecondExpansion <= RoboSupport) ?Unknown? ELSE ?Reaver Drop? } ELSE ?FastObs? } } ELSE{ IF(FirstExpansion <= Citadel) ?Fast Expand? ELSE{ IF(Legs <= Archives) ?Fast Legs? ELSE ?Fast DT? } } } ELSE{ IF(Citadel <= Stargate){ IF(Legs <= Archives) ?Fast Legs? ELSE ?Fast DT? } ELSE{ IF(RoboBay <= Stargate){ IF(RoboSupport <= FirstExpansion) ?Reaver Drop? ELSE ?Fast Obs? } ELSE  ?Carrier? } } Fig. 4. Conversion of feature-expanded decision tree into a programming code   C. ?Strategy Prediction? during the Game   Table 5 summarizes the prediction accuracy of machine learning algorithms on PROTOSS vs. PROTOSS games. It shows that the results from Weber dataset are similar to the one from YGOSU.com. As expected, the introduction of ?fog-of-war? decreases the prediction accuracy. It is interesting that the FBDT outperforms other classifiers in the later parts of the games. However, it is very poor in the early         stage of the games. Other machine learning algorithms perform well in the early stage of the game but not the best in the later part of the game. From this observation, it is meaningful to use more than one classifier during the game.

For example, Rotation Forest is used in the early stage of the game but the FBDT in the later part of the game (Fig. 5).

Table 5. The comparison of ?strategy prediction? accuracy (bold means the best accuracy)   (a) P vs. P (Weber Data)   5 min 10 min 15 min Avg_Acc NNGE 49.6 80.6 76.0 60.3 KNN 47.4 80.4 74.5 60.1  J48 [18] 43.9 81.9 85.4 62.1 FBDT 26.0 89.7 99.6 59.7  Bagging [17] 50.0 85.2 86.9 63.1 Random  Committee  50.2 85.1 85.0 64.0  Random Forest [16] 49.8 83.2 83.4 63.7  Rotation Forest [15] 50.4 85.8 89.1 65.3   (b) P vs. P (YGOSU.com)   5 min 10 min 15 min Avg_Acc  NNGE 41.1 81.0 77.5 59.6 KNN 40.4 78.3 67.6 56.0 J48 38.6 85.5 85.0 62.8  FBDT 9.4 89.1 98.5 51.0 Bagging 47.9 87.2 89.4 65.7 Random  Committee 44.6 84.7 86.0 63.7  Random Forest 45.4 84.7 86.3 63.5  Rotation Forest 47.5 87.4 90.6 66.0   (c) P vs. P (YGOSU.com) (with fog-of-war)   5 min 10 min 15 min Avg_Acc  NNGE 38.0 61.3 68.0 46.0 KNN 35.3 57.0 58.8 43.0 J48 37.2 61.5 72.5 50.2  FBDT 11.9 57.4 83.2 37.1 Bagging 40.9 63.9 75.3 53.0 Random  Committee 39.5 62.5 70.1 51.4  Random Forest 40.2 61.6 70.7 51.3  Rotation Forest 41.6 64.8 76.0 53.5        (a) P vs. P (YGOSU.com)    (b) P vs. P (YGOSU.com) (with fog-of-war)   Fig. 5. The introduction of ?fog-of-war? and the prediction accuracy during the game    D. Build Order Change We analyze the 570 replays from YGOSU.com. It shows  that the number of replays categorized into ?Fast Legs? and ?Carrier? is too small compared to other strategies. In the calculation of the winning ratio, we only consider the five strategies except the low-percentage strategies. Table 6 summarizes the winning ratio from the replays. It means that the Fast DT strategy wins 59% against the Fast Observer strategy.

Table 6. Winning ratio from the replay files (YGOSU.com)   (a) The number replays for each strategy in YGOSU.com data Fast Legs  Fast DT  Fast Obs  Reaver Drop Carrier  Fast Expand Unknown  15 162 424 200 0 265 74   (b) Winning ratio of each strategy (0~1)  Player Opponent  Fast DT  Fast Obs  Reaver Drop  Fast Expand  Unknow n  FastDT  0.50 0.59 0.67 0.64 0.00 FastObs 0.41 0.50 0.52 0.49 0.29 Reaver Drop 0.33 0.48 0.50 0.52 0.71  Fast Expand 0.36 0.51 0.48 0.50 0.31  Unknown 1.00 0.71 0.29 0.69 0.50          Fig. 6 shows the change of the E[Win] during the game. The value is calculated using the prediction accuracy of the combined models (Random Forest and FBDT) and the winning ratio in Table 6. In the early stage of the game, the prediction accuracy is not high and it is not beneficial to change the build order. In 6~7 minutes of the game, the prediction accuracy is relatively high and the E[win] becomes the maximum. After the time, the prediction accuracy increases but the possibility of the build order change goes down ?(almost buildings are constructed)?. The player can make a decision on the change of build orders based on the E[win] during the game.

Fig. 6. Expected win (0~1) from the prediction accuracy (Rotation Forest + FBDT) and the winning ratio of each strategy (P vs. P, YGOSU.com data)

V. CONCLUSIONS AND FUTURE WORKS In this paper, we propose a framework to use the replays on  the automatic design of strategy prediction and the build order adaptation. For the replay mining, we develop a new customized tool for the extraction of information from the replays considering the ?Fog-of-War.? For the strategy prediction, we propose to use the ?feature expansion? for the decision tree learning. It returns human-interpretable accurate models to predict the strategy of the game. Because the model is not good in the early stage, it is desirable to hybrid it with other machine learning algorithms (for example, rotation forest). For the build order change, we propose an equation to get the E[Win] values from the prediction accuracy and the winning ratio from the replays.

Experimental results show that the proposed FBDT is promising to build a small-size human-interpretable tree models. However, its accuracy is not good in the early stage of the game. The rotation forest is successful to predict the strategy in the early stage of the game. The combination of the two models outperforms other candidates in the strategy prediction problems. The introduction of the ?Fog-of-War? in the game reduces the prediction accuracy as expected. But there is a learning algorithm robust to the uncertainty. The build order change experiments show that the 6~7 minutes are the best timing to change the build order.

