Fast Scalable Selection Algorithms for Large Scale Data

Abstract?Selection finding, and its most common form median finding, are used as a measure of central tendency for problems in biology, databases, and graphics. These problems often require selection finding as a subcomponent where it can be called many times, and as such speed is important. The Map/Reduce framework has been shown to be an important tool for creating scalable applications. There are a number of valid implementations of the selection algorithms inside of a Map/Reduce framework, certain of which are compared in this paper. However, as the volume of data increases, subtle theoretical algorithmic implementation differences can lead to significant differences in practical application. Therefore, an efficient and scalable selection finding method has the potential to provide general benefit to a number of applications.

This paper compares algorithms that have been redesigned or created for the Map/Reduce framework for the purpose of selection finding, or, finding the k-th ranked element in an unordered set. This paper takes the concepts used from two existing selection algorithms and translates them into a novel method using the Map/Reduce framework with two variations.

Each approach uses a different methodology to reduce the total amount of workload needed for a selection. All the algorithms are compared together for scalability and efficiency in a computing cluster environment with up to 256 processing cores. The results show that the methods proposed in this paper outperform several common alternatives in identifying medians with Hadoop, including using sorting, Pig, and BinMedian methods. Our implementations are also available upon request.

Keywords-Hadoop; Map Reduce; Selection Algorithms; Me- dian Finding

I. INTRODUCTION  Finding the kth smallest element of an orderable set is a problem that underlies many applications in computer science and math and is called selection finding. The most common use of selection finding is the computation of a median, or the element of rank ?N2 ? that divides a set into equal numbers. Selection finding is often used as a subproblem to more advanced algorithms. For example, the median point is used when dividing points equally inside a space, and this action is performed recursively when building a balanced tree for indexes. For large scale datasets, and particularly algorithms that contain the selection as a subcomponent, an increase in speed can be very important.

Within the big-data community the Map/Reduce (MR) framework is commonly used for speed and scalability while  running on large scale data[1]. MR is a framework that splits algorithmic tasks into two phases: a mapping and a reduction, where each phase is done on a set of computing nodes. The mapping phase takes a key/value input pair, and produces a set of intermediate key/value pairs. These intermediate key/value pairs are then distributed to nodes based on the intermediate where the reduce phase merges together the values to the final desired result. Hadoop is an open source java implementation of the MR framework that is widely used. In this paper, our proposed methods are implemented and compared using Hadoop framework, though they should generalize to all MR frameworks.

Despite the popularity of Hadoop, little research or analy- sis has been done on selection finding within this framework.

The relative simplicity of the selection algorithm can mis- guide users into using a nave approach, a total sort of the entire dataset. For example, a search of publicly available code revealed that the naive approach, a total sort of the entire dataset, was the most common implementation. Even the median finding algorithm inside of the O?Reilly book MapReduce Design Patterns: Building Effective Algorithms and Analytics for Hadoop and Other Systems used a total sort to obtain the median. The few statistical libraries found, including the most popular Hadoop library Pig, also require the data to be completely sorted[2]. Among specific implementations, many were incorrect, storing all elements within a single main memory datastructure, or inefficient, requiring all data to be analyzed at some point by a single processor.

The computational cost of sorting the entire dataset is unnecessary for selection algorithms, and can be reduced to a much smaller cost by only sorting the small range of data that are potential kth candidates. The total sort on large datasets can be time consuming even for large systems; for example in the 32-node cluster an optimized Terasort algorithm running on 1Terabyte of data took over 40 minutes. Reducing the total sort to a partial sort can provide substantial reductions in time when only a kth element is needed.

Selection finding algorithms range in time from O(n2) to O(n), the theoretical minimum bound. Most trivial im- plementations of the selection problem simply sort the data,      Figure 1. Eliminating points by only operating on the bins that might contain the k-th element  yielding a total ordering on the data giving access to all kth elements, and a typical running time of O(n log n).

Since a total ordering is unnecessary when only the kth  element is needed, quicker algorithms for determining the selection can be created. The most well-known is the al- gorithm QuickSelect, developed by C.A.R Hoares, which has a best case run time O(n), but O(n2) worst case time.

This worst case time can be improved to a guaranteed linear time, O(n), by using the Median of Median(MoM) algorithm developed by Blum[3] to smartly select pivot points. Even with these algorithmic improvements to the lower bound, quick implementations are still necessary and desired, especially when using selection as a subcomponent of algorithms working on large data sets or when taking advantage of parallel environments.

There are multiple selection algorithms that can be trans- ferred to the Hadoop framework. But an efficient redesign of these selection algorithms for Hadoop requires under- standing the handling of data so that work is not duplicated.

Notably, many of the selection algorithms considered in this paper have a sorting step, but this step may be skipped if implemented correctly in Hadoop. Also, even with knowl- edge of the framework, the ?best? implementation of any algorithm is not obvious.

In this paper we describe and compare several different parallel algorithms for performing the kth selection and the process of moving these parallel selection algorithms and onto a MR framework. These algorithms are analyzed from the standpoint of wallclock, CPU, and network utilization to determine the best approach for the distributed systems.

Two new variations are also introduced, MrS and MrT, which use the same selection algorithm but with different implementations. These two algorithms were designed to explore the inherent properties underlying MR to achieve a fast selection that is scalable for large datasets. Both operate on the same premise of using a multi-phase MR that to perform the sorting on a small range of data.

The algorithms recursively perform partial sorts of the data into interval ranges, find the interval range that con-  tains the k-th element, and then end with a total sort of all elements within an interval range once the number of points contained are less than some threshold. All of the algorithms described in this paper that avoid a total sort of the data run significantly faster, up to 5x times, than the optimized TeraSort algorithm. While this paper focuses on the Hadoop framework for implementation and results the concepts should be generalizable to other MapReduce frameworks.



II. RELATED WORK  The work in this paper is related to the selection finding problem as well as the MapReduce framework. Selection finding is an algorithmic problem with a variety of solutions, moving from O(n2) solutions to the absolute theoretical lower bound of O(n). The naive approach is a simple sort of the data, with a running time usually of O(n log n). The sorting approach is often acceptable for smaller datasets but faster techniques are needed when speed is required or data size is large.

C. A. R Hoare devised a fast selection technique called Quickselect[4] in 1961 which was modeled after his Quick- sort algorithm[5]. Quickselect runs in expected O(n) time and works by utilizing the idea that Quicksort can be used to solve the selection problem. The difference between Quicks- elect and Quicksort is that at each step during the Quicksort only half of the data needs to be analyzed for Quickselect.

The half that needs to be sorted is based on which subset the kth element would be found in. By performing a sort on only half of the data at every step, the Quickselect algorithm can run in expected O(n) time. Unfortunately, Quickselect suffers the same drawback as Quicksort: when bad pivots are chosen, the worst case running time degenerates to a O(n2) search.

In 1973, Manuel Blum authored the paper, Time Bounds for Selection [3], in which he provides a guaranteed way of finding a ?good pivot? in O(n) time. This lowers the worst- case run time of Quickselect to the theoretical optimum ofO(n). The technique Blum proposed is called the Median of Medians (MoM) and works by choosing a pivot that will split the data into at most a 30% and 70% split. The MoM divides any set into 5 subsets where the algorithm is recursively applied on each subset until it contains less than 10 elements. At this point the middle value is returned from the 10 elements and thereafter from each of the 5 children until the final middle value is obtained, which becomes the pivot. While this is theoretically optimal, in practice the additional time spent obtaining the theoretically sound pivot causes the algorithm to run longer, in most expected cases, than it would if simply using Quickselect with a random pivot [6].

A. Map/Reduce and Hadoop  MapReduce(MR) was developed by Google in a 2004 paper and since that time has been implemented in many languages and is used in numerous applications. The most widely used implementation of the MR framework, outside of Google, is Hadoop. Hadoop is a java implementation that is currently being used in many companies such as Facebook, Adobe, Ebay and numerous others. Where the MR environment can be customized for a host of applica- tions, there are no advanced selection techniques that provide optimal speedup on the framework.

The Hadoop API is very expansive but a few particular classes are important for this paper, Mappers, Reducers, Partitioners, and Combiners. The Mappers and Reducers are classes for the Map and Reduce logic respectively. The Par- titioners and Combiners are optimizations that can be used for increased performance. Combiners are considered to be ?mini-reducers? which run on map nodes and can merge together results from mappers before the data is sent to the Reducers. The combiners are not guaranteed to be called but allow the amount of data traffic across the network to be reduced in certain cases. The other important class is the Partitioners. Partitioners provide a way of mapping particular keys emitted from a Mapper to a particular reducer. This can be very useful for load balancing, or as in the case of selection algorithms, you would like distinct nearby keys to be placed into the same reducer.

Apache Pig, [2], is a high level language that works on top of Hadoop. The language allows concise programs to be written for data analysis over a distributed framework.

Pig compiles scripts written in Pig Latin into a sequence of Map/Reduce programs. Pig is often used to speed de- velopment of Map/Reduce programs that can be difficult to reason or adapt directly to a parallel environments or for analysis due to the large number of methods for this purpose. It is also extensible and allows programmers to create their own libraries, or User Defined Functions. One of these libraries, developed by company LinkedIn, is called DataFu which has a set of functions for median, deviation, and other statistics[7].

B. Modern Work in Selection Finding  Modern work has focused on improving the speed of the selection algorithms[8], improving the multiple selec- tion problem[9], and utilizing parallelization[10], [11], [12], [13], [6], [14]. From these algorithms the Binmedian[8] was chosen as a base algorithm and is explained below.

This algorithm was chosen both because of it?s speed and more importantly because it uses an easy binning criteria, which is easily converted to a MR framework. This binning approach can also be seen in Bader?s work Ultrafast[12].

Many of the other parallel selection algorithms require a master/slave approach which are more difficult to convert into a framework that relies on node independence.

Parallelization of the selection has been done for shared and distributed systems. Work by M. Cafaro in 2009 sped up distributed memory systems [6] and proposed two al- gorithms. Both algorithms use similar methods to find the selection, one using a single weighted 3-median, and the second using two weighted 3-medians. Calculating the weighted 3-median finds three intervals where the data can be separated based on their most compact clustering. The selection finding itself is found through recursive iterations, where each iteration discards at least one-third of the data, until the median is converged upon.

Tibshirani?s 2008 paper ?Fast computation of the median by successive binning? introduced binmedian . Binmedian uses a binning approach as a means to recursively reduce the set of possible median points. The algorithm works iteratively in four steps. Given a dataset X:  1) Compute the mean ? and standard deviation ? 2) Form bins based on [?? ?, ? + ?] 3) Determine the bin containing the median 4) Recurse on the bin with the median.



III. ALGORITHMS AND IMPLEMENTATION  The algorithms described here for the selection problem all perform only a partial sort of the data. The central concept is to recursively bin the data, eliminate unnecessary bins, and then sort the bin that can contain the kth item.

The two new algorithms presented in this paper are MrS and MrT. Both use multi phase mappers/reducers to solve the selection finding problem. Both algorithms also use the internal structures within Hadoop, to save computational costs when possible.

The BinMedian, MrS, and MrT algorithms all run in O(n) total time, see the proof in [8]. This proof holds for MrS and MrT as the methodology for choosing the partitions is similar, and in some cases smaller.

A. MrT  MrT is set up in a two-stage process trying to hold closely to the MR idea of using both a Map and Reduce phase. In the first Map/Reduce the bins and bincounts are discovered, while the second Map/Reduce performs the actual binning.

The algorithm starts by taking a random sampling of ??? 1 data, where ? is the number of processors and ? is a natural constant. These sampled points are passed into a partitioner from which P number of bins are created, where P is how many reducers are available.

The first map phase takes data in as a key value pair and and uses the partitioner to determine which bin the pair will fall into. The default mapper is used as it calls the partitioner which will emit the key value to the correct bin location.

Specifically, the partitioner assigns every element x a key i based on P [i] ? x ? P [i+1]. The reducer, which represents a single bin, takes all points and emits a bin count.

Listing 1. MrT: PseudoCode 1 c l a s s MrT : 2 d e f run ( ) : 3 whi le ( | S | > t h r e s h o l d ) : 4 P a r t i t i o n e r p <? f i n d P a r t i t i o n s ( S ) 5 / / Find b i n s and b i n c o u n t s 6 Defaul tMap . run ( ) 7 Reduce1 . run ( ) 9 / / D i s t r i b u t e P o i n t s  10 i n t b i n <? g e t B i n ( ) 11 p <? f i n d P a r t i t i o n s ( b i n ) 12 Defaul tMap . run ( ) 13 S <? Reduce2 . run ( ) 14 S <? t o t a l S o r t ( S ) 16 c l a s s P a r t i t i o n e r : 17 f i n d P a r t i t i o n ( key , v a l u e ) : 18 i n t i n d e x <? f i n d C o r r e c t R e d u c e r ( key ) 19 v a l u e . c l e a r 20 re turn i n d e x 22 c l a s s Reduce1 : 23 i n t reducerNumber ; / / S e t t o a p p r o p r i a t e v a l u e 24 d e f r e d u c e ( key , v a l u e s [ ] ) : 25 c o n t e x t . c o u n t e r s . i n c r e m e n t ( reducerNumber , v a l u e s . l e n g t h ) 27 c l a s s Reduce2 : 28 d e f r e d u c e ( key , v a l u e [ ] ) : 29 f o r v a l u e i n v a l u e s : 30 emi t ( key , v a l u e )  The second stage of MrT takes the interval range of the bin that will contain the k-th element and creates a new set of P bins between this interval. The map phase then iterates over every element x and assigns it to a reducer if it belongs in the new bins, or emit nothing if it falls outside of the range of new bins.

The algorithm terminates when the amount of remaining data is less than some constant, C. Once the number of points is less than this threshold a total sort of the data is performed from which the selection is trivial. The total running time broken into parts is O(N) for counting the number of elements inside of the bins, which is then iterated O(log N) times. The final sorting step is O(N log N) but on a fraction of the original points for a total runtime of O(N).

See appendix A in the BinMedian for a complete proof of runtime for this style of algorithm[8].

B. MrS  MrS uses a similar approach to MrT but differs by using strictly Mappers in two passes over the same data. The idea is to minimize the amount of data needing to be transferred between nodes when calculating the correct bin, which happens between the map and reduce phase. Once the correct bin has been determined the second pass over the data distributes the points across the network as usual.

MrS is set up as a two-stage map, with no reducers, and is performed in a recursive manner as follows. The same precursor step of choosing the random sampling of ?? ? 1 data is performed and the Partitioner creates P bins. The first map phase though, instead of passing the data to a  partitioner, performs the bin calculation itself. Hadoop will only use partitioners if there are reducers, and in this case there are none. The mapper increments the tally of bin counts but emits nothing.

The second stage of MrS takes the interval range of the bin that will contain the k-th element and creates a new set of P bins between this interval. The second map phase then iterates over every element x and will either emit the key/value if it belongs in the new bins, or emit nothing if it falls outside of the range of new bins.

Listing 2. MrS: PseudoCode 1 c l a s s MrS : 2 d e f run ( S ) : 3 se tNumberOfReducers ( 0 ) ; 4 whi le ( | S | > t h r e s h o l d ) : 5 / / Find b i n c o u n t s 6 Map1 . s e t P a r t i t i o n s ( f i n d P a r t i t i o n s ( S ) ) 7 Map1 . run ( ) 9 / / D i s t r i b u t e p o i n t s  10 i n t b i n <? g e t B i n ( ) 11 Map2 . s e t P a r t i t i o n s ( f i n d P a r t i t i o n s ( b . g e t P o i n t s ( ) ) ) 12 S <? Map2 . run ( ) 13 S <? t o t a l S o r t ( S ) 15 c l a s s Map1 : 16 P a r t i t i o n s [ ] p a r t i t i o n s ; 17 d e f map ( key , v a l u e ) : 18 p a r t i t i o n <? f i n d P a r t i t i o n ( p a r t i t i o n s , key ) 19 c o n t e x t . c o u n t e r s . i n c r e m e n t ( p a r t i t i o n , 1 ) 21 c l a s s Map2 : 22 P a r t i t i o n s [ ] p a r t i t i o n s ; 23 d e f map ( key , v a l u e ) : 24 / / f i n d P a r t i t i o n w i l l r e t u r n n u l l i f o u t o f range 25 p a r t i t i o n <? f i n d P a r t i t i o n ( p a r t i t i o n s , key ) 26 i f ( p a r t i t i o n not n u l l ) 27 e mi t ( key , v a l u e )  C. BinMedian  The BinMedian algorithm is also an iterative algorithm that stops when the number of points remaining is less than a threshold[8]. The original BinMedian uses the exact mean and median as a starting point to calculate the number of bins within the range [???, ?+?], but to avoid an additional Map/Reduce phase this was skipped in favor of a random sampling of the distribution. From this random sampling a Partitioner was created with the P bins on the range [? ? ?, ? + ?].

The mapper in the BinMedian algorithm simply emits the key/value pair passed in and lets the Partitioner choose which Reducer will be used. To prevent the default sorting by keys that occurs inside of Hadoop the normal key was changed to the number of the reducer so that every reducer receives only one key. The Reducers simply emit the original key/value to form a series of P bins with unsorted data. The appropriate bin is then chosen and the algorithm continues.

Listing 3. Binmedian: PseudoCode 1 c l a s s BinmedianAdapted :     2 d e f run ( S ) : 3 whi le ( | S | > t h r e s h o l d ) : 4 / / Find b i n s 5 / / f i n d P a r t i t i o n s w i l l c a l c u l a t e t h e mean and 6 / / d e v i a t i o n f o r a s u b s e t o f p o i n t s 7 P a r t i t i o n e r p <? f i n d P a r t i t i o n s ( S ) 8 Map1 . run ( ) 9 Reduce1 . run ( )  11 / / g e t t h e c o r r e c t b i n 12 i n t b i n <? g e t B i n ( ) 13 S <? g e t F i l e ( b i n ) 14 S <? t o t a l S o r t ( S ) 16 c l a s s Map1 : 17 d e f map ( key , v a l u e ) : 18 p a r t i t i o n <? f i n d P a r t i t i o n ( key ) 19 e mi t ( p a r t i t i o n , append ( key , v a l u e ) ) 21 c l a s s Reduce1 : 22 i n t reducerNumber 23 d e f r e d u c e ( key , v a l u e s [ ] ) : 24 f o r v a l u e i n v a l u e s : 25 k , v <? s p l i t ( v a l u e ) 26 emi t ( k , v )  D. Algorithmic Analysis  The partial-sort stages of MrS and MrT have the following expected runtime, assuming ? processors. The map step first divides N points to ? processors where each processor performs a linear amount of work, ?, for an expected time of N?? . During the shuffle sort segment of the map step an additional ? log ? work is done for each of the ? processors leading to N?? + ?(? log ?) and assuming N >> ? leads to O(N? ).

The final total-sort stage of MrS and MrT is simply a sort of the data remaining, O(N? log  N ? ). In practice the amount  of data in this stage is small, and theoretically can be shown that the entire algorithm including this sort is still O(N), [8].

E. Test Setup  All test sets were created using the TeraGen program.

TeraGen is a program included in the Hadoop distribution which creates n data points of 100 bytes each. The generated points are a 10 byte key, 2 bytes of space, and an 88 byte random value for a total of 100 bytes per point.

Algorithm speed comparisons were run on one terabyte of data. The speed factors considered were total time, measured from the start of an algorithm until the result was written to HDFS. The time it takes to place the unsorted data into HDFS is not included. This means that generating the key/value pairs using TeraGen and the corresponding write to disk are not part of the total time. Other metrics such as Map time and Reduce time are also included when appropriate. While these times are accurate it must be noted that in most cases Hadoop starts the Reduce phase while the Map phase is still running, so there is overlap in the recorded times.

Apache Pig is a commonly used framework for writing easy queries on top of Hadoop[2]. The most commonly used  Figure 2. Algorithm CPU Comparison. Shows the average processor utilization at each time step for 1TB of data.

Figure 3. Algorithm Network Comparison. Shows the average of the bytes sent for all processors at each time step for 1TB of data.

statistical package for Pig is named DataFu which contains a method for Median selection which was used in this paper.

The Pig version used in this paper was Version 0.11.1.

Scalability results were run on 100 gigabytes of data with the cluster size ranging from 16 cores to 256 cores, as each node is 8 cores this means 4 through 32 nodes were used.

All tests were performed on TACC Longhorn. Relevant machine specs, each node is a SunBlade x6420 with two AMD Opteron Quad-Core 64-bit processors running at 2.3 GHz. Each node has 48 GB of RAM running at 667 MHz DDR2. Nodes are connected with Sun InfiniBand. Full machine specs can be found at the TACC website[15].

Software versions used for this paper, Java 7u27 with Hadoop version 0.23.7.



IV. RESULTS & DISCUSSION  The performance of the algorithms is based around the time required to complete the map/reduce phases, the num- ber of iterative steps, and the data transfer. For a parallel selection algorithm, the processing steps are as follows: first, discovery of the number and choice of bins so that points can be distributed amongst the different processors; second, comparison of the points and insertion into the appropriate     bins; and finally, either a sort of one or more bins, or a re-run of the algorithm on one of these bins. Many of these processing steps can be parallelized at the cost of transferring the data.

Pig and Terasort are significantly slower than the selection algorithms as they perform a total sort on all the data. Most of the Pig results are not shown on the charts as they are over 2x slower than Terasort. As can be seen in Figure 4, Terasorts demarcation between the map and reduce phase is very clear. In the map phase each point is assigned a bin and distributed to a reduce node. During the map phase the average CPU usage is very high but starts decreasing as nodes start finishing their point distribution and become idle. Once the Reduce phase starts then the CPUs are again mostly utilized until they finish their sort of the data. As can be seen, the data distribution during the Map phase is very low until nodes start finishing their distribution where they need to transfer their points to the Reducers. The Reducer phase shows a continuous high data transfer as the results are written back to the HDFS.

The main problems with Terasort are: during the map phase node CPU is under utilized as each node begins transferring data or finishes processing, and during the reduce phase the time spent sorting the data accounts for about half of the time of the total algorithm, 4. So we can expect that by simply not performing a total sort we should be able to create selection algorithms that are twice as efficient.

The algorithms that only do a partial sort of the final data are between 2x and 4x times faster than TeraSort on the 1TB dataset. As can be seen in Figures 5 2, the BinMedian CPU curve during the map phase is similar to Terasort but less CPU is used, and the phase is much quicker. The reduced CPU comes from emitting already sorted keys to the reducers, skipping the additional overhead of the default Hadoop sort. This leads to both less CPU used and a quicker map phase.

The CPU profiles in Figure 6 and Figure 7 show the two stage approach of finding the bin first, then distributing the points. First it is important to notice that both approaches show significant speedups over Terasort despite performing ?redundant? work in the form of repeating the bin cal- culations for each point twice. More importantly, despite having nearly the same algorithm for selecting the bins and distributing the points, they have very different CPU and network profiles. Especially noticeable is the CPU utilization and network utilization between MrS and MrT. MrS is the only algorithm to have consistently high CPU utilization that does not decrease as the mapping phase completes. The reason for this is that it needs no data transfer in between its stages. The network overhead of transferring data inside of Hadoop is a bottleneck for CPU use that is eliminated.

So it appears that performance of the algorithms is tied to the data transfer. As the Hadoop framework transfers data  Figure 8. Scalability Results showing the decrease in the total amount of time needed for each algorithm as a function of the number of processors used for 100G of data. Number of processors include the Namenode(which doesn?t perform work directly)  from one node to another the CPU utilization is reduced and the time increases. This effect is exacerbated on networks that utilize a replication factor for redundancy. Since Hadoop places the data a priori on the nodes it is more efficient to perform calculations on the same data, if possible.

The MrS algorithm performs multiple passes on the same data in successive Map only phases. The first sweep through the data merely counts how many items will be in each bin and performs no other calculations, so no data is transferred.

In the second sweep data that belongs to the appropriate bin are written out. As each phase is discrete, no reducers will be started until all maps are completed, and all network traffic is cabined to the end of each Map phase. This approach despite more passes on the same data outperforms the others because the data transfer is minimized.

The speedup of the algorithms for the 100G dataset does not show a linear improvement, see Figure 8. This shows a general trait of Hadoop, that it is built for very large scale datasets. As shown in the other results, the data transfer in Hadoop can be very costly. With this small size of dataset as the nodes increased the ratio of the network overhead versus the CPU time becomes a bottleneck for the algorithms. In this case algorithms that minimized the amount of traffic back and forth showed the best time improvement. Runs with larger data sets would be expected to demonstrate the linear speedups provided by these algorithms, because the transfer time would comprise a smaller portion of total run time.



V. CONCLUSION  Many algorithms in the fields of indexing, biology, and statistics have a subcomponent that utilizes a kth point, especially a median, and that is often called recursively.

As selection finding is often a step where a total order is not needed, having fast selection algorithms can greatly improve the overall running time of these algorithms. The MapReduce model allows for the use of these algorithms to scale linearly on large scale datasets. The open source java     Figure 4. TeraSort Profile showing the CPU usage (CPU), Bytes sent across the network (send), Bytes written to disk (writ), and the percentage completed of the map/reduce phase.

Figure 5. BinMedian Profile showing the CPU usage (CPU), Bytes sent across the network (send), Bytes written to disk (writ), and the percentage completed of the map/reduce phase.

implementation of MapReduce, Hadoop, was used as for the comparison but the underlying principles of the network and CPU utilization should generalize to other MapReduce frameworks.

Many selection algorithms can easily be made using the Hadoop framework, but, it is a non-obvious problem as to which approach will best take advantage of the Hadoop system. This paper shows the implementation of several algorithms, but more importantly shows the underlying reasons which explain why each algorithm performs the way it does. Exposing the underlying CPU, I/O access, and data transfer for each of the algorithms gives concrete evidence  about not only these algorithms, but also ideas on how to write future Hadoop programs.

Several approaches to the selection problem, utilizing a variety of algorithms and implementations, were analyzed here in an effort to understand what is the best way to use Hadoop to solve the selection problem. As with many high performance parallel systems, reducing the amount of data passed between nodes and ensuring a relative load balance among processors is important. Some of the approaches that can be used include naively sorting, changing the binning criteria from Mappers to Partitioners, moving work from the Mapper to the Reducer stage (or visa versa), or     Figure 6. MrT Profile showing the CPU usage (CPU), Bytes sent across the network (send), Bytes written to disk (writ), and the percentage completed of the map/reduce phase.

Figure 7. MrS Profile showing the CPU usage (CPU), Bytes sent across the network (send), Bytes written to disk (writ), and the percentage completed of the map/reduce phase.

even eliminating one of the Map/Reduce phases. A correct selection algorithm can be achieved through any of the approaches, but given the multitude of options, guidance is needed to direct programmers toward more optimal so- lutions. In particular it is clear that avoiding data transfer is desirable in Hadoop even if it means performing multiple stage Map/Reduce algorithms.

From the results shown here it can be seen that the preferable approach is utilizing an algorithm that performs multiple Map phases over the same data to eliminate any sorting steps or data transfer until the last possible moment.

Using this approach, selection of a point can be sped up by several times over the optimized TeraSort or other selection algorithms.

An efficient and scalable selection finding method has the potential to provide general benefit to a number of applications. The results show that the methods proposed in this paper out perform several common alternatives in identifying medians with Hadoop, including using sorting, Pig, and BinMedian methods. If incorporated into some of the popular statistical packages for Hadoop, all users will gain quicker access to the results they need.

ACKNOWLEDGEMENTS  The authors acknowledge the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for pro- viding HPC resources that have contributed to the research results reported within this paper. URL:http://www.tacc.

