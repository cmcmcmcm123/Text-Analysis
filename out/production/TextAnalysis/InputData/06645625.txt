

PARALLEL AND SCALABLE CUSTOM COMPUTING FOR REAL-TIME FLUID SIMULATION ON A CLUSTER NODE WITH FOUR TIGHTLY-COUPLED FPGAS  Kentaro Sano?, Ryo Ito, Hayato Suzuki, and Yoshiaki Kono  Graduate School of Information Sciences, Tohoku University 6-6-01 Aramaki Aza Aoba, Aoba-ku, Sendai 980-8579, JAPAN  Numerical simulation based on computational fluid dy- namics (CFD) is now an indispensable technique especially in industry due to its acquisition capability of various data at a lower cost than experiments using a wind tunnel. The lat- tice Boltzmann method (LBM) is one of the CFD schemes, which is used to compute various problems including multi- phase flow. LBM has good parallelism, but simultaneously requires many data to compute each lattice point, result- ing in a low operational intensity. Consequently, the sus- tained performance of LBM is limited by memory band- width rather than arithmetic performance when computed by using general-purpose processors and GPUs. To make matters worse, insufficient bandwidth and high-latency of an interconnection network cause a relatively big overhead in parallel computing, especially in the case of strong-scaling.

For scalable computation, acceleration with custom com- puting is a promising approach. Hardware designs dedicated to an individual application allow the system to efficiently utilize computing resources, being balanced with memory and network bandwidth. Moreover, cutting-edge semicon- ductor technology allows FPGAs to be capable of outper- forming microprocessors, even for floating-point operations, with a lot of logic elements, DSP blocks and block memories integrated with high-speed interfaces. Nowadays state-of- the-art FPGAs have a potential to bring high-performance and high-bandwidth to low-power custom computing.

Such circumstances motivate FPGA-based custom com- puting for scientific applications. In our research project, we have developed a tightly-coupled FPGA cluster where FPGA-based accelerators are directly connected by an ac- celerator domain network (ADN) to achieve scalable com- putation with high-bandwidth and low-latency data commu- nication among accelerators. For LBM computation to ef- ficiently utilize both ADN and the computing resources on FPGAs, so far, we have also proposed and prototyped a par- allel custom computing architecture for an LBM accelera- tor, which is shown in Fig.1. As reported in [1], all the computation including non-regular computation for bound- ary conditions is performed by FPGAs to avoid a bottleneck in data transfer between a host CPU and an FPGA. Another multi-FPGA LBM system [2] has significant limitation in scalability due to the data transfer. Parallel computing with low-latency ADN achieves almost linear scalability to the  ?Corresponding author. Email:kentah@caero.mech.tohoku.ac.jp  to/from neighbor FPGA  DRAM A  In te  r- FP  G A  T x/  R x  m od  ul e PE (1,1) PE (ns,1)PE (2,1)  PE (1,2) PE (2,2)  PE (1, nt) PE (ns, nt)  Memory I/F  In te  r- FP  G A  T x/  R x  m od  ul e  DRAM B  Memory I/F  to/from neighbor FPGA  Wmem  WI/O 2 WI/O 2  Wmem FPGA  Fig. 1. Parallel custom-computing architecture for LBM.

number of FPGAs [1].

The main objective of this demonstration is to present our  recent progress of scalable and real-time fluid simulation on a node of our tightly-coupled FPGA cluster. In this demon- stration, we will present a cluster node with four Stratix IV FPGAs, and real-time LBM fluid simulation and visuzliza- tion with complex boundary configuration of a flow field.

We will also show the system-on-programmable-chip design of the LBM accelerator, and linear scalability with four FP- GAs in comparison with performance of CPU-based LBM.

