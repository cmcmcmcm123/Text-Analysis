h-MapReduce: A Framework for Workload Balancing in MapReduce

Abstract?The big data analytics community has accepted MapReduce as a programming model for processing mas- sive data on distributed systems such as a Hadoop cluster.

MapReduce has been evolving to improve its performance. We identified skewed workload among workers in the MapReduce ecosystem. The problem of skewed workload is of serious concern for massive data processing. We tackled the workload balancing issue by introducing a hierarchical MapReduce, or h-MapReduce for short. h-MapReduce identifies a heavy task by a properly defined cost function. The heavy task is divided into child tasks that are distributed among available workers as a new job in MapReduce framework. The invocation of new jobs from a task poses several challenges that are addressed by h-MapReduce. Our experiments on h-MapReduce proved the performance gain over standard MapReduce for data- intensive algorithms. More specifically, the increase of the performance gain is exponential in terms of the size of the networks. In addition to the exponential performance gains, our investigations also found a negative effect of deploying h- MapReduce due to an inappropriate definition of heavy tasks, which provides us a guideline for an effective application of h-MapReduce.

Keywords-MapReduce, workload balancing, hierarchical MapReduce;

I. INTRODUCTION  Since its introduction in 2004 by Google, MapReduce paradigm has become one of the programming model of choice for processing large data sets [4]. MapReduce is a framework for developing a distributed solution for complex problems over huge data sets using a large number of computers (nodes), collectively referred to as a cluster or a grid. Favor from distributed file systems (such as HDFS) en- hanced the framework to improve its efficiency by leveraging data locality awareness. Similar to other distributed systems, MapReduce also constitutes a master and a set of workers.

The master is called ?job tracker?, while the workers are called ?task trackers?. In contrast to traditional distributed programming models [1], [3], [2], MapReduce neither pro- vides a shared memory nor a message passing interface. To compensate for the lack of communication, outputs of some set of tasks called map tasks are passed to other set of tasks called reduce tasks. A MapReduce algorithm comprises both map tasks that project given data set called input records into another data set called intermediate records, and a Reduce  Figure 1. Power law degree distribution of a Twitter user-follower network  task that combines intermediate records to a desired final result. Therefore, the reduce phase does not start until all map tasks are completed.

In general, the Map and Reduce functions divide the data that they operate on for load balancing purposes. It is not uncommon for data intensive social network analysis that a map/reduce task works on a vertex or edge. The workload of all map/reduce tasks is not uniform as the degree distribution of social networks is skewed. Social networks such as Twitter user-follower network and Document-Term network, follow power law degree distribution which is skewed. For instance, as it is shown in Figure 1, most of the vertices have a low degree, whereas only a very small portion of the vertices have extreme high degrees for a Twitter user- follower network [5]. When such network is distributed as a set of vertices among map/reduce tasks for vertex- based processing, the workload among the mappers is also skewed. The mappers that receive a high degree vertex take much longer time to complete than those that receive a low degree vertex. However, slow or bogged down machines may lead to straggler computations within the system that might lead to a longer completion time: the completion time is only as fast as the slowest computation. If one straggler computation is twice as slow as other computations, then the total elapsed time would be twice of that without the straggler computation. In such a case, a straggler detection and avoidance mechanism becomes necessary.

In this paper, we identified skewed workload in dis- tributing tasks among workers in MapReduce ecosystem.

The problem of skewed workload is of serious concern   DOI 10.1109/AINA.2013.48     for massive datasets. We address the issue by introduc- ing a hierarchical MapReduce paradigm (h-MapReduce).

Workload balancing in h-MapReduce framework is achieved by splitting the heavy tasks. The solution poses several challenges that can be addressed by h-MapReduce. Our investigations on h-MapReduce proved the performance ben- efits over standard MapReduce for data-intensive algorithms.

For the demonstration purposes, Hadoop is considered as MapReduce ecosystem and Hadoop and MapReduce are referred to interchangeably.

The rest of the paper is organized as follows. We present the skewed workload problem in Section II followed by our proposed solution h-MapReduce in Section III. An experimental evaluation of h-MapReduce is presented in Section IV and V. We review related work in Section VI.

Finally we conclude the paper with future work in Section VII.



II. PROBLEM DESCRIPTION MapReduce is intelligent in distributing tasks among task  trackers by leveraging location aware algorithms. A task can be either a map task or a reduce task. Each task tracker com- pletes its assigned task. The task assignment is carried out by MapReduce?s job tracker using specified configuration.

A task tracker reports progress of the running task and job tracker preempts if it takes longer than configured threshold time ( referred as timeout). There is no mechanism to predict beforehand that a task is heavy for a task tracker that take long time to complete. There are two types of long running tasks; 1. Tasks that run forever and never complete 2. Tasks that take long time to complete. Type 1 tasks encounter due to bugs in algorithm or code. Our work primarily focuses on the tasks of type 2. The long running tasks put the job on wait for completion. It is a serious concern if there are more such long running tasks.

Example 1: To illustrate the problem, assume a job for computing average clustering coefficient (CC) of a network.

In graph theory, a clustering coefficient is a metric which represents how likely network elements tend to come together to form clusters. Triangle is primary building block of clusters in a network and CC measures the density of triangles in a network. A triangle is formed when two neighbors of a vertex are themselves neighbors. Let ?v? be a vertex and N(v) be the neighbor set i.e. adjacency list of vertex ?v?. The number of links among the neighbors i.e.

the number of triangles with this vertex is  |triangles(v)| = |{(c, d, v)|c ? N(v), d ? N(v), c ? N(d), d ? N(c), and c ?= d}| (1)  can also be written as,  |triangles(v)| = ? ? ??c?N(v)N(c) ?N(v)  ? ? ? /2 (2)  The local clustering coefficient of a vertex in a network is proportion of number of triangles to number of possible  triangles, in other words it is the number of links between the vertices within its neighbourhood divided by the number of links that could possibly exist between them. Having the number of triangles, the local clustering coefficient of the vertex ?v? in an undirected network is computed as  CC(v) = (|triangles(v)|)/(|N(v)|?((|N(v)|?1))/2) (3) Then on, the average CC of a network is defined as  average of local clustering coefficients of all vertices in the network.

Consider a job to compute average clustering coefficient in MapReduce framework. The input key and values pair for map tasks is the vertex and its neighbors in the adjacency list. A map task is to compute local clustering coefficient of the input vertex, which can be performed by obtaining the adjacency lists of its neighbor vertices. MapReduce algorith- m to compute average clustering coefficient is presented in Algorithm 1.

Algorithm 1 Algorithm to compute average clustering co- efficient CC of a network Mapper Input: key is a vertex ?v?and value is adjacency list of ?v? i.e.

?N(v)? Output: key is a unique term i.e. ?CC?, value is local CC of vertex ?v?  1) Let v=vertex; 2) Let degreeOfv=sizeof(N(v)); 3) Let triangleCount=0; 4) for each( c in N(v) )  triangleCount+=CountCommonElements(N(c),N(v)); 5) localCC=triangleCount/(degreeOfv*(degreeOfv-1)/2); 6) emit(?CC?,localCC);  Reducer Input: key is unique term ?CC?, values are local CCs of all vertices of the network Output: key is unique term ?CC?, value is average CC of the network  1) Let avgCC=0; 2) Let ccCount=0; 3) for each( oneCC in values)  avgCC+=oneCC; ccCount++;  4) emit( ?CC?, avgCC/ccCount );  In the Algorithm 1, the function ?CountCommonEle- ments? is computationally costly and called for each vertex in the neighbor list of a vertex. The more the neighbors, the more the number of calls to the function, therefore more the time is required to compute local clustering coefficient of such high degree vertex. There is no guarantee that the degree distribution is uniform. In cases where 99% of the vertices are low degree and just 1% of the vertices are high degree (compared to low degrees), the maps that run mapper     on lower degree vertices complete very quickly and sitting idle to wait for the high degree vertices to complete that actually count just 1% of the network. The reduce tasks do not start until all the map tasks complete, thus causing long time to complete the job. Therefore, if the adjacency list of vertices is a skewed distribution like in the case of social networks, the task computations are also not balanced. One can argue that workload can balanced based on computation cost of a task for a vertex which is computed from the size of its adjacency list. The cost function cannot balance the workload in the case of skewed degree distributions such as those of social networks because this cannot change when there is one (very less number of) high degree vertex(s). In skewed distributions, task trackers who work on high degree vertices could have been working while other task trackers are sitting idle after completing corresponding tasks. One can observe the problem if the tasks cost function is skewed.

The fundamental risks from the skewed workload distri- bution are following:  ? Some task trackers are overloaded with heavy tasks.

? Heavily loaded tasks run for long, therefore the job  waits for this task to complete, which in turn increases run time of the job.

? Under-utilization of resources when the heavily loaded task tracker is working and other task trackers are sitting idle.

There are many such MapReduce algorithms, besides computing average clustering coefficient in a network, that encounter the skewed work load distribution issues.



III. H-MAPREDUCE The limitation of MapReduce paradigm prompts us to  further split heavy tasks of a job. When a task on a record of a map reduce job takes long time or needs more resources than others, the task on the record is eligible for further splitting to make use of idle resources in the infrastructure.

It is trivial to use the existing MapReduce ecosystem for the tasks that split. The new tasks run in the same MapReduce framework in the form of a new job and called ?Child Job?. The tasks that initiated child jobs, refer the tasks as parent tasks, wait until the child jobs get completed.

This way a task of a task tracker is distributed among active and available task trackers if the task is predicted to run for long time. The decision whether to distribute a task needed to be taken beforehand the processing a record of the task. The task of a heavily loaded task tracker is parallelized to complete the task in reasonable time. The parent and child jobs constitute a hierarchy of jobs, thus the proposing solution framework is called ?h-MapReduce?, short for hierarchical MapReduce. Since parent and child jobs run on MapReduce ecosystem, both jobs are developed using MapReduce programming model. Developers are free to use other distributed systems for child jobs, if one exists.

Parent task sits on hold until its child job gets back with  Figure 2. Architecture of h-MapReduce  results. The framework for the hierarchy of parent and child jobs does not alter the underlying MapReduce ecosystem but leverages it for workload balancing. The architectural view of the h-MapReduce framework is presented in Figure 2. The proposed concept of initiating a job from a task of a job is complex and poses several challenges. The challenges are addressed in the following discussion.

A. Challenges in h-MapReduce  h-MapReduce addresses design and development chal- lenges that arise from creating a child job from a task of a MapReduce job. The challenges are following.

1) Definition of heavy task (When to split a task): The basis of h-MapReduce is to develop a mechanism to draw a line between light-weight tasks and heavy tasks.

The heavy task definition decides when to split a task. A cost function of a task helps to draw such a line. Cost functions depend on the algorithm and the problem that the algorithm is attempting to address. The cost function of a task that computes the clustering coefficient of a vertex in an undirected network is the degree of the vertex. A vertex having degree more than a threshold implies a heavy task and leads to a child job for the vertex from the task.

2) Algorithm for Child Job (How to distribute a task): As we discussed, the existing MapReduce ecosystem is used to parallelize the child job. This requires developers to come up with a MapReduce-based algorithm for child jobs. To keep things simple, the child job needs to be efficiently designed for the MapReduce paradigm. The child job works on the record that could cause heavy work load in parent job. In our example of computing clustering coefficients, the child job is implemented as a MapReduce algorithm to count the number of triangles associated with the vertex in the parent task.

With the result from the child job, the parent task computes clustering coefficient of the vertex.

3) Deadlock: It is not uncommon that hierarchical job- s encounter deadlock situations and h-MapReduce is no exception. In the case that all the resources (i.e. all task trackers) are occupied by the parent job; child jobs sit waiting in queue for available resources. The parent job could not vacate the resources until child job complete as tasks in the parent job waits for child jobs. This scenario resembles deadlock where there is no progress of jobs be- cause of child job waiting for resources occupied by parent job that waiting for child job to complete. Though there are several possibilities to address the deadlock situation, a simple approach is presented here that does not disturb the MapReduce ecosystem. The simple approach is to reserve specified resources (i.e. set of task trackers) for parent jobs and the rest for child jobs. This is analogous to a division of one MapReduce ecosystem into two MapReduce ecosystems. It is not exactly dividing the ecosystem but reserving resources for dependent jobs so that they should not compete for resources. To demonstrate, reservations are accomplished in h-MapReduce through the Hadoop Fair Scheduler. Two queues are defined, one for parent jobs and aother for child jobs. The Fair Scheduler takes necessary steps to prevent competition for resources among two queues and consequently prevents competition between parent and child jobs.

4) Configuration conflicts: A configuration is necessary to initiate a new MapReduce job. Each task in a job obtains the configuration in the MapReduce ecosystem. h- MapReduce uses the configuration obtained from the parent job to initiate a new job in a task. Since the algorithms for parent and child jobs are different, there is a need for different configuration. The conflicts encountered from such inheritance necessitated attention from developers to reset configuration parameters inherited from the parent job. The ?INPUT SPLIT? parameter in the MapReduce configuration set is the best example that demonstrates the importance of attention paid to conflicts. The parent job works on massive input and needs to set a higher value for INPUT SPLIT while child jobs work on a smaller input (e.g. one adjacency list) and require a lower value to best utilize MapReduce resources.

By addressing these challenges, h-MapReduce is able to successfully initiate and complete child jobs from a MapReduce task. We investigated the performance of the h- MapReduce with standard apache Hadoop; the experiments are presented in Section IV.



IV. EXPERIMENTS  Several experiments are conducted to analyse the ef- fectiveness of the proposed h-MapReduce in comparison to traditional MapReduce framework. We adopted Apache Hadoop as the MapReduce framework and implemented h- MapReduce on top of it. Two applications are developed on both the Hadoop and h-MapReduce. The applications are:  1. Compute average clustering coefficient of a network and 2. Sorting terms of each document in a corpus.

All the experiments are performed on Hadoop cluster of 2 masters and 24 slaves each with 7 mappers and 7 reducers. Each slave has 8 processors and 16GB RAM. The experiments include a run using standard Hadoop ecosystem and other with proposed h-MapReduce framework. Each experiment run twice, and average of the run times is considered. The run time for each experiment is noted and plotted in charts for the discussion in the following section.

A. Datasets  Two types of datasets are used to evaluate the h- MapReduce framework for undirected networks. Each serves a distinct purpose. One of them is star shaped networks.

Star networks are constructed in such way that only a single vertex is connected to every other vertex in the network and there is no other connections. The rationale behind choosing this dataset is to find the gain from h-MapReduce over traditional MapReduce. In these datasets, we only see a heavy task and rest are light weighted ones. Therefore, the dataset is good for evaluating the performance in case of one heavy task. We generated several star networks of various numbers of vertices from 5,000 to 200,000,000.

The other type of datasets is generated using Benchmark network generator [7]. The primary use of the dataset is to show the negative effect of the proposed framework with wrong definition of heavy task. Benchmark networks possess low degree. Other characteristics of the benchmark networks include both the degree distribution and the size of community structures follows power law distribution.

Networks with number of vertices ranging from 10,000 to 160,000 are generated for the purpose.

B. Calculating Average Clustering Coefficient  The primary objective of the algorithm is to compute average clustering coefficient of a network. MapReduce al- gorithm for computing average CC of a network is presented in Algorithm 1. Mapper of the algorithm takes a vertex and corresponding adjacency list as key and values. The mapper emits vertex ?v? as key and corresponding local clustering coefficient as value. Reducer sums up all the local clustering coefficients and emits mean of them as value (key does not matter here). An index mechanism is implemented to retrieve adjacency list of a given vertex. The index is distributed on distributed file system and available for all tasks of a MapReduce job. Therefore the mappers leverages the index to obtain N(v), N(c) and N(d) to compute CC(v).

As mentioned in Section II, Algorithm 1 is not ef- ficient when the given network has skewed degree dis- tribution. Therefore, h-MapReduce strategy is applied to address the workload balancing problem. More specifically, h-MapReduce splits the task (which is heavy loaded) by introducing a new MapReduce job. In our experiments we     define a heavy task if N(v) > 5000 and the heavy task is to compute clustering coefficient of a high degree vertex. The rationale in choosing 5000 is that our investigations found the computation time for vertices with N(v) < 5000 is lesser than Hadoop job setup and cleanup times. The h-MapReduce paradigm based algorithm for computing average CC is summarized in Algorithm 2.

Algorithm 2 Algorithm to compute average clustering co- efficient CC of a network using h-MapReduce (Parent Job) Mapper Input: key is a vertex ?v?and value is adjacency list of ?v? i.e.

?N(v)? Output: key is a unique term i.e. ?CC?, value is local CC of vertex ?v?  1) Let v=vertex; 2) Let degreeOfv=sizeof(N(v)); 3) if(degreeOfv>5000)  triangleCount = countTrianglesInChildJob(v, N(v)); 4) else  Let triangleCount=0; for each( c in N(v) )  triangleCount+=CountCommonElements(N(c),N(v)); 5) localCC=triangleCount/(degreeOfv*(degreeOfv-1)/2); 6) emit(?CC?,localCC);  Reducer Input: key is unique term ?CC?, values are local CCs of all vertices of the network Output: key is unique term ?CC?, value is average CC of the network  1) Let avgCC=0; 2) Let ccCount=0; 3) for each( oneCC in values)  avgCC+=oneCC; ccCount++;  4) emit( ?CC?, avgCC/ccCount );  Expensive operation in the algorithm 1 is counting number of triangles associated with a vertex that is reduced in h- MapReduce paradigm by introducing a new MapReduce job for the purpose. In algorithm 2, a child job is initiated if the given vertex has high degree. The function ?count- TrianglesInChildJob? takes the neighbor list and passes the list as input to child job to run the algoirthm 3. The child job receives adjacency list as input and counts the number of triangles with each vertex in the adjacency list of ?v?.

A MapReduce algorithm for child job is developed and presented in Algorithm 3.

A mapper of the child job takes a vertex ?w? as input value. We define distance of a vertex (say ?w?) as number of hops needed for it to reach the vertex ?v? for which the child job is invoked. A vertex in adjacency list of ?v? is at distance 1 and they can reach ?v? in one hop. A mapper in child job finds neighbors of the vertex ?w? and emits its  Algorithm 3 Algorithm to count number of triangles asso- ciated with a vertex ?v? (MapReduce Algorithm for Child Job) Mapper Input: key is vertex ?w? where w ? N(v) Output: key is vertex, value is distance from ?v?  1) Let w=vertex; 2) for each( u in N(w) )  emit(u,2); 3) emit(w,1);  Reducer Input: key is vertex, value is distances from ?v? Output: key is vertex, value is number of triangles with ?v? and this vertex  1) Let w = vertex, nTriangles=0, isTriangle = false;  2) for each( distance in values) if( distance = 1)  isTriangle = true; if( distance = 2)  nTriangles++; 3) if( isTriangle )  emit( w, nTriangles );  distance to reach the vertex ?v? i.e. distance of 2 as value.

Alternately, each vertex in adjacency list of ?w? can reach ?v? in 2 hops through ?w?. The mapper also emits the vertices with distance 1 i.e. ?w? with distance 1. Reducer of the child job receives a vertex as a key and corresponding distances as values. For a vertex, if there is a value of ?1? then this vertex has direct edge with ?v? and all the distances of ?2? corresponds to triangles. Therefore the number of triangles associated with the vertex ?v? and with the vertex ?w? is number of distances of ?2? if there is at least a distance of ?1?. By the end of the reducer, we have a vertex ?w? (in adjacency list of ?v?) and corresponding count of triangles.

The flowchart of the algorithm is presented in Figure IV-B.

For performance improvement, the reducer emits a record for each triangle i.e. for each value of distance ?2?. The number of triangles with vertex ?v? in a task from parent job is number of output records from child job. The clustering coefficient of ?v? is computed using the obtained values and the Eq. 3. Therefore a task of a mapper is distributed among available task trackers in MapReduce ecosystem.

For star shaped networks, we define a heavy task if a map- per gets a vertex of degree over 5,000. The h-MapReduce framework computed average clustering coefficient faster than traditional MapReduce. A discussion on run time is presented in following section. It is obvious that a local clustering coefficient of a vertex with only 100 edges can be computed by a task for which no child task is necessary. To     Figure 3. Flowchart for computing average Clustering Coefficient  show the adverse effect of the proposed framework with a wrong definition of heavy task, a heavy task is defined with the degree over 100. Benchmark datasets are used for the purpose of showing negative effect of h-MapReduce. The result is presented in Section V.

C. Sorting terms of each document in a corpus  Sorting terms of each document in a corpus is another case study of the h-MapReduce. Assume there is a bi-partite document-term network. One type of vertices in the network is documents and other is terms (one or more words). A document is connected to several terms which occurred in the document. There are many such documents and terms.

Now the task is to sort the terms in lexicographic order in each document. In other words, to sort adjacency list of each document vertex in the document-term network.

Sorting terms can be applied in many real world applications such as information retrieval.

If there is only a document, a fast algorithm can be developed on MapReduce ecosystem. The algorithm can be extended to sort terms of more than one document in a corpus. Each mapper takes a document and sorts its terms. Input for a mapper is document identifier as key and containing terms as value. Mapper sorts the terms and emits document identifier as key and sorted terms as value.

There is no need for reducer. But if there is a document that contains tens of thousands of terms, the mapper that processes the document, i.e. sorts its terms, takes a long time because the number of terms are more than a mapper can handle. h-Mapreduce comes again to rescue when a map task is too heavy. Though there are techniques for external sorting to deploy in a map task, they do not scale for millions of items. MapReduce found its efficacy in sorting of given keys.

Taking the advantage, h-MapReduce framework distributes the task of sorting among MapReduce task trackers as a new job. As a default operation MapReduce outputs keys in sorted order. The terms of a document is given as input keys for a MapReduce job and output obtained from identity mapper and identity reducer is sorted list of terms.

Figure 4. Running time for calculating average CC of star networks  Investigation on this algorithm is also carried out on star network datasets. As discussed, the star networks show fine detailed gain of the proposed h-MapReduce over standard MapReduce for one heavy task. The analogy of document- term layout with star network is that the center vertex is a document and other vertices are terms. For this algorithm, we define a heavy task that sorts the terms of a document with more than 10,000 terms because similar to other application, the time to sort terms less than 10,000 is less than a Hadoop job setup and cleanup times.



V. RESULTS AND DISCUSSION  The results from the experiments evidenced the efficacy of h-MapReduce over standard Hadoop. The running time from each experiment is recorded for comparison. The size of the datasets is increased for repeated experiments.

A. Runtime analysis in calculating clustering coefficient  Run time for computing average CC of a network using standard MapReduce and the proposed h-MapReduce frame- work is recorded. The recorded running time is plotted in Figure 4.

The run time curve that represents standard MapReduce grows exponentially, which indicates the effect of lack of workload balancing. The plot in Figure 4 also reveals the effect of heavy task in Hadoop that slow down the com- putation. The speedup measures the gain of h-MapReduce, which is defined as follows.

Speedup = Runtime without h-MapReduce  Runtime with h-MapReduce (4)  The speedup by using h-MapReduce in comparison with standard MapReduce ecosystem increases as the network size increases. The plot showing the speedup for each network is pictured in Figure 5. The increase in speedup is significantly large with increasing size of the networks, which further justifies the need of h-MapReduce. The plots also demonstrate a significant performance improvement over the standard MapReduce ecosystem.

Figure 5. Speedup of h-MapReduce for calculating average CC of star networks  Figure 6. Run time for sorting terms in document-term networks  Figure 7. Speedup of h-MapReduce for sorting terms in document-term networks  B. Runtime analysis for sorting terms in a document  Figure 6 shows the plot of the running time for sorting terms in each document in a corpus represented as bipartite document-term networks. From the plot, it is observed that the run time by using h-MapReduce is longer for smaller networks, and as the network size grows the run time becomes much less in comparison. A longer run time for smaller networks suggests that h-MapReduce is not suitable for smaller datasets. The speedup of h-MapReduce over Hadoop increases as the network grows, which indicates that h-MapReduce is more appropriate for data intensive tasks. Figure 7 shows the speedup of h-MapReduce over the standard MapReduce.

C. Limitation of h-MapReduce  h-MapReduce has its limitations with its dependency on definition of heavy task. The efficiency of h-MapReduce  Figure 8. Run time for calculating average CC for benchmark networks shows an adverse effect  highly depends on the decision to split a task. Here negative effect of the h-MapReduce is presented, which encountered by an inappropriate definition of heavy task. For the purpose, we define a heavy task for computing clustering coefficient as a task that receives a vertex with more than 100 adjacent vertices. Benchmark networks are used for this experiment as it contains more vertices of various degrees. We generated benchmark networks in such a way that the network degrees range is minimal. The run time for computing average CC for each benchmark network is plotted in Figure 8.

For the benchmark networks, using h-MapReduce took more time than standard MapReduce ecosystems. The rea- sons for the adverse effect is due to the following reasons.

? A heavy task using a child job takes more time than it runs in the parent task.

? Creating a child job is not free, the child job setup and cleanup adds more time.

? There are many heavy tasks that do not have enough resources (task trackers) to run child jobs.

The adverse effect of h-MapReduce indicates the importance of a proper definition of heavy tasks. In practice we should only use h-MapReduce for really big data where a skewed workload distribution is a great concern.



VI. RELATED WORK  In this section, we review existing solutions for optimizing Hadoop / MapReduce performance. Zhang and Sterck de- veloped a system called ?CloudBATCH? to enable Hadoop to function as a traditional batch job queuing system with enhanced management functionalities for cluster resource management [6]. CloudBATCH constitutes of a set of HBase [10] tables that are used for storing resource management information, such as user credentials, queue and job infor- mation, which provide the basis for rich customized man- agement functionalities. Elnikety et al. proposed iHadoop, an add on for the MapReduce framework optimized for iterative applications that exhibit a producer/consumer relation of consecutive iterations on the same physical node where the cost of inter-iteration data transfer is minimal. [9]. Besides iHadoop, Zhang et al. proposed iMapReduce that extracts the common features of iterative MapReduce algorithms and     Table I EVOLUTION OF HADOOP PERFORMANCE TUNING  System Level of Scheduling Scheduling Technique  Hadoop Task Naive Speculative Hadoop[11], Mantri [12] and StarFish[13]  Task Resource based  skewTune[14] and skewReduce[15]  Record Overload based  h-MapReduce instructions in a task for a Record  Cost function based  provides the built-in support for these features [8]. Specula- tive execution is proposed to enhance Hadoop framework in heterogeneous infrastructure to reschedule straggling tasks on available other workers[11]. In addition there resource based schedulers attempt to uniformly distribute the load among tasks such as StarFish[13] and Mantri[12].

In all the MapReduce based frameworks, it is assumed that all the map and the reduce tasks are uniformly loaded, which may not be true as it is demonstrated for social net- works such as Twitter shown in section I. In contrast to the traditional way of assuming a task an atomic, there are tech- niques to repartition the tasks when found straggling[14].

SkewTune aggressively repartitions the input data for a task so that the task do not run longer than the other tasks. Similar to SkewTune, SkewReduce partitions the input records to address the skewed workload distribution based on user-define cost functions[15]. In contrast, h-MapReduce distributes a task for a record among available task trackers so to avoid straggling. To summarize h-MapReduce comple- ments existing performance tuning techniques. The evolution in Hadoop performance tuning techniques at various levels are compiled in Table VI.



VII. CONCLUSION  MapReduce is a pervasive programming model for big data analysis. Despite of its intelligent scheduling tech- niques, we identified skewed workload in distributing tasks among workers in MapReduce ecosystem. The problem of skewed workload is of serious concern for massive datasets. h-MapReduce, built on top of MapReduce, is proposed in this work to address the lack of workload balancing in MapReduce ecosystem. Workload balancing in h-MapReduce is achieved by splitting heavy tasks. The solution poses several challenges such as deadlocks, in- heritance conflicts etc.. h-MapReduce addresses these chal- lenges. Our experiments using various networks including social networks and document-term networks demonstrated the speedup of using h-MapReduce over standard MapRe- duce for big datasets where skewed workload is a great concern. The investigations also found the negative effect of h-MapReduce with a feeble definition of heavy tasks. Our future work will explore additional opportunities to further  improve the performance of h-MapReduce.

