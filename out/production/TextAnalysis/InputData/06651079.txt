High Performance GPU Accelerated Local Optimization in TSP

Abstract?This paper presents a high performance GPU accelerated implementation of 2-opt local search algorithm for the Traveling Salesman Problem (TSP). GPU usage sig- nificantly decreases the execution time needed for tour op- timization, however it also requires a complicated and well tuned implementation. With the problem size growing, the time spent on local optimization comparing the graph edges grows significantly. According to our results based on the instances from the TSPLIB library, the time needed to perform a simple local search operation can be decreased approximately 5 to 45 times compared to a corresponding parallel CPU code implementation using 6 cores. The code has been implemented in OpenCL and as well as in CUDA and tested on AMD and NVIDIA devices. The experimental studies show that the optimization algorithm using the GPU local search converges from up to 300 times faster compared to the sequential CPU version on average, depending on the problem size. The main contributions of this paper are the problem division scheme exploiting data locality which allows to solve arbitrarily big problem instances using GPU and the parallel implementation of the algorithm itself.

Keywords-Parallel Architectures, Optimization, Optimal Scheduling, GPU Computing.



I. INTRODUCTION  A. Traveling salesman problem  The traveling salesman problem (TSP)[1][2][3] is one of the most widely studied combinatorial optimization prob- lems. It has become a benchmark for new algorithms since it provides a set of standard problems making it possible to compare the results among the published works. This problem is classified as NP-hard[4], with a pure brute force algorithm requiring factorial execution time. In the symmet- ric TSP, the distance between two cities is the same in each opposite direction, forming an undirected graph, therefore the number of possible solutions is halved. The most direct solution for a TSP problem is to permute all the possible tours through n cities. Given a starting city, there can be n-1 choices for the second city, and n-2 choices for the third city accordingly. In the asymmetric TSP, paths may not exist in both directions or the distances might be different, forming a directed graph. A number of heuristics have been developed which return reasonably good approximations to the optimal tour in polynomial time.

One of the most well known methods of approaching the problem is repeating a series of steps called 2-opt exchanges [5]. According to our results, at least 90% of the execution time during the Iterated Local Search (ILS)[11] (which we are using and this algorithm is a part of - Algorithm 1) is spent on the 2-opt checks/exchanges and that number strongly increases with the problem size growing. This is easy to explain as the complexity of the 2-opt search is O(n2) and any simple perturbation method would be of order O(n), where n is the number of the cities. In order to solve the whole problem quickly using the iterative approach, this basic step has to be optimized. Recently, the use of graphics processing units (GPUs) as general-purpose computing devices has risen substantially, accelerating many non-graphics programs that exhibit a lot of parallelism with low synchronization requirements. In addition to that, the current trend in modern supercomputer architectures is to use GPUs as low-power consumption devices. Therefore, it is important to analyze possible application of CUDA on OpenCL as GPU programming languages to speedup opti- mization and provide a basis for future similar applications using massively parallel systems. The scope of this paper is to present the parallel 2-opt algorithm and the optimizations required to achieve high performance on GPU. An important thing that has to be mentioned is that this paper focuses only on the 2-opt local search, not the ILS algorithm, therefore the reported results are refer to a single 2-opt run.

Algorithm 1 Generic Iterated Local Search, Highlighted Parts are CUDA/OpenCL accelerated  1: procedure ITERATED LOCAL SEARCH 2: s0 ? GenerateInitialSolution() 3: s? ? 2optLocalSearch(s0) 4: while termination condition not met do 5: s? ? Perturbation(s?) 6: s?  ? ? 2optLocalSearch(s?) 7: s? ? AcceptanceCriterion(s?, s??) 8: end while 9: end procedure  2013 IEEE 27th International Symposium on Parallel & Distributed Processing Workshops and PhD Forum  DOI 10.1109/IPDPSW.2013.227     B. 2-opt  i i+1  j j+1  i i+1  j j+1  Figure 1. A 2-opt move  The 2-opt algorithm is one of the simplest TSP opti- mization methods. It removes two edges from the tour, and reconnects the two paths created, this is often referred to as a 2-opt move. There is only one possible way to reconnect the two paths so that the tour remains valid (Fig.

2). This step is applied only if the new tour would be shorter.

Repeating removing and reconnecting the tour until no 2-opt improvements can be found leads to an optimal route (local minimum). It improves tour by reconnecting and reversing order of sub-tour. For example, every pair of edges:  [i, j + 1] and [j, i+ 1]  is checked if an improvement is possible [d(x,y) is distance between x and y]:  d(i, i+ 1) + d(j, j + 1) < d(i, j + 1) + d(j, i+ 1)  distance(B,F) + distance(G,D)  > distance(B,D) + distance(G,F)  A B  D  F  E  C  G  A B  D  F  E  C  G  Figure 2. An example of a 2-opt step  The procedure is repeated until no further improvement can be done. It is good for finding a local optimum (a solution that cannot be improved by considering a neighboring con- figuration) but it is not guaranteed to find the best possible solution (the global optimum) out of all possible solutions [Rego and Glover 2002]. In order to find the global solution the algorithm needs a modification. A method allowing escaping from local optima has to be provided, which usually means that a local solution needs to be worsened is some way, keeping it within a certain search-space.



II. PROBLEM STATEMENT A. Parallelization  The main problem of the GPU implementation is the work division scheme. With thousands of threads running simulta- neously, the algorithm has to be simple, yet efficient enough to maintain scalability. GPU threads are very lightweight and best suited for short and simple task such as pixel processing in an image.

B. Memory limitation  There are two basic methods of obtaining the distance between 2 points. The first one, used very often to avoid unnecessary computation by data reuse utilizes a Look Up Table (LUT). It means that the distances are pre-computed and lated read from memory. The main disadvantage of this approach is that it requires O(n2) space (Table I). That is especially problematic when GPUs are considered. The other way is to read coordinates of the points only and calculate the distance each time it is needed. The space required for this task is only O(n) which exploits data locality and bigger portions of a problem can fit into cache. Due to high memory limitations and abundant compute power, GPUs are more suited to do the latter. Currently, a typical GPU is equipped with approximately 1-3GB of relatively slow (latency-wise) global memory, approximately 32kB to 64kB of much faster on-chip local memory or cache and peak computational power of over 1 TFLOP/s.

Table I 2-OPT SINGLE RUN - MEMORY NEEDED  Problem Number Memory Memory (TSPLIB) of cities needed needed  (points) for LUT for coords (MB) (kB)  kroE100 100 0.038 0.78 ch130 130 0.065 1.02 ch150 150 0.086 1.17 kroA200 200 0.15 1.56 ts225 225 0.19 1.75 pr299 299 0.34 2.34 pr439 439 0.74 3.43 rat783 783 2.34 6.12 vm1084 1084 4.48 8.47 pr2392 2392 21.8 18.69 pcb3038 3038 35.21 23.73 fnl4461 4461 75.9 34.85

III. RELATED WORKS  The factorial algorithm?s complexity motivated the re- search on two types of algorithms, exact algorithms and heuristics algorithms. The exact algorithms search for an optimal solution through the use of branch-and-bound, linear programming or branch-and-bound plus cut based on linear programming[12] techniques. These methods are usually hard to parallelize. Heuristics solutions are approximation algorithms that reach an approximate solution (close to the optimal) in a time fraction of the exact algorithm.

TSP heuristics algorithms might be based on genetic and evolutionary algorithms[13], simulated annealing [14], Tabu search, neural networks, ant systems, among others. Con- structive multi-start search algorithms, such as IHC - it- erative hill climbing, are often applied to combinatorial optimization problems like TSP.

These algorithms generate an initial solution and then attempt to improve it using heuristic techniques until a locally optimal solution, for example, one that cannot be further improved, is reached. O?Neil et al.[6] describe and evaluate a parallel implementation of iterative hill climbing with random restart for determining high-quality solutions to the traveling salesman problem. In our opinion and based on our results, an algorithm performing iterative refinement such as ours or GA-based, is a much better solution. This is the reason why we decided to keep the original ILS algorithm and parallelize the 2-opt search itself which is responsible for most of the running time, hopefully leading to strong-scaling. Most of the other works related to parallel TSP solvers involves evolutionary and genetic programming, such as Ant Colony Optimization (ACO)[7] or Genetic Algorithms (GA)[8]. The last work presents a very fast GA algorithm implemented on GPU which clearly outperforms the CPU counterpart. Although it is very fast, the main drawback is that there seems to exist a problem size limitation caused by the memory restrictions. In our opinion, our work is complementary ours, as we do not parallelize the algorithm itself, but the local optimization that can used by other by other algorithms.

Another approach to parallel GPU optimization is pre- sented by Luong et al.[20] using local search metaheuristics.

However it is hard to compare the results as that paper reports only speedups versus sequential CPU execution. In addition to that, the GPUs which are used are significantly different. In general, there are many ongoing attempts of approaching the problem of GPU TSP optimization, with the best results utilizing ACO and GA. Although this paper presents an algorithm which might not compete with the best state-of-the-art TSP solvers such as Concorde[19], to our best knowledge it is the fastest GPU accelerated local optimization algorithm at the moment.



IV. PROPOSED SOLUTION  Assuming that there are N cities in a route, the number of distinct pairs of edges can be approximated by:  (N ? 3) ? (N ? 2)/2,   1 2  3 4 5  6 7 8 9  10 11 12 13 14  15 16 17 18 19 20  21 22 23 24 25 26 27  28 29 30 31 32 33 34 35  37 38 39 40 41 42 43 44 45  0,1  0,2 1,2  0,3 1,3 2,3  0,4 1,4 2,4 3,4  0,5 1,5 2,5 3,5 4,5  0,6 1,6 2,6 3,6 4,6 5,6  0,7 1,7 2,7 3,7 4,7 5,7 6,7  0,8 1,8 2,8 3,8 4,8 5,8 6,8 7,8  0,9 1,9 2,9 3,9 4,9 5,9 6,9 7,9 8,9  N  N  dist(i,j) = dist (j,i), dist (i,i) = 0  Parallel:  Matrix of city pairs to be checked for a possible swap, each pair corresponds to one GPU job  for (int i = 1 ; i < n - 2; i++) for (int j = i + 1; j < n - 1 ; j++)  To check (n-2) x (n-3) / 2  Sequential:  Figure 3. Parallelization scheme  Algorithm 2 An overview of the basic proposed algorithm 1: Copy the tour and the coordinates to the GPU global  memory (CPU) 2: Execute the kernel (GPU program) 3: Copy the tour and coordinates to the shared memory  (fast local memory) 4: Calculate swap effect of one pair (per thread) 5: Find the best value and store it in the global memory 6: Read the result (CPU)     1 int distance ( int i , int j , float2 ?coords) { 2 // coordinates stored in fast shared memory in case of GPU execution 3 float dx, dy; 4 dx = coords[ i ]. x ? coords[j ]. x; 5 dy = coords[ i ]. y ? coords[j ]. y; 6 return ( int ) ( sqrtf (dx ? dx + dy ? dy) + 0.5f ) ; // return Euclidean distance 7 }  Listing 1. A simple function calculating Euclidean distance for GPU or CPU between points i and j  which means that i.e. in case of kroE100 100-city prob- lem from TSPLIB, there are 4851 swaps to be checked, 95703 in case of pr439 439-city or 2857245 in case of pr2392 2392-city problem. Thousands of threads can run on a GPU simultaneously and best results are achieved, when GPU is fully loaded to hide the memory latency.

Therefore we decided first to dedicate one thread to one 2-opt swap calculation. The procedure is relatively simple.

The GPU kernel receives the coordinates of the points/cities and calculates the change in distance by swapping 2 assigned edges using 2D euclidean metric. Using atomic operations the best candidates for swapping are stored in the global memory and are later accessible by CPU.

Each thread has to check the following condition and update the best edges found so far (Fig. 1 and Fig. 2):  d(i, i+ 1) + d(j, j + 1) < d(i, j + 1) + d(j, i+ 1)  A. Solving small instances   n  2n  mn  Thread 0   n+1  2n+1  mn+1  Thread 1   n+2  2n+2  mn+2  Thread 2  k  n+k  2n+k  mn+k  Thread k  Accessing global  memory  Get best local pair  Get best local pair  Get best local pair  Get best local pair  Get best global pair  Figure 4. GPU parallel approach, n - total number of threads, k < n, mn+ k ? total number of checked pairs ?= mn, {1, 2...k}- thread id  1) Optimization 1: Access pattern - Reusing shared mem- ory: The easiest approach would be to dedicate one thread to one 2-opt swap calculation. Since the large off-chip GPU memory has high latency and the fast on-chip memory is very limited, accessing pre-calculated data is not a good idea. Additionally, GPU has very high peak computational power compared to CPUs. Therefore the algorithm stores only the coordinates of the points in the fast on-chip 48kB of shared memory (however that limits us to 6144 cities1)  16144 cities: 49152 B (6144 x sizeof(float) x 2) for the coordinates  and calculates the distance each time when it is needed. As in the CPU approach, it calculates first the number of checks performed by each thread: iter = n?(n?1)2?blocks?threads . Then each thread checks assigned cell number and then jumps blocks*threads distance iter times. This allows avoiding excessive global memory usage as it is accessed only one time at the beginning of the execution (Fig. 4). I.e. For a 28 x 1024 configuration (CUDA blocks x threads) and pr2392 problem, ceil(2857245/(28 ? 1024)) = 100 iterations will be necessary for each thread. That means that each thread will reuse previously stored data in the shared memory 99 times without having to access the slow global memory.

x = coordinates[route[n]].x y = coordinates[route[n]].y  Get coordinates of point at position n  Total size: n * sizeof(route data type) + 2 * n * sizeof(coordinate data type)  0 5 3 2 6 12 4 15 13 11 14 9 1 8 10 7 Route  Array of coordinates  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  Figure 5. A simple way to obtain point information; pre-loaded coordinates are accessed via the route array  2) Optimization 2: Access pattern - ordering the coordi- nates: Preordering the coordinates (Fig. 5 and Fig. 6) brings 4 benefits:  1) No extra address offset calculation - faster 2) No route data required - less memory 3) Data is ordered when read by GPU - faster, can be  read sequentially, no bank conflicts 4) Data is ordered - Can be split into subproblems, a  basis for the general algorithm  This optimization works by reading the coordinates in the route?s order on host before copying the data to the GPU.

Then, they are stored in an intermediate array in that     order. This array is then copied to the GPU. It bring some performance degradation caused by the additional time spent on host (O(n)), however saves much more time by avoiding scattered read on GPU.

0 5 3 2 6 12 4 15 13 11 14 9 1 8 10 7  Route + coordinates = Array of ordered coordinates  x = ordered_coordinates[n].x y = ordered_coordinates[n].y  Get coordinates of point at position n  Total size: 2 * n * sizeof(coordinate data type)  Figure 6. An array containing coordinates in the route?s order  B. Solving any instance  Exploiting the fact that the coordinates are ordered, it was possible to overcome the problem size limitation with- out sacrificing the performance by the following division scheme. Assuming that the GPU?s local memory is limited and can store up to m coordinates, we propose the following approach. As presented in Fig. 7, a kernel reads coordinates of the cities from tour ranges m/2+1 to m and N?m/2+1 to N at one time. Therefore 2 coordinates? ranges are needed, which implies that the maximum subproblem size cannot be larger than 3072 ( 48kB2?2?sizeof(float) ). Since the problem is divided into several kernel launches, they can be executed independently in a parallel manner.



V. RESULTS AND ANALYSIS  The algorithm has been tested on a GeForce GTX 680 GPU, Radeon HD 7970, Intel Core i7-3960X CPU (with PCIe 3.0) using CUDA 5, AMD OpenCL v. 1.2 and Intel OpenCL v. 1.2 platforms. The table (Table II) presents a comparison of time needed to perform full 2-opt search using different TSPLIB instances. The GPU/CPU com- parison is show in Fig. 10. We used auto-parallelization provided by OpenCL and auto-vectorization features. CUDA and OpenCL implementations are much much faster even after including the time needed for data transfer, whose proportion to the calculation part decreases with the problem size growing. This can be explained by the very fast on- chip shared memory which can be treated as an explicitly managed cache and much higher peak memory throughput compared to the CPU. We believe that memory bandwidth is the limit in case of the parallel CPU implementation. Ad- ditionally, due to large data arrays being accessed randomly, cache efficiency is decreased drastically. We recorded the peak GPU performance of 680 GFLOP/s (GeForce using CUDA) and 830 GFLOP/s (Radeon in OpenCL) during the 2-opt optimization (Fig. 9). The last 3 columns show the  time needed from an initial solution based on the Multiple Fragment(Greedy) heuristic[18] to the local minimum found by the algorithm. Up to pla7397, the algorithm reaches very good solutions in less than a second. The main disadvantage of the presented algorithm is that it still requires a significant amount of time to reach the first local minimum in case of larger instances. The solutions to this problem are more sophisticated algorithms such as 3-opt, k-opt or LK. Possibly limiting the neighborhood would also bring an improvement in efficiency.

We have also implemented the Iterated Local Search algorithm and used the GPU version of 2-opt to test its performance. The assumption was made that the initial solution s0 is a random tour. We used a simple double- bridge move as a perturbation technique. Figure 11 presents the convergence of the Iterated Local Search with GPU 2- opt implementation. As for the other problems, the GPU algorithm gains more strength with the growth of instance size. Due to increased latency, the GPU ILS version does not give any substantial speedup over the CPU implementation in case of small problems (smaller than 200).



VI. CONCLUSION  In this paper we have presented a high-performance GPU implementation of 2-opt local search in Traveling Salesman Problem. We acknowledge that it may not be the fastest existing algorithm solving the TSP problem, however we believe that it is the fastest one implemented on GPU and provides a good base for more sophisticated approaches. The fastest sequential algorithms use complex pruning schemes and specialized data structures which we did not use. Instead, our algorithm solves the problem in a brute-force way, but due to the very high parallelism, the overall speed allows to tackle large TSP instances. The time needed to perform a simple local search operation can be decreased approximately 5 to 45 times compared to parallel CPU code implementation using 6 cores (The CPU parallel implementation based on OpenCL). The whole algorithm converges up to 20 times faster depending on the problem size (Figure 11), leading to very good solution in a very short time. We believe that the algorithm presents strong-scaling features, therefore we will try to paralellize it even further by using more CPUs and GPUs and possibly dividing the 2-opt task between multiple devices in order to effectively solve larger instances. The main contribution of this paper is the problem division scheme which allows to solve arbitrarily big problem instances using GPU as well as the techniques used to maintain data locality used to achieve very high performance. These optimization techniques reduce data movement, decrease read latency and use available compute resources instead of memory bandwidth.

coordinates range  (N-m/2+1,N)  coordinates range (m/2+1,m)  Problem of size N  Assuming local memory of limited size m coordinates  It is possible to calculate distances in arbitrary segments  0 N  0 N  coordinates range (m/2+1,m)  coordinates range (N-m/2+1,N)  Needed data: two arrays  Total size: 2 * 2 * n * sizeof(coordinate data type)  Figure 7. 2 subranges of coordinates are needed to compute local solution, assuming route ordered arrays of coordinates  1 int calculateDistance2D extended ( int i , int j , float2 ? cA, float2 ? cB) { 2 float dx, dy; 3 // 2 sets of coordinates needed, A for point i and B for point j 4 dx = cA[i ]. x ? cB[j].x; 5 dy = cA[i ]. y ? cB[j].y; 6 return ( int ) ( sqrtf (dx ? dx + dy ? dy) + 0.5f ) ; 7 }  Listing 2. A modified function with 2 coordinate sets used to calculate the distances between 2 points in large problems  Skip unnecessary computation 128x128  2048x2048  2. Outside a kernel  1. Inside a kernel  Run as few blocks as possible    128x128 = 16K threads  4,194,304  exchanges to be checked  Within the kernel The whole program:  Figure 8. Big problems involve either multiple kernel launches or iteration inside a one kernel     Table II 2-OPT - TIME NEEDED FOR A SINGLE RUN, CUDA, GPU: GTX680, CPU: INTEL I7-3960X  Problem GPU Host to Device GPU 2-opt Time to Initial Optimized (TSPLIB) kernel device to host total checks/s first Length Length  time copy copy time (Millions) minimum full 2-opt time time 2-opt from MF  berlin52 20 ?s 50 ?s 11 ?s 81 ?s 76.56 687 ?s 9951 8930 kroE100 21 ?s 50 ?s 11 ?s 82 ?s 282.92 897 ?s 24846 23025 ch130 21 ?s 50 ?s 11 ?s 82 ?s 483.81 1183 ?s 7849 7041 ch150 23 ?s 50 ?s 11 ?s 84 ?s 591.2 1342 ?s 7742 7120 kroA200 24 ?s 50 ?s 11 ?s 85 ?s 1015.78 2.547 ms 34601 31685 ts225 24 ?s 50 ?s 11 ?s 85?s 1145.97 1.090 ms 135193 128513 pr299 26 ?s 50 ?s 11 ?s 87 ?s 1878.46 2.555 ms 61493 54895 pr439 32 ?s 50 ?s 11 ?s 93 ?s 3307.85 3.067 ms 124127 115490 rat783 53 ?s 51 ?s 11 ?s 115 ?s 6385.53 8.827 ms 10734 9658 vm1084 80 ?s 51 ?s 11 ?s 142 ?s 8122.51 11.862 ms 287710 267210 pr2392 299 ?s 53 ?s 11 ?s 363 ?s 11065.33 89.577 ms 454068 412085 pcb3038 471 ?s 55 ?s 11 ?s 547 ?s 10689.4 168.02 ms 165688 147690 fl3795 723 ?s 54 ?s 11 ?s 788 ?s 10529.32 256.19 ms 34843 31312 fnl4461 746 ?s 58 ?s 11 ?s 815 ?s 14098.03 327.1 ms 215085 194746 rl5934 1009 ?s 59 ?s 11 ?s 1079 ?s 18172.88 291.09 ms 627417 582958 pla7397 1547 ?s 58 ?s 11 ?s 1616 ?s 18153.6 870.2 ms 26954302 24734292 usa13509 4728 ?s 66 ?s 11 ?s 4805 ?s 19481.58 6.5251 s 23271693 20984503 d15112 5963 ?s 69 ?s 11 ?s 6043 ?s 19272.07 8.6757 s 1810355 1652806 d18512 8928 ?s 75 ?s 11 ?s 9014 ?s 19268.93 14.975 s 745983 675638 sw24978 14.99 ms 85 ?s 11 ?s 15.08 ms 20863.46 37.284 s 1002187 908598 pla33810 26.81 ms 96 ?s 11 ?s 26.92 ms 21340.36 68.67 s 76680735 69763154 pla85900 168.4 ms 205 ?s 11 ?s 168.6 ms 21915.11 1109.2 s 163831861 149708033 sra104815 249.5 ms 249 ?s 11 ?s 249.8 ms 22017.38 2093.4 s 291403 264889 usa115475 302.3 ms 287 ?s 11 ?s 302.6 ms 22054.81 3337.8 s 7143419 6492848 ara238025 1.2934 s 740 ?s 11 ?s 1.2941 s 21902.18 26497 s 674838 610795 lra498378 5.6369 s 1774 ?s 11 ?s 5.6388 s 22031.65 3078 m 2467227 2264810 lrb744710 12.634 s 2833 ?s 11 ?s 12.638 s 21947.17 203.8 h 1867266 1692308  52 100 130 150 200 225 299 439 783 1084 2392 3038 3795 4461 5934 7397 13509 15112 18512 24978 33810                                  Problem size  F LO  P /s      Xeon E5?2690 @ 2.9 GHz (16 cores) ? Intel OpenCL Opteron 6276 @ 2.3 GHz (32 cores) ? AMD OpenCL GeForce GTX 680 ? CUDA GeForce GTX 680 ? OpenCL Radeon 5970 (1 Processor) ? OpenCL Radeon 6990 (1 Processor) ? OpenCL Radeon 7970 ? OpenCL Radeon 7970 GHz Edition ? OpenCL  Figure 9. GFLOP/s (distance calculation) observed during the run using CUDA and OpenCL     52 100 130 150 200 225 299 439 783 1084 2392 3038 3795 4461 5934 7397 13509 15112 18512 24978 33810                                Problem size  S pe  ed up  v s  X eo  n E  5?  (  In te  l O pe  nC L)        Radeon 7970 GHz Edition OpenCL GeForce GTX 680 CUDA GeForce GTX 680 OpenCL Radeon 6990 (single processor) OpenCL  Figure 10. Speedup of the algorithm compared to the OpenCL parallel CPU implementation running on Intel Xeon E5-2690 (2 x 6 cores @ 2.93GHz)  Figure 11. Iterated Local Search Convergence Speed (GPU) - sw24978.tsp

VII. FUTURE WORK  Our future work is to efficiently implement more complex local search algorithms such as 2.5-opt, 3-opt and Lin- Kernighan. However, these require much more effort in order to be parallelized. Also, simple ideas such as neighborhood pruning can be applied at the cost of the quality of the solution.

