Iceberg model based on meteorological data mining

Abstract- The mining efficient is not only about the designation of the mining algorithm, but also relating to the character of the data we want to explore. Different data, different focus. This paper is talking about a mining method on the meteorological data. Through building a iceberg model, and then dig the model, getting the frequent item set out further, it is important to get clear about the association rules of the weather's data.

Keywords- Data mining Iceberg model Frequent itemset Weather's data association rules BUe

I. INTRODUCTION  Data mining[l] is about the process how to dig the knowledge from the database, data cube or other huge information data, and then support the decision. It has been used in business decision, science, medical research, market analysis and other areas widely. Through data mining, we can get the interesting relation among the itemset, and then through digging the weather's data, we can get the relation among the meteorological elements. Meanwhile we also can find the meteorological characteristics of the region and time we want to know.



II. ICEBERG MODEL  American psychologist, Mike, made the famous model about the quality of iceberg in 1973. This model is created to analyze human behavior and performance, and the model built here is to extract the information to meet the users' requirements based on huge data, and this data is equal to the tip of the iceberg which is floating in the sea. And we know the data in the tip rely on the data base completely. Historical meteorological data is always complicated to the analyst, and they just focus on the information the huge data can provide.

Their information is just the result from the technology of data mining, but the efficient of mining is largely depends on the size of mining task, so it is necessary to reduce the size of the task before the extraction.

? Definition 1: Frequent itemset A transaction database TDB is a set of transactions,  where each transaction, denoted as a tuple<tid,X>,contains a set of items and is associated with a unique transaction identity tid , let I={il,i2, ..... .in} be the complete set of distinct items appearing in TDB. An itemset Y is a non-empty subset of I and is called an k-itemset if it contains k items, like {milk, tomato, fish} is 3-itemset,the number of transaction in TDB containing itemset Y is called the support of itemset Y,denoted as support(Y),given a minimum  Meiyuan Beijing union university  Beijing china e-mail: yuanmei@buu.edu.cn  support threshold,min_sup, an itemset(Y) is frequent if sup(Y?=min _sup.

Through apriori algorithm , it is clear what is frequent itemset and how to get it. After getting the k-itemset, get the candidate itemset and then scanning the TDB to find the k+ l-itemset those sup(k+ l-itemset?=min-sup. But this algorithm's fatal disadvantage is the candidate itemset is always large, so to get the k+1-itemset those support >=min _sup from the candidate itemset, the sup _ min(itemset)<sup _min would exclude, eg: to the meteorological data, when the annual precipitation meet the sup_min, but other meteorological elements don't , so the annual precipitation would get excluded. so it is important to dig the data on the users' concerned itemset, and then get the interesting information further, but if letting the default condition, so the operation is on the all frequent itemset(the amount of the itemset is the largest and it meet the threshold ).

Apriori algorithm is always used to generate the frequent itemset, it is used to increase the efficient of the iceberg query. but this algorithm here is not suitable at all, the number of the candidate items is huge , so this paper take BUC algorithm [2]to generate the frequent itemset, using top-down to generate the K-itemset. BUC algorithm is optimized aggregate algorithm. It first calculates one group by, like time and then two group by like time , station and then three .... this top-down calculated way is the most important optimized factor.

This way can fix the pruning in the iceberg model by dividing the data into parts and using recursive partitioning[3] .

? The designation of iceberg model The skill of this model is iceberg queries, and this  function mostly use aggregate functions, like count, sum and so on, then using the aggregate function's value compare to threshold, traditional iceberg query always use sorting and hash, and those way generally having huge 110 operations.

For example : for a table T, having attributes tl,t2,t3 ..... tn, threshold C, the designation of the iceberg query like below[4]:  Select T.tl, T.t2 ...... T.tn, count (*) From T Cube by T.tl, T.t2 ...... T.tn Having count (*?=C This way decrease the consuming on the calculate the  aggregation function's value.

For the Meteorological data's own characteristics,  instantiating of the iceberg queries have its special area's requirement, the threshold is a expression of percentage, and      then diminishes the scale of data through choosing appropriate threshold, according to the needs of the professional users, building the k-itemset by using the iceberg queries.

The most decisive factor is the how to choose the threshold, if the threshold is small, building this model comes meaningless, or the useful data would loss a lot. So how to decide the value of the threshold is always got by testing and testing , and it needs the professional users. The threshold here using the support S, S=count ({iI, i2, i3 .. .in}E D)/count(D), for a transaction database TDB here I is one attribute in TDB, it is a item, n is the number of the item in the itemset, D is the subitemset of the TDB.

The building of the iceberg model  ? Definition: Frequent area: it is the itemset in the transaction  database which it meets the requirement of the users, so the support of the itemset is larger than the min support. And the itemset is decided by the one area of the attribute eg: the itemset about the precipitation in the Meteorological data is between 20mm and 25mm occupying in the all data set is larger than the min support.

The model here is dynamic for the users' different focus, if the user cares more about the precipitation, then building the model on the precipitation.

eg: xl,x2 are the user's interesting attributes, and the threshold is S  Select yl, y2, y3 count (*) as countl From T Where T.xl =area[xl] and T.x2=area[x2] Cube yl, y2, y3 count (*) Having (countll(count2 in ( Select count (*) as count2 From T) ))>=S The result of the frequent itemset is used for data mining,  and then finds the associate rules. That is the destination of building the model. The above is showing what the result is.



III. ASSOCIATE RULES  Data association is one kind of important knowledge which can be found in the Data base, there exist some kind of regulation in the two or more variable's values. And this term is proposed by R.Agrawal and others, the typical example is "90% of buying behavior of buying bread would lead to buy the milk in one supermarket", data mining is to find the regulation and then make the right decision to gain better economic benefits for the supermarket's manager. But the regulation support the decision is just some critical association rules, not another set of rules.

? Definition of the associate rules It includes two factors, one is the association between the  items, eg: the gender is female and then position is secretary, the other is the association in the item, like the famous the sales of the beer is positive correlate to the sale of the diaper.

Associate rules mining is to find the relation in the huge dataset, the two threshold: min support and min confidence are often used to evaluate if a rule is a strong or not in association rules mining. Interestingness is another important threshold besides mInImum support and mInImum confidence for association rules. A transaction database TDB is a set of transactions: I (iI, i2, i3 .... .in) is a set owing in different items. Every transaction has its own unique ID, if X, Y is the subset of I, and X!l Y = <t, associate rules is the  form liking X ? Y succeed in TDB, meanwhile, meet the minimum support Sup and the minimum confidence Cof, X is the antecedent of the rules, and Y is the succedent of the rules[5].

Sup(X?Y) = [T E D I XUY]/[D]  Cof(X ? Y) =Sup(X U Y)/sup(X)  (1)  (2)  The rules meeting the two thresholds are called the strong association rules.

The relation between the frequent itemset and associate rules.

The associate rules get from the frequent itemset, so the associate rules' availability depends on the frequent itemset's precision, and another factor is the threshold is suitable or not.

? The extraction of the associate rules This test is designed to improving the efficiency of the  extract the regulations, to finding the key rules, and building model, which means that it has eliminate the outliers, and this experiment is to get the result like the rules about the diurnal water surface evaporation and the cloud cover on the certain height.

The method to get the associate rules is using support S and confidence C, this two thresholds are used to weigh the rules' role. If a rule's probability is I, it does not means it is important, but for the probability is not the highest, but the rule is the most important.

? The description of the algorithm about the meteorological data  Using Depth-first strategy, so whenever getting the new itemset, it narrows the scope of the data search, that is the BUC algorithm's main idea. But this algorithm's main job is joining the item from left to right, and those operations are less gradually from left to right, so early detection of the item does not meet the threshold set pruning to reduce UntIecessary connections.

For example:  If existing the count of item about Extreme maximum temperature>20 is 20 and Precipitation from 08 to 08 hours> 100mm is 30, the threshold is 25, and then it means the combination about Extreme maximum temperature>20, Precipitation from 08 to 08 hours> 1 OOmm would get pruned, so it is not necessary to scan the combination about Precipitation from 08 to 08, Extreme maximum temperature>20 in the DB.

V15-45     The process of pseudo-code algorithm: Input the itemset A, B and the threshold S Scanf the whole DB calculate the count of A, count  (Ai), the count of B, count (Bi) and count (*) Sli=count (Ai) /count (*), S2i=count (Bi)/count (*)  For(i=I; i<n; i++) {if (Sli>S) save Sli;  Else pruning Sli; if (Sli>S) save S2i;  Else pruning S2i; Then Join Sli and S2i; Then insert into the tree of BUC  } Repeat the operations above until the result of n times consisting with the result of n-l times.



IV. THE RESULT OF MINING THE METEOROLOGICAL DATA  The data is historical meteorological data, the data records' number is 2487059, the measure attributes' number is 15, including average site pressure, average air temperature, extreme minimum temperature, extreme maximum temperature, average water pressure, average relative humidity, average total cloud amount, average low cloud cover, average wind speed, average Ocm ground temperature, average Ocm ground, precipitation from 20 to 20,sunshine hours, large-scale evaporation, small-scale evaporation, precipitation from 8 to 8.

The threshold is 20,the results below is just part of the results, this is the resource of building the iceberg model, the first row is the support ,the second row is the number of the items, the third row is the frequent itemset.

Figure I. The first result of the Experiment  The minimum probability is 0.6 and the importance is 0.03 the part mining result is below: The first row is probability, the second is importance, and the third row is the association rules.

Figure 2. The second result of the Experiment  The most important rule is average low cloud cover < 11.6823767984 ->average wind speed < 17.5794318048, the importance is 0.82462129537459, and the probability is 0.883  Next is large-scale evaporation < 21.7302543424, average cloud cover< 17.178841904 ->average wind speed < 17.5794318048,the importance is 0.155741340754574, the probability is 1.000.



V. FURTHER DISCUSSION OF THE ALGORITHM  Through building this model, reducing the amount of aggregating time largely, the associate rules get to make clear of the frequent itemset's regulation, especially the meteorological data. BUC is a mature algorithm, it improves the iceberg model's availability, this experiment's fault is loss the exceptional data, and this data would play a more important role in the future. And deviation is a way to deal with this problem, how this concept could be used in the meteorological data is the question we should explore.



VI. CONCLUSION  Through building the iceberg model, it is available to reduce the size of objects excavated; it is based on the BUC algorithm. And this BUC tree is built by deleting the itemset ahead, thereby reducing the number of connections largely, and this algorithm support the iceberg query, it is used to prune, the data which is processing in this way are easy to get the association rules. And this is the final destination of mining the data.

