CloudRS: An Error Correction Algorithm of High-Throughput Sequencing Data  based on Scalable Framework

Abstract?Next-generation sequencing (NGS) technologies produce huge amounts of data. These sequencing data unavoidably are accompanied by the occurrence of sequencing errors which constitutes one of the major problems of further analyses. Error correction is indeed one of the critical steps to the success of NGS applications such as de novo genome assembly and DNA resequencing as illustrated in literature. However, it requires computing time and memory space heavily. To design an algorithm to improve data quality by efficiently utilizing on-demand computing resources in the cloud is a challenge for biologists and computer scientists. In this study, we present an error-correction algorithm, called the CloudRS algorithm, for correcting errors in NGS data. The CloudRS algorithm aims at emulating the notion of error correction algorithm of ALLPATHS-LG on the Hadoop/ MapReduce framework. It is conservative in correcting sequencing errors to avoid introducing false decisions, e.g., when dealing with reads from repetitive regions. We also illustrate several probabilistic measures we introduce into CloudRS to make the algorithm more efficient without sacrificing its effectiveness. Running time of using up to 80 instances each with 8 computing units shows satisfactory speedup.

Experiments of comparing with other error correction programs show that CloudRS algorithm performs lower false positive rate for most evaluation benchmarks and higher sensitivity on genome S. cerevisiae. We demonstrate that CloudRS algorithm provides significant improvements in the quality of the resulting contigs on benchmarks of NGS de novo assembly.

Availability: The source code of CloudRS is freely available at https://github.com/ice91/ReadStackCorrector.

Keywords-error correction; mapreduce; genome assembly; next-generation sequencing

I.  INTRODUCTION With the explosive growth of next-generation sequencing  (NGS) data at continually decreasing costs [1], there is a pressing need for applications to handle massive genomic sequencing data efficiently using scalable, on-demand, and inexpensive commodity cloud servers. Error correction of sequencing reads is an effective preprocessing step in many applications of NGS data analysis such as sequence assembly and DNA resequencing. There are three typical ways to correct errors for those applications, i.e., read alignment, k-mer spectrum-based [2] and graph-based assembly. Note that a mer refers to a base pair of a genome sequence. A k-mer spectrum of a genome refers to empirical frequencies of DNA k-mers in the specific genome sequence.

Read alignment depends on identifying all the overlapping reads by multiple alignments. A mismatch at a specific position of a read can be corrected to the nucleotide agreed upon by majority of the aligned reads at the same position. The idea has also been implemented in other error correction algorithms [3,4]. Another widely used approach is k-mer frequency spectrum-based error correction [5-10]. A k- mer denotes a sequence of length k in biology A k-mer frequency spectrum is derived from the frequencies of k- mers in the reads with each k-mer being categorized as trusted k-mers and untrusted k-mers, i.e., high-frequency k- mers and low-frequency k-mers, respectively. Thus, algorithms are developed to convert untrusted k-mers into trusted k-mers with minimal changes of their reads. Graph- based assembly approaches is the third type of error correction. It is based on the observation that sequencing errors may create specific types of local defects that can be detected in the assembly graph of reads, e.g. tips, bubbles, and braids [11,12]. By detecting and removing these local defects, the number of sequencing errors can be reduced and, thus, allowing an assembler to yield longer contigs.

Nevertheless, correcting sequencing errors in the huge amount of reads generated by NGS technologies is time consuming and memory demanding. Furthermore, a huge amount of intermediate data is created in the computation process. For example, a naive implementation of the read- stack algorithm of ALLPATHS-LG [13] would replicate a read for each k-mer subsequence of the read. For a 100G NGS file of read length 36 and k-mer size 25, this means that the total size of intermediate data is 1.2 tera-bytes, i.e., each   # These authors contributed equally to this work (co-First authors).

3 Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan, ROC.

4 Department of Computer Science and Information Engineering, National Chung Hsing University, Taichung, Taiwan, ROC.

? Corresponding author      central column left column  right column  R1 R2 R3 R4 R5  R1 R2 R3 R4 R5  R1 R2 R3 R4 R5  R1 R2 R3 R4 R5  branch point  branch point  k+1 mer k mer   Figure 1. Illustration of read stack construction.

read is replicated 12 times. Thus, new strategies to store and process large quantities of data efficiently are required. The MapReduce [14] framework is a scalable distributed computing framework for biologists and bioinformatician to process huge amount of genomic sequencing data. Though MapReduce and its famous implementation Hadoop [15] are available for researchers and are highly fault tolerance when processing large datasets, the design of MapReduce algorithms is not trivial. Thus, it is also the aim of this study to coordinate parallel processes in a specific way to balance the cost among disk I/O, communication and computation.

In this paper, we present our design and development of CloudRS, an error correction algorithm based on MapReduce framework. We also evaluate the performance CloudRS and give some discussions.



II. METHODS As shown in benchmark GAGE [16], the error correction  model of ALLPATHS-LG [13], denoted as Read Stack (RS) algorithm in this paper, brings significant improvement for de novo NGS genome assembly. Using RS algorithm as a starting point, our aim in this study is thus to develop an efficient algorithm based on MapReduce framework to fully exploit the advantage of on-demand computing, i.e., to acquire computing power, physical memory space and disk storage flexibly based on the scale of input NGS data size. We?ll begin this section with an overview of the RS algorithm and also present the algorithm CloudRS designed to run on the MapReduce framework.

A. Overview of ReadStack (RS) algorithm Finding read stacks and correcting possible errors by  majority voting is the basic operation in RS algorithm [13]. As shown in Figure 1, a read stack is an alignment of reads that share a common k-mer pattern in their sequences. A k-mer pattern is either a substring of length k, denoted as a substring pattern, or a subsequence composed of k fixed characters and some wildcard characters, denoted as a wildcard pattern. The substring pattern is useful for correcting errors in the two sides of read stacks and the wildcard pattern is useful for correcting errors in the middle part of read stacks. For example, in Figure 1a, the five DNA sequencing reads on top  share the same subsequence of length k+1 with a wildcard base in its middle; then they are aggregated and aligned into the read stack below, where bases located in the central wildcard column will be voted and corrected. In Figure 1b, the five reads on top share the same substring of length k; then they are aggregated and aligned into the read stack below. Each of the left and right columns will be voted and corrected except the columns outside the branching points.

There are two phases in correcting errors [13]: 1) recommendation, and 2) correction. In the recommendation phase, a stack of reads is built by aligning reads containing the same k-mer pattern. Once a read stack is built, each column of the read stack is processed to generate votes, including replacing and preserving votes, for each DNA base of each read of the read stack. After summing up the quality values of A, C, G, T, respectively on a column of a read stack, bases with scores less than a threshold are labeled as error candidates. They will receive replacing votes of modifying them to the highest-score base. Preserving vote is issued if all the bases of a column are  equal. Then in correction phase, it collects the votes and makes decisions about which base should be corrected. Note that corrections are made if the replacing votes for the same base agree with each other and no preserving vote is found. After correcting errors, RS algorithm has a screening phase to filter out the reads containing unique k-mers because they are likely to be sequencing errors [13].

B. CloudRS emulating RS algorithm in MapReduce Based on RS algorithm, CloudRS algorithm consists of three  modules: PreCorrect, FindError, and Screening. First, the PreCorrect module uses a 25-mer wildcard pattern with one wildcard in the middle to build read stacks and correct errors in the middle column (Figure 1a). Secondly, the FindError module uses a 24-mer substring pattern to build a read stack and correct errors in the columns on both sides of the 24-mer pattern of the read stack (Figure 1b). Finally, the Screening module filters out reads that contain unique 24-mer substrings. The aforementioned parameters are the same as those used by the RS algorithm [13].

In the MapReduce implementation, we utilize multiple map- reduce rounds, i.e., one map-reduce round for the recommendation phase and one for the correction phase (for both PreCorrect and FindError modules), and two map-reduce rounds for the screening module.

In the recommendation phase, each read is scanned with a sliding k-mer window by the mapper. To fit the <key, value> pair data format of MapReduce, the sliding k-mer window is defined as key and the content of a read, including the read ID and its sequence, is defined as value. After the shuffle-and-sort of MapReduce, the reads with the same k-mer are aggregated to the same reducer. We can align those reads and generate the replacing/preserving votes according to the process of recommendation phase described in Section II.A. Figure 2 shows the process of recommendation phase in the MapReduce framework. In the correction phase, the input of a mapper is read data and the corrected messages. The read ID is defined as key, whereas read sequence and corrected messages are defined as value. Thus, we can aggregate and correct sequencing errors by manipulating the replacing/preserving votes in the reducer.

The nucleotide is corrected only when the replacing action is agreed by every vote and there is no preserving vote at the specific positions. Figure 3 shows the process of correction     mappermapper mapper mapper  K2K1 R1 R2  Shuffle and Sort: aggregate values by keys  reducer reducer reducer  K1 R1 R3 K2 R2R4 K3  R1 M1  K2K1 R3 R4 K2K3 R5 R6 K2K3 R7 R8  R6 R8 R5R7R1  K3 R1  R3 M3  R2 M2  R4 M4  R6 M6  R8 M8  R1 M1  R5 M5  R7 M7  R1  R2  R3  R4  R5  R6  R7  R8  corrected message duplication   Figure 2.  The process of recommendation phase in MapReduce  framework.

R1  R2  R3  R4  R5  R6  R7  R8  R1 M1  R3 M3  R2 M2  R4 M4  R6 M6  R8 M8  R1 M1  R5 M5  R7 M7  mappermapper mapper mapper  Shuffle and Sort: aggregate values by keys  reducer reducer reducer  R1  R1 M1 ?????  bottleneck  R1?  R2?  R3?  R4?  R5?  R6?  R7?  R8?  corrected message duplication  Figure 3.  The process of correction phase in MapReduce framework.

Figure 4.  The key design of the group mechanism.

phase upon MapReduce, where the read Ri? stands for the corrected read of read Ri. The Screening module is implemented as two map-reduce rounds to generate the unique k-mer tag by counting k-mer frequencies in the first round and to remove them in the second round.

The implementation of recommendation phase and correction phase presented earlier, however, may exhibit the following side-effects. First, we need to duplicate a read for each of its k-mer subsequence and thus may cause message flooding in the recommendation phase. Second, the data and workload of the reducer may unbalance. For the first problem, it is due to that one read have different sliding k-mer. Each k-mer generates a read stack and the same read may appear in different read stacks. Therefore, one sequencing error may generate the same corrected messages distributed on different reducers.

Message flooding is the bottleneck of the correction phase, since it increases the overhead of the shuffle and sort of MapReduce framework as shown in Figure 3. The second problem is due to the repeat sequence in the genome. The k-mers produced by the reads in a repeat region lead to an uneven distribution of read stacks of each reducer.

We introduce grouping mechanism to decrease the effect of message duplication make a compromise between parallelization and communication cost [17]. Thus, we divide the k-mer into primary key and sub key, instead of using the entire k-mer as key. Figure 4 shows the design of group mechanism in relation to key partitions. The primary key is used to replace original key in the MapReduce framework. The sub key is used in the reducer to differentiate the group of read stack. A reducer in the new scheme received a group of read stacks; therefore, the duplicated corrected messages can be combined into one message in the same reducer. This mechanism can decrease the communication cost of duplicated corrected messages.

We also introduce a high frequency k-mer filter to overcome the unbalance workload problem. The read stack build by a k- mer in the repeat region are composed of reads from the different regions in the genome. It violates the basic idea of error correction, i.e., using the majority of reads to correct error; therefore, we do not need to analyze read stacks build by the k- mer in repeat regions. The straightforward way to determine the k-mer in the repeat region is using its frequency. Thus we build a high frequency k-mer list as a stop word list in natural language. First, we define a threshold of the k-mer occurrence.

In general, the threshold is set as two times of the coverage depth of read data, because the occurrence of k-mer two times than average is always belong to repeat region. We use one map-reduce round to build a high frequency k-mer list, store the list in Hadoop Distributed File System (HDF , and dispatch the list through the functionality of Hadoop distributed cache. Thus, the mappers of recommendation phase can skip computing on the high frequency k-mers according to the list. High frequency k-mer filtering decrease the communication cost between mapper and reducer, and avoid the unbalance workload in the reducer.



III. EXPERIMENTAL RESULTS  A. Datasets The seven experimental datasets, downloaded from the  sequence read archive (SRA) at NCBI, are listed in Table I. All the datasets are sequenced by Illumina sequencers. We used  datasets D1-D6 to evaluate the accuracy of error correction, these datasets are from resequencing experiments where the     TABLE I.     DATASETS  Dataset SRA  accession number  Genome (accession number)  Genome size  Read length  #Reads (Mega)  Genome coverage  D1 SRX000429 E. coli  (NC_000913) 4.6 Mb 36 bp 20.8 161x  D2 SRR022918_1 E. coli (NC_000913) 4.6 Mb 47 bp 7.0  71x  D3 SRR034509_1 E. coli (NC_000913) 4.6 Mb 101 bp 8.9 195x  D4 SRR006332 Acinetobacter  sp (NC_005966)  3.6 Mb 36 bp 17.7 177x  D5 SRX100885 S. cerevisiae (PRJNA128) 12 Mb 76 bp 52.0  329x  D6 SRR022866 S. aureus (NC_003923) 2 Mb 76 bp 25.1 953x  D7 SRA000271 African Male (NA18507) 3 Gb 36 bp 218.0 2.6x  TABLE II.    RESULTS OF THE EVALUATION ON DATASET D1-D6  Dataset Method TP FN FP TN Sensitivity (%) Specificity (%) Gain Precision (%)  D1 (E. coli)  SHREC 2819754 1183861 667435 740842474 70.430% 99.910% 0.5376 80.86% Reptile 3164394 839221 133558 741376351 79.038% 99.982% 0.7570 95.95%  CloudRS 784851 2940697 10860 741777116 21.067% 99.999% 0.2078 98.64% D2  (E. coli.) Reptile 3551764 3189748 985674 323583005 52.685% 99.696% 0.3806 78.28%  CloudRS 1659568 5005959 189119 308857044 24.898% 99.939% 0.2206 89.77% D3  (E. coli.) Reptile 17158925 2947342 1298891 874945703 85.341% 99.852% 0.7888 92.96%  CloudRS 978111 18878806 114042 876379902 4.926% 99.987% 0.0435 89.56% D4  (A. sp.) Reptile 7138883 2361813 1138666 610144638 75.141% 99.814% 0.6316 86.24%  CloudRS 2148235 5878602 19718 612737445 26.763% 99.997% 0.2652 99.09%  D5 (S. cer.)  HSHREC N/A N/A N/A N/A 36.350% 97.730% -2.4975 N/A Reptile N/A N/A N/A N/A 22.780% 99.990% 0.2243 N/A  SOAPec N/A N/A N/A N/A 18.710% 99.950% 0.1257 N/A Coral N/A N/A N/A N/A 7.090% 99.990% 0.0678 N/A  CloudRS 17383865 4737336 143295 3823800700 78.585% 99.996% 0.7794 99.18%  D6 (S. aur.)  HSHREC N/A N/A N/A N/A 22.480% 96.880% -4.2402 N/A Reptile N/A N/A N/A N/A 61.940% 99.990% 0.6131 N/A  SOAPec N/A N/A N/A N/A 32.160% 99.890% 0.2571 N/A Coral N/A N/A N/A N/A 2.760% 99.990% 0.0256 N/A  CloudRS 5484475 10653975 77957 1780836273 33.984% 99.996% 0.3350 98.60%  genome sequence is known a priori and they are also used to evaluate other error correction programs in [7,2]. We use dataset D7 to evaluate the efficiency of CloudRS algorithm, since the data size of D7 is almost 10 times than other datasets. Using dataset D7 as benchmark, we analyze the difference between the standalone RS algorithm and MapReduce CloudRS algorithm.

B. Accuracy evaluation To evaluate the accuracy of CloudRS algorithm, we used the  benchmark provide by [2]. The benchmark identifies sequencing errors by using a mapping program to align reads to the reference genome. Only uniquely mapped reads are considered and mismatched bases in the reads are defined as sequencing errors. Then, we can assess the performance of an error correction method by identifying how many of erroneous bases that are changed to the true bases [true positives (TP)], how many true bases changed wrongly [false positives (FP)], how many erroneous bases left unchanged [false negative (FN)] and how many true bases left unchanged [true negative (TN)] [7].

From these numbers, statistics like precision, specificity, sensitivity or gain can be inferred, which will be defined in the following formulas:  ? Precision = TP / (TP + FP) ? Specificity = TN / (TN + FP) ? Sensitivity = TP / (TP + FN) ? Gain = (TP-FP) / (TP + FN)  Such statistics are not entirely satisfactory to assess the quality of corrections of a method, because it is unclear which statistic will indeed yield the best performance for other applications depending on the data. However, these statistics can give an overview of the characteristics for each error correction program. Table II show the results of the evaluation on dataset D1-D6. Note that, we do not re-run error correction programs such as SHREC [9], HSHREC [18], SOAPec [19] and Coral [3] on datasets D1-D6. The evaluated results of these error correction programs are extracted from [7,2], thus some fields (TP, FN, FP, TN, Precision) do not have information.

From Table II, Reptile [7] seems perform better than other error correction programs in the viewpoint of gain measurement.

Reptile has highest sensitivity and gain for most datasets except dataset D5; on the other hand, CloudRS algorithm has lower gain and sensitivity compared to other error correction programs. However, CloudRS algorithm has the highest specificity and precision for most of the datasets, and the the number of false positives of CloudRS algorithm is much less than other error correction programs.

As discussed earlier, it is not entirely clear which statistics should be maximized in order to obtain good error correction.

Since error correction of short read data is a preprocessing step for sequence assembly, we assess performance on the impact on sequence assembly for short read data. We choose Velvet [11], a most popular short read assembler, to run on the corrected read sets.

We used the benchmarks of GAGE [16] to evaluate Velvet under 3 different assembly pipelines: (1) Velvet without error correction as preprocessing, (2) Velvet using Reptile as preprocessing and (3) Velvet using CloudRS algorithm as preprocessing. Table III show the results of these 3 assembly     TABLE III. COMPARING ASSEMBLY PIPELINES ON DATASET D1-D6  Dataset Assembly pipeline Number  of contigs N50(bp) N50  corr. (bp) Indel > 5bp Misjoins  D1 None+Velvet 515 15640 15413 2 17  Reptile+Velvet 536 15554 13382 11 53 CloudRS+Velvet 489 15926 15628 3 5  D2 None+Velvet 546 15661 15562 1 14  Reptile+Velvet 546 15661 15562 1 14 CloudRS+Velvet 458 17794 17761 0 19  D3 None+Velvet 275 35173 33558 3 7  Reptile+Velvet 275 35173 33558 3 7 CloudRS+Velvet 167 76845 60651 2 8  D4 None+Velvet 359 18646 17096 0 15  Reptile+Velvet 359 18646 17096 0 15 CloudRS+Velvet 220 31072 30625 0 17  D5 None+Velvet 5300 3401 3016 503 98  Reptile+Velvet 5300 3401 3016 503 98 CloudRS+Velvet 5230 3472 3054 518 72  D6 None+Velvet 267 32066 16037 113 8  Reptile+Velvet 267 32066 16037 113 8 CloudRS+Velvet 201 35939 18310 108 9     Figure 5.  Runtime profile of CloudRS algorithm on dataset D6.

pipelines on dataset D1-D6. From Table III, we observe that using CloudRS algorithm achieves a positive effect on the assembly results for all the datasets. For dataset D3 and D4, the corrected N50 value is about 90% higher than the values by the other two pipelines. Note that corrected N50 size of contigs is a key dimension to judge assembly results and it  is computed based on corrected contigs that have no misjoints or indels > 5bp [16]. On the other hand, the reads corrected by Reptile cannot be combined into longer contigs for all datasets and the corrected N50 drops by about 13% for dataset D1. Note that contigs assembled by Velvet using Reptile as preprocessing has the same corrected N50 with contigs assembled by Velvet without error correction on datasets D2-D6. This phenomenon may be due to the error correction mechanism built in Velvet have very similar functionality as the mechanism of Reptile.

Combining Table II, the experiments show that a critical factor to induce more successful assemblies is a low false positive rate, where false positive rate can be acquired by 1 minus precision. This may be caused by the fragile nature of low coverage areas in the sequencing data towards error correction. An overly ambitious tool can easily identify these areas as erroneous and ?correct? them towards something similar and more prevalent in the data, making them unavailable for the assembly tool [3]. In addition, similar results are obtained after we replace Velvet by CloudBrush assembler [12] to do the experiment in Table III.

C. Efficiency evaluation To evaluate the efficiency of our approach, we performed  CloudRS algorithm on 5 different sizes of Hadoop clusters using machines leased from the Amazon Elastic MapReduce.

The 5 clusters consisted of 5, 10, 20, 40 and 80 instances, respectively. The spec of instance is M1 Extra Large which had 8 compute unit and 15 GB of RAM. We used the Dataset D7 as the benchmark to analyze the runtime of CloudRS algorithm.

From Figure 5, we observed that the PreCorrect and FindError are the primary computation bottleneck of CloudRS algorithm.

However, with an increase in the number of nodes, the computation time of PreCorrect and FindError decreases substantially, while the runtime of Screening decreases only slightly. In contrast, we use the same data set to evaluate error correction model of ALLPATHS-LG [13] (standalone version of RS algorithm). ALLPATHS-LG failed to work on the single machine which has 16 GB RAM and two 4-core CPUs due to memory thrashing. By increasing memory size to 64GB, it takes 7457 seconds. The experiment shows that ALLPATHS-LG requires large physical memory to avoid running into memory thrashing. On the other hand, using CloudRS algorithm, we may dynamically allocate more computing units to deal with the additional memory requirements.



IV. DISCUSSIONS In this section, we will illustrate a major performance  bottlenecks in CloudRS algorithm. It originates mainly from the amount of intermediate data associated with each k-mer on the reads processed by the mappers of recommendation phase. These intermediate data then become communication cost in the shuffle and sort stage. The intermediate data are usually much large than original input data especially for long reads. For example, using a 10 node cluster to deal with the read data with 150 bp length, each node in the reducer  will receive intermediate data that is over 10 times bigger than original input. The reason is that the sliding k-mer window is fixed size (24-mer), it will generate the intermediate data whose size is about (150-24+1) * (original input size) by sliding the 24 mer window on the 150bp reads, and the intermediate data spread out on the 10 nodes; therefore each node receive data about 10 times bigger than original input.

To deal with such circumstance, one alternative approach is to store original input to the distributed cache. With the original input as reference, no additional information is needed in the posting except the read id; this strategy significantly decreases the size of intermediate data.

However, this strategy will cause the issue of random disk seek: as for each read content, we must look up the original input with read id to obtain the sequence and quality value.

Looking up original input implies random disk seeks, since for the most original input is too large to fit into memory. It     like a tradeoff between communication cost and cost of random disk seeking, and the interfering factor includes size of original input, length of reads and size of cluster.



V. CONCLUSION Error correction is an important step to reduce  sequencing errors and thus to provide more accurate sequencing reads for downstream bioinformatics analyses.

Our experiments show that false positive of an error correction method introduced as a result of acting too aggressively in judging whether a sequencing error occurs at a specific position of a specific read might create the negative impact the downstream analyses, e.g., de novo genome assembly. It is shown that using the error-corrected sequencing data of CloudRS algorithm as input to the de novo genome assemblers Velvet and CloudBrush indeed yields contigs with good performance indices. As shown in Tables II-IV in Results, superior performance in both specificity and precision of CloudRS algorithm is the key to better genome assemblies, despite the strategy sacrificed the sensitivity for partial datasets. Another challenge for error correction methods is the ability to handle the explosive growth of next-generation sequencing data. The proposed CloudRS algorithm for error correction meets the aforementioned demands.

In addition, CloudRS algorithm is designed based on MapReduce framework to utilize the widely available cloud computing and Hadoop/Mapreduce services. This strategy allows users of CloudRS to flexibly acquire resources of computing power, physical memory and disk space based on the size of their input data. This is especially important for big sequencing data today, e.g., Illumina HiSeq2500 sequencer is able to generate 600Gbp per run. That is, 1.2 tera bytes of DNA sequences and quality values will be the input of error correction algorithms. As shown in Figure 5 in Results, CloudRS algorithm provides a scalable way to speed up error correction for dataset D6 in MapReduce framework.

In summary, the feature of minimizing false positives of error correction as shown in CloudRS algorithm is important to avoid error propagation to downstream bioinformatics analysis, such as de novo geonome assembly. The MapReduce design of CloudRS algorithm is able to gain speed-ups for big NGS data via low-cost on-demand cloud computing services. Despite of these achievements, there still several performance bottlenecks in CloudRS algorithm which we believe might be further improved in the future.

We are also interested in finding more applications of the map-reduce techniques that we developed in designing the CloudRS algorithm, such as sequence assembly.

