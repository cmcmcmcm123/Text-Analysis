Quantitative Association Rules Mining Method Based  on Trapezium Cloud Model

Abstract?The quantitative association rules mining method is difficult for their values are too large. The usual means is dividing quantitative Data to discrete conception. The trapezium Cloud model combines ambiguity and randomness organically to fit the real world objectively, divide quantitative Data with trapezium Cloud model to create concepts, the concept cluster within one class, and separated with each other. So the quantitative Data can be transforms to Boolean data well, the Boolean data can be mined by the mature Boolean association rules mining method.

Keywords- Quantitative Association Rules; Trapezium-Cloud Model; Conception Division; Frequent Item Sets

I. INTRODUCTION  Association rule mining refers mainly to get the knowledge such as "customers bought tea. also purchased the coffee," which meet the minimum support and the minimum confidence. At present association rules can be divided into two types: Boolean association rules and quantitative association rules, and most of the research are focused on the Boolean association rules research.

First, give the formal description of association rules mining: Suppose I = (i1, i2... im) is a collection of m different items, T = (t1, t2... tn) is a transaction database, tj is a group of items of I set, tj?I. Each transaction with a unique identifier TID linked. If X is a subset of I with X?tj, we say that a transaction contains X. An association rule is a "X?Y" implication, in which X?I, Y?I, and X?Y=?.

Definition 1: If the ratio of transaction T contains X?Y is Sup, the association rules X?Y in T has a support degree Sup.

%100  n y)support(xSup ?= ?  (1)  Support (X?Y) stands for the number which support X? Y transactions, n stands for the total number of transactions.

Definition 2: If the ratio transaction T contains X also contains Y is Conf, then the confidence level of association rules X?Y in T is Conf.

%100 support(x)  y)support(xConf ?= ?  (2)  Association rules are called the strong association rules if they meet the minimum support and minimum confidence [1] [2].

Definition 3: Boolean association rules are got from the transaction database whose items values are 0 or 1.

Mining process is as follows:  (1) Identify the transaction database T in all the non-empty sets which meet user-defined minimum support degree, if the item sets meet the minimum support degree which are called frequent item sets.

(2) Using frequent item sets generate the required association rules. For each frequent item sets A, identify all  non-empty subset A?a, if confmin  support(a) support(A) ?  , then generated association rules a ?(A-a) [1][2].

The above methods only consider whether the transaction contains the item, regardless of the number of item, for example, they only consider whether a particular transaction contains tea, regardless of a pound of tea or five pounds of tea; a lot of useful information are lost[3][4].

In applications, quantitative attribute Data quantity gathered in a certain range may contain very useful knowledge.

Upper and lower limits in the expression is too rigid, for example: for <age, 18, 38>, 16-17 years old, which certainly are not divided into this interval, but if we use "young" concept to describe the age of people, 16-17 years old, which can be classified into the concept or not depending on the Individuals, or they belong to the concept partly. The method mentioned in this paper can identify association rules from the quantitative Data. Here, we first look at the mathematical theory.



II. CLOUD MODEL  Cloud model is a qualitative and quantitative conversion model; it combines ambiguity and randomness organically.

This work was supported by the Scientific and technological development projects of Shandong Province (Grant Nos.

2008GG30001030), Natural Science Fund Project of Weifang University (Grant Nos. 2008Z08)      Set U is a mathematical domain U={x}, T is the language value associated with the U. ?T (x) is a stable tendency random number which expressed the elements x subordination of T concept, subordination?s distribution in the domain is known as the cloud [5][6].

Cloud mathematical expected curve is its subordination curves from the view of fuzzy set theory. However, "thickness" of the curve is uneven, waist is the most scattered, the top and bottom of the curve are convergent, cloud's "thick" reflects the subordination degree randomness, near to or away from the concept center have smaller subordination randomness, while concept center have the largest subordination randomness, which is consistent with people's subjective feelings.

The digital features of the normal cloud characterized by three values with the expectation Ex, entropy En, excess entropy He), the expected value Ex: the center value of the concept domain, is the most representative qualitative value of the concept, it should be 100% belongs to the concept; entropy En: is a qualitative measure of the concept?s ambiguity, reflecting the accepted range values of the concept domain; hyper entropy He: can be described as entropy En of entropy, reflecting the degree of dispersion of the cloud droplets. The normal cloud is the most important cloud model, because various branches of the social and natural sciences have proved the normal distribution?s universality. The equation of normal cloud curve:  e 2En 2 Ex)(? 2  )(MEC A ??  =?  (3)  Expectation curve is a normal curve, for a qualitative knowledge, the elements outside the Ex ? 3En in its corresponding cloud model all can be ignored, because it has been proved that approximately 99.74% elements of model fall into the range of Ex ? 3En by the mathematical characteristics of normal distribution. Extending the normal cloud model to get trapezoidal cloud model, trapezoidal cloud model can be expressed by six values, they are the expected number: Ex1 and Ex2, entropy: En1 and En2, hyper entropy: He1 and He2.

Figure 1.  The digital features of Trapezoidal cloud model  Trapezoidal cloud curve equations are determined by the expectation and the Entropy are following:    ( ) ( )  ( )  A  A  A  2(x Ex1) 2 1 3 1 12En1  Ex1<=x<=Ex2  2(x Ex2) 2 2 2 3 22En2  MEC (x)  MEC (x) = 1  MEC (x)  e  e  Ex En x Ex  Ex x Ex En  ?? ? <= <=  ?? <= <= +  =  = (4)  Clearly, the left and the right half-cloud expectation curve is a normal curve.

It can complete the qualitative and quantitative transform more accurately, if there is a range belongs to the concept totally, then it can be expressed by the upper edge of Trapezoid, if only one value belongs to the concept totally, then the upper edge of Trapezoid degenerate to a point, trapezoidal cloud model also degenerated into the normal cloud model. He1 and He2 can have different values, and thus the concept of the border on behalf of different fuzzy situation, when the He1 and He2 all degenerate to 0, trapezoidal cloud model expressed a concept with accurate border subordination, when one of the He1 or He2 degenerate to 0, which expressed a concept with one r accurate border subordination and one vague border subordination, so trapezoidal cloud model has a better generality.



III. DIVIDING QUANTITATIVE DATA TO CONCEPTS USING TRAPEZOIDAL CLOUD MODEL  A. Cloud Transform  Any function can be decomposed into cloud-based superposition with allowed error range, which is Cloud Transform. The equation is:  )()(  xfxg j m  j  jc? =  ?   ))()(0(  ?<?< ? =  xfxg jj m  j c  (5)  g(x): data distribution function  fj(x): cloud-based expectations function  cj :coefficients  m: the number of superimposed cloud,  ?: user-defined maximum error  From the concept of clouds: in the domain the element?s subordination to the concept has statistical and random properties. In addition, the high-frequency elements? contributions to the concept are higher than the low-frequency elements. That is the reason to use probability density function of data distribution to get the concept set, so the concept division algorithms can be done.

According to the definition of cloud transform, the quantitative attribute?s domain dividing into m concepts can evolve to a problem to get answers from the formula:    ? += =  m  j ijjjjjjij HeHeEnEnExExfxg c  )2,1,2,1,2,1()( ?

I.e. to get Ex1j, Ex2j, En1j, En2j, He1j, He2j and cj for each cloud concept, the quantitative attribute domain is divided into a number of concepts by using cloud model, the data in each concept aggregate, and the data between different concepts separated [7][8].

B. Concept division  algorithm  Cloud transform recover the data distribution concepts from a large number of property values, the conversion is from quantitative Data to qualitative concept, is a clustering problem essentially. Local peak of the histogram is that the data aggregation part, taking it as a concept center is reasonable, the higher the peak, indicating more data convergence there, deal with it with priority. The concept division algorithm is:  Algorithm 1: Concept division algorithm  Input: the domain of quantitative attributes that need the concept division, the overall error threshold ?, and the peak height error threshold ?y, the length error ?x between trapezoidal top edge and the minimum value.

Output: m concepts and the corresponding digital features of attribute i.

{  (1) Count the each possible values x of attribute i and get  the actual data distribution function g (x);  (2)  j=0;  (3) Clouds=??g?(x)=g(x);  (4)  while max(g?(x))>?  (5)     Exj=Find_Ex(g?(x));  (6)     Ex1j=search1(g?(x),?y,?x);  (7)     Ex2j=search2(g?(x),?y,?x);  (8)     En1j=Find_En(cj,Ex1j,?);  (9)     En2j=Find_En(cj,Ex2j,?);  (10)   gj(x)=cj*Cloud(Ex1,En1j,Ex2j,En2j);  (11)   g?(x)=g?(x)-gj(x);  (12)   j=j+1;  (13) endwhile  (14) for j=0 to m-1 do  (15) Clouds(Ex1j,=Ex2j,En1j,En2j,He1j,He2j)=  Calculate_He(gj(x), g?(x),Cloud(Ex1j,Ex2j,En1j,En2j));  (16) endfor  }  In Step 1, using statistical methods to get the actual data distribution function g(x); Step 2, 3 does variable initialization; Step 4 the division of the process is ended, if the error limit less than a given error, Step 5 Search for the peak value of cj of property in the data distribution function g(x), and its corresponding value x is defined as the cloud model center (expectation); Step 6, 7 search approximate horizon line near the peak (within the error limit threshold ?y), if the width is greater than the minimum width of the threshold value ?x, where were identified as uniformly distributed, the two endpoints of the line are recorded as the trapezoidal top edge endpoints Ex1j, Ex2j; otherwise get the trapezoidal top edge endpoints are equal to the peak point value Ex1j=Ex2j=Exj, trapezoidal cloud degenerated into the normal cloud.

Trapezoidal cloud height coefficient is the function value of the Ex1j or Ex2j. The step 8, 9 calculate cloud model entropy En1j and En2j to fit g (x) for the half-liter cloud with Ex1j, half-falling cloud with Ex2j. to get En1j searching left area of the cloud model with Ex1j, to get En2j searching right area of the cloud model with Ex2j, the entropy value increase from 0 step from the smaller value until the threshold ? is greater than the difference between the half-normal cloud value and distribution histogram value; the step 10 calculate distribution function of the corresponding Trapezoidal; step 11 use the original data minus the known distribution function of trapezoidal cloud model data distribution to get the new data distribution function g'(x); Repeat step 4 to 12 until the peak value is less than the error threshold. Step 14, 15, 16 determine half hyper entropy of all cloud model with the residuals of distribution histogram.



IV. MINING ASSOCIATION RULES FROM CONCEPTS MAPPED FROM QUANTITATIVE DATA  Dividing quantitative attributes into several concepts using concept division algorithm, mapping the attributes data to the corresponding concept, sometimes, one attribute value may be mapped to different concepts, then mapping it to the concept that the attribute data has the largest subordination.

in the Boolean association rules I={i1,i2??? im}, the original meaning of ii is to buy not to buy tea, through the concept division algorithm a quantitative attribute value is extended to buy a lot of tea, buy some tea, buy little tea. Of course, every transaction belong to one of these situations only, problems can be solved by the Boolean association rules mining, using the famous Apriori frequent item set algorithm and the non-frequent item sets of FP-tree algorithm can carry out association rules mining[9][10].



V. EXPERIMENTAL ANALYSIS  Take two typical supermarket retail food rice and millet as example, the majority data scattered between 0.5-5kg, Extract 20000 transaction records randomly as experiment data, use concept division algorithm with error threshold ?=0.

5, and the peak height error threshold ?y=0.03, the length threshold error between trapezoidal top edge ?x =0.03. Then every transaction record data are divided into buy a lot of rice    (millet), buy some rice (millet), buy little rice (millet), the number of the concepts can be more or less according to the threshold values.

Using the famous Apriori algorithm to mine the database with the concepts mapped from the quantitative data, MinSup=30%,  MinConf=80%. We get the following results.

< a lot of rice> ?<a little of millet >,<a little of bean >  < some rice> ?<some millet >,<some bean >  to improve its accuracy further, reduce error thresholds, and lower the minimum support for association rules and, but this will take more time, the minimum support degree and the minimum confidence degree shouldn?t too small, so do the error thresholds, if so, the association rules does not make sense, the minimum support degree and the minimum confidence degree are given by the experts in the field.



VI. CONCLUSION  In this paper, we propose a new method to mining association rules from quantitative data based on the trapezoidal cloud model, which first take the original data distribution in the database into account, and then use the trapezoidal cloud model which combines ambiguity, randomness and uncertainty organically, and transforms the quantitative concept and qualitative data each other, in the conversion take account of the basic characteristics of human behavior fully, because of the presence of randomness, the same quantity data may belongs to different concept. for the people?s age falls into the interval (18,35), we all think that he was young, but the 16-17 year-olds should be assigned to the concept ?young? or not, that different people may have different views, even the same individuals may have different views at different times. concept division algorithm Based the trapezoidal cloud model take the above situation into account, not all the data have the above characteristics, only the elements on the conceptual border have this randomness, and the randomness can not affect the whole concept, the elements fall into the interval (Ex1, Ex2) fall into its own concept, which are their certainty. Therefore, this method can simulate the phenomena of human society better. The quantitative data are change into Boolean data in a reasonable manner, use sophisticated Boolean association rule mining algorithm, and we can achieve quantitative association rules. Using the quantitative data mining method can excavate more knowledge from database to support decision-making better.

