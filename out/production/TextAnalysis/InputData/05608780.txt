Study of Data Ming based on Apriori Algorithm

Abstract?According to the current characteristics of Internet users to search information, this article provides preprocessing steps and algorithms of log mining for non-registered and registered users. Focus on the study of the user's access pattern generated after preprocessing logs, and generated frequent access sets. It mainly studies the algorithm of web site users' personalized information recommendation and compares current various algorithms about the personalized recommendation. As the amount of Internet users log is very large, and contains a lot of useless information, this article improves the existing Apriori algorithm, by using the division and governance strategy to introduce the method of the Hash technology to complete the compressed candidate sets, reducing the times of frequently scanning the database, overcoming problems of the data mining algorithm of original association rules generating relatively large frequent sets, and needing repeatedly scanning the database .

Keyword-Web Data Mining, Web Personalized Information Recommendation, association rules  With the rapid development of the network information technology, the amount of information in the network is increasing, thereby the Internet emerges the phenomenon of "Information Explosion". In this background, users may not get the information resources they need after spending a lot of time that is the phenomenon of "Information Trek"[1-4] is produced. Therefore, by identifying the characteristics of different users' needs, for this reason the use of personalized service strategies and methods will solve this problem very well. The most important service in the intelligent personalized initiative information service is the personalized information recommendation. Resnick [5] first proposed the definition of recommendation systems, pointing out that recommendation systems is to recommend for users to realize personalized items". Schafer [6] had a deeper understanding of recommendation systems that recommending systems is a system that any providing a single recommendation result as the output or in a personalized way to guide users in the large capacity of optional items to find their own interested or useful items for themselves. Currently the combination of the Web log mining with the personalized information recommendation services has become an important research topic. While with the development of the network technology, the existing massive Internet users are available, and their browsing amount of information is also very large; the existing recommendation system is obviously not enough to process data in the speed aspect, so how to find an appropriate way  to enable to more quickly recommend the information to Internet users seems to be specially important.



I. APRIORI ALGORITHM  Apriori and others in 1993 designed a basic algorithm of Apriori[7,8], and proposed an important method of mining association rules based on two-stage frequency sets ideas, which is the most typical level algorithm and the most successful type of algorithm among Boolean association rules mining algorithms. Its core technology has been widely used in other types of Boolean association rules mining algorithms. The idea of the algorithm is [9, 10]: If S is a frequent item set, as for any non-empty subset L of S, through calculating the confidence degree, that is: conf=support(S)/support (L), and by conf ?miniconf (the minimum confidence) we can determine whether the rule of L?(S-L) is established (The rule certainly has the minimum support for S is the frequent item set).

For example: ABCD is a frequent item set, and AB is its subset.

If conf=support (ABCD)/support (AB) ?miniconf (the minimum confidence)  Then the rule of AB?CD is established, otherwise not.

For example: ABCD is a frequent item set, AB is a  subset of it If confsupport (ABCD) / support (AB) miniconf  (minimum confidence) Then the ABCD rule is established, otherwise not valid.

Specific in the page dialogue, S is the frequent item set  that is the page in S is the page often visited at the same time in a visit, while the last page in the visit sequence is always the purpose of users? visiting. Therefore, when using frequent item sets to generate the demanded rules, it is to mainly export rules of the pre-n-1 page of S to the last page; if this rule meets the minimum confidence, store this rule into the model library. The basic idea of the Apriori algorithm of frequent sets is found that, each scan of the transaction database needs constructing a frequent set; calculate the support of each candidate set, and then determine the frequent item set according to the pre-given minimum support. For example, the first scan obtains the candidate one item set, and then according to the minimum support determines the frequent one item set L1. To find the frequent 2 item set, taking the fact into account that any subset of frequent item sets has the minimum support, the Apriori algorithm uses L1?L1 to generate candidate sets,   V2-400C978-1-4244-8666-3/10/$26.00      2010 IEEE        here ? representing the join operation. C2 consists of C2|L1| two item sets. However, when |L1| is large, then C2 L1 becomes very large. Next, scan the transactions in the database, and calculate the support of each candidate set in C2 to find their support not less than the minimum support of the frequent set L2. And so on, before the k (k?1) time of scanning, first it is to use the (k-1) time of scanning result (i.e. frequent (k-1) item set Lk-1,) and the candidate k item set Ck generated by the Apriori-gen function, then in the process of the k time of scanning determine the support of each element, find the frequent k item set Lk. The algorithm ends when the candidate k item set Ck is empty. It can be found that in the process of calculating candidate item sets, generating candidate sets as small as possible is very important to the implementation efficiency, because at the time of scanning the whole database it needs calculating the support of each item set in Ck. The larger candidate sets are, the greater the cost of generating frequent sets is. Some experiments indicated that the initial scanning process has determined the cost of the entire implementation process; candidate sets generated from scanning are larger than those item sets which are real frequent item sets. Therefore, the generation of the initial candidate sets, especially of the frequent two item set, is the key to improve the association rules mining efficiency. Another factor associated with the implementation performance is the number of data which need scanning in the process of finding frequent item sets.

General algorithm is that during each iteration it needs scanning once the database of all transactions. To note that, with the increase of k value, not only the element of the frequent k item set reduces, also transactions including any frequent k item set becomes less and less. At this time, still scanning the entire database to calculate the support will be a great waste of time and space. Therefore, reducing the number of transactions needed scanning and the number of items in each transaction is an effective way to improve the mining efficiency.



II. THE IMPROVEMENT OF THE APRIORI ALGORITHM  It can be seen from the idea of the Apriori algorithm that, when there is a considerable number of frequent item sets, Apriori will generate a large number of candidate sets, and may have to repeatedly scan the database, which certainly reduces the efficiency of the algorithm; the algorithm proposed in this article is through the division and governance strategy to introduce the hash technology to improve the generation of frequent item sets, and uses the data to search the language to realize the association rule mining algorithm. To obtain users? frequent access path, if the maximum forward reference of users? each visit is very long, then it needs generating a number of k item sets; generating a large number of candidate item sets, each time it needs several repeated scanning of the same transaction database; we have proposed a strategy for improvement. For the sake of convenient discussion, here each item in I uses its item number to replace. Same as the previous, name the set of all frequent k item sets as Lk, such as L1 for the set of all frequent one item sets. Here we assume that transactions in  the transaction database and items of any item sets appearing in the algorithm are all arranged in accordance with the item number sequence. In the later algorithm, we easily see that, as long as making sure at the beginning that is items in the two item set are in order, and then the implementation of the algorithm will automatically make ensure that any item set is also in order.

Through the use of the hash technology it constructs a very small C2 to generate smaller L2 to export C3. If C2 is very large, the database can not effectively prune. After this step, the size of Li decreases rapidly as the value of i increases, thus it results in really small Li+1; then the corresponding cost is much less, greatly improving the proceeding efficiency of the entire process.

The advantage of the Hash table is to avoid the process of repeated searching, by the method of addressing to directly find the data which is needed; usually through the design of a hash function, which is a mapping, as long as it makes the hash function value obtained from any keyword be within the allowable scope of the table length.  Basic idea is as follows:  Same as the Apriori idea, first of all generate candidate one ?  item set Cl. The algorithm simply scans the transactions database, and count for each item.

Set the minimum transaction support number (i.e.

min_support), which can be calculated by the storage procedure, and can also be specified by the user himself.

Determine the L1 set of frequent one ? item sets, as the following table. Any transaction in the frequent item set L1 is larger than or equal to the minimum support degree.

Generate candidate 2 ? item sets. The algorithm uses L1? L1 to generate the candidate item set C2. Each item set in C2 is to make a connection with two frequent item sets belonging to L1 to generate  Scan the transactions database, calculate the support number of each candidate item set in C2, and store C2 with a temporary table.

Select transactions of which each transaction is not less than the minimum support degree, thus to determine the frequent two item set L2, and calculate the corresponding hash function of its transaction and the combination of item number 2.

Generate three item candidate set C3, use the characteristics of the Apriori algorithm, adopt the pruning technique, and remove all candidate item sets whose subsets are not frequent item sets, thus greatly reduce the size of 3-item candidate sets, to improve the efficiency for generating frequent item sets L3.

Generate frequent three ? item set L3, and calculate the support number of each transaction.

The circulation continues like this, continues to generate candidate sets, and from this generates frequent item sets, until the candidate item is empty.

The main purpose of using the Hash Algorithm is to generate the next candidate set by filtering out the item sets which are not needed. When scanning the database to calculate the support degree of the candidate k item set, it is to use the hash technology to pre-store the candidate (k +1)   V2-401        item set, that is, after through certain cutting, hash all possible (k +1) item set in a hash table. Each hash unit in the Hash table consists of one number, which represents so far the number of item sets hashed to this unit. The bit vector is constructed based on the Hash table; if the number of the corresponding number in the Hash table is larger than or equal to s, then the corresponding bit value is set to be 1, otherwise 0.



III. EXPERIMENTAL ANALYSIS  Figure 1 uses the example to analyze the Apriori algorithm. In the first iteration, the Apriori algorithm simply scans all the transactions, calculate the times appearing of each item, and obtain the candidate one item set of Cl; it assumes that the minimum support degree of transactions is 2, then the frequent one item set L1, is constituted by the candidate one item set which meets the minimum support degree. The Apriori algorithm uses L1 ?L1 to generate the candidate set C2. Next, scan 10 transactions in the database, and calculate the support degree of each candidate set in C2.

Then, based on the support degree of each candidate two item set in C2 can determine the frequent 2 item set L2. The candidate set C3 is produced by L2, the process as follows: first identify two frequent 2 item sets of {AB} and {AC} in L2 which are the same with the first project. Then, Apriori check the two item set {BC} composed by the previous two projects of {AB} and {AC} whether constitutes a frequent two item set. As {BC} itself is a frequent two item set, hence all subsets that can get {ABC} are frequent, then {ABC} becomes a candidate three item set. Similarly {ABE} is also a candidate three item set. This is using the above mentioned nature of Apriori to precede the connecting and pruning process. The candidate four item set cannot be constructed from L3, at this time the algorithm to be terminated.

Seen from the process of this algorithm, it is very important to produce candidate set Ci as small as possible, because in the process of scanning the entire database it needs to calculate the support degree of each project in Ci.

C 2  U se r  ID          Ite m s 2 0 2 .1 9 9 .1 3 7 .6     A B E 2 0 2 .1 9 9 .1 3 7 .6     A B D 2 0 2 .1 9 9 .1 3 7 .6     B D 2 0 2 .1 9 9 .1 3 7 .6     B C 2 0 2 .1 9 9 .1 3 7 .6     A B C 2 0 2 .1 9 9 .1 3 7 .6     A B C E 2 0 2 .1 9 9 .1 3 7 .6     A C 2 0 2 .1 9 9 .1 3 7 .6     B C 2 0 2 .1 9 9 .1 3 7 .6     A B F 2 0 2 .1 9 9 .1 3 7 .6     A C  Ite m s    S u p A        7 B        8 C        6 D        2 E        2 F        1  S c a n  D  C 1  Ite m s    S u p A        7 B        8 C        6 D        2 E        2  L 1  Ite m s A B A C A D A E B C B D B E C D C E D E  Ite m s    S u p A B       5 B C       4 A D       1 A E       2 B C       4 B D       2 B E       2 C D       0 C E       1 D E       0  S c a n  D  C 2 Ite m s    S u p A B       5 A C       4 A E       2 B C       4 B D       2 B E       2  L 2  C 3 Ite m s A B C A B E  S c a n  D Ite m s    S u p A B C      2 A B E      2  C 3  Ite m s   S u p A B C      2 A B E      2  L 3   Figure 1.  The analysis of the Apriori algorithm  Figure 2 shows an example of the specific application of the hash thinking. The purpose of constructing the hash table is to compress candidate sets by the hashing technique, and then the construction method of the hash table to some extent determines the hashing results. In this paper, it adopts the method of MVR to construct hash functions: (x * factor ?  y) mod hashtblsize. Obviously, the two uncertain parameters: the table length of the hash table and the factor coefficient are important elements to determine the hashing effects. For, if the table length of the hash table is smaller, then the hash result is that each bucket in the hash table has a number of candidate sets; if the count of each bucket is larger than the minimum support degree, then the hash table can not filter out any item; then it does not have any compression and filter effect, also the process of the construction consumes time. While the factor coefficient is an uncertain element, for example, we take x = 3, y = 2, when hashtblsize = 10, no matter which of 10,20,30 the factor takes, the results are the same; however, when hashtblsize = 20?the results will be different. Through experiments on the same test sets, under the condition of the same minimum support degree and different lengths of tables, using relevant results produced by the algorithm to analyze, comparing results, we can get, when the hash table length is longer, the effect of hashing is better. And we have verified this conclusion: the generation of initial candidate set, especially the frequent two item set, is the key to improve the mining efficiency of association rules. By use of the Hash table to effectively form frequent two item set and increase the efficiency of the algorithm.

Figure 2.  the hash technology causes frequent sets

IV. CONCLUSION  In view of the status that the present classic algorithm of association rules generating larger amount of candidate sets and more data amounts needed to be scanned, using the hash technology and the method of combining division with conquest, improving the existing Apriori algorithm, to achieve the purpose of increasing mining efficiency and raising the speed of appearing the recommended page.

V2-402        REFERENCE [1] Hai-Tao Xiao, etc. Research on Web-based Data Mining Apriori  Algorithm and Optimization Algorithms"[J], "Education," 2006 (1).

[2] J Han.M Kamber Data Mining Concepts and Techniques 2001.

[3] Ouyang River. Based on Association Rule Algorithm Improvement  Research " 2006 (21).

[4] Yao-chang proposed. Research of the Application on Customer  Analysis of an Association Rules Mining Method 2008 (4).

[5] Zhang Ling, An Improved Algorithm Based on Apriori and IUA ,"  2009 (1).

[6] Ma yu nv. An improved Apriori Algorithm based on mining  association rule," 2004 (4).

[7] Chen Yao. An Improved Apriori Algorithm Based on Graph 2008  (7).

[8] UNESCO. Medical Data Mining Based on Association Rules, 1996.

[9] SHENG Qinghong, ZHANG Jianqing, XIAO Hui. A New Approach  for Segmentation of Forest Image Based on the Color and Texture [J].

