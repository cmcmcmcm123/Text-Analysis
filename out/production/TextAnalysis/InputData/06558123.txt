A Comparative Study of Enterprise and Open  Source Big Data Analytical Tools

Abstract: In this paper, we bring forward a comparative study  between the revolutionary enterprise big data analytical tools and  the open source tools for the same. The Transaction Processing  Council (TPC) has established a few benchmarks for measuring  the potential of software and its use. We use similar benchmarks to  study the tools under discussion. We try to cover as many different  platforms for big data analytics and compare them based on  computing environment, amount of data that can be processed,  decision making capabilities, ease of use, energy and time  consumed, and the pricing.

Keywords: Big data, enterprise, open source, analytical tools,  Hadoop, business intelligence, metadata, MapReduce, SQL,  security, reliability.

1 INTRODUCTION  We are in a flood of data today. Statistics show that 90% of the world?s data was generated in the last two years itself, and it is growing exponentially.

To tackle such data and process it, we need to leave the traditional batch processing behind and adopt the new big data analytical tools. The data generated everyday exceeds 2.5 quintillion bytes which is a mind-boggling figure.

The growth of data has affected all fields, whether it is the business sector or the world of science. To process such huge amounts of data various new tools are being introduced by companies like Oracle and IBM, while on the other handle Open Source Developers continue their work in the same field.

1.1 What is Big data?

Big Data is the massive amounts of data that collect with time and are difficult to analyze using the traditional database system tools. Big Data includes business transactions, photos, surveillance videos and activity logs. Scientific data from sensors can reach massive proportions over time, and Big Data also includes unstructured text posted on the Web, such as blogs and social media.

1.2 Managing and Analyzing Big Data  For the past two decades most business statistics have been created using structured data produced from functional techniques and combined into a information factory or data warehouse. Big data significantly improves both the number of information resources and the variety and number of information that is useful for research. A significant number of this data is often described as multi-structured to differentiate it from the arranged functional data used to fill a data warehouse. In most companies, multi-structured data is growing quicker than structured data.

Two important information management styles for handling big data are relational DBMS products enhanced for systematic workloads (often known as analytic RDBMSs, or ADBMSs) and non-relational techniques (sometimes known as NoSQL systems) for handling multi-structured data. A non- relational system can be used to produce statistics from big data, or to preprocess big information before it is combined into a data warehouse.

1.3 Need for Big Data Analysis  When a business can make use of all the information available with large data rather than just a part of its details then it has a highly effective benefit over the market opponents. Big Data can help to gain ideas and make better choices.

Big Data provides an opportunity to create unmatched company benefits and better service distribution. It also needs new facilities and a new way of thinking about the way company and IT market works. The idea of Big Data is going to change the way we do things today.

The International Data Corporation (IDC) research forecasts that overall details will develop by 50 times by 2020, motivated mainly by more included systems such as receptors in outfits, medical gadgets and components like structures and connects. The research also identified that unstructured details - such as data files, email and video - will account for 90% of  Proceedings of 2013 IEEE Conference on Information and Communication Technologies (ICT 2013)     all details designed over the next several years. But the number of IT experts available to handle all that details will only develop by 1.5 times the present levels.

The electronic galaxy is 1.8 billion gigabytes in dimension and saved in 500 quadrillion data files. And its dimension gets more than dual in every two years? period of time. If we evaluate the electronic universe with our actual universe then it's nearly as many pieces of details in the electronic universe as stars in our actual universe.

1.4 Characteristics of Big Data  A Big Data platform should give a solution which is designed specifically with the needs of the enterprise in the mind. The following are the basic features of a Big Data offering- ? Comprehensive - It should offer a wide foundation and address all three size of the Big Data task -Volume, Variety and Velocity.

? Enterprise-ready - It should include the performance, security, usability and reliability features.

? Incorporated - It should easily simplify and speeds up the release of Big Technological innovation to business.

? Open Source based - It should be start resource technology with the enterprise-class overall performance and incorporation.

? Low latency flows and updates ? Solid and fault-tolerant ? Scalability ? Extensible ? Allows adhoc queries ? Little maintenance  2 ENTERPRISE BIG DATA ANALYTICAL TOOLS  VERSUS OPEN SOURCE BIG DATA ANALYTICAL  TOOLS  Ever since the research on big data started, companies like IBM, Google and Oracle have been leading the race of enterprise analytical tools and have been occupying the major section of the market. Little is known about the new age analytical tools that have been developed recently and are spreading faster than expected. In this paper, we bring to light the working characteristics of many such tools for big data.

Keeping aside all the expensive and closed source applications, we also strive to explain the working and advantages of open source analytical tools, which are as good as their counterparts in the enterprise world. We explain where these can be used and what are the issues involved in the same.

2.1 Enterprise Analytical Tool  2.1.1 Pentaho  Pentaho is a very useful tool to visualize high volumes of data and analyze it to draw conclusions. It supports all the right set of tools for the entire processing lifecycle of big data.

It provides exploration and visualization features for the business sectors; also performs predictive analysis.

It provides a 15 times boost to scripting and coding. It is very useful for interactive reports, time series forecasting, statistical learning, evaluation and visualization of predictive models. It supports Predictive Modeling Markup Language.

It provides visual tools that define instant access to data. It is built on a modern high performing, lightweight platform. This platform fully works on a modern 64 bit multi-core processor and harnesses the power of new-age hardware. Pentaho is unique in leveraging external data grid technologies such as Infinispan and Memcached to load vast amounts of data into memory.

The Instaview feature on Pentaho enables us to instantly view the reports generated by careful analysis of the data in a multi-dimensional and interactive format.

Fig1. Instaview feature on Pentaho    The problem is with the pricing. During the survey, it was found that a pricing request needs to be sent to Pentaho Big data Analytics in order to view the quotation. But reviews show that it is priced at a much lesser price as compared to other commercial BI tools.

2.1.2 TerraEchos  TerraEchos, a world leader in innovative security alternatives utilizing Big Data in Movement statistics, was known as the champion of a 2012 as it received the IBM Beacon Award for Outstanding Information Management Innovation.

TerraEchos, a next-generation big-data statistics company, has implemented the first foundation to blend and narrow large volumes of live complicated structured and unstructured data on the fly ? and at the same time to draw out, evaluate, and act upon the data, in the moment.

Unlike database-centric techniques to high-speed, high- volume data research, the TerraEchosKairos foundation is not restricted by the amount, type, or rate of the data: It consistently examines data while it is still moving, without requiring storing it. Based on a streaming operating system, and with statistics and creation segments updated to specific  Proceedings of 2013 IEEE Conference on Information and Communication Technologies (ICT 2013)     programs, the TerraEchosKairos foundati protection, intellect, facilities, and growing other marketplaces where understanding an now is crucial.

Fig2. Annual Lobbying by TerraE  2.1.3 Cognos  Cognos is known for business intel performance and strategy management as w to work for analytics applications. IBM's C known to provide an organization with wha become top-performing and analytics driv the individual, workgroup, division, mids large business, Cognos application has b ensure that companies make better choices development. This tool is for those who w company's intellect or performance.

Cognos solutions are designed to help decisions and manage performance for o results. Combine your financial and opera single, seamless source of information in t your choice for transparent and timely repo action.

IBM offers a full range of customer sup certification, and services for Cognos an Analytics customers.

Fig3. Query Analyser for Cog    An investment of over a lakh in Indian cur for the effective use of Cognos. It seems leg to invest in it, but for personal usage it may the general public.

ion is perfect for g professional, and nd performing right   EchosInc  lligence, financial well. It also extends Cognos Software is atever they need to ven.With items for size company and been developed to for their future and wish to grow their  you make smarter optimized business ational data into a the environment of orting, analysis and  pport, training and nd other Business   gnos  rrency is needed git for a company  not be possible for  2.1.4 Attivio  The AIE or Active Intelligenc huge impact on the business customers' information assets.

the situation and help them in w helps to fulfil a strategic goal wh AIE has the capabilities to platforms and brings together bo data content for an efficient an power to incorporate and rela content silos - without any requirement. It has a very a search capability for BI which analytical tools.

It offers both intuitive search powers, making structured and in ways never been thought of.

of big data and can be useful f varied technical skills and priori  2.1.5 Google BigQuery  Google BigQuery allows y against very large datasets, wit This can be your own data, o shared for you. BigQuery work of very large datasets, typically large, append-only tables.

It?s a high speed tool which c seconds. It can handle trillio terabytes of data. It?s simplicit close reference to SQL. It ha groups and user based Google secure SSL access method. It through BigQuery browser, the Script.

It has a very distinguished pri storage, interactive queries and (per GB/month), $0.035 (per G processed) respectively.

They all have default limit example is a fine example of a and optimized for processing analytics.

Fig4. Workplace for  ce Engine of Attivio has had a s through the availability of It helps in a quick analysis of whatever ways they need it. It hich has been established.

analyse anything on many oth structured and unstructured  nd agile BI. It is equipped with ate all the available data and  advance modelling of data advance intuitive Google like h incorporates all the needed  h capabilities along with SQL unstructured data more useful It has great insight in the field  for all kinds of users and their ities.

you to run SQL-like queries th potentially billions of rows.

or data that someone else has ks best for interactive analysis y using a small number of very  can analyse billions of rows in ons of records pertaining to ty stands out as it works with s a powerful sharing through Accounts. It works on a very has multiple access methods  e REST API or Google Apps  icing range which varies as per batch queries which are $0.12  GB processed), $0.02 (per GB  ts assigned too.This Netezza system that has been designed a specific workload: business   r Google BigQuery  Proceedings of 2013 IEEE Conference on Information and Communication Technologies (ICT 2013)     2.1.6 Netezza  A revolutionary data analytical tool was e and was named Netezza. It is an advanced warehouse engine that incorporates a dat storage components into a single environme run predictive analysis, business intelli applications needed in every field.

It is based on IBM blade architecture processors; an Asymmetric Massively P (AMPP) approach to process workloads; a (field programmable gate arrays ? speciali a means to filter data before it is process used to speed the process of queries and p which preprocesses. In this, computati processed by FPGAs as opposed to making the work.

The Netezza design has led to an expone field of big data analytics and is very u commodity or specialty environment.

It is not only ideal for processing comp queries such as those found when performin It preprocesses data, and then feeds the C fashion. Further, it is not burdened by structures and online transaction proces resulting in a simple code path for faster per  2.2 Open Source Analytical Tools  2.2.1 Apache Hadoop  Licensed under Apache v2, Apache Ha source system framework that not only r platform using data intensive processing. F it supports running on large clusters. Suppo and provides security and reliability for data its own Hadoop computation paradigm c where in the work is divided into vario processed on a clustered system or a grid.

It has a capacity to handle petabytes of d files within seconds and provides a very h computational processing.

Cloudera is the leader in Apache Hado and services and offers a powerful new enables enterprises and organizations to lo ? structured as well as unstructured.

Hadoop is written in the Java programmin a top-level Apache project being built and community of contributors Hadoop.

stablished by IBM, d high performance tabase, server and ent. This is used to igence and many  e which uses x86 Parallel Processing and it uses FPGAs ized processors) as sed. This FPGA is perform a function ional kernels are g the CPU do all of  ential growth in the useful as a hybrid  plex, scanning type ng deep analytics.

CPU in a balanced y legacy database ssing features ? rformance.

adoop is an open runs on distributed or commodity use,  orting data mobility a processing. It has  called MapReduce, us units and then  data in millions of high bandwidth of  oop-based software data platform that ok at all their data  ng language and is d used by a global  Fig5. Work Flow o  2.2.2 Zettaset  One of the most flexible op works on any Apache Hadoop d availability and security of the d system and is very cost effec second name.

Zettaset has created Orchestr an enterprise management too issues of Hadoop deployment w are sophisticated tools.

It has the capacity to automa installation of Hadoop on a clus is ready for enterprise usage Orchestrator is that it is not bas is a very secure open system.

2.2.3 HPCC Systems  Abbreviated as HPCC, Hi Cluster, as the name suggests is It was developed by LexisNexis versions to this tool, paid as w structured and unstructured c performing scalability from processing makes it even strong  It is commercially available a tool so easily available to the m selecting this tool is a platform includes a highly integrated capabilities from raw data pr queries and data analysis using  Working as an optimized clu high performing system resultin of Ownership) along with secu with a very good processing sp centric programming language programmer productivity for d this platform.

It has a good tolerance for processing in case of system fa warehouses, and high volume o security analysis of massive am   of Apache Hadoop  pen source analytical tools, it distribution. Its features include data. It is easy to deploy on any ctive. Simplicity is Zettaset?s  rator, software solution that is ol that addresses the common with easy-to-use interfaces that  ate, simplify and accelerate the ster management system which . The outstanding feature of sed on Hadoop distribution but  igh Performance Computing s a clustered computing system.

s Risk Solutions. There are two well as free and both work on content data. It has a high 1-1000s of nodes. Parallel  ger a tool.

nd offers a lot of features for a masses. A major advantage of  m for data-intensive computing d system environment with ocessing to high-performance a common language.

uster, it is a very low costing ng a very low TCO (Total Cost urity, scalability and reliability peed. It has an innovative data-  incorporated which increases evelopment of applications on  faults and capabilities for re- ailures. It can also manage data online applications to network  mounts of log information.

Proceedings of 2013 IEEE Conference on Information and Communication Technologies (ICT 2013)     2.2.4 Dremel  An interactive ad-hoc query system, it was developed by Google to offer analysis of nested readable data. Scalable to extremes; up to 1000s of computer systems and petabytes of data; it has the power to process trillions of rows together in just a matter of seconds by multi-level execution of trees and columns. It is not meant for replacing the old system, MR and is often used for analysis of crawled web documents, tracking install data for applications on Android Market and also for crash reporting for Google products.

Other important uses are spam analysis, debugging of map tiles on Google Maps, tablet migrations in managed Bigtable instances.

2.2.5 Greenplum HD  Greenplum HD allows customers to start with big data statistics without the need to develop an entire new venture. It is provided as application or can be used in a pre-configured Data Handling Equipment Component. Greenplum HD is a 100 percent open-source qualified and reinforced edition of the Apache Hadoop collection that contains HDFS, MapReduce, Hive, Pig, Hbase and Zookeeper. IT prevails of a finish information research foundation and it brings together Hadoop and Greenplum data resource into only one Data Handling Equipment.  Available as application or in a pre- configured Data Handling Equipment Component, Greenplum HD provides a finish foundation, such as set up, training, international support, and value-add beyond simple appearance of the Apache Hadoop submission. Greenplum HD makes Hadoop quicker, more reliable, and easier to use.

Greenplum HD facilitates Isilon?sOneFS Scale-Out NAS Storage space for Hadoop. EMC Isilon scale-out NAS is the first and only Business NAS remedy that can natively include with the Hadoop Allocated Data file System (HDFS) part. By dealing with HDFS as an over the cable method, you can quickly set up a extensive big data statistics remedy that brings together Greenplum HD with Isilon scale-out NAS storage systems to provide a very effective, extremely effective and versatile information storage and statistics environment.

The Greenplum HD DCA Component easily combines the Greenplum HD application into a product, offering an enhanced setting designed for performance and stability. The Greenplum Data Handling Equipment marries the unstructured batch-processing power of Hadoop with the Greenplum Database and the cutting-edge Extremely Similar Handling (MPP) structure. This allows businesses to draw out value from both arranged and unstructured data under only one, smooth foundation.

2.2.6 HortonWorks  Hortonworks is an authentic and free Hadoop Distribution system. It is developed on top of Hadoop and it allows clients to capture process and perform together at any broad variety  and in any framework in a simple and cost-effective way.

Apache Hadoop is a key of the Hortonworks framework.  It is ideal for organizations that want to combine the power and cost-effectiveness of Apache Hadoop with the amazing alternatives and balance required for organization deployments.

Hortonworks is the latest organization of Hadoop system and expert support, but it's an old element when it comes to working with the platform. The organization is a 2011 spinoff of Search engines, which remains one of the greatest clients of Hadoop. Actually, Hadoop was usually developed at Search engines, and Hortonworks managed an extensive broad variety of nearly 50 of its very first and most well-known associates to Hadoop.

There?s differentiator between Hortonworks and the other suppliers. Hortonworks products are 100% Begin Source and are free contrary to some of and Cloudera?s Company amazing and/or value-adding Hadoop products, which are not.

2.2.7 ParAccel  The open source tool, ParAccel data analytics platform has been used by organizations for its interactive capabilities to analyze big data in an enhanced fashion. It offers a high storage along with compression of adaptive capabilities. In- memory processing and compilation on the fly is also important, making it easy to work with and adapt to.

Fig6. ParAccel Query Analyzer  2.2.8 GridGrain  An enterprise open source system, GridGain, as the name suggests is for grid computing. This was specially made for Java and is compatible with Hadoop DFS and offers an alternative for Hadoop's MapReduce. It offers a distributed, in-memory and scalable data grid, which is the link between data sources and different applications. An open source version is available on Github or a commercial version can be downloaded from their homepage.

Proceedings of 2013 IEEE Conference on Information and Communication Technologies (ICT 2013)     3 COMPARISON OF SECURITY BETWEEN OPEN  SOURCE AND ENTERPRISE TOOLS  Although you can take an open source project, compare it against a closed source project, and say that one is more secure than the other based on some number of observations or measurements; this determination will probably be based on factors other than the nature of the project's open or closed source code. Secure design, source code auditing, quality developers, design process, and other factors, all play into the security of a project, and none of these are directly related to a project being open or closed source.

It is shocking to see the vulnerabilities in some closed source systems.  And although it certainly wouldn't mean open source software is quantitatively "more secure" than closed source software, it just means that there is a doubt in the source code auditing principles and otherwise the general security practices of certain closed source operating system vendors. However, the issue here isn't specifically related to the operating system being open or closed source, but to the processes with which the vendor approaches security.

Although this might seem to imply that open source projects are going to have less vulnerabilities than closed source projects, that's not really the case either; the number of vulnerabilities present in a given system can?t be simply associated with the openness of its source code. Ultimately, it's about the way the project and its developers handle and integrate security.

4 SELECTING THE RIGHT TOOLS FOR DATA ANALYTICS    The factors discussed in the paper have a significant impact on technology selection.Organizations are not ready to make risky investment strategies in expensive alternatives just in case there is something more to be discovered. This is where multiple alternatives come into play. Existing exclusive ? and generally expensive ? storage space and data resource alternatives are being formulated by some of the more cost- effective growing technology, generally from the Apache Hadoop atmosphere. Initial discovery and research of large information amounts, where the "nuggets" are well invisible, can be performed in a Hadoop atmosphere.

Once the "nuggets" have been discovered and produced, a decreased and more organized information set can then be fed into a current information factory or statistics system.

From that viewpoint, it makes overall sense for providers of current storage space, data resource, and information warehousing and statistics software to provide connections and APIs to Hadoop alternatives. And also put together incorporated promotions that work with both the exclusive and free components. While some of them hurry to accept Hadoop, there is no evidence that it is a sensible and suitable move. As already described, many of the new big data technology are not ready for popular business utilization, and organizations without the IT abilities of the trailblazers or  common early adopters will welcome the support from recognized providers.

5 CONCLUSION  To conclude, after the analysis of both closed and open source Big Data Tools, it is pretty evident that it's all about the usage and needs of an individual or the company. It is impossible to afford a few tools at a personal level because of the prices and complications, while using open source systems might pose an outdating and modifications problem. There is also the security issues involved in choosing the tool. Open source promotes development and innovation and supports developers.

Big data is on every CIO?s mind and for good reasons companies have spent more than $4 billion on big data technologies in the year 2012.

These investments will in turn trigger a domino effect of upgrades and new initiatives that are valued for $34 billion for 2013.

