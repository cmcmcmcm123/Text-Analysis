ANALYSIS AND IMPLEMENTATION OF   ASSOCIATION RULE MINING

Abstract Data mining Build models of the world (regression, decision trees, neural networks, association rules, fuzzy systems,..  ) from data that represent snippets of information about the world. Use these models to understand and discover patterns of interest that may provide knowledge deployable in improvingbusiness processes. The non-trivial extraction of novel, implicit, and actionable knowledge from large databases and in a timely manner. The APriori Data Mining Algorithm is used to create association rules from sets of items.

The algorithm finds patterns of items Algortihm uses knowledge from previous iteration phase to produce frequent itemsets that are frequently associated together. A confidence measure is created for each rule generated from the frequent itemsets.

Keywords: Data mining, Apriori Algorithm, Assocaition rules  1. Introduction  A set of items is defined as an itemset and represents items found in a dataset  The datasets can be obtained from real world databases representing shopping carts, retail transactions, data warehouses, sales, orders and purchases database tables or created artificially.

The APriori algorithm is used to analyze a list of transactions for items that are frequently purchased together. Considering a transaction where the sale of software is increased by the sale of e-books, Support and Confidence are two measures used to describe market based analysis association rules created with an APriori algorithm.

2. Association rule mining  Association rule mining finds interesting associations and/or correlation relationships among large set of data items. Association rules show attribute value conditions that occur frequently together in a given dataset. A typical and widely- used example of association rule mining is Market Basket Analysis.

The associations that Apriori finds are called Association rules. An association rule has two parts. The Antecedent is a subset of items found in sets of data. The Consequent is an item that is found in combination with the antecedent.

Two terms describe the significance of the association rule. The Confidence is a percentage of data sets that contain the antecedent. The Support is a percentage of the data sets with the antecedent that also contain the consequent  The input to association rule mining consists of a database T of N transactions, T = t1; t2; : : : tN over a set of m items I= fI1; I2; : : : Img. Each transaction is a subset of I. Let ? denote the size of the largest transaction2.

A subset X ? I is called an itemset. The frequency of an itemset is the ratio of the number of transactions that contain the itemset to the total number of transactions in the database. We say that an itemset X is p-frequent if its frequency is at least p. The goal of association rule mining is to discover association rules of the type W ) I where W ? I and I 2 I nW. The support of a rule W ) I is equal to the frequency of the itemset W [I. The of a rule W ) I is equal to the ratio of the frequency of W [I to the frequency of W. Given two parameters, ? ? 1 (called as support threshold) and ? ? 1 (called as confidence threshold), the goal of association rule mining is to discover association rules that have a     support of at least ? and a confidence of at least ?.

The intuitive meaning of such a rule is that transactions that contain W[I occurs frequently and a large fraction of the transactions that contain W also contain I.

3. Problem Formulation Typically, the task of association rule mining is carried out in two steps. The  fiirst step consists of finding all ?-frequent ? itemsets in the database.

The second step consists of forming the association rules among the frequent itemsets [2]. It has been observed that the ? rst step of identifying the frequent itemsets is the most computation and I/O intensive.

Therefore, we first  consider the problem of frequent itemset mining where the goal is to mine all ?-frequent itemsets.

4. APriori Data Mining Algorithm  Its Developed by Agrawal and Srikant 1994. It gives an Innovative way to find association rules on large scale, allowing implication outcomes that consist of more than one item  Apriori algorithm in pseudocode  L1= {frequent items};  for (k= 2; Lk-1 != ; k++) do begin  Ck= candidates generated from Lk-1 (that is: cartesian product Lk-1 x Lk-1 and eliminating any  k-1 size itemset that is not frequent);  for each transaction t in database do  increment the count of all candidates in  Ck that are contained in t  Lk = candidates in Ck with min_sup  end  return ?k Lk;  5. Sets of database  Transactional database D  All products an itemset I = {i1, i2,?, im}  Unique shopping event T ? I  T contains itemset X iff X ? T  Based on itemsets X and Y an association rule can be X ? Y  It is required that X ? I, Y ? I and  X ? Y = ?  up(1 ? 2)     3/5  conf((1,2)) = ---------------- = ------ = 3/4  sup(1)          4/5  relation of number of events containing both itemsets Xa and Xb to number of events containing an itemset Xa  Each set of data has a number of items and is called a transaction. The output of Apriori is sets of rules that tell us how often items are contained in sets of data  There are thousands (or more) total items to search and possibly a much larger number of combinations of those items that make up all the data sets. Apriori is a fairly efficient way to iterate through the sets of data and finds association rules depending on a particular confidence and support provided as input.

5.1 Large Item Set  A set of items that is contained in enough transactions such that the percentage of transactions that contain this item set is greater than or equal to the minimum support.

5.2 Candidate Set  A set of items that is currently being tested to see if it is a large item set or not. Candidate sets have a reference count and a current count. The reference count is the number of transactions (or sets of items) in the entire data set that contain this candidate set. The current count is used for calculating a probability of this set being a large item set as the algorithm iterates and adds candidate sets to the frontier set.

5.3 Frontier Set  The set of all Candidate Sets. This is reset at the end of each iteration of the algorithm to be the set of candidate sets that are expected to be large but are not yet known if they are really a large item set or not.

6. Limitations of Apriori algorithm  1) Needs several iterations of the data 2) Uses a uniform minimum support  threshold 3) Difficulties to find rarely occuring events 4) Alternative methods (other than appriori)  can address this by using a non-uniform minimum support thresold  5) Some competing alternative approaches focus on partition and sampling  Results Table-1  Total No.of  Items =5    Table-2  ? ?  ? ?  ??  ? ?  ? ?  ?  ????? ????? ????? ????? ??????  ?  ?  ?  ?  ??  ??  ??  ? ? ? ? ? 	 ?   ????  ????????????  ??????  ??????????  ?????????  Graphical Representation  5. Conclusions The association rules are simply taken from subsets of the large item sets. First of all, if there is a Large Item Set that has 4 items, then all the subsets of items of length ( 3, 2, and 1) are also large item sets. The association rule is found by taking the reference count of those item sets of length n-1, where n is the length of the item set, that are contained in the item set, and dividing by the reference count of the item set. The result is the confidence of the association rule. If the confidence meets your minimum confidence requirement, then the rule is output.

The APriori Algorithm basically finds the support count and confidence of itemsets eliminating those itemsets that do not meet a minimum support count and confidence measure from a final list of rules created.

????  ??????  ???????  ????????  ??????????? ??????????  ??????????  ?? ?? !? ?"????  ?? ?? ? ?"????  ?? ?? !? ?"??	?  ?? ?? ? ?"????  ??? ?? #? ?"????  .

?? ??????? ? ????  $???%? &?'??'??'??'?	(?  ?)????? &???'????'????'???	'????'???  ?'???	'????'???	'???	(?  *???  &?????'??????'?????	'??????'?  ????	'?????	'??????'?????	'?  ????	'?????	(?  +)????? &???????'???????	'???????	'?  ??????	'???????	(?  $?????? &????????	(?     6. References [1] R. Agrawal, T. Imielinski, and A. Swami.

Mining      associcion rules between sets of items in large databases. In SIGMOD Conference, pages 207{216, 1993.

[2] R. Agrawal and R. Srikant. Fast algorithms for miningassociation rules in large databases. In VLDB, pages 487{499, 1994.

[3] N. Alon and J. Spencer. The Probabilistic Method. JohnWiley, 1992.

[4] A. Ceglar and J. Roddick. Association mining. ACM Computing Surveys, 38(2), 2006.

[5] B. Chen, P. Haas, and P. Scheuermann. A new two-phase sampling based algorithm for discovering association rules. In Proceedings of ACM-SIGKDD Conference on Knowledge Discovery and Data mining (KDD), pages 462{468, 2002.

[6] H. Mannila, H. Toivonen, and A. Verkamo.

Efficient  algorithms for discovering association rules. In Proceedings of the AAAI workshop on Knowledge Discovery in Databases, pages 181{192, 1994.

[7] H. Toivonen. Sampling large databases for association rules. In VLDB, pages 134{145, 1996.

[8] M. Zaki, S. Parthasarathy, W. Li, and M.

Ogihara. Evaluation of sampling for data mining of association rules. In RIDE, 1997.

[9]M. Ester, H.P. Kriegel, J. Sander, X. Xu, A density-based algorithm for discovering clusters in large spatial databases with noise, in:Proc. of Discovery and Data Mining, Portland, Oregon, 1996, pp. 226? 231.

[10] Fayyad U , Piatetsky-Shapiro G, Smyth P, The KDD process for extracting useful knowledge from volumes of data. Communi-  cations of the ACM, 1996 , 39(11):27 35 [11] E. Knorr, R. Ng, Finding aggregate proximity relationships and commonalities in Knowledge and Data Engineering 8 (6) (1996) 884?897.

[12Jiangping Chen, An algorithm about spatial association rule mining based on cell pattern, Geoinformatics2006,wuhan.

