Identification and Tracking Using Laser and Vision of

Abstract-This paper presents a people tracker that uses a two-layered laser range sensor (LRS) and a fisheye camera. The LRS detects waists and knees of people, and the camera captures  color images of torsos of people detected by the LRS. From the data of people's position obtained by the LRS and the color histogram obtained by the camera, heuristic-rule-based and global-nearest-neighbor-based data association can identify  multiple people in crowded environments. The identified people are tracked via a model-based tracker; an interacting multiple model estimator is applied to track people maneuvers, such as walking, running, sudden starting/stopping, and sudden turning.

Simulation and experimental results show that our people identification and tracking method provides better performance than conventional methods.

K?words-Laser range sensor, Vision, People tracking, Interacting multiple model estimator, Color histogram, Data association

I. INTRODUCTION  Tracking (i.e., estimating motion) of multiple interacting people is an important issue in surveillance, security, service robotics, intelligent tr ansport system and other fields. There has been much interest in the use of stereo vision and laser range sensors (LRS) for these purposes. In this paper, we focus on laser-based tracking.

Compared with vision-based tracking, laser-based tracking is insensitive to lighting conditions and processing the data collected is quicker. Thus, many laser-based tracking systems have been developed recently [1-10]. In sparsely populated environments, laser-based tracking can provide good accuracy; however, the tracking accuracy becomes much worse in cluttered and crowded env ironments, where many interactions and occlusions of multiple people occur. To cope with this problem, multiple people tracking has been achieved using Bayesian filters such as the Kalman filter and Particle filter [5-10]. Also, data association (one-to-one matching of tracked people and laser measurements) has been implemented for reliable tracking, e.g., in the joint probabilistic data association filter (JPDAF) and the multiple hypothesis tracker (MHT) [18,   Zhitao Bai Graduate School of Engineering  Doshisha University Kyotanabe, Kyoto 610-0394 Japan  Kazuhiko Takahashi Faculty of Engineering and Science  Doshisha University Kyotanabe, Kyoto 610-0394 Japan  19, 22]. However, conventional laser-based people tracking has inherent and obvious limitations.

Most laser-based tracking systems use a laser that scans in a single plane and model tracking as a leg tracking problem, where people are represented by the states of two legs, either in a single augmented state or as a high-level track to which two low-level leg tracks are associated. This approach makes it difficult to detect several people accurately in crowded environments, leading to frequent track lost. A typical solution to this problem is to simultaneously detect different body parts of each person, such as legs and torsos, using multilayered LRS [15-17].

Moreover, laser-based tracking systems cannot provide rich information such as the color of detected objects.

Therefore, it is difficult to obtain a set of features that uniquely distinguish one person from another, leading to misidentification in cluttered environments. To cope with this problem, several systems using a laser scanner and a camera have been proposed recently to track people [11-14]; a color image taken by the camera is used for face-color and torso? color recognition to match non-confirmed feet with faces and torsos.

Most of the conventional laser-based tracking systems using Bayesian filters are built under the assumption that people walk at nearly const ant speed. Therefore, when people move randomly and flexibly (maneuver) while performing actions such as walking, runn ing, sudden starting and stopping, and sudden turning, tracking performance deteriorates significantly. People typically move and act under the constraints of an environment, making human behavior strongly place dependent. With maneuvering people, a single model (constant velocity model) is insufficient to represent the motion of people. A multiple-model-based approach in which different models run in parallel and describe different aspects of the behavior of people is an effective technique to deal with maneuvering people; in aerospace, one typical multiple? model-based approach to tracking maneuvering targets is the Interact ing Multiple Model ( IMM) estimator [20, 21].

This paper presents a method us ing a two-layered LRS and color vision for tracking people maneuvering in crowded env ironments. The two-layered LRS simultaneously detecting different body parts of people allows robust detection of people in crowded environments, while the use of color vision allows reliable identification of people. The IMM-based tracking also enables tracking maneuvering people. This paper is organized as follows: Section I I  overviews our experimental system; Sections I I I  and IV present our tracking method; in Section V, we describe a simulation and an experiment to evaluate the tracking method, followed by our conclusions.

I I. SYSTEM OVERVIEW  Figure 1 shows the overview of our experimental system, which consists of two LRSs (LRSH and LRSd and a fish eye camera. We constructed a two-layered LRS by setting the two LRSs at different heights; each LRS takes distance samples by scanning a single laser beam in a horizontal plane. LRSH observes around the waists of people, and LRSL observes around their knees. The fish eye camera was used because its view angle (185?) is similar to that of the LRSs (180?).

Figure 2 shows the sequence of people detection and tracking. The distance samples (laser measurements) related to people are extracted from the laser images obtained by LRSH and LRSL by using the background subtraction method. The distance samples coming from the same object are similar, while those coming from different objects are significantly different. Thus, the distance samples are clustered by checking the gap between two adjacent samples. The laser measurements are grouped based on sample clustering to obtain a representative point for each group. This clustering analysis is performed individually for each LRS.

A window is set around each person's torso based on the laser image taken by LRSH, as shown in Fig. 3. A color image is captured within the window with the fish eye camera to calculate a color (hue) histogram. People are detected and identified based on the distance samples obtained by the LRS and the color histogram obtained by the camera. The identified people are tracked by the method presented in the next section.

Figure 1. Experimental system.

Waist detection Knee detection ................................................. ......................................

Fisheye Camera  Data association  Figure 2. Sequence of people detection and tracking.

Figure 3. Torso area captured by the fish eye camera.

I I I. PEOPLE TRACKING  A. People Tracking using Interacting Multiple Model (IMM) Estimator  We classify motion patterns of a person into the following three patterns:  a) Constant velocity model (Modell): The person moves nearly straight at almost constant speed.

b) Sudden acceleration/deceleration model (Model 2): The person moves nearly straight, but accelerates and decelerates suddenly.

c) Sudden tum model (Model 3): The person turns quickly.

The equation of motion for each of the three models is  given by Eqs. (A. 1) and (A. 2) in Appendix. We represent the equation of motion for the n-th model, where n = 1, 2, 3, by the following vector form.

(1)  where xn = (xn ,yn ,On, vn ,onl . (xn ,yn) is the person's position in a global coordinate frame, and 0 n is the direction of motion of the person. vn and On are the linear and turning velocities, respectively, of the person. L1vn = (L1vn ,L10n)T is     a disturbance, and it is assumed to be a zero-mean white noise sequence with variance Qn = diag(q; ,q;) .

We set q:, and q? to be small; qJ = q? and q; > q? ; and q;, > q:, and q? > q? . In our experiments shown in Section V, we set their variances at  QI = diag(O. l m 2 /S4 ,0.1 rad2 /S4) , Q 2 = diag(l 00 m 2 /s 4,0.1 rad 2 /s 4) , and Q3 = diag(90 m 2 /S4 ,800 rad2 /S4) .

The LRS measurements give the waist (leg) positions of  the person in the global coordinate frame as z = (z x' Z y) T ? The measurement model related to LRSH and LRSL together is  z, = Hx; + Liz,  where H = (1 ? ? ? OJ . ? 1 ? ? ? (2)  Liz is measurement noise, and it is assumed to be a zero-mean white noise sequence with the variance R. In our experiments shown in Section V, we set the variance to be R = diag(O.O l m2 ,0.01 m2) .

On the basis of Eqs. (1) and (2), we track each maneuvering person via the IMM estimator [20, 21]. To track multiple people, as shown in Fig. 4, a validation region is set around the predicted position of each tracked person. The measurement inside the validation region is considered to come from the tracked person, and it is applied to update the track within the IMM estimator. As shown in Fig. 4 (a) and (c), in crowded environments, multiple measurements exist inside a validation region; multiple tracked people also compete for measurements. To achieve reliable data association (matching of tracked people and measurements), we apply the global nearest neighboring (GNN) algorithm [22, 23], in which the distance measure can be minimized. We defme the distance measure based on the laser measurements (distance samples related to people's waists and knees) and camera measurements (color histograms related to people's torsos).

Our methods for setting the validation region and distance measure are detailed in Section IV.

As with person #1 in Fig. 4 (b), the track of the person matched with his or her waist measurement taken by LRSH is updated using that waist measurement. The track of person #2 who is not matched with any waist measurement is updated using the knee measurements taken by LRSL? As with person #1 in Fig. 4(b) and (d), the person matched with both waist and knee measurements is updated using only the waist measurement.

B. Track management Moving people always appear in and disappear from the  sensing area of the LRSs. They also interact and get occluded.

In order to cope with such conditions, we implement track management based on the following rules:  a) Track initiation: As shown in FigA (a) and (d), measurements that are not matched with any tracked people are considered to come from new people or false alarms. The false alarms disappear soon. For this reason, we tentatively initiate tracking of the measurements with the IMM estimator.

When the laser measurements are obtained for more than 0.5  seconds, they are considered to come from new people and tracking is continued. If the measurements disappear within 0.5 seconds, they are considered to be false alarms, and the tentative tracking is terminated.

b) Track termination: When the tracked people leave the sensing area of the LRSs or when they get occluded, no measurements exist within their validation regions. If the occlusion is temporary, the measurements appear again. We thus predict the positions of the tracked people with the IMM estimator. If the measurements appear again within 1.5 seconds, we proceed with tracking the objects. Otherwise, we terminate the tracking.



IV. DATA ASSOCIATION  A. Setting Validation Region In this subsection, we describe the method for setting the  validation region. Based on the n-th motion model (where n = 1, 2, 3) of the person, the IMM estimator predicts the position of the tracked person by  Waist JtI?as:I'remel1t '\ i.) \ , '  . :, ,/ .

Person #1 \:L.. Person #2 :  (a) Tracked people and waist measurements  (c) Tracked people and knee measurements  (b) Data association with waist measurement  (d) Data association with knee measurements  (3)  Figure 4. Data association of tracked people and laser measurements. The closed circle and open diamond symbols signifY the tracked person and the measurement, respectively. The dashed line in (a) and (c) shows the validation region. The red lines in (b) and (d) indicate the result of data association.

3 Tmn It t"'-I h -n ? mn -m d mn ,.. w ere Xt = ?c xt , an c = 3  m=1 ?Tmn-m ? f-lt-I m=1  rn is the probability that the m-th model changes to the n-th model, where m = 1, 2,3, and n = 1, 2,3. xtnl and itt"'-I are the state estimate and posterior probability of the m-th model, respectively.

As shown in Fig. 5, three circle regions with a constant radius r are set around the predicted positions, Al , p/ , and N , of the tracked person, and their union is set as the  validation region for data association. In the simulation and experiment discussed in Section V, we set the radius at r = 1.0 m for LRSH and r = 0.5 m for LRSL?  B. Calculating Distance Measure  In this subsection, we describe our method for calculating the distance measure (cost) applied to the GNN-based data association.

We consider that, in a validation region, 1 people exist and J measurements are received, where 1 does not necessarily equal J. The j-th measurements taken by the LRSH and the camera are denoted by Zit and V it , respectively, where j = 1,2" " , J ;  Zit is the waist position of a tracked person detected by LRSH, and V it is a color histogram calculated from camera image related to the tracked person. The color histogram is represented within the HSV color space. We then defme the distance measure dij from the i-th tracked person to the j-th measurement, where i = 1,2"",1 and j = 1,2" " , J [7] as:  (4)  where N (i, zjt) is the Mahalanobis distance when the laser measurement zjt is matched with the i-th tracked person. E ( i, Vjt) is the normalized color distance when the j-th color histogram V it is matched with the i-th tracked person.

Figure 5. Validation region; the red marks signity the predicted positions of the tracked person. Orange dotted circles show the regions with constant radius set around the predicted positions. The black dashed line shows the validation region for purpose of data association.

The IMM estimator predicts the n-th-model-based position of the i-th tracked person. We therefore obtain the following distance:  (5)  where n = 1, 2, 3. p; is the n-th-model-based predicted position of the i-th tracked person. Sr is the covariance of the prediction error (zi - pn?  We define the Mahalanobis distance in Eq. (4) by N(i, zjt) = minimum (AI, A2, A3).

We determine a reference color histogram related to the i? th person by Vi = (Ul,112,'UY",U360)T and a color histogram calculated from the j-th VISIOn measurement by Vi = (UI,U2,U3, .. ,U360)T. The normalized color distance E(i, Vjt) in Eq. (4) is derived from the chi-squared similarity measure and defined as  (6)  The reference color histogram related to a person is generated and registered into a computer when the person is first visible in the sensing area. It is also discarded when the person disappears from the sensing area. To improve the accuracy of color recognition, we update the reference color histogram as follows:  - - Vii =aViI_1 + (1-a)Vit (7)  where we set a to be 0.4 in our experiments described in Section V.

When the tracked person is invisible to LRSH, we cannot calculate the normalized color distance E(i, Vjt) . In such cases, instead of Eq. (4), we determine the distance measure as dij = N (i,Zjt) using the knee measurements taken by LRSL?

V. SIMULATION AND EXPERIMENTAL RESULTS  A. Simulation Results a/Tracking a Single Person To evaluate the performance of IMM-based tracking, we  simulated tracking of a maneuvering person. In the simulation, the person is assumed to be always detected correctly. Figure 6 (a), (b) and (c) shows the true footprint, linear velocity and turning velocity, respectively, of the person. We set the transition probability matrix to be  [ 0.9 T = 0.05  0.05  0.05 0.05: 0.9 0.05 .

0.05 0.9     For comparison, we also conducted Kalman-filter-based  and Particle-filter-based tracking; we used only the sudden  turning model (model 3) and set the covariance  Q3 = diag(300 m 2 /s 4,300 rad 2 /s 4) for the Kalman filter and Q3 = diag(250 m 2 /s 4,250 rad 2 /s 4) for the Particle filter.

The covariance values were chosen by trial and error such that they provide the best tracking performance. In the Particle filter, the number of particles was 1000, and resampling was based on typical random sampling.

We present the results of 100 Monte Carlo simulations. To evaluate the tracking performance, we calculated the following normalized ?osition error (NPE) [24] and normalized velocity error (NVE) :  NPE=  ? )(Xit -Xit)2 +(Yit -Yit)2] i=1  ? )(Xit -ZXit)2 +(Yit -zyiti] i=1  L (vit -Vit)  NVE = -:,'",,' =;- 1 ___ _ 100 L (vit -Zvit) i=1  1 I 2 2 where Zvit =- ... ';<ZXit -Zxit-I ) + (Zyit -ZyiH) T  (8)  (9)  (Xi ,  Yi) is the true position of the person in the i-th simulation, where i = 1-100, and Vi is the true linear velocity of the person.

(Zxi , Zyi) and Zvi are the position and linear velocity, respectively, of the person calculated from the sensor  measurements. (Xi' Y i) and Vi are the filter estimates. T is the sampling period.

From the defmition of NPE and NVE, values below unity signify good estimates; the smaller the values, the better the tracking performance. Figure 7 (a) and (b) shows the results of NPE and NVE, respectively. The sampling period (scan) was 0.1 seconds. It is clear from these results that IMM-based tracking provides better performance than Kalman-filter-based and Particle-filter-based tracking.

B. Experimental Results of Tracking Five People We conducted experiments on tracking five people in a  narrow area (4 m x 2.2 m) shown in Fig. 8. They moved randomly and did maneuvers such as sudden stops, sudden starts, and sudden turns. They wore colored shirts: white (person #1), black (person #2), pink (person #3), blue (person #4), and purple (person #5).

We tracked the people for 26.2 seconds (262 scans). Figure 9 shows their estimated footprints. To evaluate the performance of our method, we counted the following two errors in our experiment: misidentification and track lost. For  I Because we cannot calculate the tuning velocity directly from sensor measurements, we do not consider the normalized turning velocity error.

comparison, we calculated the same two errors occurring when other methods were used. The results are shown in Table 1. It is clear from this table that our method provides better performance than the other methods.

Scan  7?-.-.-?-.-??-r?   ?Ll????72??3???5?? 6 X[m]  (a) Footprint  (b) Linear velocity (c) Turning velocity  Figure 6. Simulation condition; footprint, linear velocity, and turning velocity of a moving person. The LRS and camera are set at the origin of the xy coordinate frame in (a).

14,------,-------,------.------,-----,  1.2 LJ.l 0.. I Z   LJ.l3 > Z2  -IMM -KF ----PF  20 40 60 80 Scan (a) Normalized position error (NPE)  o?-?-?-?-??-?-?-?-?-? o 20 40 60 80 Scan  (b) Normalized velocity error (NVE)  Figure 7. Simulation result: thick solid, thin solid, and dashed lines are the results by IMM-, Kalman-filter-, and Particle-filter-based tracking methods, respectively.

Figure 8. Experimental condition; view by the fisheye camera.

u,  O'?' ?'I. ?, -.?, ?? ?o--?U'?'??I.'?' X1nj  (a) Person # I  0.' ??' ?.I.?' ? .?'?4 ?' ?0--?O '?'??I.'?' X1"j  (b) Person #2  2'  I2 ,. I 2

I.,  0.'  o?, ?.,?, ?.? , ??.,?o--?o.,?,??,.,?, X[ml  (c) Person #3  I2 ,.

IJ  O?  0':-, ?.I.-:- , ?.,:--??.5""""O ?O?.'?' ?'''''.5 --', X1m]  (e) Person #5  ,.



I.,  0.5  0.':-, ?., .;-, ?.;-, --';;?.'?O""""O::-:.,--:-, --:,7., ....,:, X[mJ (d) Person #4  Figure 9. Experimental results; footprints of people estimated using the IMM estimator. The LRS and camera are set at the origin of the xy coordinate frame.

Table I. Experimental result; the number of misidentifications and track lost.

2-layered LRSHand 2-layered LRS and  camera LRS camera  IMM Misidentification 0 3 3 Track lost 0 5 8  Kalman Misidentification 2 0 19 filter Track lost 5 4

VI. CONCLUSIONS  This paper presented a people tracking method that uses a  two-layered LRS and color vision via a camera. The LRS  allowed detecting people in crowded environments, while  color vision allowed identifying individual people. IMM-based  tracking enabled tracking maneuvering people. Results of  simulation and experiment validated our tracking method.

In this paper, a single tracker was set in an environment.

Our future research will involve the use of static mUltiple  trackers and a moving tracker.

ApPENDIX  The three models of a moving person are given as follows:  ? Constant velocity model (model 1) and sudden  acceleration/deceleration model (model 2)  +  1 I 2 '  2. XI_I -4(VI-IT+2dVI-IT M8t-JT sm81_1 I I 2 '  2  YI_I +4(VI-IT+2dVI-IT M81_IT cos81_1  81_1  o 1 2 (VI_IT + -dVt-JT )cos8t-J 2  (VI_IT + ? dVI_IT2 )sin 81_1 1 . 2  2d8t-JT  dVt-J T  dBI_IT  ? Sudden tum model (Model 3)  1 2 (VI_IT + 2dVI-IT ) cos 81_1  (VI_IT + ? dVI_IT2) sin 81_1 + 1 ? 2  2d81-IT  dVI_IT  dBI_IT  (A. 1)  (A.2)  where (x, y) is the person's position in a global coordinate  frame, and () is the direction of movement of the person. v  and e are the linear and turning velocities, respectively, of the     person. Liv and Lie are disturbances. r is the sampling period.

