Adapting CakeDB To Integrate High-Pressure Big Data Streams With Low-Pressure Systems

Abstract?Big Data continues to be one of the hottest topics in the computer science field and itself takes many forms. One way Big Data manifests is in the form of streams. These streams can be generally defined by their update frequency and the bandwidth they consume. They can however be further defined by the characteristics of the data they carry. The producers of these streams are generally tuned to perform a given role (such as moving large quantities of data with low latency) which can often be at odds with the requirements of a given consumer. In many cases the logistics of consuming such a stream can make the task impractical.

This paper discusses the concept of data streams as sequential data sets and having different pressures. The paper demonstrates through a use case of a financial trading company and a High Performance Compute Cluster how different applications require different pressures and why it is necessary to be able to scale down high pressure streams for low pressure applications without impacting the applications that require the full high pressure feed and the high pressure feed itself. A proposed system for classifying streams and related consumers is discussed as well as the concept of conflation as it applies to these data streams. Features in the prototype stream oriented database (CakeDB) that support adapting high-pressure streams to low-pressure applications are then discussed and further work is identified.



I. INTRODUCTION  Big Data continues to be one of the hottest topics in the computer science field today. Database research is increasingly looking to shared nothing solutions with the ability to scale out over many nodes. This approach effectively uses the power of cloud computing and the relatively low cost compute cycles to create fast databases that scale in a near linear fashion. However this approach to database design has a severe limitation, that of physical memory. As long as the value derived from the dataset exceeds the overhead of storing it, a distributed memory model can easily be appropriate.

However with terabytes of low value data (data that does not bring immediate value for its storage), this approach becomes an unrealistic solution. With the cost of fast hard disk storage plummeting, it is worth investigating disk based storage solutions. Most computers have disk storage space many times that of the physical memory in the machine. By considering the value and type of data being stored, the need for a different type of database becomes more evident [1].

This paper builds upon previous work describing different types of Big Data [1] and introduces the concept of different types of Data Streams (DS) that a Big Data system may  include, these being Low Pressure, Medium Pressure and High Pressure feeds respectively.A use case describing a financial trading company highlights the sorts of issues that a modern distributed system can face and demonstrates how the situation is getting increasingly more complex and difficult to manage.

The paper then touches on the design for a prototype stream oriented database (currently called CakeDB) with a focus on the components that allow for low latency and high throughput with respect to mediating between streams of differing pres- sures. The key features in CakeDB that allow for these are discussed followed by a discussion of future work.

The remainder of this paper is as follows. First two use cases are discussed along with the requirements for a proposed system that would satisfy their needs. This is followed by the proposed definitions for Big Data Streams and the different categories of consumer. A solution is then proposed detailing the implemented features that would fulfil the requirements. A summary then follows along with a discussion on future work.



II. USE CASES & SYSTEM REQUIREMENTS  When considering Big Data Streams, it can be easy to overlook the requirements of the consuming applications. It is common for producers to publish data at a rate that suits the producer?s needs. This could quite conceivably be a case of ?send as much data as quickly as possible?. However the downstream applications that actually consume this feed often have varied requirements and although some may be required to handle the full data stream, others are only interested in parts of it or perhaps simply wish to store the data stream with no need (or even capacity) to handle the load in real time.

This section details two use cases, one of a financial trading company that has multiple high throughput data streams but a variety of applications that utilize that data stream in very different ways. This challenge is not limited to scenarios with diverse requirements, but can also be found in compute clusters where it is quite possible that a given data stream may exceed for a short (or potentially long) time, the ability of the cluster to process it.

The ability then to adapt high throughput data streams so that they may be consumed appropriately by a given application is of considerable interest. First, it is necessary to categorize different types of data stream by the impact they have on applications and requirements rather than by using the generally accepted approach of using bandwidth. Second, it is   DOI 10.1109/CLOUDCOM-ASIA.2013.33    DOI 10.1109/CLOUDCOM-ASIA.2013.33     necessary to categorize different types of consumer so that they can be matched up to the data streams that were previously defined. Together, these sets of definitions make describing a particular scenario or use case significantly easier.

A. Use Case ? Financial Trading Company  Financial Exchanges are at the centre of world finance.

They provide regulated and structured places for trading a wide range of instruments such as stocks, bonds, futures, options, warrants and commodities. In exchange for providing these regulated trading platforms, the Exchange charges commission to market participants, generally based on the amount of volume traded. To ensure they have an environment conducive to increasing trading volume, Exchanges are constantly im- proving their technology so that orders sent to the Exchange can be processed faster and market information (such as prices and trades) can be disseminated fairly and efficiently to all participants. Quiet markets can easily generate 10 million trade updates in a trading day, and a state of the art Exchange can handle 10 billion trades a day. That does not include the orders placed that were never filled and those orders could easily be an order of magnitude higher.

Although each Exchange is slightly different, there is a trend to move towards order-based price feeds. Snapshot-based price feeds publish the best price (and available volume) for a given instrument and can include various levels of price depth below the best price. There is no calculation or processing done by the consumer and the price update can be consumed directly. However these feeds are relatively slow because the Exchange must calculate the current best price (and levels of price depth) from their internal order book. This puts considerable load on the Exchange and effectively limits how fast price updates can be published. Often instruments are published in a cycle that is measured in milliseconds. Order- based price feeds however are very different. Rather than the Exchange calculating the best price, it simply forwards any orders that it accepts to all the other market participants. It is then down to each participant to take the order update, apply it to the order book and from that, calculate the prices that were previously provided by the Exchange in the snapshot- based feed. This approach greatly increases the amount of data that consumers need to process but effectively turns the Exchange into a routing device. These feeds tend to publish data in real time with gaps between updates being measured in microseconds.

The continued improvements in performance at the Ex- change have meant that the computer systems used by market participants have needed similar upgrades and enhancements.

This includes infrastructure such as computer hardware (newer and faster machines), improved network infrastructure (low latency switches and fibre optic inter-connects) and software enhancements (ability to consume the newer and faster feeds in real time). Whichever participant can receive data from the exchange, make a trading decision and (if necessary) send an order to the Exchange in the least time, will have a very significant (and most likely very profitable) advantage over other participants. It is now considered standard in the industry to benchmark such systems in nanoseconds.

Market data is only one of many potential data sources in such a trading company and most will have distributed  trading platforms of some kind. The components of such a system have differing requirements (and capabilities) in terms of the data each can handle and the ability to adapt high speed data streams into a form that these disparate applications can consume remains a significant challenge. Figure 1 highlights how different systems may inter-operate in a given trading system.

Fig. 1. Trading System Message Flow  Financial companies are under increasing stress to manage and process more data from a myriad of sources. The ability to arbitrate between streams of different categories is a big concern as applications may not increase in performance, but the demands of a given stream will.

B. Use Case ? High Performance Computing  High Performance Computing solutions that take advantage of commodity hardware have matured greatly in recent years.

Systems such as the High Performance Computing Cluster (HPCC) [2] and similar systems such as Google?s MapReduce [3], [4], Hadoop [5], [6], SCOPE [7], Sector/Sphere [8], Datomic [9] all use commodity hardware in their respective solutions.

However, regardless of which technology is chosen, one of the challenges such a system faces is being able to process High Pressure Data Streams (HPDS) where the data coming into the cluster can exceed the clusters ability to process it.

Solutions such as Redis would not be appropriate in this case as if the cluster is unable to process the data in real time, the backlog will build up in Redis. If the stream is ?always on? there would never be an opportunity for the cluster to catch up.

Apache Kafka would be a superior choice for this challenge as it is designed specifically for handling such streams. However it does not provide database features, and so while it could buffer data, it would not be able to provide support for more complex queries such as requesting data over a time range.

CakeDB however supports basic database queries that are time bound and also supports functionality to enable streaming data both conflated and non-conflated. Such a solution would not only allow data to be buffered into the HPCC cluster itself, but would also allow for CakeDB to store data for historical queries as well. This approach gives the best of both worlds where only simple database functionality is required, but the ability to handle diverse and high speed data streams (and make them available to a range of differing consumers) is a requirement.



III. PROPOSED MECHANISMS & SOLUTIONS  Data Streams (DS) are generally categorized by the amount of data that the stream generates every second. This figure (generally given in megabits per second) allows for effective resource allocation from a network level, but does not give a great deal of information about the DS itself. When consider- ing Big Data Streams (BDS) it is often beneficial to consider the type of data being sent and how exactly that data is being sent. For example, a multicast UDP stream will behave quite differently to a TCP stream.

Streams are effectively serial in that one message or event arrives at a given time. Even streams with multiple sources ultimately arrive one behind the other in sequential order.

Whether the stream is considered to contain messages (such as in a market price feed) or events (data capture points from remote sensors), these individual elements are the fundamental component of a stream. The general definition of a stream can be expanded to better describe a BDS. First, the size of a specific message in the stream determines how much data will be received in each message. The bigger the message, the greater the load on the infrastructure (although for various reasons - such as compression - smaller messages could be more CPU intensive). The second defining factor is the time between the messages. This is significant because a message that?s twice as large but takes twice as long to arrive is usually less intensive on the infrastructure than smaller updates arriving at much higher frequency.

A. High Pressure Streams  High Pressure Streams (HPS) are those that put significant stress on infrastructure and consuming devices and applica- tions. The individual message size can be large as data is packed to increase throughput (often to the size of the network MTU) and the time between each packet being sent can generally be measured in microseconds. These streams are currently found in many financial exchanges where in many cases a single computer is unable to process the entire feed and it must be broken down into segments called partitions.

Individual streams that may not normally meet this category can be considered HPS if they combine data from other sources. For example, two Medium Pressure Streams being sent to one destination could when combined be enough to rate as an HPS.

B. Medium Pressure Streams  Medium Pressure Streams (MPS) also stress infrastructure but generally do so in only one of the two dimensions highlighted earlier. Either the packet size is large but the transmit frequency is low or the transmit frequency is high with much smaller packets. These streams can generally be consumed by most systems in its entirety although care needs to be taken to ensure that applications consuming this feed do not become overloaded. Although MPS are less intensive, it is still possible for applications to either build a backlog or to start failing under load.

C. Low Pressure Streams  Low Pressure Streams (LPS) are streams that place no real stress on the infrastructure and the vast majority of applications  would have little difficulty consuming and processing this feed.

This is the level that most general applications (particularly those with business logic) would operate at. These applications would be network bound rather than CPU or memory bound.

D. Storing the data  Regardless of the type of data stream, there is often the requirement to store it for future use. This can be especially challenging with current database technologies as those that can handle ?black box? data or highly structured data (such as JSON) tend to leverage physical memory in order to provide high performance thresholds. As physical memory is significantly less than available hard disk storage, the need to scale-out such systems (that is add additional nodes) greatly increases their cost and is therefore unable to fully utilize avail- able disk storage. This is especially important when dealing with High Volume Low Value (HVLV) data [1]. Therefore it is not only the handling of the streams that needs to be considered but also how such streams are ultimately going to be stored.

E. Types of stream consumer  Stream consumers can be broken down into four types regardless of the type of stream that is being consumed.

F. Real time non-conflated  Real time non-conflated (RTNC) consumers need to be able to process each message received in the stream within a specified time window. In addition each message in the stream must be processed. This is the most intensive form of consumer as not only must updates be handled as quickly as possible but it is a requirement for each and every update to be processed.

For these consumers, either failing to meet the time window or dropping a message designates a failure of the system. Such systems would include ?order based market price data?.

G. Real time conflated  Whilst Real time conflated (RTC) consumers have the same time constraints as RTNC consumers, there is no requirement for every message to be processed. Some systems only care about the most recent update. If a message is being processed and a new message then arrives for the same feed, such a consumer may cancel processing the current message and process the new message instead. These systems are common such as ?snapshot based market price data? where only the current price is of interest, as only the current price can be acted upon. Therefore in order to ensure that the most current value is available, such consumers should drop updates that are not relevant or can be skipped.

H. Non-real time non-conflated  Non-real time non-conflated (NRTNC) consumers have no implicit constraints on how fast each message must be handled but they are required to process each and every message.

Such systems are usually ancillary in nature. For example, an application that stores the real-time feeds discussed previously would need to see each message, but as it is only storing them, there is no requirement for the process to be inherently fast.



I. Non-real time conflated  Non-real time conflated (NRTC) consumers only care about the latest values and have no constraints on the time required to process them. For example, a system that updates a price on a web page once every minute, neither needs to process the data within a limited time frame and nor does it need to process each message in the feed. It is only interested in the latest price.

J. Conflation  Conflation allows an application to effectively consume a data stream that would otherwise overwhelm it. As discussed in the previous section, when the full feed cannot be processed in real time, either more time is needed or the effective message rate must be decreased. By conflating a data stream, messages are effectively dropped but in a way that does not impact the usage of the stream. This is similar in concept to MP3 compression where significant data is lost, but the overall impact on the use of the data stream is minimized.

1) Passive Conflation: Passive Conflation is where a data stream is conflated by the client rather than by the sender.

For example with a polling based mechanism, a client with a suitably equipped server would be able to request the latest message, process it and then again request the latest message.

Any data received in between those two requests is effectively dropped from the clients point of view. This is often desirable because it means the client can effectively rate limit the data stream to meet its own capacity to process it. However it does add additional complexity in that the client must be able to handle certain situations, such as when the latest message has not updated since the last request.

2) Active Conflation: Active Conflation is where a data stream is conflated by the sender. Conflation can be done in several ways, the simplest being that of a timed pulse. With this approach, the sender processes message as normal but does not actually send them. When the timer is triggered, the sender will then publish whatever the current value is. With a pulse cycle of 100ms, this would limit a given stream to 10 updates per second, regardless of how many messages were actually arriving at the sender for processing. Conflation could also be done on individual components inside the data stream. For example in a single stream there might be ten different message types. These could all be conflated separately. In addition, some message types may not be conflated at all.

A more application specific form of conflation is where the sender applies logic to determine if a message should be sent or if it should be dropped and replaced with a newer message. For example, if message two arrives before message one is sent, message one is dropped and only message two will actually be published.

K. Proposed implementation  As discussed in a related paper CakeDB is a stream oriented database designed specifically to handle high-pressure low-latency data streams [1]. To enable the database to handle such workloads, a new database design was necessary to meet these specific requirements. Thus the following features were key to CakeDB?s original design:  ? Does not block the client ? Has low impact on the client ? Can accept data at high speed ? Takes full advantage of available resources ? Copes with burst traffic ? Can operate on a single machine ? Queries are executed in a reasonable time In addition to these features, CakeDB also uses a simple  binary format (for both network communication and data storage) and stores data in Natural Order (NO) on disk. These features enabled CakeDB to outperform MongoDB on raw insert performance [1] and also make it possible for CakeDB to act as an intermediary for handling high-pressure data streams with low-pressure consumers.

1) Optimized binary format: CakeDB was optimized for throughput and minimal overhead. Figure 2 shows the structure of a message and these are stored one after the other in natural order.

Fig. 2. CakeDB Format - Data File  Figure 3 shows a similar structure is used in the index files.

This structure is very efficient for determining the closest byte offset to a given time stamp. This in turn allows data files of many hundreds of gigabytes to be accessed with a minimal performance penalty.

Fig. 3. CakeDB Format - Index File  2) Natural Order Storage: CakeDB implements a Natural Order Storage (NOS) system that stores data in the order it was received. This means that when the dataset is also naturally ordered ( such as prices from a stock market ) they are inherently stored in the correct order; that is events stored after a given event must also have occurred after that event.

This approach makes streaming data to disk very efficient and allows for high throughput and minimum overhead. The database simply buffers data in FIFO order and streams to disk when either a given amount of time has passed or a given amount of data has built up in the buffer.

Fig. 4. Natural Order Storage     3) Adopting Erlang: Erlang was designed initially in 1986 by a small team at Ericsson [10]. Lead by Joe Armstrong, the team developed the language (which was later open sourced in 1998) to address key issues that Ericsson were experiencing in developing software for their telecommunications hardware; that of stability and reliability in systems known to be suscep- tible to errors [11]. Armstrong?s approach takes the opposite approach of shared-everything designs (which use threads and shared memory) as he believed them to be too tightly inter- connected to allow faults to be properly isolated. Instead a process based message passing model (similar to the actor model [12]) was believed to be more than sufficient [13].

This design coupled with a powerful scheduling model makes Erlang exceptionally good at making full use of multi-core systems as this development model prevents any shared state and therefore there is very little locking in Erlang [14].

Although the message passing paradigm is not unique to Erlang, it is still a relatively rare approach to solving the concurrency problem. Scala, written for the JVM (Java Virtual Machine) [15] is one such implementation. However the challenges that are faced when trying to implement an actor based model on top of a thread based model can be significant and there are limits to what such languages as Scala can accomplish. Erlang has a custom VM that was written in C and was designed specifically from the ground up to support this paradigm [16]. This combined with libraries for implementing Erlang clusters in C and Java [17] make it very easy to integrate legacy applications into an Erlang cluster.

These features made Erlang the perfect candidate for CakeDB.

L. Commands  CakeDB implements the following commands which pro- vide the basis for stream conflation. CakeDB acts as a pressure regulator between the incoming high-pressure feed and the various low-pressure consumers. Because CakeDB acts as an intermediary, it effectively detaches slower applications from the critical path which in turn protects upstream applications from back-pressure caused by slow consumers. Consumers that require only the latest update from a given stream can use the ?Get Last Message? command whereas those that require the full feed, just in smaller chunks can use the ?Get All Messages Since? command.

1) Get Last Message: The ?Get Last Message? command returns the latest message on a given stream. This is very useful for applications where only the most recent value is of importance and where historical data (all data other than the most recent) is of no interest. This command is very efficient as it does not need to access the index or data file stored on disk. In CakeDB the last message on each stream is held in memory, and so it can reply immediately to this command.

2) Get All Messages Since: The ?Get All Messages Since? command takes a given timestamp (each message is guaranteed to have a monotonically increasing timestamp) and returns all messages since that time. It does not include the message that has that particular timestamp. This is useful for applications that need to process the whole feed but are unable to handle the full feed in real time. With this approach, CakeDB serves up data in chunks that the client can control. This query does use the index and data files, but as the most recent writes are still usually in the cache, data is still usually served from memory.



IV. RELATED WORKS  There are two key requirements for handling such data streams. There needs to be a way to store them and there needs to be a way to manage those streams so that they may be effectively consumed by the relevant applications. Combining these requirements shifts the focus away from a specific database technology. This makes designing, implementing and even understanding the issues surrounding Big Data usage a significant challenge [18].

Databases are not generally designed for this sort of workload. State of the art databases such as those created by Michael Stonebraker and his fellow researchers (H-Store [19] and C-Store [20]) offer very impressive performance over distributed datasets. However they are not designed to stream data (i.e. push updates) to multiple clients and although something similar could be implemented, this would not yield optimum performance. In addition their database structures are relational in nature and as such do not support either ?binary blob? or highly structured ?NoSQL? datasets which is a useful feature when dealing with unstructured or proprietary data feeds. Datomic treats time as a first class citizen [9] and supports ?NoSQL? datasets, however its schemas still need to be determined in advance, and aren?t as flexible when structured data such as JSON must be handled.

There are two main choices for when handling this style of data becomes necessary; MongoDB [21] and CouchDB [22].

CouchDB stores data in native JSON format and takes a unique approach to indexing data. Where RDMS systems have static data and dynamic queries, CouchDB implements dynamic data and static queries [23] leading to fast reads only when indexes are available. CouchDB also has a REST (REpresentational State Transfer) over HTTP interface which makes querying less efficient. Insert performance, even with batch jobs is relatively low, at around 700 updates per second.

In contrast, MongoDB is more like traditional SQL databases. It is possible to do dynamic queries and due to its BSON (Binary JSON) storage format offers JSON-like structures but with very fast search and parse times. However being memory mapped, physical memory is a highly limiting factor. Once data exceeds this limit, performance drops to the point of being unusable.

Although these systems could potentially handle and store the required data (although initial research suggests that there are severe bottlenecks when trying to do so [1]), there is still no native way of streaming that data to clients. The same is true for key-value store systems such as memcached [24] and Riak. Whilst highly performant and distributed in nature they were not designed to handle this sort of workload.

A potential alternative is Redis which is generally consid- ered an evolution of memcached [25]. It offers publish and subscribe support (PUB/SUB) which does give the ability to handle real-time streaming data. However, like MongoDB it is a memory-based solution (although it can backup this store to disk) and it attempts to deliver all queued messages to a given consumer. This causes problems where a consumer cannot keep up with the data stream and Redis is then required to start buffering undelivered messages in memory. In many cases a consumer is unable to catch up, and ultimately this will cause Redis to run out of memory and fail.

Kafka is an Apache project that was designed to unify multiple messaging systems into a single system [26]. The design specifically placed throughput rather than features as the primary design constrain [27]. A key design difference between Kafka and other solutions such as Redis, was the design decision to leverage a disk based system for holding data after considering research that demonstrated disk based systems when implemented appropriately can offer faster ac- cess times than a memory based system [28]. By using disk space as the primary storage medium, Kafka removed physical memory as a restriction for the amount of data that could be stored. By simply tracking where a client was in relation to a particular feed, it was possible to allow even slow consumers to consume large data streams.

However Kafka is not designed as a permanent storage system, it aggregates data streams rather than stores them. As such streams can often be very large themselves, even though Kafka might provide a solution for managing the streams in the initial case (where slower applications need to be able to manage fast data streams), it does not solve the issue of storing this data for historical purposes, archiving or a repository of data for future analysis.

An ideal solution then would implement features found in existing database technology (such as the permanent storage of data and the ability to query it) along with features found in streaming solutions. CakeDB is a prototype database that unifies features from both fields to provide a simple database that supports the following use cases.



V. SUMMARY & FUTURE DEVELOPMENTS  Big Data Streams are generally geared towards suiting the requirements of the producer rather than that of the consumer.

Indeed it is usually the producer that defines the format and requirements for a given data stream. However there are many cases where high-pressure data streams need to be consumed by applications that cannot fulfil the expectations of the producer (low-pressure consumers) and so an intermediary is required to regulate the pressure between the two. As data streams increase in throughput, complexity and number, a simple way of defining these streams and the related consumers becomes necessary.

Support for active conflation is currently being added to CakeDB. This feature allows clients to specify the number of updates per second that they can support, and then streams the data to them in real time. Although CakeDB was originally conceived primarily as a system for storing large amounts of data, the need to be able to adapt that data in real time as well as for historical uses, is driving its continued research and development.

