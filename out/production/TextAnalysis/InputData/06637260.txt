Two Stage Genetic Approach for Bio-chemical

Abstract?Determining different mentions of chemical names from texts has a wide-spread application in real life. Chemical names are complex in nature and there exist several repre- sentations and nomenclatures (like SMILES, InChI, IUPAC) which create a big challenge to their automatic identification and classification. In this paper we present a feature selection approach for appropriate feature subset selection from a well- known supervised machine learning approach namely conditional random field based classifier (CRF). Several features are identi- fied and extracted without using any domain specific knowledge and/or resources for determining mentions of IUPAC and IUPAC- like names from scientific text using some supervised classification technique. The appropriate set of features for a particular supervised classification technique is extracted from this huge collection of features using some single objective genetic algorithm based feature selection technique. Experiments are carried out on the benchmark patent dataset. Evaluation shows encouraging performance with the overall F-measure values of 70.01% by single objective optimization based approach on patent 2008 test data set.

Keywords: Mention Detection and Classification from Bio- chemical Domain, Feature Selection, Genetic Algorithm, Con- ditional Random Field.



I. INTRODUCTION  In recent years, information extraction is one of the hottest field. It has drawn huge attention to the practitioners and researchers. There is a huge collection of online information which are unorganized. In every day, a large number of data documents are added to it. So it is crucial to organize, find and extract relevant information from such a huge amount of data in our day-to-day life. In particular, in medical domain more than 2000 documents are added daily in the form of publi- cations, research papers, patents, journals etc. and we cannot classify all those documents manually [1][2]. Ddocument cat- egorization, ranking and retrieving entity-related information constitute some important steps in research and work life of general users and the researchers who can utilize these large number of textual documents like Medline [3][4]. The develop- ment of systems for automatic identification of named entities relevant to a particular domain and their possible storage to different databases have helped many emerging application areas like information retrieval, information extraction, and information aggregation. In order to automate these tasks, some techniques have been devised since the last one decade especially for determining mentions of chemical names as well as to extract relations among them (i.e. protein?protein interaction).

Chemical compounds like small signal molecules or other biological active chemical substances are the core entity classes in biological publications and patents. Many representations and nomenclatures exist for chemical names. Examples in- clude SMILES, InChI and IUPAC. The first two allow a direct structure search whereas IUPAC like names are most frequently occuring in biochemical texts. Medline [3][4] like huge text corpora databases can be beneficial if we can classify and rank all those documents. This will aid them in determining the entity related information in a fast and easy manner. The documents can be classified and ranked if we can extract, retrieve and gather the information which can be improved after identifying the entities of interest (IUPAC- like terms) with high recall rate and map those entities to corresponding entries in the database. Hence the process of identifying the entities of interest in the provided texts needs to be automated[5].

Features of training and test data sets are very much impor- tant for good performance by some classification techniques.

Feature selection [6], [7] is also having the name of variable selection, attribute selection or variable subset selection. It is the technique used in machine leraning to automatically select the relevant subset of features for developing the classification models. Feature selection helps to improve the performance of a classifier by selecting the appropriate sets of features.

Here the problem of appropriate feature selection is formulated as an optimization problem and thereafter a single objective based technique is developed for automatically determining the appropriate set of features from some supervised machine learning approaches. For single objective optimization genetic algorithm [8] is used as the underlying optimization technique.

Here we have used Conditional Random Field (CRF) [9] based classifier to find mentions of IUPAC and IUPAC-like names from scientific texts. GA is applied with CRF based classi- fier to determine appropriate set of features from individual classifiers. Finally GA results a set of solutions on the best population. The solutions on the best populations are then combined using a GA based classifier ensemble technique [10].

The proposed systems are evaluated on the benchmark patent dataset. Single objective based approach attains the overall F- measure values of 70.01% for patent test data set of 2008.

Comparative results show that the automatic feature selection technique is better than the systems with all features. The two stage approach (feature selection first followed by classifier ensembling) is much effective than only the feature selection based approach.



II. PROBLEM FORMULATION  In general, feature and parameter selection problem is formulated under the single objective optimization. It is stated as follows: Given a set of features ? and a classification quality measure D, determine the feature subset F ? such that:  D(F ?) = max F??  D(F )  In case of named entity recognition from chemical domain, the classification quality measure is F-measure. We have to select such feature combination for which corresponding F- measure value is optimized.

In general the search space for this type of problem is 2d, where d is the total number of possible features. Thus, exhaustive search strategies can not be applied in this case.



III. PROPOSED APPROACH  In this section we describe our proposed approach for chemical mention detection and classification in texts. The task is formulated with respect to the machine learning perspective where the overall task focuses on entities such as IUPAC and MODIFIER mentions. Chemical entities in general are named following different nomenclatures which are also combined by the authors of biomedical texts. Rather than concentrating only on the correct IUPAC terms, the definition is broadened by including any chemical substance mentioned in a IUPAC- like manner. Additionally it also includes IUPAC names in which a part is abbreviated, fragmented and group names. The proposed approach is based on a popular and robust machine learning technique namely Conditional Random Field (CRF).

A. Features for Chemical Name Identification  The success of any machine learning algorithm is based on the feature sets. We identify and implement a very rich set of features that exploit morphological, lexical, statistical and contextual information [5]. Most of these features are automatically extracted from the training data without using any domain dependent resources and/or tools. The below description of features is taken from [5].

Context words: These are the words occurring within the context window wi+3i?3 = wi?3 . . . wi+3, w  i+2 i?2 = wi?2 . . . wi+2  and wi+1i?1 = wi?1 . . . wi+1, where wi is the current word. We include this feature because contextual tokens carry effective information for identification of chemical names.

Word prefix and suffix. These are the word prefix and suffix character sequences of length up to n. The sequences are stripped from the leftmost (prefix) and rightmost (suffix) positions of the words. We set the feature values to ?undefined? if either the length of wi is less than or equal to n ? 1, wi is a punctuation symbol or if it contains any special symbol or digit. We set the feature values to ?undefined? if either the length of wi is less than or equal to n? 1, wi is a punctuation symbol or if it contains any special symbol or digit. We experiment with n=3 (i.e., 6 features) and 4 (i.e., 8 features) both.

Word length. We define a binary-valued feature that fires if the length of wi is greater than a pre-defined threshold. Here,  the threshold value is set to 5. This feature captures the fact that short words are likely not to be the chemical names.

Infrequent word. A list is compiled from the training data by considering the words that appear less frequently than a predetermined threshold, i.e. 10 in our current experiment.

Now, a feature is defined that fires if wi occurs in the compiled list. This is based on the observation that more frequently occurring words are rarely the chemical names.

Unknown token feature: This is a binary valued feature that checks whether the current token was seen or not in the training data. In the training phase, this feature is set randomly.

Word normalization: We define two different types of features for word normalization. The first type of feature attempts to reduce a word to its stem or root form. This helps to handle the words containing plural forms, verb inflections, hyphen, and alphanumeric letters. The second type of feature indicates how a target word is orthographically constructed.

Word shapes refer to the mapping of each word to their equivalence classes. Here each capitalized character of the word is replaced by ?A?, small characters are replaced by ?a? and all consecutive digits are replaced by ?0?. For example, ?IL? is normalized to ?AA?, ?IL-2? is normalized to ?AA-0? and ?IL-88? is also normalized to ?AA-0?.

Word class feature: Certain kind of chemical names, which belong to the same class, are similar to each other. The word class feature is defined as follows: For a given token, capital letters, small letters, numbers and non-English characters are converted to ?A?, ?a?, ?O? and ?-?, respectively. Thereafter, the consecutive same characters are squeezed into one character. This feature will group similar names into the same chemical class.

Orthographic features: We define a number of orthographic features depending upon the contents of the wordforms.

Several binary features are defined which use capitalization and digit information. These features are: initial capital, all capital, capital in inner, initial capital then mix, only digit, digit with special character, initial digit then alphabetic, digit in inner. The presence of some special characters like (?,?,?- ?,?.?,?)?,?(? etc.) is very much helpful to detect chemical names.

For example, many biochemical names have ?-? (hyphen) in their construction. Some of these special characters are also important to detect boundaries of chemicals. We also use the features that check the presence of ATGC sequence and stop words.



IV. GENETIC APPROACH FOR AUTOMATIC FEATURE SELECTION  In this paper a single objective GA is now used for solving the feature selection problem for any classifier.

A. Chromosome Representation and Population Initialization  If the total number of features is F , then the length of the chromosome is F . As an example, the encoding of a particular chromosome is represented in Figure 1. Here, F = 12 (i.e., total 12 different features are available). The chromosome rep- resents the use of 7 features for constructing a classifier (first, third, fourth, seventh, tenth, eleventh and twelfth features). The entries of each chromosome are randomly initialized to either     Fig. 1. Chromosome representation for SOO based feature selection  0 or 1. Here, if the ith position of a chromosome is 0 then it represents that ith feature does not participate in constructing the classifier. Else, if it is 1 then the ith feature participates in constructing the classifier.

If the population size is P then all the P number of chromosomes of this population are initialized in the above way.

B. Fitness Computation  We execute the following steps for fitness computation.

1) Suppose, there are N number of features present in a particular chromosome (i.e., there are total N number of 1?s in that chromosome).

2) Construct the classifier with only these N features encoded in the chromosome.

3) The training data is divided into 3 parts. The classifier is trained using 2/3 parts of the training data with the set of features encoded in the corresponding chromosome, and evaluated with the remaining 1/3 part.

4) The overall recall, precision and F-measure values of this classifier for the 1/3 training data are calculated.

5) Steps 2-4 are repeated 3 times to perform 3-fold cross validation. Then, the overall average recall, precision and F-measure values of the classifier are determined from this cross validation test.

In case of SOO, the objective function corresponding to a particular chromosome is f = F-measureavg . The objective is maximized using the search capability of genetic algorithm (GA). The objective is to maximize this objective function using the search capability of GA.

C. Other Operators  For single objective GA, normal single point crossover [11] is used. Here, mutation operator is applied to each feature entry of the chromosome where the entry is randomly replaced by either 0 or 1. Roulette wheel selection is used to implement the proportional selection strategy.

D. Combining the Solutions of the Final Population  Both the approaches are executed for some maximum number of generations. In case of SOO, we obtain a set of solutions on the best population. The final solution is the one with the best F-measure value. But some of the solutions on the best population are best with respect to recall values and some are best with respect to precision values. Thus instead of selecting a single solution we have used a classifier ensemble technique [10] to combine the solutions on the best population.

TABLE I. STATISTICS OF THE DATASETS  Set #abstracts #sentences #tokens #IUPAC  names  Training 463 3,700 161,591 3,712  Test set 26 152 4,309 411  (Patent)

V. DATA SETS  Chemical names in texts can be present in various forms, one standardized nomenclature comes from the International Union of Pure and Applied Chemistry (IUPAC) and forms a systematic way of naming organic chemical compounds that can be mapped to their structures. The below description of dataset is taken from [5].

Chemical entities in general are named following differ- ent nomenclatures which are also combined by the authors of biomedical texts. Rather than concentrating only on the correct IUPAC terms, the definition is broadened by including any chemical substance mentioned in a IUPAC-like manner.

Additionally it also includes IUPAC names in which a part is abbreviated, fragmented and group names. We used the datasets available at 1. The training set was collected from the Medline abstracts. The test data set is a collection of patent documents. The test dataset was annotated with seven classes, namely TRIVIAL (single word terms like aspirin, estragon, testosterone, Acetylsalicylate etc.), IUPAC (multi- word systematic names such as 1-hexoxy-4-methyl-hexane, 1,4-dihydronaphthoquinones etc.), PART (partial chemical names such as 8-( methylthio)-and . . . , 17beta- etc.), MODI- FIER, ABBREVIATION (abbreviations of names but not those that appear as part of IUPAC such as TPA, AMPA etc.), SUM (sum formulas like CH 3SNa, KOH etc.) and FAM- ILY (chemical families: disaccaride, pyrimidine, hydrazides; but not pharmacological/functional families:anti-inflammatory drug, chelator etc.). However, it is to be noted that training dataset has only the instances of IUPAC, PART and MODI- FIER classes. The test dataset contains the instances of all the seven classes. Statistics of the datasets are presented in Table I.

A. Experimental Results  Here a base classifier is used: conditional Random Fields (CRFs) [9]. We have used the C++ based CRF++ package 2, a simple, customizable, and open source implementation of CRF for segmenting or labeling sequential data. We set the following parameter values for genetic algorithm: pop- ulation size=100, number of generations=50, probability of mutation=0.2 and probability of crossover=0.9.

Here first the proposed SOO based feature selection tech- nique is applied for selecting relevant features from CRF based classifier. In case of SOO we have reported the best solution of the final population according to the F-measure value as well as we have combined the solutions on the final population by a genetic algorithm based classifier ensemble technique proposed in [10].

1http://www.scai.fraunhofer.de/chem-corpora.html 2http://crfpp.sourceforge.net     In this work, ?, i.e. the set of features contains the following: (i). Various context word window within the previous three and next three words (ii). Prefixes of length up to three (3 features) or four (4 features) characters (iii). Suffixes of length up to three (3 features) or four (4 features) characters, (iv). Length of the word (v). Infrequent word (vi). Unknown token feature (vii) word normalization feature (viii) word class feature (ix) several orthographic features and (x) dynamic NE information.

Initially, we construct following four baseline classifiers based on CRF frameworks using various random subsets of the above mentioned feature set. Here, C[?i,+j] denotes the context spanning from the previous ith word to the next jth  word with the current token at position 0; Prei and Sufi denote the prefixes and suffixes of character sequences up to i of the current word, respectively.

1) Baseline 1: CRF classifier trained using feature com- binations: C[?3,+3], Pre4, Suf4, and the features (iv)-(x) descried in Section III-A.

2) Baseline 2: CRF classifier trained using feature com- binations: C[?2,+2], Pre4, Suf4, and the features (iv)-(x) descried in Section III-A.

3) Baseline 3: All the classifiers of the best population generated after application of the proposed technique on CRF based classifier are combined using majority voting.

4) Baseline 4: All the classifiers of the best population generated after application of the proposed technique on CRF based classifier are combined using weighted voting.

Thereafter, we apply our proposed SOO based feature selection techniques (Section IV) to solve the problem of NER from chemical domain for a different classifier, CRF.

In the first step a CRF is trained using training data corpus from Medline and evaluation is done on an independent set of test data corpus. The recall, precision and F-measure values obtained by the proposed SOO based technique are also shown in Tables II for chemical test corpus prepared in 2008.

After application of the proposed SOO based approach on an annotated chemical test corpus prepared in 2008 we were able to obtain a F1 measure of 67.69% (recall = 62.12 and precision = 74.36).

After application of the SOO based feature selection technique for CRF based classifiers we obtain a set of best solutions on the final population. Based on the features of these solutions we have generated a set of CRF based classifiers.

Some of them are good with respect to recall and some are good with respect to precision. These solutions represent different feature combinations. By using these features we can again generate several CRF based classifiers. These CRF based classifiers are combined using a newly developed SOO based classifier ensemble technique [10]. Overall evaluation results along with the baseline models are reported in Table II for chemical test corpus prepared in 2008. Evaluation of our proposed feature selection algorithm shows the state- of-the-art performance for a test corpus. Final SOO based ensemble yields overall recall, precision and F-measure values of 63.45%, 78.08%, and 70.01%, respectively, for test corpus of 2008. Results show that SOO based feature selection is  better than automatic feature selection techniques. Table II shows that proposed approach is better than Baseline 1 and Baseline 2 for all the test data sets. Results on all the test corpus also prove that the use of SOO based ensembling to combine the solutions on the final population obtained after application of SOO based feature selection technique is more effective than the use of any other standard techniques like majority voting (Baseline 3) or weighted voting (Baseline 4).



VI. CONCLUSION  In this paper we have proposed a single objective based bio-chemical named entity recognition system. The possible performance is highly dependent on the quality and rep- resentativeness of the features that were mostly identified without using any domain dependent resources. In this work, a classifier, conditional random field (CRF), is used as the underlying classification technique. Then the problem of ap- propriate feature selection for any classifier is posed as an optimization problem. Thereafter a single objective based approach is developed for automatic feature selection from an underlying supervised classifier namely conditional random field (CRF). The proposed SOO based approach is based on genetic algorithm (GA). The proposed approach can determine the appropriate feature subsets which not only reduce the problem of overfitting but also optimizes the classification quality. We applied this technique for appropriate feature selection from a base classifier namely CRF. The proposed SOO based approach generates a set of solutions on the final best population. Solutions on the final best population are combined using a SOO based classifier ensemble technique.

The proposed systems are evaluated to solve the problem of NER from a chemical test corpus, namely 2008 test corpus. In future, we would like to add some domain dependent features.

Future works include use of some other well known classifiers like decision tree and memory based learner.

