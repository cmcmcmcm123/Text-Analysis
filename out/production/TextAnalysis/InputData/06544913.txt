Machine Learning on Big Data  Tyson Condie #1, Paul Mineiro #2, Neoklis Polyzotis *3, Markus Weimer#4

Abstract?Statistical Machine Learning has undergone a phase  transition from a pure academic endeavor to being one of the main  drivers of modern commerce and science. Even more so, recent  results such as those on tera-scale learning [1] and on very large  neural networks [2] suggest that scale is an important ingredient  in quality modeling. This tutorial  introduces current applications,  techniques and systems with the aim of cross-fertilizing research  between the database and machine learning communities.

The tutorial covers current large scale applications of Machine  Learning, their computational model and the workflow behind  building those. Based on this foundation, we present the current  state-of-the-art in systems support in the bulk of the tutorial. We  also identify critical gaps in the state-of-the-art. This leads to the  closing of the seminar, where we introduce two sets of open  research questions: Better systems support for the already  established use cases of Machine Learning and support for recent  advances in Machine Learning research.



I. INTRODUCTION  The democratization of data has fueled the collection of (even  more!) massive data sets. Platforms for large-scale analytics are  being built for extracting insights that enhance services and  optimize operations. However, the current suite of platforms  can only capture a small fraction of machine learning  algorithms at scale. Those that fall out of scope induce solutions  that abuse the intended programming model or motivate the  implementation of a separate system. This leads to production  pipelines built out of bailing wire and glue code for moving  data in-and-out of different subsystems. The problem is further  complicated by the assumption that each system assumes  ownership of the entire machine, or at best the VM, forcing  administrators to divide the cluster into machine subsets;  assigning a single system to each. Resource negotiators like  YARN [3] and Mesos [4] are being built to address this  problem with a thin virtualization layer that multiplexes higher-  level systems on a single unified machine cluster.  Yet, the data  scientist is still left with the problem of finding the right system  and programming model for their data-analysis task, assuming  one even exists, and staging their solution in production.

We believe that the database community can provide the  appropriate data management toolkits for the data scientist to  operate at scale. Large-scale (distributed) machine learning  techniques involve many of the same issues parallel databases  faced decades ago; specifically, efficient shared-nothing  architectures that support a relational algebra programming  model. Machine-learning algorithms also have characteristics  (most notably recursion) that fall directly in the scope of  deductive database query processing techniques. And finally,  we see a longer term need for real-time learning support, which  will leverage work from streaming database systems.

This seminar aims to inform the database community about  typical use cases and workflows in the machine-learning  domain. One goal in particular is to highlight the differences to  traditional data processing and thus to outline potential research  topics. Examples include:  ? The need for iteration: Machine Learning algorithms almost exclusively are iterative.

? Non-standard fault tolerance policies: Some machine learning algorithms can deal with unavailable  partitions.

? Aggregation functions over very large objects: many machine-learning algorithms involve the aggregation  of objects whose size is in the same order of  magnitude as the available main memory.

We hope that this seminar adds to the ongoing alignment of  machine learning and database research. In fact, presenters of  this seminar have in the past taken part in similar tutorials to  both the machine learning and database communities both  within companies and universities as well as at conferences.

Most recently, Markus Weimer co-presented the tutorial on  ?New Templates for Scalable Data Analysis? [5] at WWW  2012, which shares some content with this seminar proposal.



II. TARGET AUDIENCE  The seminar is targeted at database-systems practitioners and  researchers who are interested in the application of machine  learning in Big Data analytics as well as research on systems  that support large scale machine learning. The seminar does not  require prior familiarity with machine learning techniques.



III. SEMINAR OUTLINE  The seminar is structured into three sections, moving from  machine learning practice and theory, to current systems  support for machine learning at scale, to a research agenda for  the database community.

A. Machine Learning Applications and Practice  In this part of the seminar, we present a quick overview over  the theory and, more importantly, practice of machine learning.

The goal of this section is to give the audience a grasp of the  typical uses of and challenges faced by machine learning in  practice.

Drawing from the industrial expertise of the presenters in large  and small machine learning deployments, we develop a  prototypical workflow of machine learning projects. This  workflow consists of three phases: Example Formation,  Modeling and Deployment. We ground the modeling phase in  the machine learning literature, and introduce the Statistical  Query Model [6] that underlies many machine learning  algorithms and their scalable implementations [1] [7]. We also  discuss the data plumbing that goes into a typical machine  learning project as well as part of the feature extraction and the  deployment phases of the workflow.  We present this workflow  using example applications familiar to the audience such as  email spam filtering and server-hardware fault prediction.

B. Systems support for (large scale) Machine Learning today  In this section, we discuss the various tools and systems used  by machine learning practitioners today in more detail. We  cover this material according to the prototypical workflow  introduced above: (a) Example Formation, (b) Modeling and  learning and (c) Deployment.

For Example Formation, we cover the use of Pig [8],  MapReduce [9], databases and scripting.

The tools for Modeling and learning are discussed based on  their computational model. We start with relational algebra  inspired systems: MapReduce and its use in Mahout [10] and  Spark [11]. We then introduce approaches that are grounded in  numerical computation using VW [1] as the example. Lastly,  we discuss graph processing inspired approaches: Pregel [12]  and its Open Source implementation Giraph [13] and GraphLab  [14].

Deployment is very application-specific, but we cover  examples from the domains of recommender systems and  spam-filters for email, in addition to the running example of  machine fault prediction.

We close this section with a discussion of some of the gaps in  current systems with respect to their support for large-scale  machine learning.

C. Open Research Issues  In this last section, we outline open problems for large-scale  machine learning and how they relate to database research. This  section is split into two aspects:  Better systems support for the approaches covered in this  seminar: Those open problems include: (1) Query and runtime  optimizers for iterative, distributed programs. (2) Declarative  specifications and optimizers for asynchronous computations  and (3) Semantic models of non-standard fault tolerance  policies. While each of these has been studied in the past, the  specifics of machine learning computation and the latter's  importance in Big Data analysis make it worthwhile directing  the attention of the database community towards them.

Systems support for recent results in machine-learning  research: Approaches such as deep belief networks [15] [16]  and graphical models have recently received lots of attention in  the machine learning community [2] and even the mainstream  media [17]. However, their systems support is currently  severely lacking due to their high communication load. We give  an overview of those challenges and the current state of the art  solutions, as well as their shortcomings.

It is our hope that this section fosters the convergence of  machine learning and databases inside big data systems.



IV. BIOGRAPHIES  A. Tyson Condie  Tyson Condie is a principal scientist with the Cloud and  Information Services Lab at Microsoft. He received his Ph.D.

in Computer Science from the University of California at  Berkeley. His research interests center around data analytics,  distributed systems, Internet-scale query processing and  optimization, and declarative language design and  implementation. His current work focuses on building a cloud  system stack that supports machine learning and ETL  workflows.

B. Paul Mineiro  Paul Mineiro is a research software engineer with the Cloud and  Information Services Lab at Microsoft.  He has a decade of  experience in internet-scale machine learning for  computational advertising and social networking, including  positions at Overture, Yahoo, eHarmony, and Ubermedia.  His  current work focuses on scalable distributed methods for  feature engineering and feature selection.

C. Neoklis Polyzotis  Neoklis Polyzotis is an associate professor at UC Santa Cruz.

His research focuses on on-line database tuning, scaling  machine learning to large data, and declarative crowdsourcing.

He is the recipient of an NSF CAREER award in 2004 and of  an IBM Faculty Award in 2005 and 2006. He has also received  the runner-up for best paper in VLDB 2007 and the best  newcomer paper award in PODS 2008. He received his PhD  from the University of Wisconsin at Madison in 2003.

D. Markus Weimer  Dr. Markus Weimer is a principal scientist with the Cloud and  Information Services Lab of Microsoft. He works on systems  for large-scale machine learning. Prior, Markus worked at  Yahoo! Labs where he researched scalable machine learning  algorithms, declarative languages to express those in and the  systems to execute them. He applied his research to the Yahoo!

Mail spam filter where it is part of a product used by hundreds  of millions of people. In 2011, Markus co-organized KDD Cup  [18], the largest data mining competition held yearly as part of  ACM?s KDD conference. Prior to his time at Yahoo!, Markus  worked on Recommender Systems and how to apply the  learning-to-rank mechanism in that domain. He received his  PhD from TU Darmstadt in 2009.

