

Third International Symposium on Information Science and Engineering  An Algorithm of Association Rules Mining Based on Restricted Conditional Probability Distribution  Fasheng Liu Wenliang Cao Department of Computer Engineering  DongGuan Polytechnic Dongguan, Guangdong, China  CWL2222@tom.com  Xuanzi Hu Faculty of Information Engineering Jiangxi University of Science and  Technology Department of Computer Engineering  DongGuan Polytechnic Dongguan, Guangdong, China  huxuanzi@I26.com Ganzhou, Jiangxi, China  fashengliu@mail.com  Ahstract-There are excessive and disordered rules generated by traditional approaches of association rule mining, many of which are redundant, so that they are difficult for users to understand and make use of. Agrawal et al pointed out the bottleneck of transaction number increase association rules according to the index increase. To solve this problem, a new method was represented, which is based on restricted conditional probability distribution to get a condensed rules set by removing redundant rnles. Our set of rules is more meaningful, more concise and users are interested in than others. Especially, the number of rules in rules-set has been reduced greatly. We find that it is an effective method of association rules mining from examples, finally poses future research.

Klrywords-Data Mining; Association Rules; Restricted Conditional Probability Distribution  1. INTRODUCTION  Data mmmg is the process of abstracting unaware, potential and useful information and knowledge from plentiful, incomplete, noisy, fuzzy and stochastic data [1]. It is a multidisciplinary fields, including database technology [2], artificial intelligence, machine learning, neural networks, statistics, pattern recognition, knowledge-based systems, knowledge acquisition, information retrieval, high- performance computing, and data visualization. Data mining functionalities are used to speciiY the kind of patterns to be found in data mining tasks. In general, data mining tasks can be classified into two categories: descriptive and predictive.

Descriptive mining tasks characterize the general properties of the data in the database. Predictive mining tasks perform inference on the current data in order to make predictions.

Early data mining applications focused mainly on helping businesses gain a competitive edge. As data mining becomes more popular, it is increasingly used for the exploration of applications in other areas, such as biomedicine [3], financial analysis, DNA analysis, financial analysis, the retail industry, and telecommunications. In addition, the exploration of data mining for businesses continues to expand as e-commerce and e-marketing become mainstream elements of the retail industry.

In this paper, We describe an algorithm for mining association rules with restricted conditional probability distribution, which reduces the redundancy rules greatly, we find that it is an effective method of association rules mining from examples, some future research are provided as well.

DOl 10. 1109/ISISE.2010. 130

II. BRIEF INTRODUCTION TO ASSOCIATION RULES  Association rules mining [2] is becoming the core issues in data mining. Association rules mining finds interesting association or correlation relationships among a large set of data items. With massive amounts of data continuously being collected and stored, many industries are becoming interested in mining association rules from their databases.

The discovery of interesting association relationships amounts of business transaction records can help in many business decision making processes, such as catalog design, cross-marketing, and loss-leader analysis [3]. A typical example of association rules mining is market basket analysis.

The original definition of association rule mining [4, 5] by Agrawal, Imielinski, Swami is defined as: Let I = {i1, i2 , ?? " in} be a set of n binary attributes called items. Let D = {tl , t2 , ??? , tm } be a set of transactions called the database. Each transaction in D has a unique transaction ID and contains a subset of the items in I.

A rule is defined as an implication of the form X => Y where X, Y <:;;; I and X n Y <:;;; <l> . The sets of items X and Yare called antecedent and consequent (right-hand-side) of the rule respectively. The rules X=> Y holds in the transaction set D with support s, where s is the percentage of transactions in D that contain X u Y. This is taken to be the probability, p(X u Y). The rules X=> Y has confidence c in the transaction set D if c is the percentage of transactions in D containing X that also contain Y. This is  taken to be the conditional probability, p(YIX). That is,  sup port(X => Y) = p(X u Y) (1) conjidence(X => Y) = p(YIX) (2)  Rule support and confidence are two measures of rule interestingness. They respectively reflect the usefulness and certainty of discovered rules. Typically, association rules are considered interesting if they satisfy both a minimum support threshold and a minimum confidence threshold.

Such thresholds can be set by users or domain experts [6].

Many algorithms for generating association rules were presented over time. Such as Apriori, Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets, each of these itemsets will occur at  IEEE ~computer  SOCiety    least as frequently as a pre-determined minimwn support COllllt. Another step needs to be done after to generate rules from frequent itemsets fOlllld in a database, these rules must satisfy mmnnwn support and minimwn confidence.

Additional interestingness measures can be applied, if desired. The second step is the easiest of the two. The overall performance of mining association rules is determined by the first step. Apriori [7] is the best kno"\iVll algorithm to mine association rules. It uses a breadth first search strategy to COllllting the support of itemsets and uses a candidate generation fllllction which exploits the dO"\iVllward closure property of support.

Apriori [3] is an influential algorithm for mining frequent itemsets for Boolean association rules. The name of the algorithm is based on the fact that the algorithm uses prior knowledge of frequent itemset properties, as we shall see below. Apriori employs an iterative approach known as a level-wise search, where k-itemsets are used to explore(k+l)-itemsets. Figure 1 shows preudocode for the Apriori algorithm and its related procedures. Step 1 of  Apriori finds the frequent l-itemsets, 4. In steps 2-10, L k _1 is used to generate candidates C k in order to find L k .

The apriori -..Een procedure generates the candidates and then uses the Apriori property to eliminate those having a subset that is not frequent(step 3). Once all the candidates have been generated, the database is scanned(step 4). For each transaction, a subset fimction is used to find all subsets of the transaction that are candidates(step 5), and the COllllt for each of these candidates is accumulated(step 6 and 7). Fimlly, all those candidates satisfying minimwn support form the set of frequent itemsets, L . A procedure can then be called to  enerate association rules from the frequent itemsets.

Algorithm: Apriori, Find frequent itemsets usmg an iterative level-wise approach based on candidate generation.

Input: Database, D, of transactions;minimwn support tbreshold,min _sup.

Output L, frequent itemsets in D.

Method: I L,. ~findJrequent_l-itemsets(D) 2 for (k=2; L k _ 1 '" <I> ; k++)  3 Ck=apriori_gen(Lk_1 , min_sup);  4 for each transaction tED {  5 Ct=subset(Ck , t):/I  6 for each candidate c E Ct 7 c. cOllllt++:}  8 Lk = { c E Ckl c. count ;;>min_sup}  9 return L = ukLk ;  Figure I The Apriori Algorithm

III. ALGORITIlM DETAILS  In the process of mining association rules, with the transaction to increase the nwnber of association rules set to increase exponentially, particularly support and confidence thresholds when small. To avoid this situation, we can add some constraints into the algorithms for mining association rules. Based on the above point, this paper details the means of restricted conditional probability distribution to constrain the mining association rules algorithm [8].

A. Some Basic Concepts  Restricted conditional probability distribution, abbreviated as RCPD : Let X and Y are two disjoint sets of  variables, a conditional probability distribution P (r IX ), a restricted conditional probability distribution p (fiX) is a  subset of the P (y IX ) by specifYing Domoin(X) c;; Domoin(X) and  Domoin (Y) c;; Domoin (Y), The following with P (Y IX) instead of p (Y IX) .

Minimal conditional subset, abbreviated as MCS : Let X and Y are two disjoint sets of variables, if x is a subset of X and the following three conditions are met, then we say x is the minimal conditional subset of X on Y.

1) PcyIX) ~ PcyIX')  2) VZ c;; X - X', PcyIX) = PcyIX',Z).

3) Does not exist X" c X' satisfy the above conditions.

Minimal conditional probability distribution, abbreviated  as MCPD : Let X and Y are two disjoint sets of variables, and x is the minimal conditional subset of X on Y. The restricted conditional probability distribution p (fiX ') is the  minimal conditional distribution of p (fiX) on Y. If X is the minimwn on Y, then p(fIX) is the minimal conditional  probability of its own [9].

B. Detoiled Process of the Algorithm  We designed the algorithm mainly include the following steps:  1) Choose a constraint attribute item i.

2) According to the Apriori algorithm for mining  association rules generate the frequent item sets which include item i and stored in L I .

3) Use the algorithm GenAR for generating association rules. GenAR is the use of restricted conditional probability distribution to find association rules.

4) Remove redlllldant association rules. If a frequent itemset is another frequent itemsets subset, then it generates the association rules is redlllldant and should be deleted.

Figure 2 shows pseudocode for the GenAR algorithm and its related procedures.

Algorithm: GenAR(L'). i contains the given item from the collection of L' frequent itemsets generated AR.

Input: L' , an include i collection of frequent item sets, L' c L', k All entries contain L' set of items.

Output: AR, constrained association rules.

Method:  1 AR=<I>  2 for each 1 E L'k , k 2': 2 3 AR = AR u GenAC(L')  4 AR = Re move(AR)  5 return AR Figure 2 The Algorithm of GenAR  From the collection of frequent itemsets L' generated AR. For each frequent item i in the set, call the algorithm GenAC(Figure 3) corresponding to the constraints i generated association rules AR. RemoveO algorithm used to delete the redlllldant association rules.

Algorithm: GenAC(I). Generate association rules on I constraint.

Input: I, A frequent itemset.

Output: R, A association rules corresponding to the  constraints 1.

Method:  1 R=<I> 2 for item i E 1 3 1'=1 -{i}  4 S = MinSubsets U,I')  5 for each S E S  6 R=Ru{s~i}  7 return R  FIgure 3 The Algonthm of GenAC  Algorithm MinSubsets (Figure 4) Find the Minimal conditional subset of l' on i, so generate association rules corresponding to the set 1. First, determine whether p = PU I 1') 2': min can! meeting, to initialization S = {1'} . Function MaxSubSets is mainly used to generate all the largest collection subset S' of ]' on i as a candidate MCS, for example, S'=MaxSubsets(ACDE)={ACD,ACE,CDE} , this fllllction is very simple, no specific given, and verified S' in the while loop of the effectiveness of candidate MCS. For any S E S' s <S " the algorithms calculate the  conditioml probability pUis) compared with p, if they are equal, then s is a MCS of ]' on the i, s is stored in S , if there are smaller effective candidate MCS on ]' , then the   fimction DelSuperset to remove the their parent set in S .

Fllllction IntersectionSet (S',k) generates a smaller candidate MCS on l' , according to the depth of the loop k they are intersection of itemsets in S' . For example, two itemsets ACD and ACE of the intersection is AC, then AC is aMCS.

Algorithm: MinSubsets(I, l' ). Calculate a mmlffium condition subset S of ]' on i.

Input: i, and a meet the i u]' is a frequent item set of items ]'.

Output: S, a minimwn condition subset S of ]' on i.

Method:  1 S=<I>  2 P = sup(I'u{i}) / sup(I') 3 if( p 2': min can! lthen  4 S={I'}  5 S'=MaxSubsets(I')  6 k= 1 7 while(S'*<I? (  8 for each S E S'  9 PI = sup(s u {i})/ sup(s) 10 if( PI = P lthen  11 S=Su{s}  12 else S'= S'-{s}  13 for each S E S' 14 DeISuperset(s,S)  15 S'=Intersection(s',k)  16 k = k + 1J 17 return S  Figure 4 The Algorithm of MinSubsets Fllllction DelSuperset is mainly used to delete s' from  S while s' E S 1\ s' :::::> s .



IV. SAMPLE DESCRIPTION  Let's look at a concrete example of above algorithm, based on the transaction data set, of Table 1.

TABLE!. TRANSACTION DATA SET  TID Items 001 ACDE 002 CDE 003 ACDE 004 ABCD 005 BDE 006 ABD 007 BEF    asswning that the minimwn support is 0.3, minimwn confidence is 0.6, and select the item E as Constrained  v. CONCLUSIONS AND FUTURE RESEARCH In this paper, association rule mining problems arising in  Properties, the process of generating association rules is: the process to proceed, a mining method of increasing First find the frequent itemsets L' that contains item E, constraints, leading to faster discovery of interesting L'= {{E},{DE},{CE},{ADE},{ACE},{CDE},{ACDEMociation rules, and reduce redundant rule emerged, , then use the GenAR algorithm to generate association experiments show that the method is very effective.

rules. Suppose we select from L' the ] = {ACDE} , Because the algorithm can only select one attribute  algorithm GenAC first is called to generate constraint association rules. i=E, ]' = ACD , call MinSubsets algorithm to generate MCS of ACD on E. calculation peE I ACD) = 0.667 , P (E I ACD) ~ min can!

S={I'}={ACD} S'=MaxSubset(I')={AC,AD,CD} , In the while loop, check S' whether the items in the collection is MCS ACD on E, When AC checked, P (E lAC) = 0.667 = P (E I ACD) ,so we get S = {ACD, AC}, forP(EI AD)* P(EI ACD) and peE I CD) * peE I ACD) , then we took AD and CD removed from S', S' = {AC} , after call DelSuperse~ S = {AC} ,as the only one of S' during  S'= Inter sectionSet(S', k) = <I> , the loop ends and S = {AC} returned to the GenAC, association rules  {AC ~ E} were put in R. Similarly, other items in the collection of set L' , except for the one items, the same operation. The end result sho"\iVll in Table 2.

TABLE II. THE SET OF BASIC ASSOCIATION RULES  Constrained basic  10 Frequent Set association Redundancy rules Items  I C(DE) D~E  2 C(CE) C~E yes 3 C(ADE) 4 C(ACE) AC-4-E yes 5 C(CDE) C~E  6 C(ACDE) AC-4-E  We can seen from Table 2 that CE c CDE, and C(CE) <;; C(CDE) , then put C(CE) deleted. This reduces the nwnber of rules generated, enen reducing the generation of redundant rules. The resulting set of rules is { D-4-E, C-4-E, AC-4-E}.

constraints, the final result of association rules fOlmd in only a single item, whether can to select multiple attribute constraints, so that the results of a nwnber of association rules, further research is needed in the future.

