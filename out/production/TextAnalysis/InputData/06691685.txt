Advancing value creation and value capture in data-intensive contexts

Abstract?.Realizing the vast potential for value creation that Big Data has to offer to firms and public agencies requires a radical departure from the traditional data warehouse model currently in place in most organizations. Given the inability of current approaches to integrate the four dimensions of volume, variety velocity and veracity into a single and coherent framework, new business models around the Big Data paradigm will likely be developed in a collaborative regime in which technology firms, public entities and end customers will organize around ecosystems, strategic partnerships or private- collective modes of technology development and commercialization.

Keywords-component; business models, Big Data, 4Vs, value creation.

I. BIG DATA: OLD WINE IN A NEW BOTTLE ?

Big data has been used to convey all sorts of concepts, including: enormous quantities of data, social media analytics, unstructured data analysis, next generation data management capabilities, real-time response, and online data management among others. McKinsey [1], for example, refers to Big Data as datasets whose size is beyond the ability of typical database software tools to capture, store, manage, and analyze.

One of the most commonly recognized applications of big data is social media data analysis, probably due to the extensive press coverage about social network impact on customer behavior and experiences. It would be easy to conclude that big data means social media data, but this assumption would miss to capture both existing industrial applications as well as the potential of the Big Data paradigm.

Recently, it has emerged the use of Big Data techniques by national intelligence offices worldwide [2,3]. In fact organizations have already been handling big data for years.

A global telecommunications company for example, collects billions of detailed call records per day from 120 different systems and stores each for at least nine months [30].

Big Data also refers to the huge potential to create value in unrelated contexts to social media such as in genomics [4], public service delivery [5, 6] or cities [7]. Smart meters and smart grid devices installed across the US electric grid will produce more than 400 terabytes of new data every day [29].

It is foreseen to have 50 billion of connected devices by 2025 [8]. Some of that information will be part of mission critical  systems for the society like electrical grid or telecommunications system.

Big Data is not only about data storage, in addition to the storage challenge,companies have to deal with the online response time in a handful of situations (e.g. subscribers roaming away, mobile advertising, changing peaks in electricity demand, or influxes of new passengers on an underground station). In all these cases the system has to be aware and keep track of multiple variables. For many companies, therefore, the concept of big data is not new; what is the new the ability to perform large-scale analy,tics.

In this sense the Big Data paradigm refers not to a new method but the expansion of an existing one.

Big Data is often related to economic and social value creation. It remains unclear, however, the manner in which Big Data may create value for firms and public agencies.

Further research is also required to understand how part of that value can be captured by citizens (oftentimes the ones providing the data) as well as by private firms. Both value creation and value capture mechanisms are key elements for sustainable business models [9] and markets for technology [10].

Notwithstanding some of the hype, it is commonly agreed that we are in the early stages of enterprise big data adoption. In this paper, we refer to big data adoption to represent the evolution of the data, sources, technologies and skills that are necessary to create a competitive advantage in the globally integrated marketplace. We foresee that companies and institutions able to incorporate the skills to integrate, correlate and analyze corporative and worldwide data will gain a competitive advantage in the big data race.

By following this traditional approach firms are assuming that Big Data is just a bigger data warehouse where you already know what you are looking for and the answer is just ?hidden? in a larger data set, these firms assume that it is like swimming in a bigger pool, but still business as usual [34].

However, two important trends render invalid the business as usual approach to the Big Data paradigm: (1) The digitization of virtually ?everything? now creates new types of ?large? and real-time data across a broad range of industries. Much of this is non-standard data: for example, streaming, geospatial or sensor-generated data that does not fit neatly into traditional, structured, relational warehouses,(2) today?s advanced analytics and machine learning technologies enable organizations to extract valuable knowledge from data with previously unknown      levels of complexity, speed and accuracy. Technologies like Stream/online Analytics, embedded Analytics, automatic knowledge reinforcement, scalable frameworks among others, provide answers to new and exciting challenges.

Moreover, and contrary to the traditional data Warehouse paradigm in which the data was created and maintained by single firms, in the case of Big Data several stakeholders are usually involved. For example, in so-called smart cities citizens and public agencies provide the data, which is in turn processed and delivered by third parties. As a result conventional approaches to create value (e.g. a multinational harvesting data from customer?s purchases) and capture value from it (e.g. the same multinational conducting customer profiling and segmentation) will not suffice in presence of heterogeneous and unrelated parties [31-33].

This would call for novel organizational arrangements either in the form of ecosystems [11-13], strategic partnerships [14,15] or private-collective modes of technology development and commercialization [16-18].

The purposes of this paper are therefore threefold, (1)  situate the concept of business model in the context of the Big Data paradigm, (2) identify the lack of convergence of the three dimensions volume, variety and velocity as a main limitation of existing business models and (3) describe four technologies which are required to fully realize the potential of Big Data to create economic and social value.



II. DEFINING THE BIG DATA PARADIGM Probably the origin of the confusion about big data  comes from the definition itself. One valid definition for big data is described by ?today?s extreme increase of available data, from new sources and formats. The necessity to combine them with existing data repositories in order to create complex knowledge sources and the advance of analysis techniques able to extract business insights from new data environment.?   From a strategic and managerial point of view such  definition needs further refinement. In this sense it is useful to characterize the Big Data paradigm according to the well- know dimensions of: Volume, Variety, Velocity and Veracity [19].

Volume. Refers to the amount of data to be processed and analyzed. Perhaps the characteristic most associated with big data, volume denotes to the mass quantities of data that organizations are trying to harness to improve decision- making process across the organization.

Variety. Different types of data and data sources. Variety is about managing the complexity of multiple data types, including structured, semi-structured and unstructured data.

Organizations need to integrate and analyze data from a complex array of both traditional and non-traditional information sources, from within and outside the company.

In a connected environment, the data correlation between inside and outside data is as important as internal analytics.

With the explosion of sensors, smart devices and social collaboration technologies, data is being generated in  countless forms, including: text, web data, tweets, sensor data, audio, video, click streams, log files and more.

Velocity. Data in motion. The speed at which data is created, processed and analyzed continues to accelerate.

Contributing to higher velocity is the real-time nature of data creation, as well as the need to incorporate streaming data into business processes and decision-making. Velocity impacts latency ? the lag time between when data is created or captured, and when it is accessible. Today, data is continually being generated at a pace that is impossible for traditional data base systems to capture, store and analyze.

For time-sensitive use cases where real-time is a key for the success (location aware applications, automatic grid accommodation, real-time fraud detection), or those which data generation is too high and not all data necessary represent useful information (Unattended intelligent monitoring systems, sensor monitoring) the online data management without full data storage involved technology is required.

Veracity. Talking about veracity in big data is not just about technology and data correlation but it is more related to the human condition instead. Some data is inherently uncertain, sentiment in humans; weather conditions or economic factors. Dealing with these types of data, there is no data cleaning technique able to correct it, at least not today. However, despite uncertainty, the data still contains valuable information. Apparent uncertainty manifests itself in big data in some ways. It is in the uncertainty that surrounds data created in human environments like social networks; in the unknowingness of how the future will unfold and of how people, nature or unseen market forces will react to the variability of the world around them.

In order to manage uncertainty, models around the data are needed. One way to achieve this is data aggregation, where combining multiple less reliable sources creates a more accurate and useful data point, such as social comments appended to geospatial location information.

Another way to manage uncertainty is through advanced mathematics that embraces it, such as robust optimization techniques and fuzzy logic approaches   While these dimensions cover the key attributes of big data itself, it is the convergence of these four dimensions into a single coherent entity what characterizes the potential of Big Data to open up new value creation avenues for firms and public agencies.

This integration enables companies to transform the ways  they interact with and serve their customers, and allows organizations ? even entire industries ? to transform themselves. The next section investigates how value is captured and created from the perspective of business models.



III. BUSINESS MODELS IN BIG DATA CONTEXTS Formal definitions of a business model are still lacking  [20]. According to Zott [21] a business model is a system of interdependent activities that transcends the focal firm and spans its boundaries. Gambardella [22] defines it as a mechanism for turning ideas into revenue at reasonable cost, whereas Casadesus [23] considers a business model as the logic of the firm, the way it operates and how it creates value for its stakeholder.

According to Teece [9], a business model describes the design or architecture of the value creation, delivery and capture mechanisms employed. The essence of a business model is that it crystallizes customer needs and ability to pay, de nes the manner by which the business enterprise responds to and delivers value to customers, entices customers to pay for value, and converts those payments to pro t through the proper design and operation of the various elements of the value chain.

An important characteristic of a business model is the interdependency of the focal firm with external resources and stakeholders [24], this is especially relevant in the case of Big Data as: (1) data is produced, owned and processed by different parties, (2) strong network effects are likely to emerge [25]. Therefore it is likely that business models around Big Data will result from the evolution of current platforms, in high-tech industries such as Internet services, mobile or cloud computing [11, 12, 26].

As much data you are able to capture, more qualified  your decisions will be. Organizations engaged in big data require increasingly more advanced capabilities to find consumption or even movement patterns in order to target the advertising campaigns and increase the efficiency.

Advertising companies used to be concerned about one  question. What is the effect of my last campaign? Can I follow the impact in real-time? Answering that question in an accurate manner (i.e. veracity) is not a simple task. It requires combined structured and unstructured data analysis.

It implies things like measuring how many people access to my website or how many people are talking about my campaign in the social networks.

For many companies is not just about what but when.

Fraud is a very real challenge for many companies in many industries around the world. If our credit card or our mobile phone gets stolen, we do not want our bank or telecom operator to do something about the day after. We want the companies to be able to detect that an abnormal pattern is happening and do something about that now.

Firms wanting to develop business models around the Big Data paradigm can opt for two options, (1) replicate and further extend what is being done by leaders such as Google, Amazon, or (2) Develop new business models based on the integration of all (or part of) the 4Vs.

In regards to this second strategic choice, the firm must be able to put in place technologies that supports the rapidly  growing volume, variety and velocity of data created within the organization (e.g. customer purchases) as well as outside the organization (e.g. weather forecast, social media, medical data).

In order to enact novel ways of value creation and value capture around the Big Data paradigm, we contend that four main information management components are required, (1) Stream processing, (2) Stream analytics, (3) Scalable storage processing and (4) Data Orchestration.

A. Stream processing As previously defined in this document, big data comes  from the necessity to offer a technical solution to the increasing rate from sensor applications, measurements in network monitoring and traffic management, log records, manufacturing processes, call detail records, email, blogging, twitter posts and others.

All those applications share a commonality; all of them require streaming data management systems. In fact, all data generated can be considered as streaming data or as a snapshot of streaming data, since it is obtained from an interval of time.

In time sensitive applications, the discussion goes around Complex Event Processing (CEP) and the benefits of its capabilities to process huge amount of flowing data avoiding to store everything. CEP is the analysis of event data in real- time to generate immediate insight and enable instant response to changing conditions. Likewise, a seemingly similar concept is that of Business Rules engine. These again work in enterprises and help in triggering actions on the basis of the set of rules defined.

But the question emerges: how is CEP different from a Rules Engine (RE)? One main difference is that RE are stateless. An input payload is expected for a RE which is processed and output is produced. CEP engines are state-full and can be persistentin the case of failure. Another difference is that RE expects a single payload of data while as CEP can accept data and output data to multiple channels.

CEP works on real-time event data which is coming into the system where as RE would working on stored data or payload data (now some RE?s support pseudo real-time event data too) Hence RE should be used where the situation does not need to be state-full and there is no need for either real-time events or time based event correlation. CEP and Business Rules can work together and complement each other.

B. Stream Analytics Online learning algorithms are an important type of  stream processing algorithms: In a repeated cycle, the learned model is constantly updated to reflect the incoming examples from the stream. They do so without exceeding their memory and time bounds. After processing an incoming example, the online learning algorithms are always able to output a model. Typical learning tasks in stream scenarios are classification, outlier analysis, and clustering.

In data stream scenarios data arrives at high speed strictly constraining processing algorithms in space and time. To adhere to these constraints, specific requirements have to be     fulfilled by the stream processing algorithms that are different from traditional batch processing settings. The most significant requirements are the following:   ? Requirement 1: Process an example at a time, and  inspect it at most once ? Requirement 2: Use a limited amount of memory ? Requirement 3: Work in a limited amount of time ? Requirement 4: Be ready to provide with a  prediction model at any time   In the context of data streams the learning algorithms need to be capable of handling very large (potentially infinite) streams of examples.

C. Scalable storage and processing During the last 10 years researchers and organizations  have attempted to tackle the big data problem from many different angles. The angle that is currently leading the pack in terms of popularity for massive data analysis is an open source project called Hadoop.

Hadoop was inspired by Google?s work on its Google Distributed File System (GFS, the base for Big Table paper) and the MapReduce programming paradigm, in which work is broken down into mapper and reducer tasks to manipulate data that is stored across a cluster of servers for massive parallelism. Hadoop is generally seen as having two main parts:  ? A file system (the Hadoop Distributed File System) ? A programming paradigm Map-Reduce which is  considered the heart of Hadoop.

The term MapReduce actually refers to two separate and  distinct tasks that Hadoop programs perform. The first is the ?Map? task, which takes a set of data and converts it into another set of data, where individual elements are broken down into tuples (key/value pairs). The ?reduce? job takes the output from a map as input and combines those datatuples into a smaller set of tuples. As the sequence of the name Map-Reduce implies, the ?reduce? job is always performed after the ?Map? job.

Unlike traditional transactional systems, Hadoop is designed to scan through large data sets to produce its results through a highly scalable, distributed batch processing system. Hadoop is not about immediate response times, in- memory real-time warehousing, online data management or blazing transactional speed; it is about discovery and making the once near-impossible possible from a scalability and analysis perspective.

The Hadoop methodology is built around a function-to- data model as opposed to data-to-function; in this model, because there is so much data, the analysis programs are sent to the data.

There are a number of Hadoop-related projects. Some of the more notable include: Apache Avro (for data serialization), Cassandra and HBase (databases), Chukwa (a monitoring system specifically designed with large distributed systems in mind), Hive (provides ad hoc SQL- like queries for data aggregation and summarization),  Mahout (a machine learning library), Pig (a high-level Hadoop programming language that provides a data-flow language and execution framework for parallel computation), ZooKeeper (provides coordination services for distributed applications), among others.

D. Data orchestration Integrated information is a core component of any  analytics effort, and it is even more important with big data.

An organization?s data has to be readily available and accessible to the people and systems that need it.

Master data management and the integration of key data types ?sensor, customer, product, vendor, employee and the like? require cross-enterprise data that is governed according to a single enterprise standard. The inability to connect data across organizational and department silos has been a business intelligence challenge for years. This integration is even more important, yet much more complex, with big data [34].



IV. CONCLUSSIONS Baden-Fuller [20], suggests that business models have a  multivalent character as models. They can be found as exemplar role models that might be copied, or presented as nutshell descriptions of a business organisation; also business model can represent relevant instances for a class of things.

Given the state of development of the Big Data concept at present, there is not enough data from firms to characterize successful business models and extract best-in-kind patterns.

This paper contends that disruptive business models will leverage from the ability to integrate the 4Vs (Volume, Variety, Velocity and Veracity) into a single, coherent, value proposition. Moreover given some similarities shared between Big Data and existing technological paradigms (e.g.

mobile, cloud computing) we foresee an emergence of strategic federations of stakeholders [14,15] organized around technological platforms [12, 26-28].

