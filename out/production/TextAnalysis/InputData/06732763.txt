Mining Repetitive Sequences Using A Big Data  Ecosystem

Abstract? Identifying repetitive gene sequences occurring within DNA sequences that span a collection of species is a challenge that is conceptually simple yet computationally challenging. Biological research suggests that certain regions within genomic sequences may be unchanged for hundreds of millions of years; understanding and identifying these highly preserved regions is a major challenge faced by bioinformaticians. Taking an evolutionary perspective on DNA, pinpointing these repetitive sequences is the first step to understanding functional similarities and diversities. The difficulty of this problem arises from the volume of the data required for analysis; it grows with every genome that is sequenced. Traditional approaches used to identify repetitive sequences often require the pair-wise comparison of chromosomes, which takes a significant amount of time to gather results. When comparing n chromosomes, n(n-1) individual comparisons must be made. To avoid exhaustive pair-wise comparisons, we designed an algorithm that partitions genomic sequences into search key values representing potential repetitive sequences, which are hashed into bins. With the introduction of new genomes, we only process the new sequences and aggregate new results with those that were previously processed.

Keywords? repetitive sequences, Big Data, sequence matching, MapReduce, Hadoop.



I. INTRODUCTION Research on animal and plant genomes suggests that  specific regions within the genomes are highly preserved, remaining unchanged for nearly 300 million years [1]. There is great mystery associated with these seemingly static sequences; as an evolutionary mechanism, the exact role they play is uncertain[2,6]. However, identifying their existence is the first step to gaining insight. One of the most common approaches to identifying ultraconcerved elements (UCEs) is to perform a pair-wise sequence alignment [2].

This method is known to be computationally expensive.

Modern approaches utilize information retrieval based techniques to achieve similar results and have shown to be successful [1]. We suggest an alternative approach that utilizes the Hadoop MapReduce framework to address this issue.

This problem could be considered as a topic of Big Data because the data is not clean, there is uncertainty in the data, the amount of intermediate results generated by pair-wise comparison approaches is difficult to manage, and the availability of genomic data is increasing rapidly. Utilizing common pair-wise comparison techniques to compare new  genomes with existing genomic datasets requires an enormous amount of redundant computation. In addition, alignment algorithms may not provide optimal results; they may not identify all matching sequences.



II. SYSTEM ARCHETECTURE  A. Framework By utilizing the Hadoop [3] framework, we are able to parallelize the processing across multiple machines and multiple threads. Our repetitive sequence algorithm utilizes a parallel, distributed programming model developed for use in a cluster environment, MapReduce [4]. The algorithm can be thought of as a data flow consisting of two phases: Map and Reduce conceptually, shown in Fig. 1. The map phase is responsible for sorting and filtering data. First, the large- scale genomic input data is divided amongst a collection of mappers (the number of mapper is defined based on the size of our input data) and is then manipulated and translated into a set of key-value pairs; in our case, keys are subsequences and values are their respective chromosome identifiers and starting positions in the input sequence.

These key-value pairs are passed through a hashing function that will deterministically map a key to one of potentially many reducers; this ensures pairs sharing a common key will be sent the same reducer. In the reduce phase, reducers aggregate the output subsequences received from mappers.

The MapReduce programming model is appropriate for sequence matching applications. That is, by assigning a sequence to be the key, the hashing behavior of the MapReduce model may be utilized to locate iidentical sequences; exact matching sequences will be hashed to the same reduce task.

B. Cluster Setup For the purpose of this study, we simulate a cluster  environment and run Hadoop [3] in pseudo-distributed mode. We constructed a virtual cluster by partitioning a single server?s resources and allocating them to a collection of virtual machines. In total, our server has 128GB of RAM, 24 2.0GHz Intel CPUs, and 6TB of disk space. For our experiments, we have configured our cluster to have seven data nodes and one master node. Each data node virtual machine is running CentOS Linux, has approximately 6GB of RAM, and a single CPU. The master node has 8GB of RAM and 4 CPUs. Hadoop is running in pseudo distributed mode as opposed to fully distributed mode, since the cluster is housed on one physical machine. The advantage to using this sort of environment, we have complete control over    available resources and are able evaluate different cluster configuration settings such as the number of nodes and the number of CPUs and RAM to be allocated to a node.



III. METHOD Common methods for identifying UCEs across a set of  chromosomes utilize an exhaustive pair-wise comparative approach; given n chromosomes, identifying inter- and intra-chromosomal UCEs requires n2 comparison. We would prefer a technique that requires n operations, analyzing each chromosome once. Our approach, shown in Algorithm 1, utilizes the Hadoop MapReduce model to perform a single global aggregation to identify identical sequences across chromosomes simultaneously.

Algorithm1 : Repetitive Sequence in MapReduce // Map Function: input <k,v> k is offset for current file block (in bytes); v is a sequence in chromosome C  1: v = P(v)       // remove invalid characters 2: for i = 0 to m-n do 2:       TSI = code (v[i to i+n]) 3:       start_pos = i + k 4:       return (TSI, (start_pos, C))   // Reduce Function: input <k,v> k is a subsequence (TSI); v is the starting position of the subsequence w.r.t the chromosome sequence  1: if(count(v) >= 2) 2:       uce = decode(k) 3:       pos = merge(v) 4:       return (uce, pos)    As previously mentioned, the role of our Map function is to construct a set of key-value pairs; all subsequences of length n (100 base pairs in our implementation) found in a chromosome form keys and their respective starting positions along with the current chromosome identifier form the values. To avoid potential capacity issues provoked by  trying to feed in data files that are too large for a single mapper, the system first splits the file into smaller chunks.

Hence, in the map layer, our input data will be a partition of one of the original files. In the Map function, the value in each input key-value pair is a segment of the raw chromosome sequence containing values A, C, G, T and a few others such as N, M, R, etc. A few preprocessing steps are performed before generating any key-value pairs. First, only valid characters A, C, G and T are considered. That is, all subsequences that contain an N, M, R, etc., are neglected because this represents some level of uncertainty within the sequence. Second, we convert each subsequence from a string to binary representation using the code function; we refer to this conversion as a Translated Sequence Identifier (TSI). Since each position in our sequence can be occupied by one of only four possible values, using the base-2 numeric system led to a significant reduction in data size.

The character data type in Java requires two bytes of storage space; we reduce this to two bits. This will significantly reduce the size of intermediate results generated by Mapper.

In our experience using subsequences of length 100 as our key, we were able to halve the size of our intermediate results by using TSIs. The next step in the mapper is to calculate the starting position of a subsequence with respect to the raw chromosome by adding the block offset, k, to the current line offset, i. To illustrate the concept, considering a subsequence, TSI*, starting at the 50th position in the original file containing chromosome A; it would take the following key-value pair representation: <TSI*, (50,A)>.

After constructing key-value pairs in the map tasks, all like keys will be mapped to the same reducer. In the Reduce function, the input key is a TSI (potential UCE) and values are lists of starting positions paired with a chromosome id.

The main job of a reducer is to count the number of positions present in the value; if there are more than two, we have found a UCE. Following our simple example above, all key-value pairs with a key of TSI* will arrive at the same reduce task. Upon arrival to a reducer, the positions are  Figure 1. System Architecture for MapReduce setting for identifying repetitive sequences    aggregated together. This set of positions represents the location of UCEs. A sample output from a reducer may look as follows: <TSI*,((50,A);(2300,B))>. Using this representation, it becomes easy to see that a sequence of length 100 occurs in both chromosome A and B starting at the 50th and 2300th positions respectively. The final job for the reducer is to decode the TSI, translating it back to the original character representation, and storing the results.



IV. RESULT Our MapReduce implementation is written in Java and  utilizes the Hadoop MapReduce framework. We consider the task of identifying UCEs across six chromosomes, three human and three from rat, in an incremental fashion as to demonstrate the behavior of our run-time, intermediate data size, and final result data size as the number of base pairs contained in the input sequences is increased. We compare our results with that of an existing method described in [1] in terms of accuracy. We do not consider a direct comparison of the results in terms of runtime because both approaches require vastly different computing environments. However, we report the overall runtime for our method.

The first case, processing one chromosome consisting of 250 million base pairs for intra-chromosomal UCEs, resulted in a runtime of approximately 25 minutes, 68 GB of intermediate data, and 512 MB of final data containing discovered UCEs. The subsequent cases iteratively introduced new chromosomes, increasing the number of base pairs in a roughly linear manner. Our method was able to identify all known UCEs across the six chromosomes having an average length of 250,000,000 base pairs within 4.5 hours, approximately 5.6 minutes per one million base pairs. The method scaled linearly as the number of base pairs increased (see Fig. 2). Our results were validated against results achieved from [1]. To be more precise, every UCE identified by the prior method was also identified by our proposed method. It is important to note that storage space is the greatest limiting factor for our method; we generate roughly 1 GB of intermediate results per every 1 million base pairs. A fully distributed cluster containing more nodes would certainly have improved performance.



V. CONCLUSIONS This ongoing study demonstrates the potential for  conducting sequence matching with the Hadoop  MapReduce framework. Approaches defined in previous studies perform pair-wise comparisons, whereas our approach is comprised of a single global aggregation step to identify matches across all sequences simultaneously. The simplicity of our method in combination with the scalability of Hadoop is what makes our approach novel. Furthermore, introducing more nodes into the computational cluster will improve our performance until the total number of map and reduce tasks is exceeded by the number of nodes in our cluster. With the advent of online pay-per-use cluster services, individuals could quickly spin up a powerful environment composed of an arbitrary number of nodes to complete this task quickly. Additional improvements are being made on our method by employing HBase to efficiently manage memory and storage utilization; we can minimize disk I/O by accumulating results in memory and writing them to disk in one flush.

