Learning Bayesian network from event logs using  mutual information test

Abstract? A Bayesian network can be considered to be a powerful tool for various analyses (e.g. inference analysis, sensitivity analysis, evidence propagation, etc.); however, it is first necessary to obtain the Bayesian network structure of a given dataset, and this, an NP hard problem, is not an easy task. Among the available scoring metrics, the present study employed Mutual Information Test (MIT) to construct a Bayesian network from the event logs of port logistics data covering six days of observations. Additionally, dynamic programming was used to shorten the combinatorial calculation of the metrics and, later, to minimize the computation time. To validate our method, we conducted a case study of port processes using actual event logs from an Asian port.

Keywords? Learning Bayesian network, mutual information, event logs

I.  INTRODUCTION Currently many businesses are supported by information  systems that provide insight into what actually happens in business process execution. This abundant data has been studied mainly in the growing research area of process mining [1-4]. One of the important performance-monitoring concerns is the execution pattern. Suppose that a process contains an OR, according to which, it can follow one course or another course: Which is it more likely to follow?

Any different flows can differentiate the final result (e.g.

total execution time; required resources, etc.) of that process. It should be clear, then, that an event of a process might contain many other attributes, which, in the cases of some complex business processes [9-10], may have tens or even hundreds of attributes. Thus, determining which attributes influence a given pattern is not straightforward.

To accommodate this process pattern uncertainty, in the present study, we employed a Bayesian network to determine which attributes cause a particular kind of process flow.

In our previous work [5], we used a dependency graph, retrieved by Heuristic Miner [6], and decomposed any cycles found into a non-cycle structure. This methodology, though enabling quick retrieval of the constructed Bayesian  network, has drawbacks relating to the fact that its non- cycle structure is dependent solely on the structure of the dependency graph. In other words, we have to take due note of the fact that the structure is supported only by the successive occurrences between activities and not by the common information shared. To remedy this shortcoming, in the present study we refine our previous work by developing a dynamic programming procedure of mutual information score using Mutual Information Test (MIT) [7].

The data used to calculate MIT score was originally not in a form of event logs, and, indeed, MIT was not designed for the business process management field. Therefore, the formula was modified to accommodate the problem at hand.

We, also, took account the computational complexity both of the recursive procedure, proposed in [7], and our proposed dynamic programming.

This paper is organized as follows: section 2 discusses the background; sections 3 and 4 introduce the proposed method and a case study, respectively, and finally, section 5 draws conclusions.



II. BACKGROUND  A. Bayesian network A Bayesian network is defined as a directed acyclic  graph (DAG) G of tuple (V, A, P), with  ? { |1 1,..., }iV v n= = :  a set vertices of the graph, and ? { ( , ) | , }ij i j i jA a v v v v V? = ?  : a set of arcs representing  dependencies.

Let 1 2{ , ,..., }nX X X X=  be a set of random variables such that iX  is a random variable for each vertex iv  in the graph.

For each random variable iX , there exists a parent set of  iX , denoted 1 2( ) { , ,..., }ii i i isPa X X X X= . Using the chain rule, the joint probability density 1 2{ , ,..., }Ix x x x= can be written  1 2  ( , ,..., ) ( | ( )) n  n i i i  P X X X P X Pa X =  = ? .(1)   DOI 10.1109/SOCA.2013.38    DOI 10.1109/SOCA.2013.38     The Bayesian networks? powerful utility in inference analysis notwithstanding, learning a Bayesian network from a given dataset is an NP hard problem [13]. Such learning is time-consuming, especially where the data is unordered and there is no reference structure.

B. Mutual information De Campos [7] proposed the use of mutual information  and conditional independence tests as a scoring metric applicable to the learning of a Bayesian network structure.

The metric balances the degree of interaction between each random variable and its parent(s) with the correlation in terms of the independence of an 2?  distribution. Since the MIT score comprises the local scores for each iX  over its parent ( )iPa X , a higher MIT score indicates a higher dependency of each iX  on its parent ( )iPa X . The MIT score of a DAG, denoted as G, over an event log, denoted as L, is formalized as   ( ),  1; ( ) 0 ( : ) 2 . ( , ( ))  i  i ji  sn  MI T L i i l i Pa Xi j  S G L N MI X Pa X ??  ? = ?  ? ? = ?? ?  ? ? ? ? ,(2)  where 1 2{ , ,..., }nX X X X=  is the set of n variables corresponding to 1 2{ , ,..., }nr r r discrete states, N is the number  of observations of event logs L , 1 2( ) { , ,..., }ii i i isPa X X X X= is the parent of iX  in G with  corresponding 1 2{ , ,..., }ii i isr r r  discrete states, wherein | ( ) |i is Pa X=  is the number of parents of iX , ?  is a fixed  value of 0.9, 0.95 or 0.99 confidence interval, ( , ( ))L i iMI X Pa X  is the mutual information of iX  and its  respective parents, ( )iPa X , ( ), i jil ??? is the value such that  ( )( )2 ,( ) i jiij lp l ??? ? ?? =  (the Chi-square distribution at significance level 1 ?? ), and ( )ii jl ?  is the degree of freedom of:   ( ) ( ) 1( )  ( )  ( 1)( 1) , 2,...,  ( 1)( 1), 1  i i  i  i  j  i i j i k i ki j  i i j  r r r j s l  r r j  ? ? ?  ?  ?  =    ? ? =? ?= ?  ? ?? ? =? ?  ? ,(3)  where { (1),..., ( )}i i i is? ? ?=  is any permutation of the index set {1,..., }is  of ( )iPa X , which lists the corresponding states in descending order.

De Campos [7] also provided, for construction of a Bayesian network, the recursive procedure, represented by equation (4) (which procedure, because the data set considered is not in the form of event logs, can be costly).

( ) ,  ( , ( ) : ) ( , ( ) \{ }: )  min 2 . ( , | ( ) \{ })ij i r ij  r i i  r i i ij  X Pa X L i ij i ij l  g X Pa X L g X Pa X X L  N MI X X Pa X X ???  ? ?= ? + ?? ?? ?  ,(4)  where ??  is the value such that ( )2 ,( ) rijrij lp l ?? ? ?? =  and the number of degrees of freedom is  1;  ( 1)( 1) is  r ij i ik ik  k k j l r r r  = ?  = ? ? ? . The base case of the recursive procedure is ( , : ) 0r ig X L? = . Accordingly, the MIT score defined in equation (2) can also be expressed  1; ( ) 0  ( : ) ( , ( ) : ) n  MIT r i i i Pa Xi  g G L g X Pa X L = ?  = ? .(5)  C. Process structure The dataset used in the present study was in the form of  an event log, denoted L. Van der Aalst [12] proposed a hierarchical structure of process execution event logs. In his work, a process consists of cases, denoted c, and each case consists of events, denoted e, such that an event is always related to one case. For instance, suppose a tuple  , , ,A B C D< > , which represents a case in the event logs in which an event A is followed by an event B and then an event C and, eventually, an event D.

For convenience, we assume that each event in the event log is represented by one random variable X , so that AX  represents the random variable of an event A. ( )iPa X ?  is a set of candidate parent(s) of an event in the event logs. Since a case c in the event logs contains a sequential process execution, we can assume that the data in the event logs is ordered. For example, an event A has an empty candidate parent since event A is the start event, denoted ( ) {}APa X =  ? ,  while an event B has event A as its candidate parent, denoted as ( ) { }B APa X X=  ? . We should take note  that ( ) ( )i iPa X Pa X? ?  , due to the fact that a candidate parent makes no higher contribution in the iterative calculation of  ( , ( ))L i iMI X Pa X cannot actually be considered as the actual parent.



III. LEARNING BAYESIAN NETWORK Suppose that we have four random variables,  1 2 3, , ,X X X or 4X : each random variable has two states, 2r = ; the candidate parents of 1X  and 2X  are  1 2 3 4( ) { , , }Pa X X X X= ?  and  2 3 4( ) { , }Pa X X X= ?  ; and there are no candidate parents for 3X  or 4X . Using the recursive procedure in equation (4), the calculation finds the parents of  1X . The calculation starts from the top and, incrementally, calculates to its bottom layer to find the base case. The procedure has a computational complexity of  ( )2( 2 !)mO n m m? +  for n  random variables and 3m ? candidate parents for each random variable. The idea, then, is to reduce the complexity of the recursive procedure by transforming it into dynamic programming.

Fig.  1 Bayesian network resulting from given example  Let us assume that for each MIT local score we need to find the minimum score such that the (i)-th local score is from the minimum (i-1)-th local score. Unlike the recursive procedure, we would like to start our procedure from the bottom up and move incrementally to the (i)-th step from the minimum (i-1)-th step. The formal definition of the recurrence is   1 2  1 2  ( )\ ( )  ( , ( ) : )  ( , ( ) : ), ( , ( ) : )min  2 ( , | ( ) ) ij i i  r ij  k k d i i ij  k k d i i ij  k k d i i ij  X Pa X Pa X k  L i ij i ij l  g X Pa X X L  g X Pa X X L g X Pa X X L  NMI X X Pa X X ?  ?  ?  ? ?  ? ?  ? ?  ?   ? ? ?? ??= ? ? ?+ ? ?? ?? ?  ?  , (6)  where 1( , ( ) : )k kd i i ijg X Pa X X L ?? is the minimum k-th local  score that has parents 1kijX ?  that yield the minimum local  score for the (k-1)-th steps, and 12 ( , | ( ) ) r  ij  k L i ij i ij l  NMI X X Pa X X ?  ??? ? calculates the mutual  information score by adding ( ) \ ( )ij i iX Pa X Pa X? ?  . The mutual information score for finding each parent of a random variable is similar to that in equation (5).

Suppose that our recurrence for 1( , ( ) : )k kd i i ijg X Pa X X L  ?? , for the k-th step, is correct.

Accordingly, we consider two options for calculation of the (k-1)-th step: 1) the calculation of  1 2( , ( ) : )k kd i i ijg X Pa X X L ? ?? , where there is no contribution  from the addition of the remaining random variables as the parents of iX ; 2) the addition of  1 2( , ( ) : )k kd i i ijg X Pa X X L ? ??  and the remaining random  variable(s), denoted ( ) \ ( )ij i iX Pa X Pa X? ?  , if by adding it(them) a better mutual information score of iX  in (k-1)-th step is made. The minimization of the (k-1)-th step will lead to the minimization of the k-th step. Thereby, our algorithm will find the optimal mutual information score. The base case  is obviously 0 ( , ( ) : ) 0d i ig X Pa X L = , where ( )iPa X = ? .

Our algorithm has the computational complexity of ( 2 )mO n for n  random variables and m  candidate parents for each random variable.

The procedure for finding the parent(s) of 1X , using the same example from the beginning of this section. The process starts from the bottom and gradually inserts, in  sequence, the remaining parents of 1X . The last iteration shows the optimal mutual information score for 1X , denoted  1 1 3( , ( ) { } : )dg X Pa X X L? . The complete Bayesian network  for the example is shown in Fig. 1.



IV. CASE STUDY  A. Port process overview One hundred and eighty (180) of the 500 port logistics  operations in the world are required to serve more than one- half million TEU (Twenty-foot Equivalent Unit) [8] annually; this fact has compelled the adoption of more efficient operations. Most port logistics operations currently are supported by information systems that store actual operation records. Employing this data, and using the process mining technique, port managers can obtain useful insights into what actually happens in the port logistics process execution. Moreover, since port logistics are prone not only to complexity but also uncertainty, in this study we employed the Bayesian network to investigate port logistics performance.

Fig. 2 shows port logistics process in BPMN 2.0 notation. Port logistics event logs obtained consists of 395 process instances and 1009 events. It covers six days of observation, and includes the discharging and transshipment processes. The discharging process discharges a container from a vessel for stacking in the yard, after which it is either brought out of the port as an outbound container, or loaded onto another vessel. Due to privacy issues, some of the contents have been masked; this did not affect the study results. The events (activities) include yard operations for container discharge (YD), quay operations for container discharge (QD), yard operations for container loading (YL), pluggings for reefer container (RP), container shuffling (SF), quay operations for container loading (QL) and yard operations for container gate out (YO). Each event is represented as a random variable, the state(s) of which is (are) an attribute(s) value in the event logs, (i.e. the value of the originator or of another data attributes).

B. Data preparation Each event in the same process instance is ordered based  on its timestamp. This facilitates construction of a Bayesian network, since the parent of a given event is actually an event that previously occurred. To that end, first, we list the random variables, and their respective candidate parents and states from the event logs. For convenience, each random variable is represented as its abbreviation: , , , , ,YD QD SF RP YL YOX X X X X X  and QLX . For instance, a variable YDX  has its candidate parent(s)  ( ) { }YD QDPa X X= ?  , whereas QDX  has an empty set of candidate parents, since we cannot find any predecessor of event QD in the event logs. The following is a complete list of events and their respective candidate parents and states.

TABLE I. LIST OF RANDOM VARIABLE WITH RESPECTIVE STATES AND CANDIDATE PARENT( S)  Variable Number of States Candidate Parent(s) Originator(s) Attribute(s)  YDX  8 14  ( ) { } YD QDPa X X= ?    QDX  6 14  ( ) { } QDPa X = ?    SFX  8 20 ( ) { } ,SF YD RPPa X X X= ?    RPX  2 0  ( ) { } ,RP YD SFPa X X X= ?    YLX  8 10  ( ) { } YL YDPa X X= ?    YOX  8 10  ( ) { } , ,YO YD RP SFPa X X X X= ?    QLX  7 10  ( ) { } QL YLPa X X= ?     Fig.  2 Port logistics process in BPMN 2.0 notation  The states are those of the originators, which in this case are quay cranes and yard cranes.

C. Generation of Bayesian network To learn the Bayesian network from event logs, we apply  our proposed calculation steps to our case study. First, the variable with an empty set of candidate parents is left as it is.

Afterwards, starting from the first variable, we calculate the local score according to equation (5). Since YDX  has only one parent (as do YLX  and QLX ), the calculation will be straightforward. For SFX  and RPX , each having two parents.

The results indicate that between the two candidate parents, there is only one parent YDX , for both SFX  and RPX . This is due to the fact that 1 ( ,{ }: )d SF YDg X X L  and  1 ( ,{ }: )d RP YDg X X L are smaller than 1 ( ,{ , } : )d SF YD RPg X X X L  and 1 ( ,{ , } : )d RP YD SFg X X X L , respectively.

The local score calculation of YOX , denoted as ( , ( ) : )d YO YOg X Pa X L ; on this basis, we also gain the actual  parents of YOX , which are ( ) { , , }YO YD SF RPPa X X X X= . The complete Bayesian network in our case study is illustrated in Fig. 3 (a); Fig. 3(b) includes the attributes from the event logs, and also shows the observation of originator YC61 in YD  and the flow traversing through YL that affects the posterior probability of QL being late in the container loading process of VESSEL01, which also can be expressed as  ( ?  01? | ? 61?, ? ?) 0.0002QL YD YLP X Late V X YC X Traverse= = = = ; meanwhile Fig. 3(c) shows the posterior probability distribution of each possible state in the random variables.

D. Analysis Scenario After the Bayesian network is retrieved, the network  analysis, which includes inference analysis, sensitivity analysis, and others, can be performed.

? Causal inference Causal inference can also be used to investigate the  cause of working delays. In [5], we noted the lateness inference of an overall process given one delay in an event.

For example, if the container placement in the yard for loading preparation (YL) is certain, then the probability of the container loading process become late for Vessel V01 is 0.0211 and, eventually, the probability of container loading process being on time is 97.88.

? Backward inference Backward inference is used to find the probability of the  opposite direction of causal inference, that is, the probability of the caused random variable where the affected random variable is known. For instance, the probability that a container going off port through the container gate out (YO), is a reefer plugging container, is denoted  ( ' ' | ' ')RF YOP X traverse X traverse= = , and is 0.017.  It should be noted that only reefer containers go through reefer plugging event RFX . In this study, using the same means of retrieving the reefer plugging container probability, we could determine that the probability of those containers having been through a shuffling event (SF) was 0.015.

? Explaining-away inference Suppose that a port manager wants to know which event has a higher chance of triggering a gate-out process: YD, SF, RP or a combination thereof. Using explaining-away inference this time, the port manager can predict this. A more interesting analysis that can be employed using explaining-away inference is the ability to infer any possible patterns influenced by each random variable state(s). As an example, assume YD and QD as an event with yard cranes and container-bound remarks as its states, respectively. It can be inferred that any yard cranes carrying a container with ?transshipment? as its remark is more likely, with the probability of 0.09, to execute YL than another process.

This way, if YD and QD are certain, we can obtain a clearer understanding of the probability of transshipment occurring.

Fig.  3 Bayesian network of the port logistics case study

V. CONCLUSIONS This study refined our previous investigation into  Bayesian network generation. In that earlier study, we decomposed any cycles found in a dependency graph into a Bayesian network. However, in such a case, the non-cycles structure remains as the original dependency graph structure.

Moreover, the dependency graph is derived from the Heuristic miner algorithm, which derivation does not follow the principle of Bayesian network construction, namely, dependency (independency) among vertices. To remedy this shortcoming, in the present investigation, we employed mutual information test (MIT) to construct the Bayesian network. MIT was not originally designed for the field of business process management, and so the data set used to calculate MIT is not in the form of event logs. Thus, we  propose dynamic programming for MIT calculation. Our proposed method was proved to entail less computational complexity than the recursive procedure for the original MIT calculation. Moreover, we validated our method using real port process analysis. The case study reported in these pages covered six days of observation and involved almost four hundred process instances. Considering that event logs contain time-series data, we would like to enhance this method in a more applicative direction, specifically with respect to the n-order Markov chain in a dynamic Bayesian network.

