Numeric Missing Value?s Hot Deck Imputation  Based on Cloud Model and Association Rules

Abstract?Filling missing value is main task of data-processing, at present Hot Deck Imputation is preferred. Defining the similar standard of Hot Deck Imputation objectively becomes an important prerequisite. The Cloud model combines ambiguity and randomness organically to fit the real world data objectively.

first get the cloud models which present the raw no missing value, then to discrete the numeric value and do the association rules mining in the discrete value to get the knowledge base, filling the missing value with the value which generated by the cloud model from the knowledge base. The method considered the original data?s distribution as a whole and to improve its precision with association rules from the raw data for each record, it simulates the humans? behavior; this method has smaller absolute mean difference than other methods.

Keywords- Missing value; Cloud model; Association rules; Hot Deck Imputation

I.  INTRODUCTION Data in the database that contains noise, the missing data,  and ambiguity will affect the validity of data mining, knowledge discovery model should have a certain degree of fault tolerance, it is urgent to find a mechanism to handle missing value efficiently and accurately. Dealing with incomplete data sets methods can be divided into the following three categories: remove the tuples, data filling, and do the data mining with the missing data.

A. Delete the tuples Delete the tuples with missing attribute values to get the  information completeness by reducing the historical data amount, discarded a lot of hidden information in these objects.

When missing data is a larger proportion of the data, particularly there is non-random distribution of missing data; this approach may lead to deviation, which leads to wrong conclusions [1].

B. Data Filling This method is used to fill the missing values with a certain  value. Usually based on statistical theory, according to the decision table the remaining values? distribution to fill a missing value, there are several commonly used filling method [2], [3] in data mining.

1) filling manually The user fill the missing data, this method is time-  consuming. It is not feasible with large-scale data or a huge number of missing values.

2) Treating Missing Attribute values as Special values Deal with the missing value as a special value, [4]  indicate that the method may lead to serious data deviation.

Generally these kinds of methods are not recommended.

3) Mean/Mode Complete  If the missing value is numeric, to fill the missing value with the average of all the other non missing value; if the missing value is not numeric, to fill the missing value with the most likely value in all the other tuples without missing value.

Reference [1] thinks in real applications, generally people have very little prior knowledge with the owned data, so this approach is obviously not satisfactory.

4)  Hot deck imputation To fill the missing values with its most similar values in  all the data set. The concept of this method is very simple, and to estimate the missing value using relationship between the data of a tuple. At present [5], [6], [7] think that the hot deck method is superior to other method.

There are several other filling methods such as EM (Expectation maximization), and MI (Multiple Imputation) and so on, these method are relatively complex, and the EM method convergence rate is slow, time complexity is large.

From the use of the frequency and extent of research, now, generally [8] think that filling method is the most commonly used method to deal with missing values. Its main disadvantage is difficult to define similar standards, there are more subjective factors.

Cloud model can transform qualitative and quantitative each other, considering the original data distribution in the database, using the cloud model to discrete non-missing values of database, then use the association rules to generate knowledge base with non-missing value tuples, when fill the missing value, first search  knowledge base to find associated knowledge, then use association rules to find the corresponding numerical cloud model, to fill missing value with the cloud generator, the model itself has the universal nature of social phenomena and nature, the filled data has the same distribution with the original data, the association rules  This work was supported by the Scientific and technological development projects of Shandong Province (Grant Nos.

2008GG30001030), Natural Science Fund Project of Weifang University (Grant Nos. 2008Z08)  2010 Second International Workshop on Education Technology and Computer Science  DOI 10.1109/ETCS.2010.299       limit makes filled value of a record relevant to the other attribute information of itself, so the similarity of Hot deck imputation are defined objectively.



II. CLOUD MODEL Cloud model is a qualitative and quantitative conversion  model; it combines ambiguity and randomness organically.

Set U is a mathematical domain U={x}, T is the language  value associated with the U. ?T (x) is a stable tendency random number which expressed the elements x subordination of T concept, subordination?s distribution in the domain is known as the cloud [9].

Fig 1 presents a cloud model; cloud mathematical expected curve is its subordination curves from the view of fuzzy set theory. However, "thickness" of the curve is uneven, waist is the most scattered, the top and bottom of the curve are convergent, cloud's "thick" reflects the subordination degree randomness, near to or away from the concept center have smaller subordination randomness, while concept center have the largest subordination randomness, which is consistent with people's subjective feelings.

Figure 1. The digital features of the normal cloud  The digital features of the normal cloud characterized by three values with the expectation Ex, entropy En, excess entropy He) (see Fig 1), the expected value Ex: the center value of the concept domain, is the most representative qualitative value of the concept, it should be 100% belongs to the concept; entropy En: is a qualitative measure of the concept?s ambiguity, reflecting the accepted range values of the concept domain; hyper entropy He: can be described as entropy En of entropy, reflecting the degree of dispersion of the cloud droplets. The normal cloud is the most important cloud model, because various branches of the social and natural sciences have proved the normal distribution?s universality. The equation of normal cloud curve:  e 2En 2 Ex)(? 2  )(MEC A  Expectation curve is a normal curve, for a qualitative knowledge, the elements outside the Ex  3En in its corresponding cloud model all can be ignored, because it has been proved that approximately 99.74% elements of model fall into the range of Ex  3En by the mathematical characteristics  of normal distribution. Extending the normal cloud model to get trapezoidal cloud model, trapezoidal cloud model can be expressed by six values, they are the expected number: Ex1 and Ex2, entropy: En1 and En2, hyper entropy: He1 and He2.

Trapezoidal cloud curve equations are determined by the expectation and the Entropy are following:  A  A  A  2(x Ex1) 2 1 3 1 12En1  Ex1<=x<=Ex2  2(x Ex2) 2 2 2 3 22En2  MEC (x)  MEC (x) = 1  MEC (x)  e  e  Ex En x Ex  Ex x Ex En  Clearly, the left and the right half-cloud expectation curve is a normal curve.

It can complete the qualitative and quantitative transform more accurately, if there is a range belongs to the concept totally, then it can be expressed by the upper edge of Trapezoid, if only one value belongs to the concept totally, then the upper edge of Trapezoid degenerate to a point, trapezoidal cloud model also degenerated into the normal cloud model. He1 and He2 can have different values, and thus the concept of the border on behalf of different fuzzy situation, when the He1 and He2 all degenerate to 0, trapezoidal cloud model expressed a concept with accurate border subordination, when one of the He1 or He2 degenerate to 0, which expressed a concept with one r accurate border subordination and one vague border subordination, so trapezoidal cloud model has a better generality.



III. GENERATED NUMERICAL CHARACTERISTICS OF CLOUDS WITH EXISTING NON-EMPTY ORIGINAL DATA  A. Cloud Transform Any function can be decomposed into cloud-based  superposition with allowed error range, which is Cloud Transform. The equation is:  )()(  xfxg j m  j  jc ))()(0(  xfxg jj m  j  c  g(x): data distribution function fj(x): cloud-based expectations function cj :coefficients m: the number of superimposed cloud, : user-defined maximum error  From the concept of clouds: in the domain the element?s subordination to the concept has statistical and random properties. In addition, the high-frequency elements? contributions to the concept are higher than the low-frequency elements. That is the reason to use probability density function of data distribution to get the concept set, so the concept division algorithms can be done.

According to the definition of cloud transform, the quantitative attribute?s domain dividing into m concepts can evolve to a problem to get answers from the formula:     m  j ijjjjjjij HeHeEnEnExExfxg c  )2,1,2,1,2,1()(

I.e. to get Ex1j, Ex2j, En1j, En2j, He1j, He2j and cj for each cloud concept, the quantitative attribute domain is divided into a number of concepts by using cloud model, the data in each concept aggregate, and the data between different concepts separated.

B. The original non-empty data discrete algorithm Counting non-missing values of the original data to get the  histogram of data distribution, local peak of the histogram is that the data aggregation part, taking it as a concept center is reasonable, the higher the peak, indicating more data convergence there, deal with it with priority. The discrete grouping algorithm is:  Algorithm 1: Discrete algorithm Input: the domain of property that need the concept  division, the property values attributes i corresponds to them, the overall error threshold , and the peak height error threshold y, the length error x between trapezoidal top edge and the minimum value .

Output: m concepts and the corresponding digital features of attribute i.

{ (1) Count the each possible values x of attribute i and get the actual data distribution function g (x);  (2) j=0; (3)Clouds= g?(x)=g(x); (4) while max(g?(x))> (5)   Exj=Find_Ex(g?(x)); (6)    Ex1j=search1(g?(x), y, x); (7)    Ex2j=search2(g?(x), y, x); (8)    En1j=Find_En(cj,Ex1j, ); (9)    En2j=Find_En(cj,Ex2j, ); (10)   gj(x)=cj*Cloud(Ex1,En1j,Ex2j,En2j); (11)   g?(x)=g?(x)-gj(x); (12)   j=j+1; (13) endwhile (14) for j=0 to m-1 do (15) Clouds(Ex1j,=Ex2j,En1j,En2j,He1j,He2j)=Calculate_  He(gj(x), g?(x),Cloud(Ex1j,Ex2j,En1j,En2j)); (16) endfor} In Step 1, using statistical methods to get the actual data  distribution function g (x); Step 2, 3 does variable initialization; Step 4 the division of the process is ended, if the error limit less than a given error, Step 5 Search for the peak value of cj of property in the data distribution function g(x), and its corresponding value x is defined as the cloud model center (expectation); Step 6, 7 search approximate horizon line near the peak (within the error limit threshold y), if the width is greater than the minimum width of the threshold value x, where were identified as uniformly distributed, the two endpoints of the line are recorded as the trapezoidal top edge endpoints Ex1j, Ex2j; otherwise get the trapezoidal top edge endpoints are equal to the peak point value Ex1j=Ex2j=Exj, trapezoidal cloud degenerated into the normal cloud.

Trapezoidal cloud height coefficient is the function value of the Ex1j or Ex2j. The step 8, 9 calculate cloud model entropy En1j and En2j to fit g (x) for the half-liter cloud with Ex1j,  half-falling cloud with Ex2j. to get En1j searching left area of the cloud model with Ex1j, to get En2j searching right area of the cloud model with Ex2j, the entropy value increase from 0 step from the smaller value until the threshold  is greater than the difference between the half-normal cloud value and distribution histogram value; the step 10 calculate distribution function of the corresponding Trapezoidal; step 11 use the original data minus the known distribution function of trapezoidal cloud model data distribution to get the new data distribution function g'(x); Repeat step 4 to 12 until the peak value is less than the error threshold. Step 14, 15, 16 determine half hyper entropy of all cloud model with the residuals of distribution histogram.

Through the above processing, the non-missing numeric values are divided into several cloud models, the non-empty numerical attribute can be discrete based on their subordination to the cloud model. The data after discretization can be easily carried out Boolean association rule mining.



IV. ASSOCIATION RULE MINING Association rule mining refers mainly to get the  knowledge such as "customers bought tea  also purchased the coffee," which meet the minimum support and the minimum confidence. At present association rules can be divided into two types: Boolean association rules and quantitative association rules, and most of the research are focused on the Boolean association rules research.

First, give the formal description of association rules mining: Suppose I = (i1, i2... im) is a collection of m different items, T = (t1, t2... tn) is a transaction database, tj is a group of items of I set, tj I. Each transaction with a unique identifier TID linked. If X is a subset of I with X tj, we say that a transaction contains X. An association rule is a "X Y" implication, in which X I, Y I, and X Y= .

Definition 1: If the ratio of transaction T contains X Y is Sup, the association rules X Y in T has a support degree Sup.

%100  n y)support(xSup   Support (X Y) stands for the number which support  X Y transactions, n stands for the total number of transactions.

Definition 2: If the ratio transaction T contains X also contains Y is Conf, then the confidence level of association rules X Y in T is Conf.

%100 support(x)  y)support(xConf  Association rules are called the strong association rules if they meet the minimum support and minimum confidence.

Discrete grouping algorithm divided non-empty original quantitative attribute into multiple concepts, if one attribute value is mapped to a different concept, mapped to the concept whose subordination value is the greatest. A quantity data corresponding to the original meaning is to buy not to buy tea, through the concept discrete algorithm is now extended to buy very much more tea, buy a lot of tea buy a few tea. The question naturally converted to Boolean association rule     mining, using the famous Apriori algorithm which generated frequent item sets and FP-Growth algorithm which does not produce the frequent item set to generate association rules database, which contains knowledge like <tea, more> <coffee, little> knowledge.



V. GENERATE THE MISSING VALUE WITH CLOUD MODEL AND ASSOCIATION RULES  Use three digits values of cloud model as a basis to fill the cloud droplets, the cloud droplets and the original data which generated three digits values of the cloud model are in the same distribution, characteristics the same data group, with these data to fill the missing values. This method combined organically the fuzzy nature and the randomness, which has a better exactness matching the human society behavioral characteristics. The accuracy of each record is limited by association rules, such as <tea, more> <coffee, very little>, that is, tea cloud model if it is "more", then the missing value to fill the coffee just use the "few". Generate missing value to fill the value is to make the greatest possible reduction of the raw data.

Algorithm 2: The missing value reduction algorithm Input: association rule knowledge base, and the original  database with a quantitative missing value Output: the database with no quantitative missing value {(1) Locate the records with missing value; (2) Find association rule "A B" according to record?s non-  missing attribute A in the Knowledge Base (3) Get cloud model digital features (Ex, En, He) from B (4) Generate the normal random number En' with En as  expectation, He as mean square error; (5) Generate the normal random number x with Ex as  expectation, En' as mean square error to fill the missing value; (6) Repeat steps 1-5 until the required number of missing  values produced.}

VI. EXPERIMENTAL ANALYSIS Take two typical supermarket food rice and bean as  example, the majority retail sales data scattered between 0.5-5kg, Extract 6000 transaction records as experiment data, set empty value in 600 records use random method, respectively (1) filling manually; (2) Treating Missing Attribute values as Special values; (3) Mean/Mode Complete; (4) Hot deck imputation: is given in this article based on cloud model and association rules. Calculate Absolute mean difference between the fill value and the original value of various methods, the smaller absolute mean difference, the higher the method accuracy. The Data are showed in table 1.

TABLE I. EXPERIMENTAL DATA ANALYSIS  Method Method 1 Method 2 Method 3 Method 4 Absolute mean difference 0.463 1.391 0.562 0.486  Seen from the experimental results, filling manually method?s overall error is not large, because the people know actual fact well, but it cost too much time. Method 2 and method 3 change the raw data distribution, they are  meaningfulness. Methods 4 take into account both the distribution of the raw data, but also each record data correlation, so the method has high accuracy, to improve its accuracy further, reduce discrete particle size of the original non-empty data, and lower the minimum support for association rules and, but this will take more time, the minimum support degree and the minimum confidence degree shouldn?t too small, if so, the association rules does not make sense, the minimum support degree and the minimum confidence degree are given by the experts in the field.



VII. CONCLUSION Filling an empty value based on cloud models and  association rules first consider the distribution of the original data, use the cloud model to discrete the original non-missing data, the do the data association rules mining with the Boolean data and get knowledge base. Use the knowledge in knowledge base and the numerical cloud model features to fill the empty values, the missing value substitute have the same characteristics with the cloud model that generated the concept of the original data. The individual differences reflected the double features in ambiguity and randomness; it simulates the real-world general similarities and slight differences of the same group. For each record, use association rules in the knowledge base to enhance the substitute precision of each record further, the accuracy of the method is higher than others.

