FChain: Toward Black-box Online Fault Localization for Cloud Systems

Abstract?Distributed applications running inside cloud sys- tems are prone to performance anomalies due to various reasons such as resource contentions, software bugs, and hardware fail- ures. One big challenge for diagnosing an abnormal distributed application is to pinpoint the faulty components. In this paper, we present a black-box online fault localization system called FChain that can pinpoint faulty components immediately after a perfor- mance anomaly is detected. FChain first discovers the onset time of abnormal behaviors at different components by distinguishing the abnormal change point from many change points caused by normal workload fluctuations. Faulty components are then pinpointed based on the abnormal change propagation patterns and inter-component dependency relationships. FChain performs runtime validation to further filter out false alarms. We have implemented FChain on top of the Xen platform and tested it using several benchmark applications (RUBiS, Hadoop, and IBM System S). Our experimental results show that FChain can quickly pinpoint the faulty components with high accuracy within a few seconds. FChain can achieve up to 90% higher precision and 20% higher recall than existing schemes. FChain is non- intrusive and light-weight, which imposes less than 1% overhead to the cloud system.



I. INTRODUCTION  Infrastructure as a Service (IaaS) clouds [1], [2] allow  multiple tenants to share a common physical computing infras-  tructure in a cost-effective way. However, applications running  inside the IaaS cloud are prone to performance anomalies such  as service level objective (SLO) violations due to various rea-  sons such as resource contentions, software bugs, or hardware  failures. It is particularly challenging to localize the faulty  components in a complex distributed application that consists  of many inter-dependent components.

Previous work on distributed system debugging can be  categorized as white-box, grey-box, or black-box techniques.

White-box and grey-box techniques (e.g., [3]?[5]) require  modifications or instrumentations to applications or underlying  middleware platforms. Those intrusive techniques often im-  pose significant runtime overhead and are difficult to deploy,  which make them impractical for performing online fault  localization in the production IaaS clouds. Existing black-box  techniques [6]?[8] mostly focus on anomaly detection or sys-  tem metric attribution. In contrast, our goal is to pinpoint faulty  components in a distributed application. Other schemes [9]?  [11] only work for certain types of faults or applications. We  will discuss related work in detail in Section IV.

In this paper, we present FChain, a black-box online fault  localization system for diagnosing performance anomalies in  IaaS clouds. FChain can pinpoint faulty components immedi-  ately after a performance anomaly is detected. FChain does  ???? ??? ????  ?????? ????? ?	???? ????  ??	???????????????? ? ?? ????  ???????	? ??  ???????????? ?	????????????  ?????? ??? ???? ???????	?????????????????  ??????  ???? ??	???? ????? ????	???  ???  ??!

???	?? ???? ??!

???	?? ????  ??????? ????  ????"?!?#??  $?? ???%  Fig. 1. The overall architecture of the FChain system. The FChain slave continuously collects system-level metrics (e.g., CPU, memory) on different application VMs running inside the cloud and learns normal fluctuation patterns. When a SLO violation is detected, the FChain master triggers all related FChain slaves to look for abnormal change points. The FChain master then pinpoints faulty components based on the change point timing information and inter-component dependencies. FChain also performs online validation to remove possible false alarms.

not perform any intrusive tracing in the cloud system and  only relies on low-level system metrics (e.g., CPU, mem-  ory, network statistics) that can be easily obtained from the  guest OS or hypervisor. FChain does not assume any prior  application knowledge, which makes it practical for IaaS  clouds. Moreover, FChain does not require any training data  for anomalies, which can diagnose both previously seen and  unseen performance anomalies. Figure 1 shows the overall  architecture of the FChain system.

When a performance anomaly is detected, FChain first  discovers abnormal changes at all system metrics of dif-  ferent components. Next, FChain extracts abnormal change  propagation paths for the diagnosed performance anomaly  and localizes the faulty components based on the abnormal  change propagation patterns. Our design is based on two  key observations: 1) performance anomalies often manifest as  abnormal system metric fluctuations that are distinctive from  the normal fluctuation patterns; and 2) the abnormal system  metric changes often start from the faulty components and then  propagate to other non-faulty components via inter-component  interactions.

To achieve robust fault localization, FChain needs to ad-  dress a challenging problem: how to distinguish the abnormal  change point that marks the onset of the fault manifestation  from many other change points that are caused by normal  workload fluctuations. It is insufficient to use a fixed filtering  threshold since some applications are inherently more dynamic  than others. To address this problem, FChain captures the  normal fluctuation patterns using online system metric value  prediction models [12] and uses a predictability metric to iden-   DOI 10.1109/ICDCS.2013.26    DOI 10.1109/ICDCS.2013.26    DOI 10.1109/ICDCS.2013.26     tify abnormal fluctuations. As we will show later in Section  III, this abnormal change detection scheme can achieve higher  accuracy than traditional anomaly detection schemes [10].

Due to inter-component interactions, abnormal changes in  the faulty component(s) often propagate to other normal  components. If we examine each component in an isolated  way, we might produce many false alarms by mistakenly  pinpointing normal components as faulty ones. Our previous  work [13] has shown that it is feasible to pinpoint faulty  components by sorting all affected components based on their  fault manifestation time and identifying the component with  the earliest manifestation time as the faulty one. However, we  observe that it is insufficient to only rely on the chronological  order to perform fault localization since we may derive spuri-  ous abnormal change propagations between two independent  components. To address the problem, FChain integrates the  dependency relationships with the fault propagation model to  achieve more accurate pinpointing than existing schemes.

This paper makes the following contributions:  ? We present FChain, a practical online fault localization  system for large-scale IaaS clouds. FChain does not  require any intrusive application monitoring, which can  diagnose both previously seen and unseen anomalies.

? We describe a predictability-based abnormal change point  selection scheme that can distinguish abnormal change  points that are related to the fault manifestation from  those normal change points that are caused by normal  workload fluctuations.

? We introduce a new integrated fault localization scheme  considering both fault propagation patterns and inter-  component dependencies to achieve higher pinpointing  accuracy.

We have implemented FChain on top of the Xen platform  and tested it on NCSU?s Virtual Computing Lab (VCL) [2], a  production cloud computing system that operates in a similar  way as Amazon EC2 [1]. We conducted extensive experiments  using a set of common faults and different types of appli-  cations (IBM System S data stream processing system [14],  Hadoop [15], and RUBiS online auction benchmark [16]).

Our experimental results show that: 1) FChain can achieve up  to 90% higher precision and 20% higher recall than existing  black-box fault localization schemes; 2) FChain can complete  the fault localization within a few seconds and works for  different types of applications; and 3) FChain is light-weight,  imposing less than 1% CPU overhead during system runtime  execution.

The rest of the paper is organized as follows. Section II  describes the design of the FChain system. Section III presents  our experimental evaluation. Section IV compares our work  with related work. Finally, the paper concludes in Section V.



II. SYSTEM DESIGN  In this section, we first present an overview of the FChain  system. We then describe the abnormal change point identifica-  tion algorithm and the integrated faulty component pinpointing  scheme.

????? ??? ???????? ? ????  ?& ?'  ?(  ?)*?)(  ?)+  ?)&  ?),  ?)'  ?)-  ? ?	????????????  ??	?? ????????????????? ??  .	???? /??0	? ? ????? ???  1$? ? ?0??  Fig. 2. Abnormal change propagation in an IBM System S application. The fault propagates along the path: PE3 ? PE6 ? PE2. The propagation PE6 ? PE2 is caused by the back-pressure effect.

A. System Overview  FChain is decentralized consisting of a set of slave modules  (i.e., normal fluctuation modeling, abnormal change point  selection) and master modules (i.e., integrated fault diagnosis,  online pinpointing validation), shown by Figure 1. The slave  modules run inside the domain 0 of different cloud nodes while  the master modules run on dedicated servers. FChain treats  each guest virtual machine (VM) as one component.

The normal fluctuation modeling module continuously mon-  itors the system metrics for each VM to capture its normal  fluctuation pattern. We employ a light-weight online learning  model [12] to continuously learn the evolving pattern of  each system metric value. If the change is caused by normal  workload fluctuations, the prediction model must have seen  and learned the change before. Thus, the prediction errors on  those normal change points will be small. In contrast, the  fluctuations caused by faults are not captured by the online  learning model, which will probably incur high prediction  errors.

When a performance anomaly (e.g., SLO violations) is  detected1, the FChain master is invoked to pinpoint the faulty  components in the failing distributed application. The FChain  master first contacts the slaves on all related distributed hosts  to identify whether a component exhibits abnormal changes  and when the abnormal change begins. If the performance  anomaly occurs at time tv, the FChain slaves check a window  ([tv?W, tv]) (e.g., W = 100) of recent metric values before tv.

The abnormal change point selection module uses the normal  fluctuation model to filter out those change points that are  caused by normal workload fluctuations.

The integrated fault diagnosis module comprehensively  examines the abnormal change point information from all  components and the inter-component dependency information  to pinpoint the culprit component(s). FChain first derives the  abnormal change propagation pattern in the examined dis-  tributed application by sorting the timestamps of the abnormal  1Note that FChain focuses on fault localization rather than performance anomaly detection that has been addressed by previous work(e.g., [17], [18]).

Fig. 3. Abnormal change point selection for the DiskWrite metric of a faulty map node and the CPU usage metric of a normal reduce node in a Hadoop sorting application.

change points of different components.2 For example, Figure  2 shows the abnormal change propagation path in a sample  IBM System S application consisting of seven distributed  components called processing elements (PEs). The application  consists of one faulty component PE3 with a memory leak  bug. The memory usage metric of PE3 shows the abnormal  change at time t1. The anomaly first propagates to the com-  ponent PE6 at time t2 and then to the component PE2 at  time t3. Since t1 < t2 < t3, FChain pinpoints PE3 as the  suspicious root cause component. Note that our scheme does  not assume any knowledge about the application including  its topology. FChain leverages the black-box dependency  discovery tool [11] to filter out spurious abnormal change  propagations between independent components.

Finally, FChain performs online pinpointing validation us-  ing the dynamic resource scaling technique in a similar way  as [20]. Since FChain monitors various system metrics, it can  not only pinpoint faulty components but also identify which  system metrics are related to the fault. We can then adjust those  metrics on the faulty components to validate the accuracy of  the pinpointing results by observing the resource adjustment  impact to the application?s SLO violation status.

B. Abnormal Change Point Selection  To localize the faulty components, FChain first examines  i) which components exhibit abnormal changes in different  system-level metrics; and ii) when those abnormal changes  start. As mentioned before, system-level metrics are inherently  fluctuating under dynamic workloads. If we apply standard  change point detection algorithms, we often discover many  change points. For example, Figure 3 shows the set of change  points discovered on the ?Disk Write? metric of a map task  node and the ?CPU usage? metric of a reduce task node in a  Hadoop application, using the common change point detection  algorithm ?CUSUM + Bootstrap? [21]. We can see many  change points are just random peak and bottom values, which  are not actually related to the fault. We can use smoothing  2Since FChain relies on timing information to infer abnormal change propagations, we synchronize the clocks of all hosts using the network time protocol (NTP), which has an error of less than 0.1 ms in LANs and less than 5 ms on the Internet [19]. In our experiments, we observe that all of the anomaly propagation delays between two dependent components are at least several seconds. Therefore, our system can tolerate small time skews (i.e., tens of milliseconds).

Fig. 4. Expected prediction error for the CPU usage metric on a dual-core host.

and change magnitude outlier detection to filter some normal  change points [13]. However, it is insufficient to only rely  on smoothing and outlier detection to select abnormal change  points since some metrics (e.g., the ?Disk Write? metric in  Figure 3) have large variations during normal execution.

FChain uses a predictability metric to achieve robust ab-  normal change point selection. We employ an online learning  model [12] to continuously learn the change pattern of each  system metric. The online learning model can capture the  transition probability between different metric values using  a discrete time Markov chain model. Intuitively, the value  transition patterns at a normal change point should be able  to be captured by the online learning model and thus easier  to predict. We calculate a prediction error for each outlier  change point by comparing the predicted value with its true  value. If the prediction error is high, we consider this outlier  change point as an abnormal change point. However, it is a  non-trivial problem to pick a proper prediction error threshold  for filtering normal change points since some metrics (e.g.,  bursty ones) are inherently harder to predict than others and  vary from application to application. Thus, it will be imprecise  to apply a fixed filtering threshold.

To address this problem, FChain dynamically computes a  proper prediction error threshold for each change point based  on the burstiness of the time series surrounding the change  point. The intuition behind our scheme is that a bursty metric  is expected to have a higher prediction error than a non-  bursty metric. Thus, we want to use a higher prediction error  threshold when the metric values are bursty. Specifically, we  extract a small window of time series data surrounding the  change point xt: X = xt?Q, ..., xt+Q (e.g., Q = 20 seconds) and apply the fast Fourier transform (FFT) algorithm on X  to determine the coefficients that represent the amplitude of  each frequency component. We consider the top k (e.g., 90%)  frequencies in the frequency spectrum as high frequencies.

We apply inverse FFT over the high frequency components  to synthesize the burst signal. We then use the burst signal  magnitude (e.g., 90th percentile of the burst value) as the  expected prediction error for the change point xt. If the real  prediction error exceeds the expected prediction error, the  change point is selected as one abnormal change point. For  example, Figure 4 illustrates the expected prediction errors  for a system metric time series. We can see that the expected  prediction error is higher when the original time series are     ?? ???????  ? ?? ?????????????????	?	??????  ?? ???????  ?		??????????	????????	???  ?? ??????	????  ? ????????  ? ????????  ??? ??????  ?? ??????  ????? !?"??? ????????  #?? ??$ ????  ?? ???????  ?? ?????????????????	?	??????  Fig. 5. Faulty component pinpointing for the RUBiS online auction benchmark application.

bursty, and is lower when the time series become stable. In  Figure 3, our scheme correctly filters out the outlier change  point on the normal reduce node and only selects the abnormal  change point on the faulty map node.

After identifying a component exhibiting any abnormal  change, we need to know when the abnormal change starts.

During our experiments, we found that the selected abnormal  change point sometimes resides in the middle of the fault  manifestation process instead of at the beginning, depending  on the evolving pattern of the fault manifestation (e.g., gradual  change or bursty change). Thus, FChain performs tangent-  based rollback to identify the precise start time of the abnormal  change. Specifically, starting from the abnormal change point,  we compare the tangent of the current change point with  that of its preceding change point. If their values are close  (e.g., < 0.1), we roll back to the preceding change point. We repeat the roll-back process until the tangents of two adjacent  change points are distinct. If a component has multiple metrics  exhibiting abnormal changes, we pick the earliest abnormal  change start time as the component?s abnormal change start  time.

C. Integrated Faulty Component Pinpointing  Our faulty component pinpointing algorithm consists of  three steps: 1) deriving the abnormal change propagation  paths in the examined distributed application by sorting the  start time of abnormal changes at different components; 2)  pinpointing faulty components based on the selected abnormal  change propagation paths; and 3) refining pinpointing results  by filtering out spurious abnormal change propagation paths  using inter-component dependency information. For example,  Figure 5 shows the pinpointing process for the RUBiS online  auction benchmark application [16].

If the abnormal change onset time of C1 is earlier than that  of C2, we say that the abnormal change propagates from the  component C1 to the component C2. For example, in Figure  5, the application server 1 starts to exhibit abnormal change  at time t1 = 200 seconds and the database server starts to show abnormal change at time t3 = 210. Thus, we can infer that the abnormal change starts at the application server 1 and  propagates to the database server.

To pinpoint faulty components, FChain first sorts all the  components into a chain based on their fault manifestation  time. We first pinpoint the source component in this chain  as the faulty component since it has the earliest fault man-  ifestation. We continue to examine the other components in  the chain following the time order. If the fault manifestation  time of the next component is close to the pinpointed com-  ponent (e.g., time difference ? 2 seconds), we infer that the component?s abnormal behavior is probably not caused by the  anomaly propagation but a concurrent fault that occurs at a  similar time. Thus, we will pinpoint this component as one  faulty component as well.

If all the application components contain fault manifesta-  tions and the changes at all the components follow the same  upward or downward trend, FChain infers that the performance  anomaly is probably caused by some external factors such  as workload increases (i.e., upward trend) or a NFS server  problem (i.e., downward trend). In this case, FChain will  not pinpoint any component within the application as faulty.

Although previous work [22] has also addressed the problem  of distinguishing workload changes from anomalies within  a single component, our work provides workload change  detection for distributed applications.

We may derive spurious abnormal change propagations  between independent components. For example, in Figure 5,  we will derive an abnormal change propagation path from  the application server 1 to the application server 2 based on  the timestamps of their abnormal change points. However,  this propagation actually does not exist. We propose to use  inter-component dependency relationships to filter out spurious  propagation paths. For each suspicious component that con-  tains the abnormal change point, we examine whether there  is a path in the dependency graph from any pinpointed faulty  components to this component. If no path in the dependency  graph can be found, we pinpoint this component as a faulty one  since the anomaly propagation is unlikely and the component?s  anomalous behavior must have been caused by an independent  fault. We leverage previous black-box dependency discovery  tools [11] to discover inter-component dependencies.3  However, we cannot solely rely on the dependency in-  formation for fault localization since the abnormal change  propagation does not always follow the dependency path.

For example, in RUBiS, the faulty application server can  cause its upstream component (the web server) to exhibit  abnormal behavior due to a back-pressure effect, that is, a  faulty component might cause its upstream component to  show anomalous behavior.4 If we only rely on the dependency  information, we will pinpoint the normal web server as the  faulty component and miss the true culprit component, that is,  the application server. Our experimental results in Section III  will confirm this observation.

We also found that existing network trace based depen-  3To achieve high accuracy, the black-box dependency scheme needs to accumulate sufficient amount of network trace data [11]. Luckily, the appli- cation dependency information rarely change during application runtime. We perform the dependency discovery offline and store the results in a file for later reference.

4The cause of the back-pressure varies among different applications. One common reason is that after the input buffer of the faulty component becomes full, it forces the upstream component to drop data or pause processing.

dency discovery scheme fails to discover any dependency  information in the data stream processing system [14]. The  reason is that the dependency discovery algorithm relies on  the gap between network packets to separate network flows.

However, the stream application processes continuous data  packets, which do not contain gaps between network packets.

Note that FChain can still pinpoint faulty components based on  the abnormal change propagation paths when the dependency  information is unavailable. In contrast, the dependency-only  scheme will fail the fault localization task for distributed  stream processing systems.



III. EXPERIMENTAL EVALUATION  We have implemented the FChain system on top of the  Xen platform [23], and conducted extensive experiments us-  ing the RUBiS multi-tier online auction benchmark (EJB  version) [16], the IBM System S data stream processing  system [14], and the Hadoop MapReduce framework [15].

In this section, we first describe our evaluation methodology  followed by the experiment results.

A. Evaluation Methodology  Our experiments were conducted on the NCSU?s Virtual  Computing Lab (VCL), a production cloud computing infras-  tructure that operates in a similar ways as Amazon EC2 [1].

All the VCL hosts used in our experiments have a dual-core  Xeon 3.00GHz CPU, 4GB memory, and 30GB disk, which  are connected to Gigabit networks. Each host runs 64 bit  CentOS 5.2 with Xen 3.0.3. The guest VMs also run 64 bit  CentOS 5.2. FChain monitors each guest VM from Domain 0  using the libxenstat and libvirt libraries. Monitored  metrics are cpu usage, memory usage, network in, network  out, disk read, and disk write. The metric sampling interval is  1 second. To evaluate FChain in multi-tenant cloud computing  environments, we run three benchmark systems concurrently  on the same set of VCL hosts. We first describe the benchmark  systems used in our experiments as follows.

RUBiS online auction benchmark: We use the three-tier  online auction benchmark system RUBiS (EJB version). The  topology of the RUBiS system is shown in Figure 5. We  run each application component in one guest VM. In order  to evaluate our system under workloads with realistic time  variations, we use a client workload generator that emulates  the workload intensity observed in the NASA web server  trace beginning at 00:00:00 July 1, 1995 from the IRCache  Internet traffic archive [24] to modulate the request rate of  our RUBiS benchmark. The client workload generator also  tracks the response time of the HTTP requests it made. An  SLO violation is marked if the average request response time  is larger than 100ms.

Hadoop: We run Hadoop sorting application, one of the  sample applications provided by the Hadoop distribution. The  application consists of three map nodes and six reduce nodes.

The data size we process is 12GB, which is generated using  the RandomWriter application. We measure the progress score  of the job by calling the Hadoop API. An SLO violation is  marked when the job does not make any progress for more  than 30 seconds.

IBM System S: We use the commercial high-performance  data stream processing system [14], System S, developed by  IBM. In our experiments, we used a tax calculation appli-  cation, one of the sample applications provided by System S  product distribution. The topology of the System S application  is shown in Figure 2. Each PE runs in a separate guest VM.

In order to evaluate our system under workloads with realistic  time variations, we used the workload intensity observed in  the ClarkNet web server trace beginning at 1995-08-28:00.00  from the IRCache Internet traffic archive [24] to modulate the  data arrival rate. We measured the average per-tuple processing  time and an SLO violation is marked if the average processing  time is larger than a pre-defined threshold (e.g., 20ms).

Fault injection. We inject different faults (e.g., common  software bugs, bottleneck) during an application runtime. Each  application run lasts one hour. We inject one fault at a  random time instant to test FChain under different workload  conditions. For each fault, we use 30 to 40 application runs.

We test both single-component faults and multi-component  concurrent faults.

For RUBiS, single-component faults include: 1) MemLeak:  we start the program that has a memory leak bug in the  VM running the database server; 2) CpuHog: a CPU-bound  program competed CPU with the database server inside the  same VM; and 3) NetHog: we use httperf [25] tool to  send a large number of HTTP requests to the web server.

Multi-component concurrent faults include: 1) OffloadBug:  the application server 1 wants to offload some EJBs to the  application server 2. However, the program bug (JIRA #JBAS-  1442) in the application server 1 makes the remote server  lookup return the local server binding by mistake; and 2)  LBBug: a load balancing bug (mod jk 1.2.30) causes the web  server to dispatch requests unevenly. These two faults are real  software bugs found in the JBoss and Apache load balancer  software.

For Hadoop, we injected concurrent faults in all the map  nodes: 1) Concurrent MemLeak: we injected a memory leak  bug into all the map tasks, which allocated memory from the  heap without releasing; 2) Concurrent CpuHog: we injected  an infinite loop bug in all the map tasks; and 3) Concurrent  DiskHog: we start a disk I/O intensive program in the Domain  0 of each host running the map tasks.

For System S, we inject the following single-component  faults: 1)MemLeak: We inject a small snippet of code that has  a memory leak bug into a randomly selected PE; 2) CpuHog:  a CPU-bound program competes CPU with a PE within the  same VM; 3) Bottleneck: we make one randomly selected  PE the bottleneck by setting a low CPU cap over the PE.

The multi-component concurrent faults include: 1) Concurrent  MemLeak: we start the memory leak program simultaneously  in two randomly selected PEs; and 2) Concurrent CpuHog:  we start the CPU intensive program simultaneously in two  randomly selected PEs.

We compare FChain with a set of existing black-box fault     localization schemes:  1) Histogram: This scheme computes an anomaly score  for each system-level metric using Kullback-Leibler diver-  gence [26] between the histogram of the most recent data  contained in the same look-back window as FChain and the  histogram of the whole data. It then pinpoints abnormal com-  ponents based on the anomaly scores. We vary the anomaly  score threshold to show the tradeoff between the true positive  rate and the false positive rate. This scheme has been used by  previous work for detecting anomalies (e.g., [10]).

2) NetMedic [9]: It is a recently developed application-  agnostic multi-metric fault localization tool. The abnormal  component pinpointing is based on the application topology  and the inter-component impact learned from the historical  data. This scheme needs to assume the knowledge of the ap-  plication topology. For estimating the inter-component impact,  we use the same 1800 seconds of recent data as specified in  [9]. Different from FChain, NetMedic just gives a ranked list  of all components based on their likelihood of being the faulty  components. We first pinpoint the top impact component as the  faulty component. We also pinpoint the following components  whose impact difference with the top ranked component is less  than a certain threshold ?. We adjust the value of ? to show the  tradeoff between the true positive rate and the false positive  rate that can be achieved by NetMedic.

3) Topology: This scheme assumes the knowledge of the  application topology. It first detects abnormal components  using the outlier change point detection algorithm developed  in our previous work PAL [13]. It then pinpoints faulty  components based on the topology information, that is, if the  abnormal component C2 depends on the abnormal component  C1, we pinpoint C1 as the faulty component. By comparing  FChain with this scheme, we want to show that it is insufficient  to just consider the application topology for pinpointing faulty  components.

4) Dependency: Instead of assuming the application topol-  ogy knowledge, this scheme uses the black-box dependency  discovery tool [11] to dynamically extract the inter-component  dependency information. It first detects abnormal components  using the same outlier change point detection algorithm as the  Topology scheme. It then pinpoints faulty components based  on the discovered dependency information. If no dependency  information is discovered, this scheme will output all the com-  ponents that have outlier change points as faulty components.

By comparing FChain with this scheme, we want to show that  it is insufficient to just rely on the dependency information for  pinpointing faulty components.

5) PAL [13]: This is the initial version of our change propa-  gation based fault localization system. However, different from  FChain, PAL does not perform predictability-based abnormal  change point selection or consider the dependency information  in fault localization. It also does not support online validation.

6) Fixed-Filtering: This scheme uses the same pinpointing  algorithm as FChain except that it employs a fixed prediction  error filtering threshold to select the abnormal change points.

We varied the filtering threshold to show different accuracy re-  Fig. 6. Fault localization accuracy comparison for the single-component faults for RUBiS.

Fig. 7. Fault localization accuracy comparison for the single-component faults for System S.

sults that can be achieved by this scheme. We compare FChain  with this fixed filtering scheme to show the effectiveness of  our burst based filtering scheme.

To quantify the accuracy of different fault localization  schemes, we use the standard precision and recall metrics.

Let Ntp, Nfn and Nfp denote the number of true positives  (correctly pinpoint a faulty component), false negatives (miss  a faulty component), and false positives (pinpoint a normal  component as faulty), respectively. We calculate the precision  and recall metrics in the standard way as follows,  Precision = Ntp  Ntp +Nfp , Recall =  Ntp  Ntp +Nfn (1)  We evaluate the accuracy of different pinpointing algorithms  using the commonly used ?receiver operating characteristic?  (ROC) curve whose X-axis and Y-axis show the recall and  precision, respectively. A perfect pinpointing scheme should  achieve 100% precision and 100% recall.

In our experiments, we configure the FChain system as  follows. We set the look-back window (W ) to be 100 seconds  for all the tested faults except the DiskHog fault in the  Hadoop application. The reason is that the DiskHog fault  takes much longer time to manifest than the other faults.

The 100 seconds look-back window will not cover the initial  stage of the fault manifestation. Thus, we used a longer look-  back window (500 seconds) for the DiskHog fault. We use  a concurrency threshold of 2 seconds to classify concurrent  faults, that is, if the abnormal change point time difference  between two components is less than 2 seconds, we consider  these two components as concurrent faulty components. The  burst extraction window Q is set as 20 seconds. We use the     top 90% frequencies to synthesize the burst signal and use the  90th percentile of the burst value as the expected prediction  error. We found those parameter configurations work well for  all the applications tested in our experiments. We also conduct  sensitivity study on those parameters and will show the results  in Section III-F.

B. Single-Component Fault Localization Results  Figure 6 shows the pinpointing accuracy results for RUBiS  under three single-component faults. We observe that FChain  consistently achieves the highest precision and recall for all the  faults. We observe that the Histogram scheme does not work  well for the CpuHog and NetHog faults that manifest very  quickly. The reason is that when the performance anomaly  is detected, the histogram of the recent data has not shown  significant difference from the histogram of all historical data  yet since the fault manifestation duration is very short. The  histogram scheme works better for gradually changing faults  such as memory leak although it is still less accurate than  FChain. We observe that NetMedic could not achieve high  accuracy during this set of experiments. After examining the  logs, we found that the pinpointing errors are caused by unseen  states that make the system assign inaccurate impact values.5  During the fault injection, the faulty component and the other  affected components often have a state that is not present in  the historical data. In contrast, we observe that FChain is not  susceptible to the problem of unseen values.

By employing predictability-based filtering and leveraging  dependency information, FChain effectively removes irrelevant  change points caused by normal workload fluctuations. Thus,  FChain can achieve higher accuracy than the other change  point based schemes such as Topology, Dependency, and PAL.

Since the dependency discovery scheme accurately identifies  all the dependencies in the RUBiS system, the Dependency  scheme has the same accuracy as the Topology scheme in this  case. Particularly, the Topology and the Dependency schemes  have very low accuracy for the MemHog and CpuHog faults.

The reason is that we injected those two faults at the database  server that is the last tier in the RUBiS system. We observed  the ?back-pressure? symptom mentioned in Section II-C. The  faulty database server causes its upstream component (the web  server or the application server) to exhibit anomalous behav-  iors. If we perform pinpointing based on the dependency or  topology, we will mistakenly pinpoint the upstream component  of the culprit component as the faulty one. We injected the  NetHog fault in the web server that is the first tier in RUBiS.

Thus, both Topology and Dependency perform well since the  back-pressure problem does not exist. In contrast, FChain is  not sensitive to the location of the faulty component, which  can achieve high accuracy for all situations. Although FChain  also considers the dependency information, we observe that  when a fault propagates back to the upstream components, its  impact becomes smaller. The abnormal change point selection  step can effectively filter out those change points.

5NetMedic assigns a default high impact value (0.8) to an edge connecting to the abnormal component with a previously unseen state.

Figure 7 shows the fault localization accuracy comparison  results for the System S single component faults. Similar  to the RUBiS experiments, FChain consistently achieves the  highest precision and recall values for all the tested faults. The  dependency discovery scheme fails to detect any dependency  relationship for System S due to the reason mentioned in  Section II-C. Thus, the Dependency scheme pinpointed all the  components that have outlier change points. This is reason why  the Dependency scheme has low precision for all the cases.

The Topology scheme does not perform well for the MemHog  and the bottleneck faults because of the same back-pressure  problem mentioned in the RUBiS results. We also observe that  all the schemes have low precision for the bottleneck fault. The  reason is that the fault propagates very quickly due to high-  throughput communication between stream processing com-  ponents. Thus, it is difficult to distinguish single-component  faults from concurrent multi-component faults, which explains  why the precision is low. Luckily, we can use the online  validation to quickly remove those false alarms, which will  be shown in Section III-D.

C. Multi-Component Concurrent Fault Localization Results  We now evaluate FChain using multi-component concurrent  faults. Figures 8, 9, and 10 show the pinpointing accuracy  results for RUBiS, System S, and Hadoop, respectively.

We observe that FChain consistently achieves high precision  and recall results in all the tested cases except the concurrent  CPUHog in System S. After examining the log file, we find  the diagnosis errors are mostly caused by the side-effect of  smoothing. Although our previous work [13] showed that  smoothing helps to remove the random noise in the raw  monitoring data, smoothing in this case causes the time of  the abnormal change point in the affected normal component  to become earlier than those of true culprit components. We  need to perform adaptive smoothing to address this problem,  which is part of our on-going work.

Compared to RUBiS and System S, Hadoop is much more  dynamic with highly fluctuating system metrics. In this case,  the simple change point detection schemes such as PAL have  low accuracy, especially for the CpuHog and DiskHog faults.

In the Hadoop experiments, we inject faults into all the map  nodes that are the first components in the topology order.

The ?back-pressure? problem does not exist in this case.

This explains why Topology and Dependency achieve high  accuracy. NetMedic also achieves high precision and recall  values in the MemLeak and CPUHog faults. The reason is  that the default high impact value for unseen states happen to  be correct. However, for the DiskHog fault, the default high  impact value is incorrect, which causes NetMedic to have low  accuracy. In contrast, FChain can handle previously unseen  values and consistently achieve high accuracy.

D. Online Pinpointing Validation Results  We now evaluate our online validation scheme. We pick two  most challenging faults where all the schemes do not perform  well. They are the Bottleneck fault and the concurrent CpuHog     Fig. 8. Fault localization accuracy comparison for the multi-component faults in RUBiS.

Fig. 9. Fault localization accuracy comparison for the multi-component faults in System S.

Fig. 10. Fault localization accuracy comparison for the multi-component faults in Hadoop.

fault in System S. Figure 11 shows the pinpointing accuracy  results for different schemes. The ?FChain+VAL? denotes  the FChain scheme with the online pinpointing validation  activated. Note that the results for the FChain scheme shown  before are the results achieved by FChain without the online  validation. We observe that our online validation scheme can  successfully remove most false alarms for these two faults.

FChain can quickly identify the true faulty component(s)  by properly scaling the right resource metric. However, our  current online validation scheme cannot help to improve the  recall value, which is part of our on-going work.

E. Comparison with Fixed Filtering Schemes  We now compare FChain with the Fixed-Filtering scheme.

Due to space limitations, we only show a subset of our results.

Fig. 11. Online validation effectiveness for two challenging System S faults.

Fig. 12. Fault localization accuracy comparison with the Fixed-Filtering scheme for LBBug in RUBiS and DiskHog in Hadoop.

Figure 12 shows the accuracy of the Fixed-Filtering scheme for  a subset of faults in RUBiS and Hadoop. We observe that the  Fixed-Filtering scheme is very sensitive to the prediction error  filtering threshold value. In contrast, FChain can automatically  infer the optimal (or near optimal) filtering threshold based on  the burstiness in the metric values.

F. Sensitivity Study  FChain parameters NetHog CPUHog Diskhog  RUBiS System S Hadoop  Look-back 100 P=1, R=1 P=0.97, R=1 P=0.56, R=0.63 window W 300 P=0.98, R=1 P=0.95, R=0.95 P=0.88, R=0.9 (sec) 500 P=0.98, R=1 P=0.92, R=0.95 P=0.88, R=0.92  Concurrency 2 P=1, R=1 P=0.97, R=1 P=0.88, R=0.92 threhold 5 P=1, R=1 P=0.93, R=1 P=0.88, R=0.88 (sec) 10 P=0.97, R=1 P=0.93, R=1 P=0.83, R=0.88  TABLE I PRECISION (P) AND RECALL (R) VALUES UNDER DIFFERENT  CONFIGURATIONS OF THE KEY FCHAIN PARAMETERS.

We have conducted sensitivity study to evaluate the impact  of key FChain system parameters to its pinpointing accuracy.

Due to space limitations, we only show a subset of our results  in Table I with the optimal parameter settings highlighted in  bold. Overall, we found that FChain is not sensitive to different  parameter values. We observe that FChain can achieve the  optimal performance using default setting (100 seconds look-  back window, 2 seconds concurrency threshold) for all the  tested faults except one case that is the look-back window     System Modules CPU cost  VM monitoring (6 attributes) 1.03?0.09 milliseconds Normal fluctuation modeling 22.9 ? 2 milliseconds (1000 samples) Abnormal change point selection 602.4 ? 105.2 milliseconds (100 samples) Integrated fault diagnosis 22 ? 1 microseconds Online validation (per-component) 30 ? 1 seconds  TABLE II FCHAIN OVERHEAD MEASUREMENTS.

size for the DiskHog fault in Hadoop. The reason has been  described in Section III-A. Generally, the look-back window  should be long enough to capture the fault manifestation.

We are currently investigating an adaptive look-back window  configuration scheme by examining the metric changing speed.

G. FChain System Overhead Measurements  We now evaluate the overhead of the FChain system. Table  II lists the CPU cost of each key module in FChain. We  observe that most modules in FChain is light-weight. The  most computation-intensive module is the abnormal change  point selection component, which is triggered only when  a performance anomaly occurs. FChain also distributes the  change point computation load on different hosts and executes  them in parallel to achieve scalability. The online validation  takes about 30 seconds for each component since we need  some time to observe scaling impact for deciding whether we  have made a pinpointing error. However, the online validation  is only performed on those suspicious components pinpointed  by the integrated fault diagnosis module. The FChain daemon  running inside the Domain 0 of each host imposes less than 1%  CPU load and consumes about 3MB memory during normal  execution.



IV. RELATED WORK  Our work is first closely related to previous black-box  fault localization schemes. For example, NetMedic [9] pro-  vided detailed application-agnostic fault diagnosis by learning  inter-component impact. NetMedic first needs to assume the  knowledge of the application topology. To perform impact  estimation, NetMedic needs to find a historical state that is  similar to the current state for each component. However, for  previously unseen anomalies, we might not be able to find a  historical state that is similar to the current state for the faulty  components. Under those circumstances, NetMedic assign a  default high impact value, which sometimes lead to inaccurate  diagnosis results as shown in Section III. In comparison,  FChain can diagnose previously unseen anomalies and does  not assume any prior application knowledge. Oliner et al. [10]  proposed to compute anomaly scores using the histogram ap-  proach and correlates the anomaly scores of different compo-  nents to infer the inter-component influence graph. As shown  in Section III, it is difficult for the histogram-based anomaly  detection to perform online fault localization over suddenly  manifesting faults. Moreover, unrelated components can have  indirect correlations caused by workload fluctuations, which  will cause their system to raise false alarms. In comparison,  FChain is more robust to different types of faults and workload  fluctuations.

To achieve black-box diagnosis, researchers have also ex-  plored various passive network traffic monitoring and analysis  techniques such as Sherlock [11], Orion [27], SNAP [28].

However, those analysis schemes can only achieve coarse-  grained machine-level fault localization. Additionally, during  our experiments, we found that previous network trace analysis  techniques cannot handle continuous data stream process-  ing applications due to the lack of gaps between packets  for extracting different network flows. Project5 [29] and  E2EProf [30] performed cross-correlations between message  traces to derive causal paths in multi-tier distributed systems.

WAP5 [31] extends the black-box causal path analysis to  support wide-area distributed systems. Orion [27] discovers  dependencies from network traffic using packet headers and  timing information based on the observation that the traffic  delay distribution between dependent services often exhibits  typical spikes. LWT [32] proposed to discover the similarity  of the CPU usage patterns between different VMs to ex-  tract the dependency relationships between different VMs.

However, as shown in our experiments, dependency-based  fault localization techniques are not robust, which can make  frequent pinpointing mistakes due to various reasons (e.g.,  the ?back pressure? effect in distributed applications, common  network services pinpointed as culprits). Furthermore, existing  dependency discovery techniques need to accumulate a large  amount of trace data to achieve reasonable accuracy. Particu-  larly, network trace based techniques only support request-  and-reply types of applications, which fail to discover any  dependency in continuously running applications such as data  stream processing systems. In contrast, FChain provides online  fault localization, which does not require any training data  for anomalies or a large amount of training data for normal  behaviors. FChain is fast, which can quickly localize faulty  components with high accuracy after the performance anomaly  is detected.

A flurry of research work has proposed to use end-to-end  tracing for distributed system debugging. Magpie [3] is a  request extraction and workload modelling tool that can record  fine-grained system events and correlate those events using an  application specific event schema to capture the control flow  and resource consumption of each request. Pinpoint [4] takes a  request-oriented approach to tag each call with a request ID by  modifying middleware platform and applies statistical methods  to identify components that are highly correlated with failed  requests. Monitor [33] tracks the requests exchanged between  components in the system and performs probabilistic diagnosis  on the potential anomalous components. X-Trace [5] is an  integrated cross-layer, cross-application tracing framework,  which tags all network operations resulting from a particular  task with the same task identifier to construct a task tree.

Spectroscope [34] can diagnose performance anomalies by  comparing request flows from two executions. In contrast,  our approach does not require any instrumentation to the     application or middleware platform to collect request flows.

Thus, it is much easier to deploy FChain in large-scale IaaS  clouds.

Blacksheep [35] correlates the change point of system-level  metrics (e.g., cpu usage) with the change in count of Hadoop  application states (i.e., events extracted from logs of DataN-  odes and TaskTrackers) to detect and diagnose the anomalies  in a Hadoop cluster. Kahuna-BB [36] correlates black-box data  (system-level metrics) and white-box data (Hadoop console  logs) across different nodes of a MapReduce cluster to identify  faulty nodes. In comparison, FChain is a black-box fault local-  ization system, which is application-agnostic without requiring  any knowledge about the application internals. We believe that  FChain is more practical and attractive for IaaS cloud systems  than previous white-box or gray-box techniques.



V. CONCLUSION  In this paper, we have presented FChain, a robust black-  box online fault localization system for IaaS cloud com-  puting infrastructures. FChain can quickly pinpoint faulty  components immediately after the performance anomaly is  detected. FChain provides a novel predictability-based ab-  normal change point selection scheme that can accurately  identify the onset time of the abnormal behaviors at different  components processing dynamic workloads. FChain combines  both the abnormal change propagation knowledge and the  inter-component dependency information to achieve robust  fault localization. FChain can further remove false alarms by  performing online validation. We have implemented FChain on  top of the Xen platform and conducted extensive experimental  evaluation using IBM System S data stream processing system,  Hadoop, and RUBiS online auction benchmark. Our exper-  imental results show that FChain can achieve much higher  accuracy (i.e., up to 90% higher precision and up to 20%  higher recall) than existing schemes. FChain is light-weight  and non-intrusive, which makes it practical for large-scale IaaS  cloud computing infrastructures.

