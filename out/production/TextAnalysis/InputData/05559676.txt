Data Quality Improvement using Fuzzy Association Rules

Abstract? The activities and decisions of organizations and companies are based on data and the information obtained from data analysis. Data quality plays a crucial role in data analysis, because the incorrect data leads to wrong decisions.

Nowadays, improving the data quality manually is very difficult and in many cases is impossible as data quality is one of the complicated and non-structured concepts and data refinement process can not be done without the help of professional domain experts, and detection and correction of errors require a thorough knowledge in the related domain of the data. Thus, the necessity of using (semi-)automatic methods is discussed to find data defects and errors and solve them. Because data mining methods are designed to discover interesting patterns in datasets, we can use them efficiently to improve different dimensions of data quality. In this paper, a new method is presented to measure the accuracy dimension of data quality using fuzzy association rules. Finally, Experimental results of the proposed method show the effectiveness of the proposed method to find incorrect values in datasets.

Keywords-data quality; data mining; fuzzy association rules

I. INTRODUCTION Widespread use of data and decisions based on data  analysis focuses on data quality in today?s business success.

Nevertheless, a study by the Meta group reveals that 41% of projects based on data analysis will be failed. As one of the main reasons, they identified insufficient data quality leading to wrong decisions [1].

Therefore, traditional methods of cleaning data can be used rarely. It is normally infeasible to guarantee data quality by manual inspection, especially when data are collected over long periods of time and through multiple generations of databases. Therefore, (semi-)automatic data cleaning methods have to be used [1].

Since the early 90s, knowledge discovery in databases  (KDD) has been introduced as a well established field of research, and over the years new methods together with scalable algorithms have been developed to analyze efficiently very large datasets. Unfortunately most of the orientation is towards the particular and theoretical problems.

The application of data mining methods to improve data quality is a relatively new and promising approach from research and usage viewpoint and can present new domains of their application outside the domain of pure data analysis [2].

Data quality studies have been accomplished using data mining methods. For example, Hipp and et al proposed developed an algorithm to extract association rules for data cleaning in which they ranked deviation from these rules based on confidence of violated rules [3].

In addition, Brodley and Friedl used data mining in preprocessing data in the process of knowledge discovery in database. By filtering the probably incorrect training data, they can significantly reduce the misclassifications, while their goal is to improve final classification accuracies, not detection of errors in training data [4]. Marcus and Maletic used different methods of data mining which include statistical methods, clustering, pattern-based methods and ordinal association rules for automatic error detection in real dataset. Their experiments showed that ordinal association rule is more efficient than the other methods, but it is appropriate only for the datasets whose lots of their attribute type is decimal or date [5].

Gr?ning showed that classifiers can be used in detection of conflicts in datasets and gave practical recommendations for data correction.  This approach uses support vector machines as a classification algorithm [6].

In this paper fuzzy association rules are used to enhance the data quality. The structure of this paper is as follows: Section 2 presents an introduction of data quality and different dimension of data quality. In Section 3, the data mining and its application for improving data quality is stated, and in Section 4, the proposed method is explained.

The experimental results of proposed method have been given and analyzed.

V1-468 Volume 1   C978-1-4244-7681-7/$26.00      2010 IEEE

II. DATA QUALITY Definition of data quality depends on the considered  purpose, so there is no unique definition which can be stated formally.  In literature, "appropriate for use" or "to meet end user needs" are used.  According to the general definition of quality management, we define quality of data to meet customer needs. Contrary to popular belief, the quality is not necessarily error at the level of zero [7]. Data will have high quality if they are appropriate for applications, decision making and planning.

Researches have defined different dimensions for data quality. Each dimension shows a particular view of quality.

The more interesting dimensions are as follow:  ? Accuracy: the distance between value of v  and value of 'v  that represents the entity which v  tries to portray it.

? Completeness: the database includes all of the related entities.

? Update: the rate of fast updating of data.

? Consistency: the rate of violation from the defined  significant rules on the dataset.

? Availability: the rate of data availability, being easy  and the speed of data retrieval.

? Timeless: over the time some of information looses  its credit. For example, information about the address of person at the time of entrance is completely correct and has enough quality but just this information would loose its credit over the time.

? Value added: the rate of being useful.

Different dimensions of data quality are not independent  of each other, and there may be a direct or indirect relation between them [6].

In this paper, the accuracy dimension has been considered.



III. DATA MINING AND ITS APPLICATION IN DATA QUALITY IMPROVEMENT  Data mining is an automatic process to extract the patterns as an implicit knowledge in the database. Data mining utilizes multiple scientific fields simultaneously, such as the technology of data base, artificial intelligence, machine learning, neural networks, statistics, pattern recognizer, knowledge based systems and information retrieval.

In 2001, Hipp introduced data quality mining as a new approach from research and application viewpoint. The goal of data quality mining is to employ data mining methods in order to detect, quantify, explain and correct data quality deficiencies in huge databases.

Generally, four important aspects are considered in data quality mining [3]:  a) Employment of data mining methods to measure and explain the data quality deficiencies  b) Employment of data mining methods to correct deficient  data  c) Extension of knowledge discovery process models to reflect potential of data quality mining  d) Development of specialized  process models for pure data quality mining

IV. THE PROPOSED METHOD Among data mining methods, association rules is a very  active research topic that has been used widely in lots of cases such as basket analysis, decision maker support systems, theft detection, etc. Data quality is also one of the topics where using association rules can be useful, because:  ? In comparison with other methods of data mining, association rules are understandable and can clearly discover and describe the dependency between data.

? Association rules are independent of each other.

Therefore, recision of simpler rules has no effect on the other rules.

? The applications of association rules have proved its usability in data quality. The evaluations done in [3, 5] show that association rules can improve data quality.

In [3], a method has been presented which uses association rules to discover error of transactions. In this method, rules and patterns available in data are initially discovered via Apriori algorithm. Then, each of the transactions is compared with discovered rules and any deviation from rules will be marked as error. In this method discovered association rules have a very important role in error detection.

One of the defects of presented method in [3] is that association rules can not show properly the associations between quantitative values. As an example in Table I, occupation and education are categorical attributes and income is numerical attribute. If we want to obtain association rules in these data with support >1, via Apriori algorithm, only we can find some BS degrees, whereas there is an association between income and occupation or education and income. For example, some one who have a high educational level have more income than the people who have a mean educational level. Apriori algorithm is not capable of discovering such associations between quantitative values. As much of data in real world contain quantitative values and there may be errors in this type of values, the proposed method in [3] cannot be an applicable to detect error. In this paper, the presented method in [3] will be extended and association rules between quantitative values will be discovered with the help of fuzzy sets concept.

A. Fuzzy Association Rule Fuzzy association rules are the rules which are extracted  from a fuzzy dataset. A fuzzy rule in the form of BA ? where A, B are fuzzy sets and 0?? BA , is called fuzzy association rule.

Using of fuzzy concepts has some advantages. First, the discovered association rules are more understandable. These concepts make a transition between numerical values of data and categorical concepts. Second, these concepts help discover the rules between quantitative values. For example,  V1-469 Volume 1     the following fuzzy sets are defined for the data shown in Table I:  High occupational level = {manager, deputy} Mean occupational level = {designer} High educational grade = {PHD, MS} Mean educational grade = {BS, 12th} High income = {500-700} Mean income = {200-300}   TABLE I.  INSTANCE DATABASE  OccupationIncome Education Transaction_id designer 200 BS 1 designer 250 BS 2 designer 300 12th 3 manager 500 PHD 4 manager 550 MS 5 deputy 600 PHD 6   The following fuzzy association rules may be stated by  the mentioned fuzzy sets: High occupational level? High educational grade Mean occupational level? Mean educational grade High occupational level? High income Mean occupational level? Mean income  Now assume the following transaction: Transaction_id 7: occupation= deputy, education= PHD,  income =200 Considering the dataset shown by Table I, an error can  be detected in the above transaction, as the person who is deputy cannot have income of 200, while his income is expected to be much higher. This transaction violates "High occupation level? High income" fuzzy rule. It should be noted that the error would not be detected by the rules extracted by Apriori algorithm.

B. proposed framework The proposed framework is consisted of steps shown in  figure 1.

Figure 1.  Proposed framework  The description of each step is as follows: a) Data preprocessing: in this step, data is converted to  standard format and lost values are managed. For example, the values of date attribute in some transactions may be in  the form of YYYY/MM/DD and in some other in the form of YYYY-MM-DD. To obtain a correct model of the associations, implementation of this step is necessary.

b) Mapping data to fuzzy values: fuzzy sets and fuzzy  membership functions for quantitative data are defined in accordance with the knowledge of experienced experts.

Then, data values are mapped to fuzzy values by fuzzy membership functions. Table II shows fuzzy mapping for income and occupation attributes of Table I.

TABLE II.  MAPPING TABLE I TO FUZZY VALUES  Mean income  High income  Mean occupational  level  High occupational  level  Fuzzytrans _id  1 0 1 0 1 0.98 0.01 1 0 2 0.90 0.02 1 0 3 0.08 0.85 0.03 0.95 4 0.04 0.90 0.01 0.95 5 0.03 0.93 0 1 6     c) Fuzzy association rules extraction: Fuzzy association rules are extracted, via the presented algorithm in [9], in which all attributes are weighted equally. In this step, the rules with high confidence level are considered.

Thus, rule set R is limited to  ? ??? ? )(| rconfidenceRrR  where ?  is the minimum confidence.

Table III shows some extracted rules from Table II.

TABLE III.  FUZZY ASSOCIATION RULES EXTRACTED FROM TABLE II  ConfidenceFuzzy association rule 0.52 High occupational level? High income 0.41  Mean occupational level? Mean income    d) Consistency check of transactions with discovered  rules: the consistency of transactions with discovered rules is checked in this step. Each of transactions may violate some rules and may be consistent with other rules. Also, it may not fire some of the rules. Let R be the set of fuzzy association rules and Let D be the database of fuzzy transactions. Let YXr ??  be a Fuzzy association rule and X? be membership function of X.  the mapping that determines whether a fuzzy transaction DT violates a rule Rr , is defined as:    ?  ? ??	?  ??  else satisfyruleTsatisfyruleTifT  RDViolate  YXY  _)(_)()(1  ]1,0[: ???  ???  rule_satisfy parameter is the threshold for fuzzy rule satisfaction.

Database  Data preprocessing  Mapping data to fuzzy values  Association rules extraction  Consistency check of transactions with discovered rules  Scoring and ranking the transactions  V1-470 Volume 1     In (1), )(1 TY??  shows the degree that fuzzy transaction T violates r, the more )(1 TY?? , the more inconsistency of transaction T with rule r.

As an example, the consistency check for transaction 6 is shown in Table IV.

TABLE IV.   CONSISTENCY CHECK FOR TRANSACTIONS 6 WITH RULES IN TABLE III  Inconsistent rule  Consistent rule  Non-correlated rule  Fuzzytrans_id  _  High occupational level? High  income  Mean occupational level? Mean  income      e) Scoring and ranking the transactions: In order to assign scores to the transactions, the highest confidence of violated rules will be assigned to the corresponding transaction. Then the transactions are sorted in descending order. It is well-worth mentioning that the highest confidence of violated rules is not only simple and acceptable measure, but also eliminates extra calculations.

Because, if a rule is violated by a transaction, there will be no need to check rules having confidence less than violated rule.



V. EVALUATION RESULTS In this section, proposed method is evaluated and the  results are compared with presented method in [3]. All the evaluations have been implemented on a system equipped with Core 2 Duo 2.26 GHz CPU, 3GB RAM and Win 7.

Usually for evaluating the performance of information retrieval and classification algorithms, precision and recall measures are used. These measures are defined by (2) and (3) based on the confusion matrix entries, shown in table V. In this paper, in addition to these two measures, the runtime of the two methods has been also compared.

TABLE V.  CONFUSION  MATRIX  Transactions classification by proposed  method   Incorrect  transactions Correct  transactions FN  TP Correct transactions  Real  Classification TN  FP  Incorrect transactions    Precision = FPTP  TP ?  (2)  Recall = FNTP  TP ?  (3)    In order to evaluate the proposed method, Adult dataset from UCI has been used. This dataset includes two numerical attributes called income and hpw and two categorical attributes called occupation and education. 1000 instances of the transactions of this dataset have been selected and manipulated to make imitation errors. Then, proposed method and the presented one in [3] are applied to the manipulated dataset. In the tests related to precision and recall, min_support, min_confidence and min_rule_satisfy are considered to be 0.2, 0.7 and 0.4, respectively.

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8  0 20 40 60 80 100  Number of Error IN Dataset Pr  ec is  io n  Fuzzy Association Rule Apriori   Figure 2.  The precision measure for the proposed method and the method  described in [3]   Figure 2 compares the precision of the proposed method  with the method proposed in [3], in terms of number of created errors in dataset. The figure shows that the proposed method outperforms the method proposed in [3], as lots of the association between quantitative data in dataset are not discovered by Apriori algorithm and play no role in transactions? ranking. Some attributes? values are in a range which causes rules? support would not be satisfied (above example). Precision curves have wavy shapes, because any change in data causes a change in the support of rules which can affect their detections. Figure 5 also compares the recall measure for the two methods. It is clear that the proposed method delivers better results.

In the evaluation related to runtime of the two methods, min_support parameter has been changed to 0.05 in order to discover more rules by both algorithms. According to figure 4, runtime for the proposed method is better. The main reason is due to the difference in runtime of association rules discovery algorithm and ranking the transactions.

Fuzzy association rules algorithm is much faster than Apriori algorithm, because Apriori algorithm must check all of the numerical and categorical data in the dataset to find frequent itemsets. Also, it should find the support of each of itemsets which is time consuming due to variety of numeric values. While fuzzy association rules algorithm only works with the fuzzy sets whose numbers are more limited.  The ranking of transactions, based on highest confidence of violated rules, needs fewer computations, because when a  V1-471 Volume 1     transaction violates a rule, there would be no need to check rules having confidence less than that of violated rule.

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9   0 20 40 60 80 100  Number of Error In Dataset  R ec  al l  Fuzzy Association Rule Apriori   Figure 3.  The recall measure for the proposed method and the method  described in [3]           0 100 200 300 400 500 600 700 800 900 1000  Number of Transactions  R un  Ti m  e( se  co nd  )  Fuzzy Association Rule Apriori Figure 4.  The runtime for the proposed method and the mothod presented  in [3]

VI. CONCLUSION Automated data quality improvement is a necessity for  finding incorrect data in large databases. In this paper, a method based on data mining approaches is presented to improve data quality. This paper?s proposed approach for data quality improvement uses fuzzy association rules, by which hidden rules in datasets will be discovered. Then, the consistency of transactions with these rules is checked and a score is assigned to each transaction. This score generally shows inconsistency of a transaction with the rules set. In the proposed method, high scores are assigned to the transactions which are suspicious to have defects. User?s knowledge on suspicious transactions and background knowledge about data will be used in the process of determining the accuracy of transactions and ultimately total quality of dataset. Evaluation results show the effectiveness of the proposed method.

