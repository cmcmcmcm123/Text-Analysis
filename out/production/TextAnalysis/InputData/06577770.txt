Towards Reconfigurable Processors for Power-Proportional Computing

Abstract?Today we witness a major shift in microelectronics engineering priorities. The cost of computation is no longer dominated by the cost of building computation hardware. Almost every consumer electronic device nowadays contains a powerful processor with one or, increasingly often, multiple computation cores that are cheap to manufacture. Growing demand for high- definition content and associated processing capabilities revealed the true cost of computation ? energy. ?Big data? companies face large electricity bills; in other applications, such as healthcare and infrastructure monitoring, the energy constraint has a different form: instead of a large bill, one faces limited energy availability and has to resort to such measures as energy- harvesting. In both cases it is essential for the computation system to be adaptable to the inflow of energy and computation load which often vary dramatically during runtime.

In this paper we discuss an approach to building processors with reconfigurable computation pipeline capable of changing the way they fetch and execute instructions depending on energy availability and application requirements. We show how to use Conditional Partial Order Graphs to model the microarchitecture of such a processor, discuss implementation details, and present preliminary experimental results on an example of Intel 8051 microcontroller.



I. INTRODUCTION From 2007 our society uses more energy for browsing  the Internet than for air travel [3]. It is also predicted that the energy (and environmental) footprint of computation and data traffic will steadily increase in the near future: Data centres will grow and so will the network infrastructure together with the number of terminal nodes of the global information network such as computers, phones, gadgets and other connected cyber-physical devices (so called Internet of Things). Energy-efficiency of components at all levels of the computation hierarchy is thus becoming a major concern for the microelectronics industry. A serious factor impeding the progress in addressing this concern is a wide gap between the ways of how energy efficiency is approached by hardware and software engineers, and this gap is matched by the lack of common understanding between the two communities.

In this paper we discuss an approach to bridging this gap by developing a shared design criterion, called power- proportionality, on the basis of which both electronics and programming solutions can be judged. A computing sys- tem, for it to be considered power-proportional, has to keep power consumption and computation load proportional to each other [8]. That is, an idle system would ideally consume no power, whereas given a small energy budget the system would adapt itself by reducing its computation flow and lowering  the delivered Quality-of-Service (QoS), and still remain func- tional; see Section II for a detailed discussion. The state-of- the-art systems have generally poor power-proportionality; for example, the servers used in data centres typically consume 50-60% of peak power under 10% of peak load [4]. Why are modern systems not power-proportional? We hypothesise that because they are designed to operate in a narrow scope of conditions, typically optimised for either high performance or low power consumption. This approach is inherently flawed because all the design effort is focused on one particular operation mode disregarding others.

Applications Operating System Drivers/Libraries  Protocols/Interfaces Microcontrollers  Components  Software layers  Hardware layers  Meters  feedback control  Figure 1: Cross-layer power proportionality  Another issue that makes it difficult to handle unpredict- able environmental conditions is the absence of a cross-layer feedback-control loop between energy supply components (in hardware) and an operating system or an active application (in software); see Figure 1. As a result, software is unable to steer, or modulate, computation with the aim of maximising QoS by prioritising tasks, trading accuracy for energy, and, in general, by strategic planning of energy resource utilisation.

In this paper we discuss cross-layer hardware-software mech- anisms for online energy monitoring and reconfiguration with particular focus on design and implementation of a power- proportional microprocessor, capable of adapting to varying operating conditions (such as low or even unstable voltage level) and application requirements in runtime.



II. RECONFIGURABILITY AND POWER PROPORTIONALITY How can one build a reconfigurable and adaptable pro-  cessor? The first thought could be to implement it in the field- programmable gate array (FPGA) technology allowing static and, sometimes, dynamic reconfiguration. Such a processor would be capable of adjusting its internal structure or beha- viour by rewiring interconnections between its components       Sy st  em  Service provision  Service demand  Power supply  Power consumption  Service provision  Power supply  saturation  power-proportional interval of operation  ideal power-proportionality minimum operating power  interval of unutilised energy  Power consumption  Service demand  saturation  power-proportional interval of operation  ideal power-proportionality  minimum power  interval of wasted energy  Figure 2: Power proportionality: power supply/consumption and service demand/provision interplay  or even by changing functionality at the level of individual gates. This technology can provide a very fine-grain control over a system at runtime, however, the associated overheads are extremely high. In particular, in terms of energy consump- tion, FPGAs are typically 10-15 times more expensive than application-specific integrated circuits (ASICs).

Since the fine-grain reconfigurability offered by FPGAs is overly costly, one shall consider the coarse-grain reconfigur- able architectures in the ASIC realm. They significantly lower the overheads by dropping reconfigurability in datapath com- ponents and utilising their custom designed versions instead.

The control logic and interconnect fabric, however, retain the capability to reconfiguration. In this setting, the key design and implementation challenge is to formally describe and synthesise the controller whose task is to coordinate hard system resources (the datapath components and interconnects) according to the runtime information on availability of soft system resources (energy, time); the latter can also include information on hardware faults in a system, thereby allowing the controller to bypass faulty components whenever possible.

The conventional approach to control logic specification and synthesis is to employ Finite State Machines (FSMs) [7] or interpreted Petri Nets (PNs) [2] as an underlying modelling formalism. Within this approach the designer explicitly de- scribes the controller?s behaviour for each combination of available resources and operating conditions. The number of such combinations and corresponding behaviours grows extremely fast with the size and degree of adaptability of the system. This leads not only to the state space explosion problem, but also to explosion of the specification size [6], thus slowing down the synthesis tools, reducing productivity, and increasing the overall cost of ASIC development.

Our approach is based on the crucial observation that the controller?s behaviours are strongly related to each other in different operating conditions. Indeed, when a system config- uration is changed incrementally, e.g., a datapath component goes offline and another one is used in its place, the overall behaviour of the controller is affected in the same incremental manner, therefore one would want to have a joint behaviour specification for these two configurations.

It has been demonstrated that the FSM and PN formalisms are not well-suited to describing families of many related behaviours [6] and the design methodologies based on them have poor scalability in the context of reconfigurable systems.

As an appropriate alternative, the Conditional Partial Order Graph model was introduced by Mokhov et al. [5][6]. The model was devised to allow implicit description of families of related behaviours in a compact form ? see Section III.

Figure 2 illustrates the concept of power-proportionality and its duality. On the leftmost chart one can see depend- ency of power consumption on service demand. Ideally, the former should grow proportionally to the latter: when there is no service demand, the system should consume (near) zero power, while any increase in service demand should cause a proportional increase in power consumption until the point of saturation, when the system can no longer serve all the incoming requests. Our design goal is to reduce the interval of wasted energy as much as possible.

On the right hand side of Figure 2 one can see the dual chart illustrating dependency of service provision on power supply.

Indeed, an ideal computation system should provide service proportionally to the inflow of energy. From this perspective, our design goal is to build computation systems that can start delivering certain level of QoS on as low power supply as possible, thus minimising the interval of unutilised energy.

The duality of power-proportionality creates a four di- mensional space of power-service interplay: a computation system can be considered a black box which has two inputs (service demand and power supply) and two outputs (service provision and power consumption) as illustrated in the centre of Figure 2. Our combined design goal is therefore to achieve proportionality between the inputs and outputs of the system.



III. CONDITIONAL PARTIAL ORDER GRAPHS  A Conditional Partial Order Graph [6] (further referred to as CPOG or graph) is a quintuple H = (V,E,X, ?, ?) where: ? V is a set of vertices which correspond to events (or  atomic actions) in a modelled system.

? E ? V ? V is a set of arcs representing dependencies  between the events.

? Operational vector X is a set of Boolean variables. An  opcode is an assignment (x1, x2, . . . , x|X|) ? {0, 1}|X| of these variables. An opcode selects a particular partial order from those contained in the graph.

? ? ? F(X) is a restriction function, where F(X) is the set of all Boolean functions over variables in X . ? defines the operational domain of the graph: X can be assigned      a: 1  d: 1  b: 1  c: x e: x_  x  x   x _  x _  x _  x _  ?(x)=1  (a) Full notation  a  d  b  c: x e: x_  x  x  x _  x _  x _  x _  ?(x)=1  (b) Simplified notation  Figure 3: Graphical representation of CPOGs  only those opcodes (x1, x2, . . . , x|X|) which satisfy the restriction function, i.e. ?(x1, x2, . . . , x|X|) = 1.

? Function ? : (V ? E) ? F(X) assigns a Boolean condition ?(z) ? F(X) to every vertex and arc z ? V ?E in the graph. Let us also define ?(z) df= 0 for z /? V ?E for convenience.

CPOGs are represented graphically by drawing a labelled circle for every vertex and drawing a labelled arrow for every arc. The label of a vertex v consists of the vertex name, a colon and the vertex condition ?(v), while every arc e is labelled with the corresponding arc condition ?(e).

The restriction function ? is depicted in a box next to the graph; operational variables X can therefore be observed as parameters of ?.

Fig. 3(a) shows an example of a CPOG with |V | = 5 vertices and |E| = 7 arcs. There is a single operational variable x; the restriction function is ?(x) = 1, hence both opcodes x = 0 and x = 1 are allowed. Vertices {a, b, d} have constant ? = 1 conditions and are called unconditional, while vertices {c, e} are conditional and have conditions ?(c) = x and ?(e) = x respectively. Arcs also fall into two classes: unconditional (arc c ? d) and conditional (all the rest). As CPOGs tend to have many unconditional vertices and arcs we use a simplified notation in which conditions equal to 1 are not depicted in the graph; see Fig. 3(b).

The purpose of conditions ? is to ?switch off? some ver- tices and/or arcs in a CPOG according to a given opcode, thereby producing different CPOG projections. An example of a graph and its two projections is presented in Fig. 4.

The leftmost projection is obtained by keeping in the graph only those vertices and arcs whose conditions evaluate to 1 after substitution of variable x with 1 (such projections are conventionally denoted by H|x=1). Hence, vertex e disappears (shown as a dashed circle ), because its condition evaluates to 0: ?(e) = x = 1 = 0. Arcs {a? d, a? e, b? d, b? e} disappear for the same reason; they are shown as dashed arrows . The rightmost projection is obtained in the same way with the only difference that variable x is set to 0; it is denoted by H|x=0, respectively. Note that although the condition of arc c ? d evaluates to 1 (in fact it is constant 1) the arc is still excluded from the resultant graph because one of the vertices it connects, viz. vertex c, is excluded and naturally an arc cannot appear in a graph without one of its vertices. Each of the obtained projections can be regarded as specification of a particular behavioural scenario  a  d  b  c: x e: x_  x  x  x _  x _  ?(x)=1 a  d  b  c e  x=1  a  d  b  c e  x=0 x _  x _  Figure 4: CPOG projections: H|x=1 (left) and H|x=0 (right)  of the modelled system, e.g. as specification of a processor instruction. Potentially, a CPOG H = (V,E,X, ?, ?) can specify an exponential number of different instructions (each composed from atomic actions in V ) according to one of 2|X|  different possible opcodes.



IV. IMPLEMENTATION  In this section we give a brief outline for our power- proportional implementation of Intel 8051 microcontroller designed using the CPOG model.

A. Design overview  The instruction set of Intel 8051 processor contains 255 instructions and our implementation supports all of them. We generally follow the standard 8051 architecture with two on- chip RAMs; the program is stored in a reprogrammable off- chip ROM. However several important changes were made: ? Our implementation is asynchronous, which gives certain  advantages over clocked design in case of varying oper- ating conditions. The communication between the control logic and the datapath units is arranged by means of request and acknowledgement signals.

? We use adjustable delay lines to achieve robust operation in a wide range of supply voltages (Subsection IV-B).

? We can choose how to execute an instruction depending not only on application or environmental aspects, but also on the functional correctness of individual components, thus addressing the issues of fault tolerance.

The overall CPOG specification of the microcontroller is shown in Figure 5. As explained in Section III, vertices cor- respond to datapath components and arcs model dependencies between them (see [5] for details on specific components).

B. Adaptability features  The most power and time consuming datapath compon- ents are adders, multipliers and dividers. To enable power- proportionality in a wide range of power supply and service demand conditions, we designed two sets of these arithmetic units: one optimised for low energy consumption and the other one optimised for high performance. Depending on whether      Figure 5: CPOG model of Intel 8051 control logic  there is a shortage of energy or on any other restrictions imposed by an active application, we can choose the most appropriate datapath unit to be used during an instruction execution. The decision can be made at different levels of system control: software level, sensors, external signals, etc.

Delay code Delay wrapper  req ack  variable delay line  Inputs Outputs  Register  Datapath computational unit  Figure 6: Adjustable delay lines Our implementation of self-timed datapath components is  based on bundled-data approach, where each component is ac- companied with a matching delay line to signal completion of its computation. In order to correctly function in a wide range of operating conditions (e.g., supply voltage or temperature), a component needs to adjust the latency of its completion signal.

We propose to address this issue by use of adjustable delay line, whose latency is selectable by the delay code in runtime, as shown in Figure 6.

C. Experiments  We implemented the design in STMicroelectronics 130nm technology process and conducted preliminary experiments to estimate the level of adaptability we can achieve using the discussed approach. Here we briefly report the results.

The nominal voltage is 1.2V but the chip is capable of operating in the range from 0.2V to 1.5V of supply voltages: ? 0.89V to 1.5V: full capability mode.

? 0.74V to 0.89V: at 0.89V the RAM starts to fail, so the  chip can only operate using internal registers.

? 0.22V to 0.74V: at 0.74V the program counter starts  to fail, so the chip gets stuck at executing the same  instruction forever. Surprisingly, the rest of the control logic synthesised using the CPOG model continues to operate correctly down to 0.22V.

In terms of performance, we measured the range from 67 millions of instructions per second on nominal voltage (1.2V) down to ~2700 instructions per second at 0.25V.



V. CONCLUSIONS We discussed an approach to architecting power-  proportional computation systems. The approach was tested by designing and implementing the Intel 8051 microcontroller capable of operating in the range from 0.2V to 1.5V of supply voltages.

Our future research is to push the boundaries of power- proportionality even further by using the self-timed SRAM capable of operating at low voltages [1], as well as by implementing the key control structures, such as program counter, using high-reliability logic with low fan-in gates.

ACKNOWLEDGEMENT This work was supported by EPSRC grant EP/I038357/1  (eFuturesXD, project PowerProp).

