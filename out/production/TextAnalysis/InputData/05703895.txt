On Classification Compatibility Laws of Zhongjing Prescriptions

Abstract?Zhongjing prescriptions are selected as data set in this paper. First, a classification association rules mining algorithm, which selected features based on information gain, short for CARM-IG, is achieved. The algorithm is used to mine compatibility laws of Zhang Zhongjing from cold-heat, deficiency-excess in eight principals of TCM (Traditional Chinese Medicine). Second, 21 rules which are applied to the treatment of cold, heat or cold-heat mixing syndromes are mined. And 29 rules which are applied to the treatment of deficiency, excess or deficiency-excess mixing syndromes are discovered too. The reduction of  run time and memory cost of algorithm CARM-IG had been proved, at the same time, the concision and precision of classification compatibility laws are guaranteed because of the deletion of irrelevant Chinese medicines with classification. Finally, the compatibility laws of Zhang Zhongjing are listed in this paper, which would be used to direct clinical practice sometimes.

Keywords-prescription compatibility laws of Zhang Zhongjing;  classification association rules;  features selection

I. INTRODUCTION Zhongjing prescriptions, which are created by Zhang  Zhongjing, have experienced approximately two thousand years of Clinical verification. The prescriptions are commended by the later generation as classical prescriptions because of strict compatibility and remarkable treatment result. Over the years, Zhongjing prescription research is more confined to personal summary about compatibility principle, clinical research, experimental study, classification of prescription etc. Zhongjing prescriptions have not been collected completely to establish a database, and mathematical, statistical and data mining methods have not been used to sum up compatibility principle of Zhongjing prescriptions objectively so far.

Then, 268 prescriptions from ?Shang Han Lun? and ?Jin Kui Yao Lue? which are written by Zhang Zhongjing are selected from the prescription database of TCM information system through excluding duplicate prescriptions and deleting prescriptions without medicine in order to establish data set of Zhongjing prescription in this research firstly.

Then Zhongjing prescriptions are classified by experts in the TCM field from two dimensions of cold-heat and deficiency- excess in eight principles. Finally, a classification association rules mining algorithm based on information gain feature selection CARM-IG is applied to mining latent compatibility  principles of Zhang Zhongjing, so that the compatibility principles of Zhang Zhongjing for the treatment of cold syndrome, heat syndrome, deficiency syndrome and excess syndrome etc. can be summed up objectively.



II. RELATED WORKS Currently, association rules mining [1], cluster [2],  correspondence analysis [3] etc. are data mining methods commonly used to discover prescription compatibility principle. Furthermore, several improved algorithms based on association rules mining have emerged [4]. A prescriptions data mining system integrated data preprocessing, high frequent item sets and association rules mining, cluster, classification etc. is developed in project DartGrid [5, 6]. New research way is explored because of the application of methods above for prescriptions compatibility principle mining, which is becoming one of the most important research directions for the interdisciplinary research of TCM in union with computer technology gradually.

Algorithm CMAR (Classification based on Multiple Association Rules) based on FP-growth method is proposed in 2001 [7]. The objective of algorithm CMAR is to solve the problem of classification through mining classification association rules. But it is assumed that decision attributes (or category attribute) are closely related any conditional attributes (the attributes except decision attributes) in algorithm CMAR. Higher memory cost and running time will be spend when super high dimension data set such as prescriptions data sets of TCM with hundreds dimensions is processed. The assumption above causes that irrelative attributes exist in mined classification association rules. So the index of IF * ICF (the inverse class frequency) is proposed in paper [8], which is used to delete the attributes with higher frequent but lower contribution for classification from VCFP-Tree in order to improved the performance of algorithm. The index of IF * ICF usually is useless When the classification number is lower so that a Chinese medicine has appeared in each type of prescription. The method of feature selection based on information gain is introduced in algorithm CMAR. A classification association rules mining algorithm CARM-IG for TCM prescription high dimension data set is established, which can reduce the time and space cost, and mine classification association rules with higher quality through the deletion of irrelevant Chinese medicines      with classification. Finally, the compatibility principles of Zhongjing prescriptions are summed up from different dimensions of classification.



III. FEATURE SELECTION BASED ON INFORMATION GAIN The basic idea behind feature selection is to compute  some measure that is used to quantify the relevance between an attribute and a given class label. Information gain is one of effective measures for feature selection. Such a measure is referred to as an attribute selection measure or a measure of the goodness of split. Information gain is defined as the difference of the expect information needed to classify a give  sample 1( ,..., )mI s s  and the expect information needed to  classify a give sample based on conditional attributes ( )E C .

Specific definition is given by  1( ) ( ,..., ) ( )mIG C I s s E C= ?                    (1) Let S be a set consisting of s data samples. Suppose the  class label attribute has m distinct values defining m distinct classes Di (for i=1,?, m ). Let si be the number of samples of S in class Di. The expect information needed to classify a give sample is given by  1 2  ( ,..., ) log ( ) m  m i i i  I s s p p =  = ?? (2)  Where pi is the probability that an arbitrary sample belongs to class Di, and is estimated by si/s.

Let attribute C have v distinct values{ }1,..., vc c . Attribute C can be used to partition S into v subsets{ }1,..., vS S , where Sj contains those samples in S that have value cj of C. Let sij be the number of sample of class Di in a subset Sj. The entropy, or expected information based on the partitioning into subsets by C, is given by    ...

( ) ( , ..., )  v j m j  j m j i  s s E C I s s  s=  + + = ?  (3)  Where 1 ...j mjs s  s + +  acts as the weight of the jth subset, and is the number of samples in the subset divided by the  total number of samples in S. 1( ,..., )j mjI s s  is defined  as  lo g ( )  m  ij ij i  p p =  ? ? , and | |  i j i j  j  s p  S =  is the probability that a sample in Sj belongs to class Di.

Anyway, the smaller the entropy value ( )E C , the greater  the information gain value ( )IG C , the greater the purity of the subset partitions, and the higher the correlation between conditional attributes and decision attributes. The lowest threshold of information gain is usually been set, and the attributes whose information gain value is higher than the threshold will be chosen as the distinguish attributes for classification. The threshold of information gain is set to 0.1 in this research refer book [11].



IV. CLASSIFICATION ASSOCIATION RULES MINING BASED ON INFORMATION GAIN FEATURE SELECTION  A. Variant of Classification Frequent Pattern Tree A variant of classification frequent pattern tree (VCFP-  Tree) is a compress storage structure of data set in order to mine classification association rules without candidate frequent item sets generation. A VCFP-Tree must satisfy condition below [9]:  1. A VCFP-Tree consists of one root labeled as "null", a set of item prefix subtrees as the children of the root, and a frequent-item header table F-list.

2. Each node in the item prefix subtree consists of five fields: item, child-link, parent-link, node-link, and class-list, where item registers which item this node represents, child- link links to child nodes of each node, parent-link links to parent node of each node, node-link links to the next node in the VCFP-Tree carrying the same item, or null if there is none, and class-list is a classification list which records every class label and its corresponding frequency.

3. Each entry in the frequent-item header list consists of three fields, (1) item-name; (2) start of node-link, which points to the first node in the VCFP-Tree carrying the same item; (3) information gain value of the attribute.

As shown in table  below, Chinese medicines usually is designated as attributes of prescriptions data set such as a and b etc. Whether a Chinese medicine appears in a prescription is been indicated by 0 or 1. Classification attributes often list in the end, such as classification tag. A VCFP-Tree (i.e., figure 1) is constructed based on the data set T. The order of items in the frequent-item header table F- list is a: 4-b: 3-c: 3-d: 3 with 3 as the threshold of support count. The information stored in the node indicated by deep color arrow is that there is one prescription containing Chinese medicines abd and belonging to class C.

TABLE I.  PRESCRIPTION DATA SET T.

No. a b c d E classification tag 1 1 0 1 0 1 A  2 1 1 1 0 0 B  3 0 0 0 1 1 A  4 1 1 0 1 0 C  5 1 1 1 1 0 C     Figure 1.  VCFP-Tree based on data set T.

B. Classification Association Rule Class association rule is an implication of the form R: C  D, where C D= , sup(R) min_sup, and conf (R) min_conf. C indicates conditional attribute (or item). D indicates decision attribute (or classification attribute). Sup (R) is the support of rule R, min_sup is the lowest threshold of support; Conf (R) is the confidence of rule R, min_conf is the lowest threshold of confidence. The support and confidence of rule R is defined below:  Sup (R) = P (C?D)                            (4) Conf (R) = P (D|C)                            (5)  P (C?D) indicates the joint probability of C and D. P (D|C) indicates the conditional probability that belongs to class D under the condition of Chinese medicine C appearance. Rule support and confidence are two important measures of rule interestingness. They respectively reflect the usefulness and certainty of discovered rules. The rules with higher support and confidence are usually chosen as interesting pattern.

For example: {Ramulus Cinnamomi *  Rhizoma Zingiberis Recens *  Radix Paeoniae Alba} {cold syndrome} (Support count=15, frequency of rule former piece=25) Where {Ramulus Cinnamomi * Rhizoma Zingiberis  Recens * Radix Paeoniae Alba} before  is the rule former piece; {cold syndrome } after  is the rule rear piece. The support count indicates the frequency of three Chinese medicines above appearing in a same prescription and treating cold syndrome, and is also called the frequency of rule; the frequency of rule former piece indicates the frequency of three Chinese medicines above appearing in a same prescription. There are 268 prescriptions in Zhongjing prescriptions. So the rule support is 0.056, which is computed through support count 15 divided by 268. The rule confidence is 60%, which is computed through support count 15 divided by the frequency of rule former piece 25.

C. Efficient Mining of Classification Association Rules based on Information gain Feature Selection The concrete steps of algorithm CARM-IG which are  used to mine classification association rule effectively are as following.

Algorithm: CARM-IG Input: min_sup: the lowest threshold of support  min_conf: the lowest threshold of confidence min_IG: the lowest threshold of information gain.

Output: CAR the set of classification association rules.

Begin (1) Scans Zhongjing prescription database once. The  information of relevant frequencies is recorded, so that the information gain value of every conditional attribute can been computed completely.

(2) Conditional attributes which are higher than min_sup and min_IG are selected. The frequent-item header table F- list is constructed in attributes support descending order.

(3) Scans Zhongjing prescription database second, and a VCFP-Tree T is constructed according to section .

(4) CAR=CFP-growth (T, min_sup, min_conf);  (5) Return (CAR); End Algorithm CARM-IG scans Zhongjing prescription  database firstly. The frequency ix f  for every Chinese  medicine xi, the frequency jy f  for each type of prescription,  and the frequency ( | )j if y x  for each type of prescription  containing Chinese medicine xi are recorded. jy f  is used to  compute 1( ,..., )mI s s , and ( | )j if y x  is used to computer ( )E C , so that information gain value IG of every  conditional attribute can be computed according to (1). The conditional attributes (or Chinese medicines) with  ix f  >min_sup and IG>min_IG are chosen. The header table  F-list is constructed in accordance with ix f  descending. The objective of the second scan of prescription data set is to create a compress storage structure for Zhongjing prescription, namely a VCFP-Tree T. Finally, the algorithm of CFP-growth (Class-Frequent Pattern growth) is achieved as following [7].

1. Based on F-list in Figure 1, the set of class-association rules can be divided into subsets without overlap: (1) the ones having d; (2) the ones having c, but no d; (3) the ones having b, but no d  nor c ; and (4) the ones having only a. So algorithm CARM-IG finds these subsets of class-association rules one by one.

2. To find the subset of rules having d, algorithm CARM- IG traverses nodes having attribute value d and look ?upward? to collect a d-projected database, which contains {(a,b,c,d):C; (a,b,d):C; d:A} three tuples. It contains all the tuples having d. The problem of finding all frequent patterns having d in the whole training set can be reduced to mine frequent patterns in d-projected database.

3. Recursively, in d-projected database, a and b are the frequent attribute values, i.e., they pass support threshold. (In d-projected database, d happens in every tuple and thus is trivially frequent. We do not count d as a local frequent attribute value.) We can mine the projected database recursively by constructing VCFP-trees and projected databases. Please see [10] for more details.

4. It happens that, in d-projected database, a and b always happen together and thus ab is a frequent pattern. a and b are two subpatterns of ab and have same support count as ab. To avoid triviality, we only adopt frequent pattern abd. Based on the class label distribution information, we generate rule abd  C with support count 2 and confidence 100%.

5. After search for rules having d, all nodes of d are  merged into their parent nodes, respectively. That is, the class label information registered in a d node is registered in its parent node. The VCFP-tree is shrunk as shown in figure 2. Please note that this tree-shrinking operation is done at the same scan of collecting the d-projected.

Figure 2.  VCFP-Tree after merging nodes d  The remaining subsets of rules can be mined similarly.

Anyway, the information used to compute information gain value for every conditional attribute is collected completely through algorithm CARM-IG without increase in computing time basically. The scale of VCFP-Tree created in memory has been reduced moderately after dimension reduction based on information gain filter. Then the mining of classification association rule becomes more quickly and efficient, and experiments show that the run time and memory cost of algorithm CARM-IG has been reduced respectively[12].

D.  Experiment Result Experimental platform configured to Intel 2G / 1G,  Windows XP. Algorithm CARM-IG using java programming is achieved. The 268 Zhongjing prescriptions are chosen as data set, which are classified by experts in the TCM field from two dimensions of cold-heat and deficiency- excess in eight principles. The lowest support, confidence and information gain threshold set 2%, 50% and 0.1 respectively.

Experiment results are summarized in table and below. The two classification association rules sets on the treatments of cold-heat (21 rules) and deficiency-excess (29 rules) etc. syndromes of Zhongjing prescription are listed respectively.

TABLE II.  CLASSIFICATION ASSOCIATION RULES SETS ON COLD-HEAT  Former piece Rear piece Conf  Sup radix bupleuri heat 100 3.36 talcum heat 100 2.24 bulbus lili heat 100 2.24 rhizoma zingiberis * rhizoma coptidis  cold-heat 100 2.24  rhizoma zingiberis recens * radix aconiti lateralis praeparata  cold  75 2.24  radix aconiti lateralis praeparata * rhizoma atractylodis macrocephalae  cold  75 2.24  rhizoma zingiberis * radix aconiti lateralis praeparata  cold  72.72 2.99  radix aconiti lateralis praeparata cold  72.22 9.70 radix et rhizoma asari cold  70.58 4.48  fructus schisandrae chinensis cold  66.66 2.24 ramulus cinnamomi * herba ephedrae  cold  64.28 3.36  ramulus cinnamomi * rhizoma zingiberis recens * radix paeoniae alba  cold  60.00 5.60  rhizoma coptidis heat  57.14 2.99 rhizoma zingiberis recens * radix paeoniae alba  cold 56.66 6.34  gypsum fibrosum heat  55.55 3.73 fructus aurantii immaturus other 55.55 3.73 ramulus cinnamomi * rhizoma zingiberis recens  cold  55.00 8.21  ramulus cinnamomi * rhizoma zingiberis  cold-heat 53.84 2.61  rhizoma zingiberis cold  53.65 8.21 ramulus cinnamomi * poria other  53.33 2.99 ramulus cinnamomi * radix paeoniae alba  cold  51.51 6.34  TABLE III.  CLASSIFICATION ASSOCIATION RULES SETS ON DEFICIENCY-EXCESS  Former piece Rear piece Conf Sup radix et rhizoma rhei * fructus aurantii immaturus  excess  100 2.99  radix et rhizoma rhei * natrii sulfas excess  100 2.99 radix et rhizoma rhei * cortex magnoliae officinalis  deficiency  100 2.61  bulbus lili deficiency 100 2.24 semen lepidii semen descurainiae excess  100 2.24 radix et rhizoma rhei * fructus aurantii immaturus * cortex magnoliae officinalis  excess  100 2.24  natrii sulfas excess  90.90 3.73 semen persicae excess  88.88 2.99 fructus aurantii immaturus * cortex magnoliae officinalis  excess  87.50 2.61  radix et rhizoma rhei * semen persicae  excess  85.71 2.24  poria * rhizoma atractylodis macrocephalae  deficiency 80.00 4.48  radix rehmanniae deficiency 77.77 2.61 radix aconiti lateralis praeparata deficiency 75.00 10.07 rhizoma atractylodis macrocephalae  deficiency 73.33 8.21  rhizoma zingiberis * radix aconiti lateralis praeparata  deficiency 72.72 2.99  herba ephedrae * gypsum fibrosum deficiency- excess  72.72 2.99  fructus aurantii immaturus excess  72.22 4.85 radix et rhizoma rhei excess  71.87 8.58 cortex magnoliae officinalis excess  71.42 3.73 gypsum fibrosum deficiency-  excess 66.66 4.48  fructus jujubae * radix scutellariae deficiency- excess  66.66 2.99  fructus trichosanthis deficiency- excess  66.66 2.24  fructus jujubae * radix paeoniae alba  deficiency 65.51 7.09  rhizoma coptidis deficiency- excess  64.28 3.36     radix paeoniae alba * herba ephedrae  deficiency- excess  60.00 2.24  rhizoma zingiberis * radix scutellariae  deficiency- excess  60.00 2.24  radix scutellariae deficiency- excess  58.62 6.34  fructus jujubae * herba ephedrae deficiency- excess  53.84 2.61  herba ephedrae deficiency- excess  50.00 5.97   To sum up, the run time and memory cost of algorithm is  reduced[12]. At the same time, the concision and precision of classification compatibility principles is guaranteed because of the deletion of irrelevant Chinese medicines with classification. The mined rules more follow with the prescription law directed by basic theories of traditional Chinese medicine by experts? confirmation preliminary.

Follow-up study focuses on the careful and thoroughgoing explanation for classification compatibility principle of Zhongjing prescriptions through literature research and expert advice. Further study and explanation for these rules are in progress.

