Association Rule Mining Analysis on Concept  Lattice and Rough Set

Abstract?Rough set is used extensively in the field of date mining, which is also an useful tool for mining rules from decision tables, then solves the bottleneck of knowledge acquisition effectively, while concept lattice represents knowledge shown with hierarchical structure on the Hasse diagram, thus it is properly applied to the description of mining association rules in databases.

Association rules mining based on rough set and concept lattice can be shown that they perform the same function. However the latter represents the association rules more visual, vivid and concise than the former, therefore, association rule mining on concept lattice is applied to find common relations among different knowledge clusters in computer education.

Keywords: association rule;concept lattice; rough set;data mining

I. INTRODUCTION Association rule[1,2] mining is an important data mining  method for determining consumer purchasing patterns in transaction databases. Nowadays, many applications have used to discover useful information.

Rough set is used extensively in the field of date mining or knowledge discovery in databases, which is firstly proposed by Z. Pawlak[3], nowadays has become an useful tool for mining rules from decision tables, then solves the bottleneck of knowledge acquisition effectively, while Galois (Concept) Lattice was presented by R. Wille[4], reflects entity-attribute relationships between objects, represents knowledge shown with hierarchical structure on the Hasse diagram, thus it is properly applied to the description of mining association rules in databases, Literature[5,6,7] has testified that what rough set and concept lattice represent is isomorphic, despite their algebraic structures are not different.

As we known, association rules mining on rough set is predominant on data mining, which should deal with multi- step attribute reduction that deleting the reduplicative rows or columns. However, attribute reduction[8-14] is not just only one set, usually, finding out all attribute reductions or the least attribute reduction has been testified that it is NP-Hard problems, in the meantime, providing that all attribute reductions or the least attribute reduction have been found, association rule mining from the attribute reductions is a very difficult work because how to generate and express association rules are not very easy tasks.

In this paper, the contribute is that association rule mining based on concept lattice can perform the same function as that of on rough set, but the latter represents the rules more visual, vivid and concise than the former.

The paper is organized as follows: In section II, we review the preliminaries of association rules, basic notions of concept lattice, then discuss the generation algorithm of concept lattice in section III, association rules mining on concept lattice is described in section IV, in addition, preliminaries of mining rules from decision tables on rough set are reviewed in section V, association rule mining on concept lattice and rough set are illustrated and the comparison between them is made in section VI, finally, we reach the conclusion in section VII.



II. BASIC NOTIONS OF ASSOCIATION RULE AND CONCEPT LATTICE  A. Basic notions of association rule Mining association rule is an important branch of data  mining, which describes potential relations among data items (attribute, variant) in databases, The well-known Apriori algorithm was proposed by R. Agrawal et.al., in 1993. Mining association rules can be stated as follows:  Let I={i1, i2, ? im} be a set of items. Let D, the task- relevant data, be a set of transactions, where each transaction T is a set of items such that T I. The quantities of items bought are not considered. Each transaction is assigned an identifier, called TID. Let A be a set of items, A transaction T is said to contain A if and only if A T. An association rule is an implication of the form A B, where A I, B I, and A B= . The rule A B holds in the transaction set D with support s, where s is the percentage of transactions in D that contain A B (i.e., both A and B). This is taken to be the probability, P(A B). The rule A B has confidence c in the transaction set D if c is the percentage of transactions in D containing A that also contain B. This is taken to be the conditional probability, P(B|A).

That is, Support (A B)=P(A B)=s; Confidence(A B)=P(B|A)=Support(A B)/Support (A)=c.

Mining association rules should be mining strong association rules that satisfy the user-specified both minimum support threshold and confidence threshold.

This paper is supported by innovation program of shanghai municipal education commission under grant No. 08YZ120  2010 Second International Workshop on Education Technology and Computer Science  DOI 10.1109/ETCS.2010.545     B. Basic notions of concept lattice A context is defined as a triple C=(O,D,R , where O is a  set of objects, D is a set of attributes, and R is the binary relation between O and D, according to the inclusion relations between extents and intents, there is an unique ordered set that describes the structure of inherent lattice, which defines natural groupings and relationship descriptions between the objects and their attributes in the context, this structure is also known as (Galois) Concept Lattice[15](for short, CL).

Definition 2.1: Each pair such as (A,B) derived from the context is called a concept, where A P(O), B P(D), and P(O), P(D) are the power sets of O and D respectively, A and B create a connection in terms of the following properties.

A?={m D| g A, gRm}, B?={g O| g B, gRm}.

Where A?=B, B?=A. A is called the extent of concept, and B is called the intent, denoted as Extent(C) and Intent (C) respectively.

Definition 2.2: In a CL, partial order relation ?< ? between concept C1=(A1, B1) and C2=(A2, B2) is defined as C1<C2 B2 B1, then C1 is the sub-concept (son) of C2, and C2 is the sup-concept (father) of C1, that is, the relation between C2 and C1 is the relation as of father and son.

If C1<C2, there exists no a concept C=(A,B), such that satisfies C1<C<C2, C1<C2 is called a direct-sub-concept-direct- sup-concept-relation between C1 and C2, the Hasse diagram of concept lattice is generated according to the partial order relation: If C1<C2 is a direct-sub-concept-direct-sup-concept, there exists an edge from C2 to C1.

From the definitions, the hierarchical relations between the concepts are shown to us, which can be used as an efficient tool for data analysis and knowledge acquisition.



III. GENERATION ALGORITHM OF CONCEPT LATTICE From above the discussion, we can obtain the algorithm  to generate concept lattice, which should be inserted initial concepts (corresponding to initial concepts) one by one, then adjust it, the algorithm is described as following:  Algorithm Insertion (CL, C):  insert a concept into CL.

A concept C=(A,B) is inserted in the CL, if there   is an equivalent concept of C in the CL such that C1i (C1i C), we merge the intent of C into the intent of C1i then finish the operation of insertion, that is, Intent(C1i):=Intent (C1i) Intent(C);  Else, on the assumption that C0 is a direct-sup-concept of C, firstly insert C as a sub-concept of C0 into the CL, finish the operation in turn as follows:  Each C1i{C1i<C0,C1i<C} connects C as a direct- sup-concept of C1i in the same time, then we remove the connections between C1i and C0.

Insert the concept Ck (Ak,Ik) that is generated when C intersects concepts into the CL, by the way, the judgment of the equivalent relations of sup-concept and sub-concept  in the algorithm can be implemented.

Attribute set A={a1,a2,? ,an} in the context (O,D,R), partial relation generated by attribute ai can be represented by  {Oi1/vi1, Oi2/vi2,?,Oik/vik}, where Oij/vij is an object set Oij of attribute ai,  whose value is vij, therefore, we can gain an array of initial concepts CL(ai)= {(Ai1,ai=vi1), (Ai2,ai= vi2), ?(Aik,ai=vik)}  by attribute ai.

The generation algorithm of the CL:  Create-In-Attr(CL) is below:  Initialization: Generate CL with the complete concept (O,{}) and the null concept ({},all) as only two concepts.

for i:=1 to n do for Ci C(ai) do Insertion(CL, Ci).



IV. MINING ASSOCIATION RULES BASED ON CONCEPT LATTICE  As for concept C=(A, B) in a CL, we can introduce support(B)=|A|/M of intent B to concept C, M is the sum of transactions in D, |A| is the extent cardinal of concept C.

Definition 4.1: If Support (B) of concept C in a concept lattice is big than the support threshold, the concept C is called a frequent concept, where intent B of concept C is frequent items.

Theorem 4.1: Frequent items is corresponding to the intents of concepts lattice each other.

Theorem 4.2: If C=(A, B) is a concept in a concept lattice, the support of B is |A|/M, where |A| is the extent cardinal of C, M is the sum of transactions in D. If B is frequent itemsets whose support is bigger than the support threshold, B must be a group of equivalent intents of the concepts.

According to the theorems mentioned above, association rules can be mined in a given concept lattice[16-18].

Theorem 4.3:  If concepts C1=(A1,B1), C2=(A2,B2) satisfy C2 sup(C1), we can min association the rule: B2  B1-B2, its confidence is |A1| / |A2|, otherwise, B1-B2  B2, its confidence is 100%.

If concepts C1=(A1,B1) C2=(A2,B2) do not satisfy C2 sup(C1), and there exists nonempty common maximum- sub-concept C=(A,B), namely, C?=(A?,B?), which makes C sup(C?) C1 sup(C?) C2 sup(C?) true, there are association rules between B1 and B2: B1 B2, B2  B1, their confidences are |A|/|A1|, A|/|A2| respectively[16].



V. PRELIMINARIES OF MINING ASSOCIATION RULES FROM DECISION TABLES ON ROUGH SET  Decision table can be stated as follows: DT=(U,AT {d},f), where U is a non-empty, finite set  called the universe, U={u1,u2,u3,?un}, elements of U are called objects, At is a non-empty, infinite set of attributes, AT={c1,c2,c3,?cm}, which is a set of conditions, Vci  is the value of ci , i=1,2, m,  d is the decision attribute, its value is Vd, f is information function f: U  Va  (a  AT d).

The key step of mining rules algorithm[8] from decision table is to find out the dependence relationship between condition attributes AT and decision attributes d.

Definition 5.1:  X AT, if u, v U, there exist d(u)=d(v).

Where ?X(u)=X(v)? denote the a X, satisfying a(u)=a(v), that is u and v have the same value of attribute a, ? ? is the implication function, then decision attribute ?d? is dependent of condition attribute subset X, denoted as X d.

From the definitions, we can reach the conclusion: If we find out those condition attribute sets X, whose the object of decision attribute values d(u)=d(v) is the same object of the condition attribute X(u)=X(v), the certain or uncertain rules can be mined[9-13].



VI. COMPARISON OF MINING ASSOCIATION RULES BETWEEN CONCEPT LATTICE AND ROUGH SET  There is a decision table as shown in the decision table 1, attribute a, b, c are condition attributes, where the condition attribute a is divided in two classes {a1(low), a2(high)}, as like as the condition attribute b and c, the condition attribute b is divided in three  classes {b1(black), b2(red), b3(yellow)}, and the condition attribute c is divided in two classes {c1(blue), c2(brown)}, at last, attribute d is the decision attribute, and decision attribute d is divided into two classes {d1,d2}.

We can mine rules from the decision table 1 [19] using rough set, taking the condition attribute b as an example:   U/b={X1, X2 , X3 }={{1, 2, 3},{4},{5, 6}}  U/d={Y1, Y2}={1, 2, 3, 6}, {4, 5}}, according to the attribute reduction ,then the certain rules can be mined:  If hair=?b1(black)? then the class=?d1?, that is: b1(black)  d1 ;  If hair=?b2(red)? then the class=?d2?,  that is: b2(red)  d2; the confidence of the two rules is 100%.

X3 Y1={6}, X3 Y2={5}, the uncertain rules can be  mined: If hair=?b3 (yellow)? then the class=?d1?, or the class=?d2?,  that is: b3 (yellow)  d1 , whose confidence of the two rules is 50%.

If hair=? b3 (yellow)? then the class=?d1?, or the class=?d2?, that is: b3 (yellow)  d1, whose confidence of the two rules is 50%.

According to the data in the decision table 1, the concept lattice can be built against the algorithm in section 3, and the Hasse diagram corresponding to the concept lattice has been drawn as Fig 1, on the concept lattice we can mine the association rules, just as Apriori algorithm[22] does.

As for the same example above, the rule b1 (black)  d1 has mined based on rough set, In Fig.1, the support of frequent itemsets is not divided by M=6 in order to be  convenient for explanation, we can easily obtain frequent itemsets, for example, such as the support of frequent itemsets {b1d1} is the extent cardinal of concept({1,2,3},{b1d1} is 3, but frequent itemsets {b1} does not exist, there is an common maximum-sub-concept {b1d1},  whose support is 3, then the confidence of the rule b1(black)  d1  is 3/3=100%.

However, there exist some uncertain rules, the rules b3 (yellow)  d1 mined in the decision table 1 based on rough set, the support of itemsets {b3} is the extent cardinal of concept({5,6},{b3}) is 2, but the itemsets {b3d1} does not exist, there is a common maximum-sub-concept({5},{a2b3c2d1}), whose support is the extent cardinal is 1, then the confidence of the rules b3 (yellow)  d1 is 1/2=50%, Finally, it is tested that the results are identified.



VII. CONCLUSIONS In this paper, from the comparison of association rules  mining based on between rough set and concept lattice, we can obtain the conclusion that they perform the same function, however, in the course of association rules mining, the latter represents the rules more visual, vivid and concise than the former. The approach of mining association rules based on concept lattice presents association rules more visual, vivid and concise than that of on rough set.

In view of finding out all attribute reductions or the least attribute reduction has been testified that it is NP-Hard problems, in the meantime, providing that all attribute reductions or the least attribute reduction have been found, mining association rules from the attribute reductions is a very difficult work because how to generate and express association rules are not very easy tasks, in addition. Since frequent itemsets and their supports are shown on the Hasse diagram of  Tab. 1  Decision Table U a(High) b(hair) c(eyes) d(class) 1 a1(low) b1(black) c1(blue) d1 2 a2(high) b1(black) c1(blue) d1 3 a2(high) b1(black) c2(brown) d1 4 a2(high) b2(red) c1(blue) d2 5 a1(low) b3(yellow) c1(blue) d2 6 a2(high) b3(yellow) c2(brown) d1  Fig 1 The  Hasse diagram of concept lattice according to Tab. 1     concept lattices, it is more convenient for us to mine association rules, thus improves mining association rules efficiency.

