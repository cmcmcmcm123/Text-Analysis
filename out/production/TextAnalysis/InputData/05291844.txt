September 14-16, 2009

Abstract?Associative models are usually applied in knowledge discovery problems in order to find patterns in large databases containing mainly nominal data. This work is focused on two different aspects, the predictive use of association rules and the management of quantitative attributes. The aim is to induce class association rules that allow predicting software size from attributes obtained in early stages of the project. In this application area, most of the attributes are continuous; therefore, they should be discretized before generating the rules.

Discretization is a data mining preprocessing task having a special importance in association rule mining since it has a significant influence on the quality and the predictive precision of the induced rules. In this paper, a multivariate supervised discretization method is proposed, which takes into account the predictive purpose of the association rules.

Keywords-Associative classification, class association rules, discretization, software size estimation

I.  INTRODUCTION Estimation is one of the earliest and most important  activities in software projects. It determines the project schedule and drives the decision making along the whole project. So, the reliability of estimations has a great repercussion in the duration and cost of the project as well as in the quality of the produced software. The most usual software attributes to be estimated are software size and project effort or cost. Traditional estimation methods, such as COCOMO or other methods based on software metrics (function points, use case points, design metrics, etc.), are widely known and used for many years, nevertheless, nowadays they are being replaced by prediction models based on data mining techniques whose superiority with regard to the first ones has been  demonstrated in numerous studies.

Application of data mining techniques in the project management field has two main drawbacks: On the one hand, the number of available examples is scarce and, on the other hand, most of the involved attributes are continuous. First problem leads to a lesser precision of classification models, and second problem forces to carry out a discretization process previous to the application of many data mining techniques, which work with categorical and discrete attributes. In addition, the discretization of continuous attributes can provide many benefits: the predictive models induced by the algorithms are more accurate and more understandable; the induction  process is faster and the analysis of the results can be more comprehensive and helpful [13].

Association analysis is one of the data mining areas where discretization has more relevance. Association rules can only be generated from discrete attributes, thus, an adequate discretization has a significant impact of on the rules? quality.

Association and classification techniques have been recognized as useful tools in making decisions in many application domains. Association is considered an unsupervised data- mining task, thus it is mainly used in the area of knowledge discovery in databases, while classification problems are treated by means of supervised learning algorithms.

Traditionally, both kinds of techniques have been used to solve different classes of problems. However, recent studies show that knowledge discovery algorithms, such as association rule mining techniques can be successfully used for building accurate and efficient classifiers [9] [12] [22] [15] [16].

Associative classification methods obtain a model of association rules that is used for classification. These rules are restricted to those containing only the class attribute in the consequent part. This subset of rules is named class association rules (CARs) [12].

Association rule mining presents several problems that numerous proposed methods try to solve. The main are the high cost of generating the rules, the large number of rules generated, rules representing weak associations and uninteresting patterns, use of inadequate metrics for evaluate the validity of the rules, etc. The way of dealing with these problems depends on the application, because a rule may be useful for one application but not for another. In this paper we consider the predictive use of the association rules in the field of project management. In this area, as we commented before, the attributes involved are numeric, contrary to the traditional application domain of association rules where most of them are nominal. When attributes used for inducing the rules take continuous values an efficient data discretization contributes significantly to simplify the rule set, to generate strong and interesting patterns and to improve the algorithm performance.

We propose a multivariate supervised discretization method suitable for the induction of CARs. The procedure selects the best attributes for classification in order to simplify the rules and the induction process as well as to obtain a more accurate classifier. All selected attributes are discretized simultaneously,      including the class attribute, taking as reference clusters generated by a supervised clustering algorithm.

The proposed approach is applied in order to estimate software size by using software attributes obtained in early stages of software life cycle. The method has been validated with data from real projects.

The rest of the paper is organized as follows. Section II contains a review of relevant works in the literature concerning associative classification and discretization. In Section III the discretization algorithm is presented. The experimental study is showed in section IV and conclusions are given in section V.



II. RELATED WORK  A. Classification based on association Since Agrawal and col. introduced the concept of  association between items [1][2] and proposed the Apriori algorithm [3], association rules have been the focus of intensive research. Most of the efforts have been oriented to simplify the rule set and improve the algorithm performance.

The first pass of the Apriori algorithm consists of counting the number of occurrences of each item to determine the large itemsets, whose supports are equal or greater than the minimum support specified by the user. There are algorithms that generate association rules without generating frequent itemsets [9]. Some of them simplifying the rule set by mining a constraint rule set, that is a rule set containing rules with fixed items as consequences [5] [6]. The procedure causes loss of information, but it does not represent a drawback for predictive purposes when a single item, the class, constitutes the consequence part of the rules. Many other algorithms for obtaining a reduced number of interesting rules have been proposed, however this work is focused in the use of the rules in classification problems. The number of papers in the literature that consider this aspect is lesser. A proposal of this category is the CBA (Classification Based on Association) algorithm [12] that consists of two parts, a rule generator based on Apriori for finding association rules and a classifier builder based on the discovered rules. CMAR (Classification Based on Multiple Class-Association Rules) [10] is another two-step method, however CMAR uses a variant of FP-growth instead Apriori. Another group of methods, named integrated methods, build the classifier in a single step. CPAR (Classification Based on Predictive Association Rules) [23] is the most representative algorithm in this category. In [22], the weight of the evidence and association detection are combined for flexible prediction with partial information. The main contribution of this method is the possibility of making prediction on any attribute in the database. Moreover, new incomplete observations can be classified. The algorithm uses the weight of the evidence of the attribute association in the new observation in favor of a particular value of the attribute to be predicted. This approach uses all attributes in the observation, however in many domains some attributes have a minimal influence in the classification, so the process can be unnecessary complicated if they are taken in consideration. Our proposal evaluates the importance of the attributes on the classification.

Our aim is to propose an effective procedure for classification problems that is very suitable for application domains, such as software project management, where many quantitative attributes are involved.

B. Discretization Diverse types of attributes can be used in the data mining  area. There are different ways for classifying them. A first division in categorical and quantitative (numerical) data is the most common. Categorical data can be subdivided in nominal and ordinal ones. Nominal data are named values without an order between them. Ordinal data are nominal data types with ordered values. Quantitative attributes can be continuous or discrete. While continuous data can take an infinite number of values, discrete attributes represent intervals in a continuous spectrum and therefore, they take a finite number of values [13].

Data mining process involves a preprocessing step in order to assure the data have the quality and the format required by the algorithms. Data transformation is a preprocessing task including any procedure that modifies the original form of the data. A common transformation consists on splitting a continuous numerical domain into intervals, obtaining discrete attributes required by the algorithms. This procedure, denominated binning or discretization, is critical in classification and knowledge discovery. Discretization algorithms should be adapted to the data mining methods in order to improve the induced models.

Several discretization approaches are available [13]. They can be classified in supervised and unsupervised techniques.

The first ones are usually applied with knowledge discovery algorithms while the last ones are used by machine learning algorithms. The performance of such algorithms is significantly affected by the discretization process.

Two simple unsupervised techniques commonly used are equal-width and equal-frequency, which consist on creating a specified number of intervals with the same size or with the same number of records respectively. The purpose of the discretized data and the statistical characteristics of the sample to be treated should be kept in mind when one of these algorithms is selected. For example, equal-with discretization give poor results when the sample distribution is not uniform, however it is more adequate for obtaining coherent and comprehensible intervals of values for some attributes like age, lines of code, etc., in order to achieve a better interpretation of the results.

Supervised methods, used mainly in classification problems, consider entropy measures. These are often embedded within machine learning algorithms such as those of decision trees induction. In these cases, the supervised discretization carried out considers class information for generating the intervals while unsupervised discretization does not. The intervals obtained are those that better discriminate the classes. One known supervised method is the proposed by Fayyad and Irani [8], which is an entropy-based multi-interval discretization procedure.

This work has been financed by the Spanish ?Junta de Castilla y Le?n?      On the other hand, discretization can be univariate or multivariate. Univariate binning quantifies one continuous attribute at a time while multivariate discretization considers concurrently multiple attributes, that is, the threshold values of the intervals are searched simultaneously for all continuous attributes. Though the univariate discretization is more used and simple than the multivariable one, the latter presents more advantages, particularly in discovery of association rules since all available attributes are concurrently involved in the mining process. Multivariate discretization allows to obtain interdependences between attributes that can improve the quality of the association rules, specially when they are used for classification. This one is the scenery of our work, in which we use a model of association rules for building a classifier.

Extracting all association rules from a database requires counting all possible combination of attributes. Support and confidence factors can be used for obtaining interesting rules which have values for these factors greater than a threshold value. In most of the methods the confidence is determined once the relevant support for the rules is computed.

Nevertheless, when the number of attributes is large, computational time increases exponentially. For a database of m records of n attributes, assuming binary encoding of attributes in a record, the enumeration of subset of attributes requires m x 2n computational steps. For small values of n, traditional algorithms are simple and efficient, but for large values of n the computational analysis is unfeasible. When continuous attributes are involved in the rules, the discretization process is critical in order to reduce the value of n and to obtain high confident rules at the same time.

Attribute discretization methods for mining association rules have been treated in the literature. Nearly everyone take the support factor of the rules as the main feature for splitting the attribute values into intervals, that is, they consider the weight of the records in the interval in relation to the total number of records. In [18] the measure of ?partial completeness?, based on confidence and support factors, is defined for quantifying the information lost by partitioning quantitative attributes. Values are fine-partitioned and then, adjacent intervals are combined if it is necessary.

Lian et al. [11] introduce the concept of ?density? to capture the characteristics of quantitative attributes and propose an efficient procedure to locate ?dense regions? for mining association rules. The density of the regions representing rules is a measure used to evaluate the interestingness of the rules instead of using support and confidence.

An alternative to the conversion of continuous attributes into discrete data is to consider the distribution of the continuous data, via standard statistical measures such as mean and variance [4]. In [4] a rule is considered a subset of a population. A validity testing is proposed taking account if the mean of the subset is significantly different to the mean of its complement in the database. The rules are also evaluated depending on other measure of probability distributions such as variance.

Recently, several partition methods based on the fuzzy set theory have been proposed [20]. The mined rules are expressed in linguistic terms, which are more natural and understandable.

In these works either both the antecedent and consequent parts of the rules are formed by a single item or the consequent part is not fixed. In our case the consequent part must be fixed because it is the class attribute and the antecedent parts is an itemset. So, it is more suitable a multivariate discretization that consider all the attributes.



III. MINING CARS FROM QUANTITATIVE ATTRIBUTES In order to manage quantitative attributes we propose a  multivariate discretization algorithm based on clustering (CBD, Clustering Based Discretization). When the number of available attributes is large a previous selection process is carried out, which take into account de classification purpose of the associative model.

A. Finding the best attributes for classification The class label attribute is the target of prediction in  classification problems. Class association rules have the consequent part formed only by this attribute. For the antecedent part we select, among all available attributes, the most influential in the prediction of the classes. Importance of columns is a technique that determines the degree of utility of the descriptive attributes (columns) in discriminating the different values of the label attribute. The purity measure (a number from 0 to 100) informs about how well the columns discriminate the classes. It is based on the amount of information (entropy) that the column provides [14]. The expression for that information (I) is:   n  I (P(c1), ..., P(cn)) = ?  - P(ci) logn P(ci)  (1)   i=1    where P(ci) is the probability of the class i and n is the number of classes.

The information is 1 when the classes have the same probabilities.

The purity is defined as 1 ? I. Every set in the partition has its own purity measure, and the purity of the partition is a combination of these individual measures. For a given set in the partition, the purity is 0 if each class has equal weight and 1 (100%) if all records belong to the same class. The proposed selection procedure takes the attributes with higher values of purity for mining the rules. Although before generating the rules the continuous values of these attributes must be split into intervals according to the following procedure  B. Discretization procedure A clustering technique was applied for discretizing the  selected attributes simultaneously, including the class label.

Clusters of similar records were built by using the k-means algorithm with an Euclidean distance metric [13]. This distance D(p,q) between two points p and q in a space of n dimensions is:  [D(p,q)]2 = || p ? q ||2 =   ?i  (pi - qi) 2  (2)      Where pi and qi are the coordinates of the points p and q respectively. In our case, the points are the records to be compared, and the coordinates are the n attributes of each record.

In our proposal the clusters are built giving more weight to the class attribute. This is a supervised way of obtaining the best intervals for classification. The distribution of attribute values in the clusters was used for making the discretization according to the following procedure, based on the algorithm that we proposed in a previous work for mining conventional quantitative association rules without classification purpose [17]:  1. The number of intervals for each attribute is the same of the number of clusters. If m is the mean value of the attribute in the cluster and ? is the standard deviation, the initial interval boundaries are (m - ?) and (m + ?).

2. For adjacent intervals i and i+1:  If (mi > mi+1? ?i+1) or (mi+1 < mi+ ?i) the two intervals are merged into one:  (mi ? ?i) , (mi+1 + ?i+1) else  the cut point between intervals i and i+1:  (mi+1 ? ?i+1+ mi+ ?i)/2 This procedure was applied for creating intervals of values  simultaneously for all attributes in order to generate the class association rules.



IV. EXPERIMENTAL STUDY The proposed method was applied in the software project  management area. This is a field where most of the attributes come from the application of software metrics and therefore, these attributes are numeric. The target of the CARs model built from project data is the estimation of software size. That is a critical issue in the project management area. Good early estimates are essential for a reliable prediction of project effort and cost as well as for an efficient planning and scheduling.

It is convenient to specify that we propose a method for building estimation models, thus, the model obtained in this study is applicable to projects with similar characteristics, nevertheless, for other projects, it would be necessary to develop another model from suitable historical data. In addition, models must be regularly updated due to the dynamic nature of the software development field.

A. Data Description The data set used in the empirical study was obtained from  experiments carried out by Dolado [7]. The data comes from academic projects in which students developed accounting information systems that have the characteristics of commercial products.

The information available was divided in two groups. One group containing global data from each project (42 records) and other group containing attributes from each component of  the projects (1537 records). The code of the applications was classified into three types of components according to Verner and Tate [21].

The component attributes are: TYPECOMP (type of component, 1: menu, 2: input, 3: report/query), OPTMENU (number of choices, for menus), DATAELEMENT (number of data elements, only for inputs and reports/queries), RELATION (number of relations, only for inputs and reports/queries) and LOC (lines of code).

The project attributes described below are those used in Mark II method (MKII) [19]: LOC (lines of code), NOC (number of components), NTRNSMKII (number of transactions MKII), INPTMKII (number of total inputs), ENTMKII (number of referenced entities), OUTMKII (number of total ouputs), UFPMKII (number of unadjusted functions points MKII).

The target of the work was to estimate de software size of entire projects, hence new project attributes were calculated from the module attributes of each project: NOC-MENU (total number of menu components), NOC-INPUT (total number of input components), NOC-RQ (total number of report/query components), OPT-MENU (total number of menu choices), DATAELEMENT (total number of data elements), RELATION (total number of relations). These computed attributes and the global ones were used in the mining process whereas individual module attributes were discarded. Therefore, we used for mining the rules 42 records corresponding to the 42 projects. Due to the sample had only three projects with LOC greater than 5,000 LOC, we removed the three corresponding records.

B. Results We aim to build a model that relates some project attributes  that can be obtained early in the life cycle with the final software size. Thus, LOC can be considered as the class attribute, but it must also be discretized. The first step is to search the best attributes for discriminating LOC by calculating their cumulative purity. The attributes found by means of Mineset, a Silicon Graphics tool [14], were RELATION, NTRNSMKII, OPT-MENU, DATAELEMENT and OUTMKII with a cumulative purity of 97.95%. These attributes were used in the discretization process in order to generate the class association rules later.

The clusters were created with a weight for the class attribute LOC three times greater than for the other attributes, in order to obtain the most suitable clusters for predicting the class, which appear in the consequent part of the rules. In the study, the number of clusters used as input to the clustering algorithm was established considering the necessities of the problem domain. Finally five clusters were generated, which were used for splitting the values of the selected attributes according to the procedure described above. Figure 1 shows the intervals of values obtained for the class attribute (LOC). The distribution of records for each interval of values of the attributes is presented in figure 2. We can see in it that the algorithm CBD finds different number of intervals for the different attributes. We can also observe in this figure, by means of colors, the proportion of records of each class.

Figure 1.  Intervals of values for the class label (LOC)                        Figure 2.  Distribution of records for intervals of values of the attributes  In order to validate the proposed Clustering Based Discretization (CBD) procedure we examined the precision obtained by applying several classifications methods with continuous data (except the class attribute) and several associative classification methods with discretized data by different discretization algorithms.

Results from classification algorithm are showed in Table 1. We applied Bayes Net, the decision tree J4.8 and two multiclassifiers, Bagging with RepTree and Staking with CeroR. For multiclassifiers we proved several base classifiers but we only report the most precise. These methods do not work with quantitative class attributes, therefore we split the LOC values into five equal width intervals since five was the number of obtained clusters.

For associative classification we selected CBA and CMAR because our experience and the literature have demonstrated their superiority against others methods. Usually CMAR has more predictive precision than CBA, however it has higher computational cost. We also obtained better results with CMAR and the computational cost was similar, given that the data set is very small.  Obtained results are presented in Table 2. We have applied CMAR with data discretized by means of four different algorithms: equal width, equal frequency, Fayyad and Irani method, and the proposed CBD procedure. For applying Fayyad and Irani method, five equal width intervals of LOC were used since the algorithm works with nominal class attributes. CMAR was executed with a minimum support of 10% and a minimum confidence threshold of  75%.

TABLE I.  RESULTS FOR CLASSIFICATION METHODS  CLASSIFICATION METHOD PRECISION Bayes Net 38.46% Decision Tree J4.8 58.97% Bagging (RepTree) 56.41% Stacking (CeroR) 33.33%  TABLE II.  RESULTS FOR CMAR  DISCRETIZATION METHOD PRECISION Equal width 27.50% Equal frequency 1.67% Fayyad and Irani 80.83% CBD 85.83%    As we can see in table I, traditional classifiers gave very poor results. It can be a consequence of the reduced number of records available in the data set, which is insufficient for the learning process.

CMAR gave better results only with two discretized data sets, which were obtained by applying Fayyad an Irani method and the procedure proposed in this word, the CBD algorithm.

The last one provided the best results, a precision of 85,83%, which can be considered a high precision, keeping in mind the small available data set. Precision differences between the two methods are not very significant, however, Fayyad an Irani method has the inconvenience that it works with nominal class attributes while CBD does not, but rather it discretizes simultaneously all attributes, including the class. Besides, CBD achieve a better precision with a lesser number of rules.



V. CONCLUSIONS In the project management field it is very difficult to obtain  a great quantity of historical data for making predictions. That fact leads to a low precision of classification models built from the available data sets. On the other hand, most of the involved attributes, obtained by applying software metrics, are continuous. Therefore, they cannot be directly used as input to the data mining algorithms. In this work we have used project management data in order to estimate the software size by means of an associative classification method.

One of the main problems of quantitative association rule mining is the discretization of continuous attributes. In this      work a supervised multivariate discretization procedure is presented, which is specially indicated for class association rules (CARs). Our proposal evaluates the importance of the attributes on the classification and split simultaneously their values into intervals by means of a procedure that takes as reference the clusters generated by a supervised clustering algorithm. The algorithm produces the most suitable rules for prediction, due to the supervised discretization and the use of the most important attributes in discriminating the different values of the class attribute, which constitutes the consequent part of the rules. Our experiments demonstrated that, in the studied application field, associative classification methods combined with a good discretization procedure overcome widely to the classic supervised learning methods.

