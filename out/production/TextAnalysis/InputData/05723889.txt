Mining Classification Rules via an Apriori Approach S M Monzurur Rahman, Mohammed Rokibul Alam Kotwal, Xinghuo Yu?

Abstract Classification rules are the interest of most data miners to summarize the discrimination ability of classes present in data. A classification rule is an assertion, which discriminates the concepts of one class from oth- er classes. The most classification rules mining algo- rithm aims to providing a single solution where multiple solutions exist. Moreover, it does not guarantee the op- timal solution and user has not any control over the classification error rate. In this paper, we addressed these problems inherent in mostly used classification algorithms. A solution has been proposed to solve these problems and it has been tested with experimental data.

Keywords: Data Mining, Association Rules, Classifi- cation Rules, Characteristic Rules, Classification Algo- rithm, Apriori.



I. INTRODUCTION The data mining has been a new and exciting research field receiving an increasing attention.  The first task of data mining is to construct a model that represents a huge data set of interest. One common representation of data is by means of rules.  There are mainly three kinds of rules; association rules, classification rules and cha- racteristic rules. The acquisition of classification rules is of interest in this paper [1]. Classification is about classifying given data into categories or groups which have some common features. Typical examples of min- ing classification rules include, for example, target mail- ing, franchises location, credit approval, treatment, ap- propriateness determination etc [1]. Classification is a well-studied problem in AI and Mining classification rules have been studied since data mining is introduced in [1-5]. There are several models for classification e.g.

Neural Networks [4], Statistical models [6], Genetic models [7], Decision trees [8-10], among which deci- sion trees and neural networks appear to be used very often.

A decision tree is a directed graph consisting of nodes and directed arcs. The nodes frequently correspond to a question or test on an attribute of the given data set. The arcs contain values of attributes. Classes are at the leaf level. The most popular ID3 [9-10] algorithm can be used for generating decision trees. This algorithm parti- tions the given data set into smaller and smaller subsets by growing the tree. Decision trees have many advan- tages over other classifiers such as Neural Networks in that they are easy to construct, relatively fast, easy to be converted into SQL queries and can handle non- numerical data. The rule extraction using decision trees is simple as well. However, its classification perfor-  mance may be compromised in the situation when the data set has complicated (nonlinear) domains. Since the mining goal of classification problem is to assign each example in the data set to one of many categories, the decision tree approach has to generate many branches for each node, and may result in large mining errors. On the other hand, Neural Networks can learn the classifi- cation rules by many passes over the training data set, which takes a longer time for learning. The classifica- tion functions generated by Neural Networks are buried in the weights which make the extraction of classifica- tion rules difficult as well. Some attempts have been made in this direction [13]. Decision functions can be made of many forms e.g. equations, trees or production rules. Then each class can be classified or partitioned by the decision function associated to it. Efficient functions can be acquired by using, for example, the information theory. The weakness of this approach is that it is una- ble to deliver all the solutions i.e. how many decision functions can be constructed for each class and which one is best on the basis of compactness. Another prob- lem associated with mostly used classification methods is that they do not allow relaxation and tolerance in the mining process. Consequently these methods may end up with decision functions with larger classification errors. The present paper attempts to address the afore- mentioned problems. Some new measures such as glob- al and local measures have been introduced to study classification rules in this paper. Furthermore, it has provided a method to mine classification rules from real data based on well-known data mining algorithm apri- ori. The proposed algorithm is also tested with real data and result is reported in this paper.

This paper is organized as follows. Section II discusses the data-mining problem with examples and formalizes it. Section III describes our proposed method to mine sets of classification rules from submitted data. The proposed method is tested and results are reported in Section IV. Finally, Section V gives some concluding remarks.



II. PROBLEM STATEMENT First, we will illustrate common problems associated with most currently available classifiers by considering a known animal world data [12]. There are three classes: Bird, Hunter and Peaceful animals, which are assumed to have been identified already. We choose the mostly used classifier C4.5 from Quinlan [9] as a typical clas- sifier. The result of the classification using C4.5 is shown in Fig. 1. It is found in the experiments that this classifier C4.5  and other as well only gives a single solution for the classification problem as shown in Fig.

23-25 December, 2010, Dhaka, Bangladesh     1 regardless of whether other solutions may exist. If we look at the data carefully, we can find that there is an alternative solution with the same size of tree as shown in another part of the figure, which cannot be obtained using C4.5. The downside of this single solution may have some consequence. For example, consider a test data of an imaginary animal, which has hooves, 2-legs, can fly and also has feather. If we want to determine the class of this animal using the tree from C4.5, we will be given the answer that this animal belongs to Peaceful animal. But we will be more convinced if it is classified as a Bird since it has most characteristics of a bird. In this situation, it may be helpful if the classifier can give multiple solutions to choose from.

Fig. 1. Decisions tree and function for animal classifier produced and does not produce by C4.5.

Another limitation is that the classifiers such as C4.5 do not allow enough relaxation in producing rules. This limitation restricts the classification ability of most clas- sifiers as real data sets do contain noise and fuzziness and certain relaxation may help to produce more con- vincing rules. For example, some members of one class may have overlaps with members of other classes. This may incur a classification error during constructing the classification model. This problem has been addressed to some extent in the existing classifiers, for example, C4.5 tries to keep this error as low as possible. When classification is not well defined by data given to C4.5, C4.5 constructs simple trees at the cost of complying an amount of classification. Note that C4.5 works accord- ing to Information Theory in the construction of the root node to leaf nodes and users have no influence in guid- ing the construction. Obviously it will be more desirable if users can have certain control in order to obtain their desired results. To demonstrate this point, let us consid- er again the animal world data. If a user agrees to accept the classification error of 1%, a good classification solu- tion may be given as follows, in comparison with the solutions in Fig. 1.

Decision Function 3: IF Animal Size_Is (Small) THEN Class = Bird ELSE  IF Animal Has (Hooves) Then Class = Peaceful IF Animal Has_Not (Hooves) Then Class = Hunter ENDIF  We now look at the problem formulation. The classifi- cation problem can be considered as finding the set of classification rules that partition the given data into dis- joint sets or groups. The problem of inferring classifica- tion rules from a set of given examples (data) has been formalized in many literature [11]. The formulation can be stated as follows.

Let C  be a set of m  classes { }mCCC ,,, 21 ?  and A be a set of n attributes ( )naaa ?,21 , . Define )( iadom as the set of possible values for attribute ia . The conven- tional classification problem formulation is given a large data set or database D , each example of which is a n -tuple of the form nvvv ?,, 21 where )( ii adomv ? .

Obtain an appropriate set of classification rules R whose member classification rule is of the form in conjunctive normal form kii Caar l ????1: , where nimk j ?? ,, , lj ,,= 1 ?  and n li ?, . It is obvious that the member classification rules in R are disjoint, i.e. if ii r C ?  and jj r C?  and ji ? , then ji r r? . If a new example fires a classification rule  ir  then it is said that this example is a member of iC class. The problem is to obtain an appropriate set of classification rules, denoted as ? , from given data with decidability and classification accuracy ?  and ?  re- spectively. The membership of set ?  is rule subset. For example, for the ith member subset   rule sets e.g.

j i? where i denotes the class information and j denotes  the member index of the class iG .

j i? partition data  between the iG  class and non-class and it consists of  classification rules of the  form kji Gaa ???? ,  where jimk ?? ,,  and nji ?, . We will explain these two terms in the following sections.  If a new example fires a classification rule from ji?  then it is said that the example is a member of iG .



III. MINING CLASSIFICATION RULES USING APRIORI APPROACH  The mining task is to derive a set of classification rules for each class iC . Before proceeding further, we first introduce the following measures, which will be used later.

Definition 1: Rule Class Support: An l-tuple classifica- tion rule is expressed in the form of  k  l  i i a C ?  = ?   where  ia denotes ith attribute in the class kC  and ?  is the AND operator. Support of a classification rule ir of  class kC is defined in sub example space kD as  k  i k  ki N n  Drs =),( , where i kn is the number of examples in  kC that support the rule ir  and kN the number of ex- amples in kC that support the rule ir and kN the num- ber of examples in kC . For example, from the data we have,  0.1 7 ),)(( ==? birdDBirdfeatherHass  and     85.0 6 ),)(( ==? BirdDBirdflyDoess .

Definition 2: Rule Class Global Support: A gklobal support of classification rule ir of class kC is as  N n  Drs i k  i =),( , where i kn  is the number of examples  of all class kC supporting the rule ir and N is the num- ber of examples of all classes in D. For example, from the data we have,  44.0 7 ),)(( ==? DBirdfeatherHass  and  38.0 6 ),)(( ==? DBirdflyDoess .

Defenition 3: Rule Set Class Support: Class support of  a rule set is defined as )),((),(  i  l  i ik  j k drsMaxDRS ?  = = ,  where jkR is jth classification rule set of class kC and j  ki r R? . An example can only be considered once in the  rule level support calculation i.e. ?= = ?  l  i id   and  k  l  i i d D ?  = ?   . Here id is the subset of examples of the  class Dk. All id are mutually exclusive. For example, from the data we have two classification rule sets for the class bird as follows:  { }bird)(bird;)(1 ??= huntdoessmallsizeRBird { }bird)(bird;)(2 ??= flydoesmediumsizeRBird  Their rule set class support is then, 0.1),( 1 =BirdRS bird , 85.0  )06(),( 2 =+=BirdRS bird .

Definition 4: Rule Class Confidence: The rule class  confidence is defined as N  nDrc kiki <>?= 1),( , where  kin <>  is the number of examples from examples of  kDD \ (D excluding kD ) that fires the rule ir  and N is the total number of examples of all classes. For example in Table I 0.100.1),)(( =?=? BirdBirdswimDoesc  8125.0 3 0.1),)(( =?=? BirdBirdmediumSizec .

Table I. Classification Example.

Definition 5: Rule Set Class Confidence: The rule set class confidence is defined as )),(()( krcMinRC i  j k = for  li ?1= and jki r R? . For example, from the data, con- sider the classification rule set foe the class bird as  { }bird)(bird;)(1 ??= huntdoesmediumsizeRBird  its rule set class confidence is then calculated as  6875.0)6875.0,8125.0()( 1 == MinRC bird .

Definition 6: Compactness of Classification Rule Sets: The compactness of a classification rule set can be  measured as follows [15]: ? =  ?= r  i i  i  Cn t  r E   ) 1(1 , where n is  the total number of tuples in the set, r is the cardinality of the rule set, it the number of tuples classified by rule i in the rule set, and iC  the number of conditions in the antecedent part of rule i. It is obvious a higher value of E  indicates the goodness of the class ruleset. In the algorithm to be developed below, we will minimize the value of r as well as iC . As in our case we are able to generate different set of classification rules, so we can find out the best set i.e. compact with that measure.

In the following we shall develop our algorithm for mining classification rules. This algorithm consists of three parts: Pattern Construction, Mining classification rules and Construction of Classifications Rule Sets. In our algorithm, we will allow users to control the accura- cy of the classification rule mining solution. There may be two types of errors: one is the indecisive error due to less rules set support, the other the misclassification error due to rule set confidence. If the rule set confi- dence is chosen too high, obviously the classification error will be small. However this may result in no solu- tion at all. On the other hand, keeping the class rule set support we can get the closer picture of the class in terms of rules but it declines the prediction accuracy as discussed in the previous section. The other notable feature of our solution is that a rule cosseting of same logical predicates may appear for different classes. It does not mean that they do not have any discrimination power. They will have different confidence measure and from that we can have the idea it is more likely to be appeared in which class.

3.1 Pattern Construction Before using our mining algorithm, data needs to be transformed into an appropriate format so that it can be processed. This transformation can be done by con- structing logic predicates from attributes, which may be continuous or discrete. We produce rules in logical pre- dicates from attributes and then truth-values of these m attributes along with the class information to make the so-called patterns for next step of processing.  If the attribute is discrete then the predicate is constructed directly as j ii a v = , where  j iv  is the j  th discrete value of  attribute ia . Continuous attribute values can be discre- dited via the existing approaches such as Chi-merge and Chi2 [14], and the resulting predicates would be of the form 1 +<<= j ii  j i vav , where  1 +< ji jv i v  are discrete  values found, for example from Chi2. After that, all patterns will be further constructed with two parts the     predicate and class information, from all examples. We now illustrate the above idea via examples given in Ta- ble 1. From Table 1, we have three attributes Age, Sala- ry and Sex where Age and Salary are continuous but Sex is discrete.  Arbitrarily we discretize Age and Sala- ry in three ranges, that is ),40[),40,30[),30,[ ???  ),5000[),5000,3000[),3000,[ ??? , and construct the predicates as )30(1 <= Age 40p  ) 30(2 <<== Agep ,  )40(3 >= Agep , )3000(4 <= Salaryp , )50003000(5 <<== Salaryp  and )5000(6 >= Salaryp .

The other two predicates for Sex are constructed as )(7 MaleSexP ==  and )(8 MaleSexP == . In this ex-  ample we have three classes which are denoted as Pro- grammer=C1, Secretary=C2 and Accountant=C3. The resulting corresponding patterns are given in Table II.

Table II. Classification rules produced by C4.5 and X2r  rule generator.

The algorithm for normalizing attributes is given below: =1.Set 1 j  and predicate set P empty.

2.Attribute normalization starts at here. For all attributes naa ?1  do the following steps  a.If ia  is discrete then find its discrete values and set  them as il ii v v ? 1 . Construct il  logical predicates of the  form jva j ii ?=  , and add them to the predicate set P; b.If ia is ordinal then discretize its domain using  Chi2 method [14] and get its discrete ranges. Define ranges as +??? ,, 1 il ii v v ? . Construct il  logical predi-  cates of the form  1 +<<= j ii j  i vav where )10 +=<<= ilj , and add them to the predicate set P;  3.Pattern construction starts at here. Given pn  predi- cates in P, for all examples in the data set do the follow- ing steps:  a.Apply all predicates in P to the examples and get their truth-value (1 or 0) accordingly;  b.Get the class member index of the example; c.Construct the pattern for the example by concate-  nating pn  truth-values of predicates to the class mem- ber index e.g  classvavvavvavvav mn m n  m m  n mmn m ,,,,,,, 11    1 1 <>=<>=<>=<>= ????  where m is the number of attributes and in is the num- ber of ranges of ith attribute.

3.2 Mining Classification Rules We now mine classification rules from constructed pat- terns obtained in a) using the popular data mining algo- rithm apriori [1]. Assume that patterns have up to n attributes. The idea our algorithm is as follows. We start from 1-item classification rules (one condition in antecedent part) first, and then graduatelly to reach n- item classification rules (n conditions in antecedent part).  The novelty of our method is each classification rule is derived using the apriori algorithm with their rule class support and rule class confidence levels given.

This will enable us to discriminate those which are un- likely to get the desirable level of support and confi- dence of all classes. The main idea of the apriori algo- rithm lies in constructing n-item classification rules from (n-1)-item classification rules. The rule of thumb is the present set of rules must have all of its subset rules in the previous iteration and there should not be conflict conditions for the same attribute in any rule as conditions of an attribute are disjoint.

We use the data in Table I and II to explain it. In order to mine classification rules for C1 (Programmer class), the 1-itemset-classification rules with their associated rule class support and rule class confidence levels can be obtained as  [0,0.75],1  ,[0.33,0.5]1  ,][0.66,0.751  ],75.0,0[  1 4321 JpJpJpJp ???? [0,0.75]1  ,[1,0.75]1  ,[1.0,1.0]1  ],25.0,0[  1 8765 JpJpJpJp ????  If we set the minimum rule class support to be larger than zero and minimum rule class confidence level to be 0.5, we then have the classification rules for 1-itemset rules as  , 1  2 J , p ?  1  3 J 1p ?    , 1 76 JpJp ?? .

Next, we can construct 2-itemset rules from the three groups of conditions }{},,{ 632 ppp and  from three attributes having a support higher than 0 in the data set and they are:  ],1,1[  1],1,1[  1],1,1[  1 637262 JppJppJpp ?????? ]75.0,33.0[  173 Jpp ??  The classification rules from 2-itemset is:  1,  1, 1 637262 JppJppJpp ?????? 173 Jpp ?? After 2-itemset rules we have still the three groups of conditions }{},,{ 632 ppp  and }{ 7p  from three attributes having a support higher than 0. We now create 3-itemset rules as  ]1,33.0[  1],1,66.0[  1 763762 JpppJppp ?????? and the classification rules are mined as   1, 1 763762 JpppJppp ?????? .

The next step is to mine 4-itemset rules and as we have only three group of conditions we have no rule in the 4- itemset rules and hence it concludes the iteration. We now summarize the final result of classification rules for the programmer is  , 12 p J ?  1  , 1 76 JpJp ?? 1,  1, 1 637262 JppJppJpp ?????? 173 Jpp ??   1, 1 763762 JpppJppp ??????  We can construct sets of classification rules from the above result as follows considering class set support 0.95 and confidence 0.75 by combining one rule with other rules as long as the criteria is not met. The sets are     6 1Set 1:  p J ? [1, 1] Set 2:  17 p J ? [1, 0.75] 1Set 3:   62 Jpp ?? ,  172 Jpp ?? [1, 0.75]  1Set 5:   , 1 763762 JpppJppp ?????? [1, 1] Any set of the above has the discrimination ability to partition examples of the programmer from the rest. The algorithm for mining classification rules his given as below. The algorithm for Mining Classification Rule sets is given as below.

Input: Examples of m classes, user defined class confi- dence and user defined class rule support and set sup- port.

Output: Number of rule sets obtained in examples for each class.

Steps: For class index  to m do the followings: 1.Find the distinct attributes of class j which are at least present once in the example set of class j and lets say they are { }j m j j jbbbB ,, 21 ?= ; 2.Construct all 1-item rules 1F  from B of the form  ji jb g ?  for Bb ji ? ? ;  3.Calculate rule class support, global support and rule class confidence of all rules found in step (b).

4.Add all classification rules of 1F  to the solution set  jG  which has class confidence and support equal or higher than user defined class confidence and support.

=5.Set 1 k  and do the followings until kF  is empty

I.Increment k and construct k-itemset rules as super-  sets of (k-1) itemset rules as Apriori algorithm as well as taking account that at most one condition may appear in a rule from each attribute;

II.Calculate rule class support, global support and rule class confidence of all rules found in step I;

III.Remove those rules which has class support lower than user defined support from kF ;

IV.Add all classification rules of kF to the solution set jG  which has class confidence equal or higher than user defined class confidence.

6.Finally, construct all rule sets ji? of class j from  jG which has user defined class rule set support.

The class rule set ji?  of each class gives us the whole picture of the class and we can partition the entire data set by ji? . The identification of an example pattern is straightforward. If the pattern obeys any rule from  j i? but does not obey any other rule from  kji k? ?, then it is obvious that the pattern is belong  to the class j.  Since we are considering control parame- ters class confidence and support, the test pattern has the chance to fire some rules from ji?  and some rules  from kji k? ?, . In this case we will consider the rule  level confidence to determine the membership of the test pattern and if any rule fired by the test pattern from  j i? has the higher confidence than all rules fired from k i?  then we conclude that the test pattern belong to the  class j.



IV. EXPERIMENT RESULTS To study the performance of the proposed method for mining classification rules, we used the IRIS data set in [16]. It has four numeric continuous attributes: sepal- length, sepal-width, petal-length and petal-width. IRIS data consists of three classes: Setosa, versicolor and virginica. It has 150 examples in total and each class has 50 examples.  Table III contains some available results about the data in [17] as well as the result about class support and classification error added. It should be noted that only single solution was given in [17], more- over, the introduced default rules do not present the discrimination characteristics of the class.

Table III. Intervals of attributes for IRIS data.

Now we report the results using our method. First, each attribute was discretized. We used a method similar to Chi2 taking account of different stop criteria. As this paper is concerned about extracting classification rules from discretized attributes, details of the discretization algorithm used in this experiment is omitted here. How- ever, any discretization algorithm can be used. The pre- dicates for classification rules were constructed as shown in Table IV. Three experiments were set with high value for user defined class error and lower value for support.  The set support was kept the same for two experiments. The first experiment gave few rules as shown in Table V, VI and VII but with strong class sup- port and confidence. We constructed the sets of rules from these rules and every set had the discrimination power to partition data from the associated class to the rest. The second experiment was conducted with less support and confidence value and it produced many rules and sets which have also the discrimination power.

It should be noted that our result provides multiple solu- tions without any default rule.

Table IV. Classification rules produced proposed me- thod with the parameters Min class support 80%, Min class confidence 99% (experiment 1).

Table V. Classification rule sets from experiment 1 Min set support 95%, Min set confidence 99%.

Table VI. Intervals Classification rules produced pro- posed method with the parameters Min class support 50%, Min class confidence 90% (experiment 2).

Table VII. Classification rule sets produced from expe- riment 2 Min set support 95%, Min set confidence 90%.



V. CONCLUSION The classification problem in the context of data mining should be solved taking account that it should provide as many as solutions possible by meeting user-defined criteria. The more solutions means more classification rule sets which increase the discrimination, prediction and characteristics exhibition of classes found in the data set.  In the paper we have demonstrated multiple solutions construction with our algorithm to mine classi- fication rules. Our algorithm can be tuned with many user-defined controls such as intervals, class support, class confidence, set support, set confidence etc. It re- sults different solutions as well as it also guarantees the discrimination ability. Our method has been tested with widely used Iris data set for testing machine learning algorithms. The result has been reported which shows the effectiveness of our proposal in the classification rule-mining field. We would like to do more research at this field in near future.

