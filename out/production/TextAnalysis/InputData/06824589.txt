Collaborative Book Recommendation Based on Readers? Borrwoing Records

Abstract?Book recommendation is an important part and task for personalized services and educations provided by the academic libraries. Many libraries have the readers? borrowing records without the readers? rating information on books. And the collaborative filtering (CF) algorithms are not proper under this circumstance. To apply the CF algorithms in book recommendation, in this paper, we construct the ratings from the readers? borrowing records to enable the CF algorithms. And to evaluate the traditional CF algorithms, we show that linearly combining (blending) a set of CF algorithms increases the accuracy and outperforms any single CF algorithms. At last, we conduct the experiments based on the real world dataset and the results invalidate the efficiency of the blending methods.

Keywords-library recommendation; collaborative filtering; supervised learning; ensemble learning

I.  INTRODUCTION Because of the recent rise in library 3.0, academic  libraries have become medium facilitating interactions between the community and librarians. Numerous academic libraries have begun to provide personalized services, such as book recommendations, to attract readers to use library resources [1, 2, 3, 4].

Recommender systems help users to discover items in large web shops, to navigate through portals or to find friends with similar interests. And in recommendation systems, CF algorithms are widely used and have enjoyed much interest and progress [5]. It predicts the potential interests of a given user (called an active user) by taking into account the opinions of users with similar taste (i.e., social wisdom). Compared to other recommendation technologies (e.g., content-based filtering [6]), collaborative filtering technologies have the capability of working in domains where items? attribute contents are difficult to obtain or cannot easily be parsed by automatic processed. In addition, CF algorithms can provide serendipitous recommendations and can be easily adopted in different recommender systems without requiring any domain knowledge [7]. In other words, CF helps users discover new items. And there are various research works on the evaluation of the CF algorithms, such as real time online recommendation [8, 9] and sparse problem [10, 11] etc.

In order to establish recommendations, CF algorithms need to relate two fundamentally different entities: items and users. There are two primary approaches to facilitate such a comparison, which constitute the two main techniques of CF:  the neighborhood approach and latent factor models.

Neighborhood methods focus on relationships between items or, alternatively, between users. An item-item approach models the preference of a user to an item based on rating of similar items by the same user. Latent factor models, such as matrix factorization, comprise an alternative approach by transforming both items and users to the same latent factor space. The latent space tries to explain rating by charactering both products and users on factors automatically inferred from user feedback [12].

From the above description about the traditional CF algorithms, CF algorithms are based on the history of the feedbacks from the users, especially the rating of the items.

But in the history of the borrowing records recorded in the academic libraries, there is no rating information but just the records of borrowing and returning with time stamp. And the traditional CF algorithms cannot be adopted well for that situation.

In this paper, we focus on how to implement accurate CF algorithms based on readers? borrowing records without the feedback ratings. We transform the users? ratings based on the borrowing records with time stamp for implementing CF algorithms. And at the same time, to make the traditional CF algorithms more accurate, we linearly combine (blend) the traditional ones by different methods. The results from the experiment on the real world data set invalidate the efficiency and accuracy of our methods. And as the ratings on the books can be transformed into time, by predicting the ratings, we can easily figure out how long the reader will borrow the book.

This paper is organized as follows: in section II, we give some details of some traditional CF algorithms and describe them in mathematics; in section III, we introduce the algorithms for blending. And in section IV, we show how to transform the readers? borrowing records to the rating and give the results by implementing the algorithms on the real world data. In section V, we give the conclusion and some plans about the future works.



II. COLLABORATIVE FLITERING Collaborative filtering uses the history of user-items  events in order to predict future ones. These events can be any source of user-generated information, such as purchases, rating, clicks, bookmarks, favorite-adds etc. Within this work we focus on rating based collaborative filtering.

The problem of rating prediction can be described with the help of the user-item rating matrix [ ]uiR r=  where   DOI 10.1109/CBD.2013.14     entry uir  is the rating of user u for item i . This matrix has the size U M? with U being the number of users and M the number of items. In general R is sparsely filled, because a user typically dose not rate every item. The goal of the prediction model is to accurately predict missing values of this matrix, i.e. to produce predictions u?ir for how an item i would be rated by user u . In collaborative filtering, the system infers a model from all available data in R . One possibility to do so is to use a low-rank matrix factorization.

And for testing the accuracy and efficiency, the dataset usually contains two parts: the train set and the test set. We train the parameters for CF models based on the train set and test the accuracy and efficiency of the CF models based on the test set.

There are two main techniques for CF: the neighborhood approach and latent factor models. And in this section, we give details on the neighborhood approaches including k- nearest neighbor (KNN) methods (KNNitem, KNNuser) and latent factor model (LFM) [13].

A. KNNitem  A prediction u?ir for the rating of a user u on item i  is an item based k-nearest neighborhood model is obtained by calculating a weighted sum over the rating of the user u for the k items most similar to item i . The weights and the similarities are proportional to the correlations ijc between item i  and other item j . Therefore, a precalculated item- item correlation matrix C is useful, due to the constant access time to any item-item correlation ijc in this case. For one prediction, the KNNitem algorithm selects the k best correlated items to item i . And usually employ the following equation to predict the rating:  ( )  ( )  ( ) ?  k k  k  k  k  ii ui k i S i  ui ii  i S i  c r i r i  c ?  ?  ? = +  ? ?  (1)  In the above equation, i  and ki  represent the average ratings of item i  and ki . And ( )S i represents the neighbor set of the k best correlated items to item i .

The training time is dominated by the calculation of the item-item correlation matrix C , which needs  2( * )O U M operations. And to select the k best correlated items, this can take up to ( )O M  operations. The sorting of this list takes ( log( ))O M M  time.

B. KNNuser  This model is exactly the same as the KNNitem, but items and users are flipped. Hence the prediction and training bounds are also reversed. In this model, a precalculated user-  user correlation matrix 'C and usually employ the following equation to predict the rating:  '  ( ) '  ( )  ( ) ?  a a  a  a  a  uu u i a u S u  ui uu  u S u  c r u r u  c ?  ?  ? = +  ? ?  (2)  In the above equation, u  and au  represent the average ratings of user u  and au . And ( )S u represents the neighbor set of the k best correlated users to user u .

C. LFM (latent factor model)  This is probably the most popular collaborative filtering technique. A prediction is given by the dot product of a user feature vector up  and the item feature vector iq , then the rating is predicted as follows:  ? Tui u ir p q=                                                                      (3) The LFM learns two factor matrices, user features  1 2[ , ... ]UP p p p= and item features 1 2[ , ... ]MQ q q q= via stochastic gradient descent. The model parameterizes two matrices with f  rows. And to train the parameters, we have to minimize the following cost function:  2 22  , min {( ) }  ui  T ui u i u iP Q r R  r p q p q? ? ?  ? + +?          (4) In the above equation, the parameter ?  is the  regularization ratio.  And to implement the stochastic gradient descent, we have the learning rate ? . The parameter f is usually predefined before parameter training.

In practice the whole training needs a few tens of epochs over whole dataset until convergence, leading to (| |)O ? .

And for storing the two parameter matrices, the memory consumption is ( )O M U+ .

Both training and prediction time have optimal  asymptotic runtime behavior, which makes LFM an excellent candidate for large scale recommendation applications. And it usually can predict all the missing values in R while the KNNuser and KNNitem usually predict a part of the missing values.

Sometimes, LFM can be acted as SVD and there many extensions to SVD in the literature, for example the SVD++ model described in [14].

In this section, we give some details of the traditional CF algorithms on how to predict the ratings. And there are many other CF algorithms and we do not focus on them in this paper.



III. BLENDING The combination of different kinds of collaborative  filtering algorithms leads to significant performance improvements over individual algorithms. Blending predictions is supervised machine learning problem. Each     input vector ix is the F -dimensional vector of the predictions in the ensemble. For N data samples, one collection the vectors 1 2, ... Nx x x in an N F? matrix X of predictions. The target values for these n data points are collected in an N -dimensional vector y . In some film recommendation systems, the entries of y are integer rating between 1 and 5. The blending algorithm is formally a function : F? ? ?? . The input x is a vector of individual predictions, the output is a scalar.

And sometimes, we cannot predict all the missing values, so have another parameter for CF algorithms?SR (successful rate). SR is the ratio of the predicted values in all the missing values.

A. Linear Regreesion - LR In statistics, linear regression is an approach to modeling  the relationship between a scalar dependent variable y  and one or more explanatory variables denoted as X .  In linear regression, data are modeled using linear predictor functions, and unknown model parameters are estimated from the data.

Such models are called linear models. Most commonly, linear regression refers to a model in which the conditional mean of y  given the value of x  is an affine function of X .

Assuming a quadratic error function, optimal linear combination weighs w (vector of length N ) can be obtained by solving the least squares problem. For any input vector x , the prediction is ( ) Tx x w? = .

We use the traditional cost function while calculating the parameter of the model. And then compare the prediction with the real data for the test set.

B. Neural Network - NN Neural networks are a different paradigm for computing:  von Neumann machines are based on the processing/memory abstraction of human information processing; neural networks are based on the parallel architecture of animal brains. Neural networks are a form of multiprocessor computer system with simple processing elements, a high degree of interconnection, simple scalar messages and adaptive interaction between elements.

Small neural networks can be trained efficiently on huge data sets. The training of neural networks is performed stochastic gradient descent. The output neuron has a sigmoid activation function with an output swing of -1 to +1. To generate rating predictions in the range between two integers, we can use a simple output transformation. For example, the output is multiplied by 1.6a = and the constant 1b = is added. The learning rate is ? .



IV. EXPERIMENETS In this section, we implement the traditional CF  algorithms and the blending methods on the real world dataset. Before implementing CF algorithms to predict the integer between a certain ranges, we have to transform the borrowing records.

A. Dataset and transformation This real world dataset is from some academic library.

And it contains 82, 987 records form 2011-06-07 to 2012- 06-27 which contains five different operations including borrowing, returning, renewing, booking and removing the booking with the time stamp.

While a student borrow a book, he should renew it after 30 days. And he can renew twice, after one renew, he can borrow the book for another 15 days. It means that a book can be borrowed at most for 60 days without penalty on the extra days.

To generate the ratings for the user on the item, we first calculate the interval t?  between the book borrowing record and the corresponding returning record. And we generate the ratings based on the interval as in table I. And the distribution of scores over the whole dataset is shown figure 2. And we can easily figure out that most of the books just have ratings with 1.

TABLE I.  RATING TRSANFORMATION  t? (Days) Rating [0,15) 1  [15,30) 2  [30,45) 3  [45,60) 4  [60,+ ? ) 5    Figure 1.  Distribution of the scores  B. Set the data set and parameters We first extract the borrowing records from the whole  dataset. And we have 26, 897 pairs of borrowing and return records between 334 users and 25, 018 books. We separate the data set into two parts: the train set and the test set. The records between 2011-06-07 and 2012-04-30 are set to be the train set and it has 26, 640 records. And we can see that R is very sparse with density is 0.3188%. The records between 2012-04-30 and 2012-06-30 are set to be the test set and it has 257 records.

To implement CF algorithms and the blending methods, we have the following parameters:     TABLE II.  PARAMETERS  Parameter(s) Related  Methods/A lgorithms  k  KNNitem, KNNuser f  LMF ?  LMF ?  LMF, NN  ,a b   NN And we have the two parameters to evaluate CF algorithm  and the blending methods: SR and RMSE. We first denote the number of predicted value is 'N and as there N data samples to predict. The SR is defined as follows:  ' /SR N N=                                                                 (5) And RMSE is defined as follows:  '  '   1 (P( ) ) N  i i i  RMSE x y N =  = ??                             (6) In the above equation, P( )ix  represents the predicted  value by the individual CF algorithm or the blending of different CF algorithms.

C. Results We first implement the three traditional algorithms and  the performance is shown in table III.

TABLE III.  PERFORMANCE OF THE INDIVIDUAL CF ALGORITHM  name RMSE description  KNNuser-1 1.33426 KNNuser, Pearson correlation, k =1  KNNitem-1 1.43217 KNNitem, Pearson correlation, k =1  LFM-1 1.110883 LMF, ? =0.01, ? =0.05, f =5  LFM-2 1.110819 LMF, ? =0.01, ? =0.05, f =10  LFM-3 1.110815 LMF, ? =0.01, ? =0.05, f =15  LFM-4 1.110747 LMF, ? =0.01, ? =0.05, f =18  LFM-5 1.110839 LMF, ? =0.01, ? =0.05, f =20  From the above table, we can see that LFM-4 performs the best of all and its RMSE is 1.110747. There is a certain  value of f which make LFM change towards differently directions.

1) LR:We use LR to blend the different CF algorithms.

And the performance is shown in table IV.

TABLE IV.  RESULTS OF LR BLENDING  name weights RMSE  KNNuser- 1+KNNitem-1  0,0 1.0  KNNuser-1+LFM-4 0,0 1.0  We can see that both of the results are with RMSE=1.0 and are much better than the individual algorithm.

2) NN:We use NN to blend the different CF algorithms.

Here, we set NN with 3 hidden layers and run NN 1000 times with different initial parameters to find the best one by using PyBrain [15].

While we use NN for blending the traditional CF algorithms, we set 0.01, 1a? = = and the constant  0b = in our experiment and the algorithm performs well with the above parameters.

And the training error of 1000 times is shown in figure 2.

Figure 2.  Train Error with different initializations  From the figure 2, we can see that most of the time the train error is below 1 and NN is a good at training models and extracting the parameters.

And the performance on the test set is shown in table V.

TABLE V.  RESULTS OF NN BLENDING  name Average Training Error(1000 times)  RMSE(the best)  KNNuser- 1+KNNitem-1  0.0567 0.6321  KNNuser-1+LFM-4 0.0559 0.5126  D. Comparision We list the results of both the individual CF algorithms  and the combined CF algorithms in table VI.

TABLE VI.  PERFORMANCE COMPARISON     name RMSE Evaluation KNNitem-1 1.43217 0%  KNNuser-1 1.33426 6.84%  LFM-4 1.110747 22.44% KNNuser- 1+KNNitem-1 (LR)  1.0 30.18%  KNNuser- 1+KNNitem-1 (NN)  0.6321 55.86%  KNNuser-1+LFM-4 (NN)  0.5126 64.21%  By blending the traditional CF algorithms, the performance is more than 60% at the most better than the individual CF algorithm. And the lowest RMSE is 0.5126, which means that while a reader borrows a book we can predict how long he will borrow with error not exceeding 0.5126*15 8? days.



V. CONCLUSION AND FUTURE WORKS In this paper, we focus on how to implement efficient and  accurate CF algorithms for the academic library. By transforming the rating information from the borrowing records, we can use the traditional CF algorithms to predict how long the reader will borrow the book.

And by blending the individual different CF algorithms, we can have much more accurate algorithms for predictions.

And NN blending method performs better than LR blending method. By using NN with KNNuser and LFM, the error of the prediction will not exceed 8 days.

In the future, we will focus on how to evaluate SR under the individual CF algorithms and the blending methods as while we use NN with KNNuser and LFM, SR is just 2.95%.

And we will try also more blending methods which are described in [16].

