Clustering Association Rules*

Abstract  We consider the problem of clustering two-dimensional as- sociation rules in large databases. We present a geometric- based algorithm, BitOp, for performing the clustering, em- bedded within an association rule clustering system, ARCS.

Association rule clustering is useful when the user desires to segment the datu. We measure the quality of the segment- ation generated by ARCS using the Minimum Description Length (MDL) principle of encoding the clusters on several databases including noise and errors. Scale-up experiments show that ARCS, using the BitOp algorithm, scales linearly with the amount of data.

1 Introduction Data mining, or the efficient discovery of interesting pat-  terns from large collections of data, has been recognized as an important area of database research. The most commonly sought patterns are association rules as introduced in [ 3 ] .

Intuitively, an association rule identifies a frequently occur- ing pattern of information in a database. Consider a super- market database where the set of items purchased by a single customer at the check-stand is recorded as a transaction.

The supermarket owners may be interested in finding ?asso- ciations? among the items purchased together at the check- stand. An example of such an association is that if a cus- tomer buys bread and butter then it is likely he will buy milk.

Given a set of transactions, where each transaction is a set of items, an association rule is an expression X j Y ,  where k? and y are sets of items. The meaning of this rule is: trans- actions that contain the items in X also tend to contain the items in Y .

Generally, association rules are used in conjunction with transaction (or basket) type data, but their use is not limited to this domain. When dealing with customer demographic data, for example, the database schema defines a fixed set  *This research was initiated by the first two authors while at Silicon Graphics Computer Systems, Inc. Continuing support for the first author has been provided by a graduate fellowship of the Department of Defense Office of Naval Research.

of attributes for each record, and each customer is repres- ented by one record. Each record contains a value for each attribute, i.e., (attribute = value). By replacing the sets of items in the traditional definition of association rules with conjunctions of (attribute = value) equalities, we can gen- eralize the above definition of association rules to include this type of non-transactional data. For example, (age = 40) A (Salary = $50,000)  When mining association rules from this type of non- transactional data, we may find hundreds or thousands of rules [9] corresponding to specific attribute values. We therefore introduce a clustered association rule as a rule that is formed by combining similar, ?adjacent? association rules to form a few general rules. Instead of a set of (attribute = value) equalities, for clustered rules we have a set of value ranges using inequalities. For example, the clustered rule (40 5 age < 42) (ownhome  = yes) could be formed from the two association rules (age = 40) (own-home 1 yes) and (age = 41) d ( o w n h o m e  = yes). The problem we consider is how to efficiently mine clustered association rules from large databases, using an as- sociation rule mining algorithm as one step in the overall process.

In practice it is very important that rules mined from a given database are understandable and useful to the user.

Clustered association rules are helpful in reducing the large number of association rules that are typically computed by existing algorithms, thereby rendering the clustered rules much easier to interpret and visualize. A practical use of these clusters is to perform segmentation on large customer- oriented databases. As an example, consider a marketing company that sends a direct mail catalog to its current cus- tomer base promoting its products. Orders are taken for each customer and placed in a transactional database, recording a list of those items purchased along with some demographic information about the purchaser. At some point the com- pany decides to expand its current customer base. By group- ing its existing customers by total sales, for instance, into groups of ?excellent?, ?above average?, and ?average? prof- itability, the company could use clustered association rules  (own-home = yes).

1063-6382/97 $10.00 0 1997 IEEE  mailto:widom}@cs.stanford.edu   on the attributes of the demographic database to segment its customers. The company can then use this segmentation in selecting the new customers it should target for future mail- ings who are most likely to respond. Hence, we would like a segmentation that defines a specific criterion (e.g., custom- ers rated ?excellent?) as a function of the other attribute val- ues. This corresponds to considering clustered association rules of the form  A l A A z A  . . .  AA,*G where G is one of ?excellent?, ?above average?, or ?av-  erage?, and the A, are attributeranges such as (40 5 age < 42).

In this paper, we consider association rule clustering in the two-dimensional space, where each axis represents one attribute from the database used on the left-hand side (LHS) of a rule. (We feel two attribute segmentation is easily un- derstandable by the user, however, in Section 5 we discuss possibilities for handling higher dimensionality data.) Be- fore processing by our algorithms, the two LHS attributes A, and A, are chosen by the user. Statistical techniques for identifying the most influential attributes for a given dataset, such as factor analysis [lo] and principal component ana- lysis 1111, have been well-studied and could also be used.

The user also selects a value from the third attribute as a cri- terion to be used for segmentation. We use an adjacency definition that ensures that the clusters are rectangular re- gions of the data. This helps with readability of the final clustered association rules.

Clustering in general is a difficult problem and as a re- search topic has been studied for quite some time 119, 211.

Even under the two-dimensional attribute assumptions we make, the problem of identifying the fewest clusters in  a two-dimensional grid is a specific instance of the k-decision set-covering problem, which has been shown to be NP- complete [ 5 ] .

Our clustering approach begins by taking source data in tuple form and partitioning those attributes that take values from a continuous domain. We then perform a single pass over the data using an association rule engine to derive a set of association rules. Next, we cluster all those two-attribute association rules where the right-hand side of the rules sat- isfies our segmentation criteria. Our approach to the clus- tering problem is heuristic, based on the geometric proper- ties of a two-dimensional grid, and produces an efficient lin- ear time approximation to an optimal solution. This cluster- ing produces the desired segmentation. We test the segment- ation for accuracy, and if necessary modify certain system parameters to produce a better segmentation and repeat the process.

1.1 Related Work The problem of taking sets of attribute-value pairs, such  as (Age=x, Salary=y), as points in two-dimensional space  and then finding optimal regions, with respect to some spe- cified criteria, within this space was introduced in [7] .  The authors in [7] considered two types of regions, rectangles and connected x-monotone, but focus on the latter. We fo- cus on rectangular regions because they usually are more understandable. Unlike 171 we do not require: the user to choose any of the parameters necessary for generating asso- ciation rules, but instead provide a fully automated system for identifying clustered association rules.

In [22] the authors consider the problem of mining quant- itative attributes, in contrast to previous algorithms designed for transaction-type items. The authors in [22] partition attributes using equi-depth bins (where each bin contains roughly the same number of tuples), and introduce a new measure to help the user to decide how many partitions there should be for each attribute. They address issues of com- bining adjacent intervals and use a ?greater-tham-expected- value? interest measure for the rules. One component of our work also requires partitioning quantitative attributes, but differs in that we are using the partitioned quantitative attrib- utes as a means to form clusters that ultimately define a seg- mentation of the data. Further, our system is fully automated and does not require any user-specified thresholds as does the work in  1221. In addition, we have provided an alternat- ive definition of an ?interesting? association rule that gives an intuitive means of segmenting the data. Finally, whereas 1221 uses user-specified parameters and a partiid complete- ness measure to determine attributepartitioning, we use geo- metric properties of the attribute space to form clusters.

A related problem is class$cution, where data is categor- ized into disjoint groups based upon common characterist- ics of its attributes. Techniques for classification include instance-based classifiers, decision trees, artificial neural networks, genetic algorithms, and various statistical tech- niques [ 171. Classification has been studied primarily in the AI community, and the computational complexity of these algorithms generally inhibits the performance efficiency ne- cessary when mining large databases 1121; furthermore, the algorithms do not scale well with increasing database size.

In the database community, the work in [I ,  13, 201 has fo- cused on designing efficient classification algorithms for large databases. The goal of classification is to compute a predictive model for each of the groups. The emphasis is to produce the most accurate model that can predict the group to which an unseen instance belongs. An understanding of the model that the classification algorithm computes is usu- ally of secondary importance. By contrast, in cur work we want to segment the space of attribute values, based on some criterion grouping attribute, into manageable segments that are understandable to the user.

Also related is the area of image processing:. Although a rich literature exists, traditional image segmentation al- gorithms tend to focus on obtaining exact boundaries for  22 1    clusters and as a result do not limit themselves to rectangu- lar clusters that we feel are necessary for understandability by the user [8]. Our system has been designed, however, so that it would be a simple matter of exchanging our clustering algorithm with any other.

1.2 Paper  Organization In Section 2 we present formal definitions and an over-  view of the work. Section 3 describes our Association Rule Clustering System, ARCS, and describes the BitOp al- gorithm for clustering. We also discuss a preprocessing step to smooth the grid, and dynamic pruning that removes un- interesting clusters. Statistical measures for setting the ini- tial threshold values are also described in Section 3. In Sec- tion 4, we present experimental results on synthetic data and show the scalability of ARCS for large databases. Using a classifier as an alternative way of segmenting the data, we compared a well-known classifier to our approach that in- stead uses clustered association rules for segmentation. Sec- tion 5 concludes and presents our ideas for future work.

2 Preliminaries In this section we present formal definitions and termin-  ology for the problem of mining clustered association rules, and then we give an overview of our approach.

2.1 Terminology An attribute can be either categorical or non-categorical.

Categorical attributes are those that have a finite number of possible values with no ordering amongst themselves. Ex- amples of categorical attributes include ?zip code?, ?hair color?, ?make of car?, etc. Non-categorical attributes, which we will call quantitative attributes, do have an im- plicit ordering and can assume continuous values usually within a specified range. Examples of quantitativeattributes include ?salary?, ?age?, ?interest rate?, etc.

Let D be a database of tuples where each tuple is a set of attribute values, called items, of the form (attributei = value,). An association ruEe is an expression of the form X Y ,  where X and Y are sets of (attribute=value) items such that no attribute appears more than once in X U y .  We will refer to X as the left-hand side (LHS) of the rule, and y as the right-hand side (RKS) of the rule.

Two common numeric measures assigned to each asso- ciation rule are support and conjidence. Support quantifies how often the items in X and Y occur together in the same  where JDJ denotes the total number of tuples. Confidence quantifies how often X and Y occur together as a fraction of the number of tuples in which X occurs, or ?>?I. When generating a set of association rules using any o F- the known algorithms, the user must specify minimum threshold values for both support and confidence.

tuple as a fraction of the total number of tuples, or - lXYl ID1  The following is an example of an association rule:  (Age = 32) A (Salary = $38,500) (Rating = good)  Because quantitative attributes will typically assume a wide range of values from their respective domains, we par- tition these attributes into intervals, called bins. In this pa- per, we consider only equi-width bins (the interval size of each bin is the same). Other choices are possible, such as equi-depth bins (where each bin contains roughly the same number of tuples), or homogeneity-based bins (each bin is sized so that the tuples in the bin are uniformly distributed) [14, 231. Our approach can easily be extended to handle these cases as well. When a quantitative attribute is parti- tioned into bins, the bins are mapped to consecutive integers; when attribute values are mapped to bins their value is re- placed with the corresponding integer for that bin. For cat- egorical attributes we also map the attribute values to a set of consecutive integers and use these integers in place of the categorical values. Since this mapping happens prior to run- ning an association rule mining algorithm, the binning pro- cess is transparent to the association rule engine. This bin- ning and mapping approach is also used in [221 and [15].

Clustering, as defined here, is the combination of adja- cent attributes values, or adjacent bins of attribute values.

For example, clustering (Agedo)  and ( A g e 4 l )  results in (40 5 Age < 42). A clustered association rule is an ex- pression of the form X C  * Y c .  X C  and Y C  are items of the form (Attribute = value) or (bini L: Attribute < bini+l), where binj denotes the lower bound for values in the ith bin.

Clustered association rules will always have a support and confidence of at least that of the minimum threshold levels.

2.2 Overview of Approach In this paper, we consider the problem of clustering as-  sociation rules of the form A A B - C where the LHS attributes ( A  and B )  are quantitative and the RHS attribute (C)  is categorical.? The RHS attribute could be quantitative, but would first require binning with the resulting bins then treated as categorical values. We define a segmentation as the collection of all the clustered association rules for a spe- cific value C of the criterion attribute.

Given a set of two-attribute association rules over binned data, we form a two-dimensional grid where each axis cor- responds to one of the LHS attributes. On this grid we will plot, for a specific value of the RHS attribute, all of the cor- responding association rules. An example of such a grid is shown in Figure 1. Our goal is to find the fewest number of clusters, shown as circles in the figure, that cover the asso- ciation rules within this grid. These clusters represent our clustered association rules and define the segmentation.

?We consider only quantitative LHS attributes because the lack of or- dering in categorical attributes introduces additional complexity. We are currently extending our work to handle categorical LHS attributes.

$60-$65k $55-$60k  $50-$55k  $45-$5Ok  $40-$45k .$ $35-$40k  $30-$35k  $25-$30k  $20-$25k  S l5-$20k $lO-$15k  $O-$lOk  m  <20 21 22 23 24 25 26 21 28 29 3 0  31  Age  Figure 1. Sample grid with clustered associ- ation rules.

The algorithm we introduce to cluster association rules using a 2D grid solves only a part of the overall problem.

Recall that in order to mine a collection of association rules we must first define minimum thresholds for both support and confidence. Further, for the quantitative attributes we must determine the number of bins over which to partition the attribute domain. Finding the values for each of these parameters that gives us the best segmentation is a difficult combinatorial optimization problem. We present a unifying framework that uses heuristics for searching this space ef- ficiently and has very good results. Figure 2 shows a high- level view of our entire system to compute the clustered as- sociation rules, which we now describe.

While the source data is read, the attribute values are par- titioned (by the binner) as described earlier. While the num- ber of bins can be changed by the user, doing so restarts the system. The association rule engine is a special-purpose al- gorithm that operates on the binned data. The minimum sup- port is used along with the minimum confidence to generate the association rules.

Once the association rules are discovered for a particu- lar level of support and confidence, we then form a grid of only those rules that give us information about the group (RHS) we are segmenting. We apply our BitOp algorithm (Section 3.3) to this grid, along with certain other techniques described in Sections 3.4 and 3.5, to form clusters of adja- cent association rules in the grid. These clustered associ- ation rules are then tested for their accuracy (by the verijer) against a sample of tuples from the source database. The ac- curacy is supplied to a heuristic optimizer (Section 3.7) that adjusts the minimum support threshold and/or the minimum confidence threshold, and restarts the mining process at the association rule engine. These heuristic adjustments con- tinue until the verifier detects no significant improvement in the resulting clustered association rules, or the verifier de- termines that the budgeted time has expired.

Record Data I  # of x-bins  Binner  y j a r r a y o f h i n n e d  data  , criteria Engine I asssociation rules  Heuristic Optimizer  conversion  Clustering . ? I I I \  clustered association rules i , \ test data \ ,  Figure 2. Architecture of the Association Rule Clustering System  Association Rule Clustering System (ARCS)  The ARCS framework was shown in Figure 2. We now detail components of the system.

3.1 Binning Data The binner reads in tuples from the database and re-  places the tuples? attribute values with their coriesponding bin number, as previously described. We first determine the bin numbers for each of the two (LHS) attributes, A ,  and A,. Using the corresponding bin numbers, bin, and bin,, we index into a 2D array where, for each bin,,bin, pair, we maintain the number of bin,,bin, tuples having each pos- sible RHS attribute value, as well as the total number of bin,,bin, tuples. Thesizeofthe2Darray is n,*ny*(nseg+ 1) where n., is the number of x-bins, ny is the number of y- bins, and nSeg  is the cardinality of the (RHS) segmentation attribute. In our system we assume this array can fit in main memory.

Although we are typically interested in only one value of the segmentation criteria at a time (e.g., ?customer-rating = excellent?), by maintaining this data structure in memory we can compute an entirely new segmentation for a different value of the segmentation criteria without the need to re-bin the original data. If memory space is at a premiurn, however, we can set nSeg  = 1 and maintain tuple counts for only the one value of the segmentation criteria we are inlerested in.

3.2 The Association Rule Engine While it is possible to use any of the existing association  rule mining algorithms to mine the binned data, we describe a more efficient algorithm for the special case of mining two- dimensional association rules using the data structure con- structed by our binning process. Deriving association rules from the binned data in the BinArray is straightforward. Let Gk be our RHS criterion attribute. Every cell in the BinAr- ray can be represented by an association rule whose LHS values are the two bins that define the BinArray cell, and whose RHS value is Gk:  where ( X  = i )  represents the range (bin; 5 X < binktl), and (U = j) represents the range (bin; 5 Y < bin;+l), and bin: and bin; are the lower bounds of the ith x-attribute bin and the i th  y-attribute bin, respectively. The support for this rule is and the confidence is .w, where N is the total number of tuples in the source data, I (i, j )  I is the total number of tuples mapped into the BinArray at location (i, j ) ,  and I(i, j ,  Gk)l is the number of tuples mapped into the BinArray at location (i, j )  with criterion attribute value Gk. To derive all the association rules for a given support and confidence threshold we need only check each of the oc- cupied cells in the BinArray to see if the above conditions hold. If the thresholds are met, we output the pair (i, j )  cor- responding to the association rule on binned data as shown above. The ( i ,  j )  pairs are then used to create a bitmap grid that is used by the BitOp algorithm, described in the follow- ing section, to locate clusters of association rules.

Our algorithm, shown in Figure 3, requires only a single pass through the data, in  contrast to using some existing al- gorithms that may need to make several passes to find all association rules. It is also very important to note that by maintaining the BinArray data structure, we can apply dif- ferent support or confidence thresholds without reexamin- ing the data, making the ?re-mining? process dramatically faster than with previous algorithms In  our system, chan- ging thresholds is nearly instantaneous.

(x = i) A (Y = j )  j Gk  3.3 Clustering We begin by presenting a very simple example of the  clustering problem to illustrate the idea. Consider the following four association rules where the RHS attribute ?GroupAabel? has value ?A?:  Input: 11: The BinArray computed from the binning component.

12: The value Gk we are using as the  13: The min-support threshold (%).

14: The min-confidence threshold (%).

15: N, the total number of tuples in the source data.

16: n,, the number of x-bins.

17: ny, the number of y-bins.

0 1  : A set of pairs of bin numbers, ( i ,  j ) ,  criterion for segmentation.

output:  representing association rules of the form (x = i) A (Y = j )  =+ Gk.

Procedure GenAssocationRules() minsupport-count = N * min-support; /* Association rule generation from the binned data */ for(i=l; i< n,; i++) do begin  for(j=l; j <  n,; j++) do begin if ((BinArray[i, j ,  Gk] 2 min-supportzount) and  (BinArray[i, j ,  Gk]/BinArray[i, j,Total] 2 min-conf)) Output (i j)  end-for end-for  Figure 3. The association rule mining al- gorithm  (Age = u3)  A (Salary = s5) =+ (Group-label = A) (Age = u4) A (Salary = s6) =$ (Group-label = A) (Age = u4)  A (Salary = s5) (Group-label = A ) (Age = u3)  A (Salary = 8 6 )  ==+ (Group-label = A)  We represent these four rules with the grid in Figure 4.2 Clearly linear, adjacent cells can be combined to form a line segment, and we can naturally extend this idea to rectangu- lar regions. In doing so, all four of the original association rules are clustered together and subsumed by one rule: (a3 5 Age < a4)A(ss 5 Salary < sg) + (GroupAabel = A )  Assuming the bin mappings shown in Figure 4, the final clustered rule output to the user is: (40 5 Age < 42) A ($40,000 5 Salary < $60,000) + (GroupAabel = A )  Intuitively, starting with a grid as in Figure 4, we are searching for the rectangular clusters that best cover all of the occupied cells. For example, given the grid in Figure 5, our algorithm would select the two clusters shown if we are interested in finding the fewest number of clusters.

The problem we investigate is how to find these non-  (Age = 40) A (Salary = $42,350) ==+ (GroupAabel = A ) (Age = 41) A (Salary = $57,000) ==$ (Grouplabel = A ) (Age = 41) A (Salary = $48,750) ===+ (Grouplabel = A) (Age = 40) A (Salary = $52,600) a (Groupdabel = A )  If theLHS Age bins areal,  u2 ,  . . . I and theLHS Salary bins are S I ,  sz, . . . , then these rules are binned to form the corres- ponding binned association rules:  overlapping clusters, Or more Precisely, how to find a good  *The grid is constructed directly from the (a,, sJ ) pairs output by the association rule engine.

Age  Figure 4. Grid showing the four association rules  mask  Figure 5.

clustering of rules given some criterion. An example of a simple criterion is minimizing the total number of clusters; in Section 3.6 we will describe the criterion we use. The process of clustering is made difficult by noise and outliers.

Noise occurs in the data from those tuples that belong to other groups than the group we are currently clustering. For example, a customer database could contain data about cus- tomers whose group label (RHS attribute) would be one of several values (customerxating is ?average?, ?above aver- age?, or ?excellent?), of which ?excellent? may be our cri- terion value for segmenting the data using clusters. Tuples having any other value will be considered noise that will af- fect both the support and confidence of the clusters we com- pute. Outliers are those tuples that belong to the same group but lie outside of any existing clusters for that group. We will give examples of these occurrences and show how our clustering system can mitigate their effects in  the following sections.

01 1 1 0 1  1 1 11 10 10  3.3.1 The BitOp Algorithm The algorithm we present enumerates clusters from the grid.

We select the largest cluster from the enumerated list, it- eratively applying the algorithm until no clusters remain.

It has been shown in  [5] that such a greedy approach pro- duces near optimal clusters in O ( ~ , , ,  15?1) time, where C is the final set of clusters found. We experimented with pure  Bitmap Masks starting at row 1  The first mask, mask:, is defined to be the same as the first row, while the second mask, mask:, is the first mask bitwise ANDed with the second row. Since mask: is not identical to mask:, we evaluate mask: for clusters finding a 2-by-1 cluster, indicated by the solid circle. VVe see that mask: identifies a 1-by-2 cluster: 1 since a single bit is set     and 2 since it is in the second mask meaning the cluster extends two rows. This cluster is indicated by the dashed circle. Finally, mask: identifies no clusters since the mask contains no set bits, signifying there are no clusters that be- gin at I-ow l and extend through row 3.

We now repeat this process, beginning with the second row of the bitmap, producing the two clusters as shown:  Bitmap Masks starting at row 2  row3  1 0 0 mask; row2  mask: row 1 0 1 1  The process ends when the mask for the last row is com- puted. Our algorithm can be implemented efficiently since it only uses arithmetic registers, bitwise AND, and bit-shift machine instructions. We assume that the size of the bitmap is such that it fits in memory, which is easily the case even for a 1000x1000 bitmap.

3.4 Grid Smoothing As a preprocessing step to clustering, we apply a 2D  smoothing function to the bitmap grid. In practice, we have often found that the grids contain jagged edges or small ?holes? of missing values where no association rule was found. A typical example is shown in Figure 7(a). These features inhibit our ability to find large, complete clusters.

To reduce the effects caused by such anomalies, we use an image processing technique known as a low-pass jilter to smooth out the grid prior to processing [8]. Essentially, a low-pass filter used on a two-dimensional grid replaces a value with the average value of its adjoining neighbors, thereby ?smoothing? out large variations or inconsistencies in  the grid. The use of smoothing filters to reduce noise is well known in other domains such as communications and computer graphics [8]. Details of our filtering algorithm are omitted for brevity, but Figure 7(b) nicely illustrates the res- ults. Experiments using the association rule support values instead of binary values were also performed yielding prom- ising results (See Section 5).

3.5 Cluster Pruning Clusters that are found by the BitOp algorithm but that  do not meet certain criteria are dynamically pruned from the set of final candidate clusters. Typically, we have found that clusters smaller than 1% of the overall graph are not use- ful in  creating a generalized segmentation. Pruning these smaller clusters also aids in reducing ?outliers? and effects from noise not eliminated by the smoothing step. In the case where the BitOp algorithm finds all of the clusters to be suf- ficiently large, no pruning is performed. Likewise, if the al-  Input: 11: R, the number of bins for attributex.

12: C, the number of bins for attribute Y.

13: BM, the bitmap representation of the grid.

(BM[i] is the ith row of bits in the bitmap) output:  0 1  : clusters of association rules  row = 1; while (row i R) do begin  RowMask = set all bits to ? 1?; PriorMask = BM[row]; height=O; for (r=row; riR; r++) do begin  RowMask = RowMask & BM[row]; if (RowMask == 0) do begin I* Locate clusters in PriorMask * I process_row(PriorMask,height) ; break;  end-if if (RowMask != PriorMask) do begin  processxo w(PriorMask,height) ; PriorMask = RowMask;  end-if height = height+l; I* extend height of possible clusters *I  end-for processJow(PriorMask,height);  end-while  Figure 6. The BitOp algorithm  * B  e m  attribute X (4  Figure 7. A typical grid (a) prior to smoothing; (b) after smoothing.

computed cluster Function 2 false-positives 1 ( ( a g e  < 40) A (50K L: s a l a r y  5 1 O O K ) )  j GroupA 2 ((40 5 age < 60) A (7511? 5 s a l a r y  5 125K)) GroupA 3 ( ( a g e  2 60) A (25K 5 salary 5 751q)) 3 Group A  LI B Y  else + Group other 0 a E  false-negatives - .- Y a  Figure 8. The function used to generate syn- thetic data  gorithm cannot locate a sufficiently large cluster, it termin- ates. The idea of pruning to reduce error and to reduce the size of the result has been used in the AI community, espe- cially for decision trees [ 171.

3.6 Cluster Accuracy Analysis To determine the quality of a segmentation by a set of  clustered association rules we measure two quantities: (i) the number of rules computed, and (ii) the summed error rate of the rules based on a sample of the data. The two quantities are combined using the minimum description length (MDL) principle [ 181 to arrive at a quantitative measure for determ- ining the quality compared to an ?optimal? segmentation, We first describe our experiments, then we describe our error measure for a given rule, then we describe our application of the MDL principle.

In [2], a set of six quantitativeattributes (salary, commis- sion, age, hvulue, hyears, loan) and three categorical attrib- utes (educationdevel, cur; zip code) for a test database are defined. Using these attributes, 10 functions of various com- plexity were listed. We used Function 2, shown in Figure 8, to generate the synthetic data used in our experiments. The optimal segmentation for this data would be three clustered association rules, each of which represents one of the three disjuncts of the function. The clustering process is made dif- ficult when we introduce noise, random perturbations of at- tribute values, and error due to binningof the input attributes age and salary.

An intuitive measure of the accuracy of the resulting clustered rules would be to see how well the rectangular clusters overlap the three precise disjuncts of the function in Figure 8. We define the notion of false-positives and fulse-negatives graphically, as shown in Figure 9, and seek to minimize both sources of error. In Figure 9, the light- grey rectangle represents an actual cluster according to the function, and the dark-grey rectangle represents a computed cluster. Note that in general an optimal cluster need not be rectangular. The false-positive results are when the com- puted cluster incorrectly identifies tuples outside of the op- timal cluster as belonging to the specified group, whereas the false-negatives are tuples that should belong to the group but are not identified as such by the computed cluster. The total summed error for a particular cluster is the total (false-  Figure 9. Error between overlapping rc-g? * ions  positives + false-negatives). However, unless the optimal clustering is known beforehand, such as here where a func- tion is used to generate the data, this exact measure of error is not possible. Because we are interested in real-world data where the optimal clustering is not known, we instead se- lect a random sample of tuples from the database and use these samples to determine the relative error of the com- puted clusters. The relative error is only an approximation to the exact error since we are counting the number of false- negatives and false-positives based only on a sample of the original database. In order to get a good approximation to the actual error, we use repeated k out of n sampling, a stronger statistical technique.

The strategy we use to measure the quality of a segmenta- tion given a set of clustered association rules is b<asecf on the MDL principle. We are using a simplified model of MDL that has worked in practice. The MDL principle states that the best model for encoding data is the one that minimizes the sum of the cost of describing the model and the cost of describing the data using that model. The goal is to find a model that results in  the lowest overall cost, with cost typ- ically measured in bits.

In the context of clustering, the models are the descrip- tions of the clusters and the data is the sampled data de- scribed above. The greater the number of clusters used for segmentation, the higher the cost necessary to describe those clusters. The cost of encoding the sampled data using a given set of clusters (the model) is defined to be the sum of all errors for the clusters. The intuition is that i f  a ruple is not an error, then it is identified by a particular cluster and hence its cost is included with the description of the cluster.

Otherwise, if the tuple is an error, then we must specifically identify it as such and this incurs a cost. We use the follow- ing equation to determine the cost of a given set of clustered association rules:  cost = w, log,(ICI) + we log,(errors) where IC/ is the number of clusters and e r r o r s  is the sum of (false-positives + false-negatives) for the clusters C.  The logarithmic factor is used because having more clusters re- quires a logarithmically increasing number of bits to enu-     merate, and the logarithmic factor provides a favorable non- linear separation between close and near-optimal solutions.

Based on empirical evidence, we made the simplifying as- sumption the clusters themselves have a uniform encoding cost. The constants wc and we allow the user to impart a bias towards ?optimal? cluster selection, providing greater flex- ibility in  finding a representation of the segmentation that is the most usable. If w, is large, segmentations that have many clusters will be penalized more heavily since they will have a higher associated cost, and segmentations of the data that have fewer clusters will have a greater probability of be- ing ?optimal?. Likewise, if we is large, the system will favor segmentations where the error rate is lowest. If both con- stants are equal, wc = we = 1, as in thedefault case, neither term will bias the cost.

Our heuristic optimizer (recall Figure 2), by means de- scribed in the following section, seeks to minimize the MDL cost.

3.7 Parameter Heuristics In this section we describe the algorithm our overall sys-  tem uses to adjust the minimum support and confidence thresholds based upon the accuracy analysis from the previ- ous section. (currently the number of bins for each attribute is preset at 50. We discuss this issue more in the following section.) We desire values for support and confidence that will result in a segmentation of the data that optimizes our MDL cost function. The search process involves successive iterations through the feedback loop shown in Figure 2. We identify the actual support and confidence values that appear in the binned data, and use only these values when adjusting the ARCS parameters. We begin by enumerating all unique support thresholds from the binned data with one pass, and then all of the unique confidence thresholds for each of these support thresholds with a second pass. A data structure sim- ilar to that shown in Figure 10 is used to maintain these val- ues. Note that as support increases, there become fewer and fewer cells that can support an association rule, and we have found a similar decrease in the variability of the confidence values of such cells.

Given a choice to either begin with a low support threshold and search upwards, or begin with a high sup- port threshold and search downwards, we chose the former since we found most ?optimal? segmentations were derived from grids with lower support thresholds. If we were using a previous association rule mining algorithm, for efficiency it might be preferable to start at a high support threshold and work downwards, but our efficient mining algorithm al- lows us to discover segmentations by starting at a low sup- port threshold and working upwards. Our search starts with a low minimum support threshold so that we consider a lar- ger number of association rules initially, allowing the dy- namic pruning performed by the clustering algorithm to re-  Confidence List - I I I I I I 5 rl 12% I 25% I 40% I 56%I  90%) Sumort I  List? //-Wra*luoal  Figure 10. Ordered support thresholds  . . .

lists of confidence and  move unnecessary rules. The support is gradually increased to remove background noise and outliers until there is no im- provement of the clustered association rules (within some E ) .

4 Experimental Results  The ARCS system and the BitOp algorithm have been implemented in C++ (comprising approximately 6,300 lines of code). To assess the performance and results of the al- gorithms in the system, we performed several experiments on an Intel Pentium workstation with a CPU clock rate of 120MHz and 32MB of main memory, running Linux 1.3.48.

We first describe the synthetic rules used in generating data and present our initial results. We then briefly compare our results with those using a well-known classifier, C4.5, to perform the segmentation task. Finally, we show perform- ance results as the sizes of the databases scale.

4.1 Generation of Synthetic Data We generated synthetic tuples using the rules of Func-  tion 2 from Figure 8. Several parameters affect the distri- bution of the synthetic data. These include the fraction of the overall number of tuples that are assigned to each value of the criterion attribute, aperturbation factor to model fuzzy boundaries between disjuncts, and an outlier percentage that defines how many tuples will be assigned to a given group label but do not match any of the defining rules for that group. These parameters are shown in Table 1.

4.2 Accuracy and Performance Results We generated one set of data for Function 2 with ID1 =  50,000 and 5% perturbation, and a second set of data for the same function but with 10% outliers, i.e., 10% of the data are outliers that do not obey the generating rules. In every ex- perimental run we performed, ARCS always produced three clustered association rules, each very similar to the gener- ating rules, and effectively removed all noise and outliers     Attribute 1 Value salary  age  uniformly distributed from $20,000 to $150k uniformly distributed from 20 to 80  PI fracA  fracother  P U  Number of tuples, 20,000 to 10 million Fraction of tuples for ?Group A?, 40% Fraction of tuples for ?Group other?, 60% Perturbation factor, 5% Outlier percentage, 0% and 10%  Table 1. Synthetic data parameters  from the database. The following clustered association rules were generated for Group A clusters from the set of data containing outliers, and with a minimum support threshold of 0.01% and a minimum confidence threshold of 39.0%:  (20 5 Age 5 39) A ($48601 5 Salary 5 $100600) j Grp A (40 5 Age 5 59) A ($74601 5 Salary 5 $124000) + Grp A (60 5 Age 5 80) A ($25201 5 Salary 5 $74600) + Grp A The reader can compare the similarity of these rules with those used to generate the synthetic data in Figure 8.

We measured the error rate of ARCS on these databases and compared it to rules from C4.5. C4.5 is well known for building highly accurate decision trees that are used for classifying new data, and from these trees a routine called C4.5RULES constructs generalized rules [ 171. These rules have a form similar to our clustered association rules, and we use them for comparison, both in accuracy and in speed of generation. Figures 11 and 12 graph the error of both sys- tems as the number of tuples scale, using the two sets of gen- erated data. The missing bars for C4.5 on larger database sizes are due to the depletion of virtual memory for those ex- periments, resulting in our inability to obtain results (clearly C4.5 is not suited to large-scale data sets.)  From Figure 11, we see that C4.5 rules generally have a slightly lower error rate than ARCS clustered association rules when there are no outliers in the data. However, with 10% of the data appearing as outliers the error rate of C4.5 is slightly higher than ARCS, as shown in  Figure 12. C4.5 also comes at a cost of producing significantly more rules, as shown in Figures 13 and 14. As mentioned earlier, we are targeting environments where the rules will be processed by end users, so keeping the number of rules small is very im- portant.

The primary cause of error in the ARCS rules is due to the granularity of binning. The coarser the granularity, the less likely it will be that the computed rules will have the same boundary as the generating rules. To test this hypo- thesis, we performed a separate set of identical experiments using between 10 to 50 bins for each attribute. We found a general trend towards more ?optimal? clusters as the number of bins increases.

100 10002000 4000 8000 10000 Number of Tuples (in ?000s)  Figure 15. Scalability of ARCS  Table 2. Comparative execution timers (sec)  4.3 Scaleup Experiments  To test scalability, we ran ARCS on several databases with increasing numbers of tuples. Figure 15 shows that ex- ecution time increases at most linearly with the size of the database. Because ARCS maintains only the ElinArray and the bitmap grid, ARCS requires only a constant amount of main memory regardless of the size of the database (assum- ing the same number of bins). This actually gives our system significantly better than linear performance, as can be seen by close inspection of Figure 15, since some overhead ex- ists initially but the data is streamed in faster from the U 0 device with larger requests. For example, when the num- ber of tuples scales from 100,000 to 10 million (a factor of loo), the execution time increases from 42 seconds to 420 seconds (a factor of lo). In comparison, C4.5 requires the entire database, times some factor, to fit entirely in main memory. This results in paging and eventual depletion of virtual memory (which prevented us from obtaining execu- tion times or accuracy results for C4.5 rules from databases greater than 100,000 tuples). Both C4.5 alone and C4.5 to- gether with C4.5RULES take exponentially higher execu- tion times than ARCS, as shown in Table 2.

10 r- --- -______? 20   20 50 100 200 500 1000 Number of Tuples (in ?000s)  Figure 11. Error rate with U = 0%  20 50 100 200 500 1000 Number of Tuples (in ?000s)  Figure 12. Error rate with U = 10%   4 30 II: 6 25  ~~  v) --  --  L  3 20 -- 5 15 --  z  10 --  5 -  O T     20 50 100 200 500 1000 Number of Tuples (in ?000s)  Figure 13. Number of rules produced, U = 0%  5 Conclusions and Future Work  In this paper we have investigated clustering two- attribute association rules to identify generalized segments in large databases. The contributions of this paper are sum- marized here:  0 We have presented an automated system to compute a clustering of the two-attribute space in large databases.

0 We have demonstrated how association rule mining technology can be applied to the clustering problem.

We also have proposed a specialized mining algorithm that only makes one pass through the data for a given partitioning of the input attributes, and allows the sup- port or confidence thresholds to change without requir- ing a new pass through the data.

e A new geometric algorithm for locating clusters in a two-dimensional grid was introduced. Our approach has been shown to run in linear time with the size of   !$ 14 - z 12 O 10 kl  z 6    Lc  20 50 100 200 500 1000 Number of Tuples (in ?000s)  Figure 14. Number of rules produced, U = 10%  the clusters. Further, parallel implementations of the algorithm would be straightforward.

0 We apply the Minimum Description Length (MDL) principle as a means of evaluating clusters and use this metric in describing an ?optimal? clustering of associ- ation rules.

0 Experimental results show the usefulness of the clustered association rules and demonstrates how the proposed system scales in better than linear time with the amount of data.

The algorithm and system presented in the paper have been implemented on several platforms, including Intel, DEC, and SGI. So far we have performed tests only us- ing synthetic data, but intend to examine real-world demo- graphic data. We also plan on extending this work in the fol- lowing areas:  0 It may be desirable to find clusters with more than two attributes. One way in which we can extend our pro- posed system is by iteratively combining overlapping     sets of two-attribute clustered association rules to  pro- duce clusters that have an arbitrary number of attrib- utes.

Handle both categorical and quantitative attributes on the LHS of  rules. To obtain the best clustering, we will need to  consider all feasible orderings of categorical at- tributes. Our clustering algorithm has been extended to handle the case where one attribute is categorical and the other quantitative and we achieved good results. By using the ordering of the quantitative attribute, we  con- sider only those subsets of the categorical attribute that yield the densest clusters.

Preliminary experiments show that segmentation can be  improved if the association rule support values, rather than binary values, are considered in  the smooth- ing filter, and more advanced filters could be  used for purposes of  detecting edges and corners of clusters.

It may be beneficial to  apply measures of information gain [16], such as entropy, when determining which two attributes to  select for segmentation or for the op- timal threshold values for support and confidence.

The technique of factorial design by Fisher [6 ,  41 can greatly reduce the number of experiments necessary when searching for ?optimal? solutions. This tech- nique can be applied in the heuristic optimizer to re- duce the number of runs required to  find good values for minimum support and minimum confidence. Other search techniques such as simulated annealing can be also be used in the optimization step.

Acknowledgements  We are grateful to Dan Liu for his work in extending functionality in the synthetic data generator and for the ex- periments using C4.5.

