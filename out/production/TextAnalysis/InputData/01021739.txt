Mining Negative Association Rules?

Abstract  The focus of this paper is the discovery of negative as- sociation rules. Such association rules are complementary to the sorts of association rules most often encountered in literatures and have the forms of X ! :Y or :X ! Y .

We present a rule discovery algorithm that finds a useful subset of valid negative rules. In generating negative rules, we employ a hierarchical graph-structured taxonomy of do- main terms. A taxonomy containing classification informa- tion records the similarity between items. Given the tax- onomy, sibling rules, duplicated from positive rules with a couple items replaced, are derived together with their esti- mated confidence. Those sibling rules that bring big con- fidence deviation are considered candidate negative rules.

Our study shows that negative association rules can be dis- covered efficiently from large database.

1. Introduction  Data mining is the process of extracting implicit, pre- viously unknown, and potentially useful information from large quantities of data. Through the accretion of current data with historical data, enterprises find themselves in pos- session of larger data sets in electronic form than at any time heretofore. Various techniques have been employed to con- vert the data into information, including clustering, classi- fication, regression, association rule induction, sequencing discovery, and so forth. In general, an association rule rep- resents a relationship between two sets of items in the same database. It can be written in the form X ! Y , where X and Y are item sets (i.e., values from stipulated domains) and X \ Y = ;. The left-hand side (LHS) of the rule is called the antecedent, while the right-hand side (RHS) is called the consequence.

1This work was supported in part by DoD EPSCoR and the State of Louisiana under grant F49620-1-0351.

Much effort has been devoted and algorithms proposed for efficiently discovering association rules [1, 2, 13, 5, 12, 6]. In applying these algorithms, quantity measurements for generality and correctness are developed, which are sup- port and confidence, respectively. The support measures the frequency with which an item set appears in the database and the support for a rule is defined to be the frequency of occurrence that contains the union of items in the an- tecedent and consequent. For instance, in a given database, 266 out of 1000 records contain ?bread?, and then the sup- port is 26.6%. A rule with low support usually indicates it is not very important. However, the relative frequency of the occurrence of the combinations, which is measured by confidence, is essential in discovering strong rules. Con- fidence is computed as the ratio of the frequency of co- occurrence of antecedent and consequence divided by the support of the antecedent. It measures the correctness of the rule with the predictability of the rule. In the above example, if 218 records contain both ?bread? and ?butter?, then the frequency of occurrence of ?bread and butter? is 21.8%. Thus the confidence of the rule ?bread ! butter?, which means if bread is purchased, butter is also purchased, will be 0.218/0.266, which is 81.95%. Thus, this rule can be restated as ?81.95% of transactions that include purchasing bread also include purchasing butter.? The mining problem consists of finding all association rules having support and confidence that satisfy both predefined minimum support (minsup) and minimum confidence (minconf ), respectively.

In addition to the sort of association rule discussed above, there are rules that imply negative relationships.

Such rules are called negative association rules [9]. A neg- ative association rule also describes relationships between item sets and implies the occurrence of some item sets char- acterized by the absence of others. In contrast to negative association rules, the classical rules most frequently stud- ied are called positive association rules here. Algorithms for discovering negative rules are occasionally discussed [9, 10]. Ashok, et al. [9] generates negative rules based on a complex measure of rule parts. Brin, et al. [10] generalize  Proceedings of the Seventh International Symposium on Computers and Communications (ISCC?02)    positive and negative rules into correlations and induce with the dependence between two item sets.

In brief, mining negative association rules consists of defining a heuristic search procedure. Heuristics tend to fa- vor one set of solutions over others. The heuristics cited above are not designed to find all possible negative asso- ciation rules. It is the ones that are most useful that the heuristic should target for discovery. Here we argue that cri- terion for negative association rules utility is its relationship to a valid positive rule. In this article, we present a method to discover negative rules following the support-confidence scheme together with prior knowledge of the relationships between term values in the item sets. In the remainder of this paper, we start with presenting rule effectiveness mea- sures in section 2 then give the problem statement and for- mal description in section 3. Section 4 explains the al- gorithm and the experimental results follow in section 5.

Discussion and conclusions are in section 6.

2. Rule Effectiveness Measures  Rule induction is the process of discovering rules that reach a predefined support-confidence threshold. If we con- sider the problem of determining which rules are meaning- ful from an information-theoretic aspect, those with high occurrence rate and corresponding low entropy, tend to be rules. P. Smyth et al. [11] proposed an alternate en- tropy measure of information level for rules and Gregory Piatesky-Shapiro [8] used the measure based directly on probabilities. However, in mining association rules, the purpose is to find the most often regularity. That means the way to measure ?goodness? should be different. Fortu- nately, in our situation, namely, mining negative association rules, measuring the deviation is more critical than identi- fying information level itself.

Generally, items from the same ?market basket? tend to appear in the same set of transactions. If rules are modi- fied through substitution of items within the given basket, we naturally expect the truth to still hold with respect to the same measure. However, careful computation can show that the measure of some rules can deviate greatly from what would be expected. Such rules include negative associa- tion rules. A simple example of a negative rule is ?78% of transactions that include purchasing bread and milk, do not include butter.?  Example: Table 1(a) shows synthetic data for vehicle purchase information. Assumption 1: The minimum sup- port is 30% and minimum confidence is 70%. Assump- tion 2: The numeric attribute AGE ranges from 18 to 70 and is quantized into two groups - less than thirty and over thirty. Item sets that satisfy minsup (=30%) are listed in Ta- ble 1(b). The rule that satisfies both minimum support and minimum confidence criterion is ?age < 30 ! coupe?, the  Table 1. (a) vehicle purchase information, (b) large item sets with minimum support of 30%  SSN Name Age Vehicle Type 711205331 John Smith 23 Coupe 831213362 Tom Lee 45 Truck 914223733 Rachel White 50 Van 751238334 Jack Gates 27 Sedan 116243395 Peter Cruise 38 Sedan 271253036 Sally Lent 20 Coupe 318261337 Mike Swami 41 Truck 491273328 Erin Buckles 22 Coupe 510283339 Nick Park 39 Coupe 611294340 Jane Chen 40 Sedan  (a) Item sets Support Age < 30 40% Age > 30 60%  Coupe 40% Sedan 30%  Age < 30, coupe 30% (b)  confidence of which is 75%. However, if we are also look- ing for negative association rules, there exists a rule: ?age >30 ! ?not purchasing coupe??, which has a confidence of 83.3%. For the purpose of identifying purchase pattern, it is obvious that the latter has better predictive ability.

The preceding example illustrates that negative associa- tion rules are as important as positive ones. However, there are at least two aspects making mining negative rules dif- ficult. First, we cannot simply pick threshold values for support and confidence that are guaranteed to be effective in sifting both positive and negative rules. Second, in a practical database, thousands of items are included in the transaction records. However, there may be a large number of items in the domain of an attribute that do not appear in the database or appear an insignificant number of times. An absent item is defined to be one that occurs an insignificant number of times in the transaction set. If we consider neg- ative rules for which the LHS or RHS is a combination of absent as well as frequently used items, then there will be a great many of them. In this case, a valid rule, however, is not necessarily a useful one. This article addresses both issues.

3. Problem Statement  The negative association rule differs from its positive counterpart not only in the mining procedure but also in  Proceedings of the Seventh International Symposium on Computers and Communications (ISCC?02)    form. Before we present the problem of discovery of nega- tive association rules, let?s state its formal definition.

3.1. Definition of Negative Association Rule  Let I = fi1; i2; : : : ; ing be a set of items, where each item ij corresponds to a value of an attribute and is a member of some attribute domain Dh = fd1; d2; : : : ; dsg; i:e: ij 2 Dh. For simplicity, we make do- mains discrete. Let T = f?1; ?2; : : : ; ?pg be a table, where T ? D1 ? D2 ? : : : ? Dq. The Cartesian product spec- ifies all possible combinations of values from the underly- ing domains [3]. Each transaction ?j = hg1; g2; : : : ; gmi in table T is associated with an unordered attribute value set tj = fg1; g2; : : : ; gmg using the obvious transform, where tj ? I . We say that tj contains itemset X , which is a subset of I , if for 8ik 2 X; ik 2 tj . A positive association rule represents a relationship between two sets of items in the form of X ! Y , where X ? I , Y ? I and X \ Y = ;.

It might be noted that our definition of rules differs from previously published study [1, 2] in that our item domains are not necessary binary. We say that tj does not contain itemset X ? I , if 9ik 2 X , ik 62 tj . A negative association rule is an implication of the form X ! :Y (or :X ! Y ), where X ? I , Y ? I and X \ Y = ;(Note that although rule in the form of :X ! :Y contains negative elements, it is equivalent to a positive association rule in the form of Y ! X . Therefore it is not considered as a negative as- sociation rule.) In contrast to positive rules, a negative rule encapsulates relationship between the occurrences of one set of items with the absence of the other set of items. Our explanation is based on the former negative rule, X ! :Y ; however, we note in the following descriptions where there are differences.

The rule X ! :Y has support s% in the data sets, if s% of transactions in T contain itemset X while do not contain item set Y . The support of a negative association rule, supp(X ! :Y ), is the frequency of occurrence of transactions with item set X in the absence of item set Y . Let U be the set of transactions that contain all items in X . The rule X ! :Y holds in the given data set (database) with confidence c%, if c% of transactions in U do not contain item set Y . Confidence of negative as- sociation rule, conf(X ! :Y ), can be calculated with P (X [:Y )=P (X), where P (?) is the probability function.

Previous algorithms for discovering positive association rules, such as Apriori and AprioriTid [2], require multiple passes over the database. The support and confidence of item sets are calculated during iterations. However, it is difficult to count the support and confidence of non-existing items in transactions. To avoid counting them directly, we can compute the measures through those of positive rules.

Given supp(X ! Y ) and conf(X ! Y ), the support and  confidence of the negative rule X ! :Y can be computed as:  supp(X ! :Y ) = supp(X)? supp(X ! Y ) (1)  conf(X ! :Y ) = 1? conf(X ! Y ) (2)  The computation of the support and confidence of nega- tive rules of form :X ! Y differ, which is given in equa- tion 3 and 4.

supp(:X ! Y ) = supp(Y )? supp(Y ! X) (3)  conf(:X ! Y ) = P (Y )  1? P (X) (1? conf(Y ! X) (4)  3.2. Mining Negative Association Rule  Mining association rule as most frequently presented in- volves finding rules that satisfy both user-specified mini- mum support and minimum confidence. We cannot intu- itively ?complement? the thresholds to find negative rules.

That is, we cannot find positive rules with small support and confidence values, such that by applying equations 2 and 4, the corresponding negative rules are exposed. That will result in many uninteresting rules. To eliminate un- wanted rules and focus on potential interesting ones, we predict possible interesting negative association rules by in- corporating domain knowledge of the data sets.

3.2.1 Locality of Similarity  Let T be the taxonomy of the given item sets. Taxonomy T consists of vertexes and directed edges. Each vertex repre- sents a class. The vertex with in-degree of zero is the most general class and the vertexes having out-degrees of zero are the most specific classes and correspond to items. Two ver- texes are connected with a directed edge, which represents an is-a relationship. An example is shown in Figure 1.

Figure 1. Example of taxonomy  In most cases, taxonomies over the items can be devised.

They provide domain knowledge of the item sets. Within  Proceedings of the Seventh International Symposium on Computers and Communications (ISCC?02)    the taxonomy, two kinds of relationships are important ? vertical relationships and horizontal ones [13, 4, 7]. The vertical relationship semantics is that the lower level vertex values are instances of the values of immediate predecessor vertexes, i.e., the is-a relationship. In [13], a vertical rela- tionship is used to discover generalized association rules.

The semantics of the horizontal relationship is that the ver- texes on the same level having the same immediate prede- cessor (siblings to borrow from rooted tree terminology) en- capsulate similarity among classes. We call sibling relation- ships locality of similarity (LOS). Items belonging to the same LOS tend to participate in similar association rules.

This is generally true because members of such groups tend to have similar activity patterns. For example, in a retail database, instances are items involved in transactions and customers are participants. If there is no preference for each person, the purchase probability of each item will be evenly distributed over all brands. Items fall into the same LOS can be denoted as [i1; i2; : : : ; im], where i1; i2; : : : ; im 2 I are members of a LOS and are siblings. In the taxonomy shown in Figure 1, ?IBM Aptiva? and ?Compaq Deskpro? are on the same level and are inherited from ?Desktop Sys- tem?, thus they belong to one LOS that can be written as [?IBM Aptiva?, ?Compaq Deskpro?]. However, the LOS can be extended to different levels following the same parent node. For instance, it is more reasonable to put ?IBM Ap- tiva?, ?Compaq Deskpro?, ?Notebook?, and ?Parts? into one LOS when viewing the database at a more abstract level.

Intuitively, siblings are in the same LOS. Further, similarity is inheritable. Formally, the similarity assumption in this paper is stated as follows:  (a) Let X = fi1; i2; : : : ; ih; : : : ; img; Y = fj1; j2; : : : ; jlg, and X;Y ? I;X \ Y = ;;  (b) 9ih 2 I; ik 62 X; and[ih; ik], such that supp(ih) ? minsup, supp(ik) ? minsup;  If 9(r : X ! Y ), where rule r consists of an- tecedent X and consequence Y , that is supp(X ! Y ) ? minsup; conf(X ! Y ) ? minconf , there is a possibility that the inequality conf(X 0 ! Y ) ? minconf is true if the item set X 0 = fi1; i2; : : : ; ik; : : : ; img is the same as X except item ik is substituted for ih. We denote the rule X 0 ! Y as rule r0. We say that rule r : X ! Y and rule r0 : X 0 ! Y are sibling rules to each other.

3.2.2 Discovering Negative Rules  The use of localities of similarity provides clues to poten- tially useful negative rules. Suppose that items ih; ik 2 I are members of a LOS, that is, [ih; ik]. If rule r : X ! Y is true and ih 2 X , then based on the similarity assumption, by substituting ik for ih in the antecedent of rule r, a sibling  positive rule r0 : X 0 ! Y is generated. However, if such association is not supported, which means the occurrence of Y is not related to X 0, then the corresponding negative association may exist. A salience measure needs to be de- fined. In the context of the taxonomy, the salience measure is defined as a distance between confidence levels:  SM = jconf(r0)?E(conf(r0))j (5)  Where conf(r0) is the actual confidence of rule r0, com- puted with equations 2 or 4. E(conf(r0)) is the estimated confidence of rule r0, which is defined to be equal to the confidence of r based on the similarity assumption. A large value for SM is evidence for accepting the hypothesis that X 0 ! Y is false. That is, X 0 ! :Y may be true. From the information theoretic perspective, a large SM value signi- fies large information gain. In a sense, ?interestingness? is associated with high entropy. If the candidate negative as- sociation rule satisfies both support and confidence criteria, it is kept. In brief, to qualify as a negative rule, it must sat- isfy two conditions: first, there must exist a large deviation between the estimated and actual confidence and, second, the support and confidence are greater than the minima re- quired.

3.2.3 Pruning  A major consideration in mining negative rules is avoiding the many uninteresting rules. This is overcome mostly by a prediction procedure. However, in addition to the prun- ing process in discovering positive rules, some redundancy introduced by negative rules need to be described. In con- structing candidate negative rules, there are possibilities that an equivalent or similar pair is generated, such as X ! :Y and Y ! :X . Obviously, if both rules appear in the re- sult, based on the equivalence theory, only one of them is enough to represent the information carried by both. Thus, one is pruned. Another redundancy exists when items from a LOS [i1; i2; : : : ; im] constitute m ? k negative rules for which k rules are coupled with positive ones, and all are sib- ling rules. The pruning will either keep all positive ones or keep all negative ones that have high confidence. An exam- ple is the pruning between rule ?Female ! `BuyHat0?, and ?:Male ! `BuyHat0?. In this case, the domain of attribute gender only includes two values: Female and Male, which means :Male is equivalent to Female. Thus only one rule, either Female ! `BuyHat0 or :Male !

`BuyHat0, is saved.

4. Algorithm  The discovery procedure can be decomposed into three stages: (1) find a set of positive rules; (2) generate negative  Proceedings of the Seventh International Symposium on Computers and Communications (ISCC?02)    rules based on existing positive rules and domain knowl- edge; (3) prune the redundant rules. The algorithm is shown in List 1. In generating negative rules, both RHS negative rules and LHS negative rules are developed. However, in identifying candidate negative rules, salience measures are computed for each type of negative rules based on different equations, namely equation 2 and equation 4.

List 1. Mining negative association rules 1) // Finding all positive rules 2) FreqSet1 = ffrequent 1-itemsetsg; 3) k = 2; 4) while (FreqSetk?1 6= ;) 5) for all transactions g 2 DataSet 6) CandidateSett = subset (CandidateSetk, g); 7) for all candidates c 2 CandidateSett 8) c.count = c.count + 1; 9) endfor 10) endfor 11) FreqSetk = fc 2 CandidateSettj c.count ? minsupg; 12) k=k+1; 13) endwhile 14) // Generate Positive Rules with Apriori 15) postiveRule = genRule(FreqSetk); 16) Rule = postiveRule; 17) // Generate Negative Rules 18) Delete all items t from the taxonomy, t 62 FreqSet1 19) for all rules r 2 postiveRule 20) tmpRuleSets = genNegCand(r); 21) for all rules tr 2 tmpRuleSets 22) if SM(tr.conf, t.conf) > confDeviate) 23) Rule = fRule, Neg(tr) j Neg(tr).supp > minsup,  Neg(tr).conf> minconf g; 24) endif 25) endfor 26) endfor 27) // Pruning 28) if all members of LOS have common itemset that form  fr1; r2; : : : ; rng ? Rule 29) delete rk, where rk falls in the categories (see 2.2.4) 30) endif  5. Results  Our experiments are performed on a database of TV ca- ble transactions. A few sample records are listed in Table 2.

Different support-confidence thresholds were tested. Table 3(a) gives the LOS for discovering negative rules. A few positive and negative rules are listed in Table 3(b). They are generated under the support and confidence constraints of 18% and 70%, respectively. Note that rules in the right col- umn are negative rules discovered with respect to positive  rules in the left column. Figure 2 shows the number of pos- itive rules and negative rules vs. user-specified minimum support and minimum confidence. Thus, the algorithm can successfully generate negative rules and the number of neg- ative rules discovered is reasonable. From Figure 2, we can see that the number of negative rules tends to be related to the number of positive rules. However, it is inversely pro- portional to the minimum support threshold. The reason is less and less high-support 1-item set survives with increas- ing support threshold, which reduces the number of candi- date negative rules significantly. During the discovery of negative rules, positive rules, which are prior information, generally remain unchanged. Nevertheless, after negative rules are generated, both positive and negative rules are re- considered together to reduce redundancy. That is, positive rules and negative rules are allowed to interact and some of the rules are eliminated because they contain the same pre- dictive information as other rules. In this case, only rules that have higher confidence are kept.

Table 2. Sample data  Sex Marital TV Type TV Program Promotion M Married Subscription Sports New Customer M Married Subscription Sports Addon Package M Married Subscription ABC News Renewal M Married Subscription ABC News Renewal F Single Subscription ABC News No Promotion M Married Subscription ABC News Renewal F Single Subscription Super Station No Promotion F Single Pay Per View Movies New Customer M Married Subscription Movies No Promotion F Single Pay Per View Movies No Promotion M Married Specific Events Sports New Customer F Single Specific Events Sports No Promotion  6. Discussion and Conclusion  We described a novel strategy for mining negative rules.

The algorithm predicts useful negative rules with respect to existing positive rules by employing domain knowledge, namely, taxonomy of the data set. From the given taxon- omy, sibling rules are derived together with their estimated confidence. Rules that have a large confidence deviation are considered candidate negative rules. Negative rules are gen- erated from those candidates that satisfy support and confi- dence constraints. Furthermore, the pruning step reduces redundancy among the negative rules. Our experiments in- dicate the efficacy of this strategy. Given the number of positive rules P and the average size of the LOS L, the complexity of the algorithm is O(P ? L). Note that the  Proceedings of the Seventh International Symposium on Computers and Communications (ISCC?02)    Table 3. (a) LOS used in the example (b) pos- itive and negative rules generated  [M, F] [Single, Married, Coreside] [Subscription, Specific Events, Pay Per View] [ABC, Fox, SUper Station, TNT, Movie, Sports] [Addon Package, New Customer, No Promotion, Renew]  (a) Positive Rules Conf. Negative Rules Conf.

ABC News!MAR 72.3% : Sports!MAR 70% Movies! SUB 99% : Sports! SUB 86%  Movies!: SE 99% MAR: Married, SUB: Subscription, SE: Specific Events  (b)  Figure 2. Positive and negative rules dis- covered under different supports and confi- dences  complexity does not depend on the number of transactions since it is assumed that the supports of item sets have been counted and stored for use in this as well as other min- ing applications. However if we are considering discov- ering positive rules, which is necessary in generating neg- ative rules, the algorithm must browse all combinations of items. The complexity of discovering positive rules depends on not only the number of transactions, but also the sizes of attribute domains as well as the number of attributes. The overall complexity will be proportional to that of discov- ering positive rules. The performance is also affected by the choice of minimum support. A lower minimum sup- port produces more numerous item sets and, with the same confidence constraint more positive rules will be generated, which adds to computation expense. From figure 6 we can  see the trend in the number of negative and the number of positive rules with different minimum support. A contribu- tion of the strategy described here versus the mining of neg- ative rules described elsewhere [9, 10] is that our strategy provides a uniform measure of usefulness based on domain knowledge and takes advantage of ready-to-use statistical data produced during the generation of positive rules. The computation of negative support and confidence is grounded in statistical theory. The experimental data suggests that by using LOS to predict the occurrence of negative rules com- bining with reasonable interestingness measurement as well as a viable pruning strategy, our algorithm yields results that are more accordance with expectation.

