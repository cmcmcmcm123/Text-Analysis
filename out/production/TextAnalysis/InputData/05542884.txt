Sequential Pattern Mining with Multiple Minimum  Supports: a Tree Based Approach

Abstract?Frequent pattern mining is an important data-mining method for determining correlations among items/itemsets. Since the frequencies for various items are always varied, specifying a single minimum support cannot exactly discover interesting patterns. To solve this problem, Liu et al. propose an apriori- based method to include the concept of multiple minimum supports (MMS in short) on association rule mining. It allows user to specify MMS to reflect the different natures of items.

Since the mining of sequential pattern may face the same problem, we extend the traditional definition of sequential patterns to include the concept of MMS in this study. For efficiently discovering sequential patterns with MMS, we develop a data structure, named PLMS-tree, to store all necessary information from database. After that, a pattern growth method, named MSCP-growth, is developed to discover all sequential patterns with MMS from PLMS-tree.

Keywords: Sequential pattern, multiple minimum supports, pattern growth

I. INTRODUCTION Sequential pattern mining is one of the most important  research issues in data mining, which was first introduced by Agrawal and Srikant[2] and can be described as follows: we are given a set of data-sequences, as the input data. Each data- sequence contains an ordered list of transactions (itemsets).

Given a user-specified minimum support threshold (minsup), sequential pattern mining finds all subsequences with frequencies larger than minsup in a sequence database.

Some previous studies on association analysis [1, 5-7] indicate that using only minsup as single frequency threshold does not deliberate on the nature of items in real-life applications. That is, single minsup implicitly assumes that all items in the database have similar frequency. However, some items may appear very frequently in the database while others rarely appear. Under such circumstance, if we set the value of minsup too high, we will not find those rules involving rare items in the database. On the other hand, if we set that value too low, it will generate a huge amount of meaningless patterns.

Therefore, Liu et al. [5] first address this dilemma (also known as the rare item problem) and propose the concept of multiple minimum supports (MMS in short) to redefine the problem of association rule mining. The method allows users to specify different minsups for different items (MIS values) to reflect  their unique natures and generate different association rules, depending on which items are in the patterns.

Recently there have been some extended studies on MMS [1, 5, 7-9]. These researches show that MMS can both enhance the efficiency of the mining process and generate more interesting patterns. However, most of previous studies deal with the rare item problem on association rule mining but rarely on sequential pattern mining. In this study, we first extend the definitions of sequential pattern with consideration to MMS, which allows users to specify multiple item supports to reflect different frequencies of items. Items with low frequencies will be specified with lower minimum supports and patterns involving these items can be easily retrieved for further decision support.

The major problem on frequent pattern mining with MMS is that the downward closure property no longer holds in the mining process, which means that a super-pattern of an infrequent pattern might be a frequent pattern. To effectively reduce the search space in level-wise methods, Liu et al. [5] proposes the sorted closure property, where all items in the dataset are sorted in ascending order by their MIS values. Keep this ordering in subsequent operations and let k-patterns generated from (k-1)-pattern share the same prefix, i.e. the smallest minsup among all items in k-patterns, the complete set of frequent patterns with MMS can be easily discovered. The sorted closure property, however, is invalid in sequential pattern mining since the order of events in a data-sequence cannot be altered. Therefore, to discover complete set of sequential patterns with MMS is not straightforward. Base on the new definitions of sequential pattern with MMS, we first propose an extended version of the PLWAP-tree structure [4], named Preorder Linked Multiple Supports tree (PLMS-tree), to compress and store all necessary information from sequence databases. After that, we extend PLWAP-Mine to develop an efficient algorithm, named Multiple Supports Candidate Pattern growth (MSCP-growth), for discovering complete set of sequential patterns with MMS. We extend PLWAP-tree and PLWAP-Mine because they have been proven to be a compact data structure and a highly efficient algorithm on sequential pattern mining.

The structure of this paper is as follows. In Section 2, we first review the concept of PLWAP-tree, which is considered as the basis of our approach. Section 3 introduces the definition of sequential pattern mining with MMS. Our data structure     PLMS-tree and the mining algorithm MSCP-growth will be presented in Section 4, and we have a conclusion in Section 5.



II. RELATED WORK Preorder Linked WAP tree (PLWAP-tree) algorithm is  proposed by Ezeife et al. [4]. Extended from WAP-tree [3], PLWAP-tree enhances its performance by avoiding reconstruction the intermediate tree. The PLWAP-tree stores the sequence databases into a preorder linked tree, and each node in the PLWAP-tree has a unique binary code to indicate its position. The position code makes this tree be able to directly recognize the ancestors and descendants of a node by comparing the assigned position codes between nodes and avoid re-constructing the intermediate trees during the mining process. In order to makes it easier to explain how to implement the position codes, the PLWAP-tree uses an equivalent binary tree to represent the original tree. Given a node n, n?s first child node in the original tree is n?s left child node in equivalent binary tree and n?s first sibling node in the original tree is n?s right child node in equivalent tree. Fig. 1 is an example on describing how to represent WAP-tree as an equivalent binary tree and how to assign the position code on each node.

The rule of assigning the position code is as follows, given an equivalent binary tree, and nodes in the tree including a node n1, a left child of n1 named n2 and a right child node of n1 named n3. Given n1?s position code is ?a?. n2?s position code will be assigned with an additional 1 in the bottom of ?a? and becomes ?a1?. n3?s position code will be assigned with an additional 1 in the bottom of ?a? and becomes ?a0?. For example, the leftmost child of root node in tree is B, node B will be the root node of equivalent binary tree. The leftmost child node of B is D, and the first sibling node of B is C, so the equivalent binary tree is represented as Fig. 1 and the position codes of B, D and C are ?1?, ?11? and ?10?, respectively. The other nodes are assigned in the same way.

Figure 1. Identify the decendants by position code  Fig. 1 also explains how to use position codes on identifing relationships between ancestors and descendants of a given node. Given a node n with positon code ?a?, suppose we assighs an additional 1 in the bottom of ?a? and becomes ?a1?, all nodes with the same prefix ?a1? are descendants of n. For example, the position code of node B is ?1?. We assigns an additional 1 in the end of B?s postion code, i.e. ?11?, and each node with prefix ?11? is the descendant of node B.



III. PROBLEM DEFINITION In this section, we formally give definitions used in the  discovery of sequential pattern with MMS. Let I denote the set of items in the database, and a subset of I is called an itemset.

A customer?s data-sequence is an ordered list of items with time stamps. Therefore, a sequence, say ?, can be represented as <(a1: t1), (a2: t2), (a3: t3), ?, (an: tn)>, where aj is an item, and tj stands for the time when aj occurs, 1 j n, and tj-1 tj for 2 j n. If several items occur at the same time in the sequence, they are ordered alphabetically.

Definition 1. Given an itemset Iq = (i1, i2...im), we say itemset Iq occurs in ? if integers 1 k1 < k2 <?< km n exist such that, i1 = ak1, i2 = ak2, ..., im = akm and tk1= tk2=...= tkm. We refer to k1 and tk1 jointly as the position and the time that Iq occurs in ?, respectively.

Definition 2. Let ? = <I1, I2...Is> (Iq ? I for 1 q  s) be a sequence of itemset. Assume that each Iq in ? occurs in ?.

Then we say sequence ? occurs in ?, or is a subsequence of ? if tI1 < tI2<...< tIs, where tIq (1 q s) is the time, at which Iq occurs in ?.

Definition 3. A sequence database S is formed by a set of records < sid, s >, where sid is the identifier of this data- sequence and s is a data-sequence. For a given sequence ?, the support count of sequence ? in S are defined as follows:  supp(?) = |{(sid, s)| (sid, s) ? S ? ? is a subsequence in s}|  The following definitions are related to the concept of MMS. In this model, the definition of the minimum support is changed. Each item in the database can have its minsup, which is expressed in terms of minimum item support (MIS). In other words, users can specify different MIS values for different items.

Definition 4. Let MIS(i) denote the MIS value of item i (i ? I). Given an itemset Iq = (i1,i2,...,im), the MIS value of itemset Iq = (i1,i2,...,im) (1 k m), denoted as MIS(Iq), is equal to:  min[ MIS(i1), MIS(i2),?, MIS(im)]  Definition 5. Given a sequence ? = <I1.I2...Is> (Iq ? I for 1 q s), the minimum support threshold of ?, denoted as  MIS(?), is equal to:  min[ MIS(I1), MIS(I2),?, MIS(Is)]  Definition 6. Given a sequence database S and a sequence ?, we call ? is frequent in S or ? is a sequential pattern in S if supp(?)  MIS(?).

By providing different minimum item support thresholds for different items, the user can effectively express different support requirements for different data-sequences. MMS allow us to have higher minimum supports for sequences involving highly frequent items, and also allow us to have lower minimum supports for sequences involving rare items. In summary, the problem we addressed can be stated as follows.

Given a sequence database S and a set of MIS values for all items in S, we discover all sequential patterns that satisfy MIS(?).



IV. THE PROPOSED ALGORITHM In this section, we first proposed a PLWAP-tree-like  algorithm, called PLMS-tree, to store all necessary information from sequence database. After that, we than propose a new pattern growth algorithm, named MSCP-growth, to generate the complete set of sequential patterns with MMS based on the PLMS-tree.

A. PLMS-tree construction The PLMS-tree is an extension of PLWAP-tree. But there  are still some differences between these two tree structures.

The first is that PLWAP-tree contains only items with support no less than minsup while PLMS-tree retain items with support no less than MIS(I), i.e. the smallest MIS value among all items.

It is because items with support no less than MIS(I) have possibilities to be part of sequential patterns. The second is that in PLMS-tree we need to add some additional information into the tree structure for discovering complete set of sequential patterns. This part will be discussed later.

Similar to PLWAP-tree, PLMS-tree is composed of a header table and tree nodes. The header table stored related information of items in the sequence database, where iname records item?s name, mis records the MIS value of this item and nlink records the link to the first node in the tree node shares the same item-name with this item in prefix order. Items in the header table are sorted in increasing order based on its MIS value.

The tree node stores the information including iname, iflag, icount, mps, pcode and the related node links (plink, clink, slink and nlink). The iflag and mps are two new fields. The iflag is used to identify the last item of a transaction. Since we consider multiple items in an event, the mining process has to distinguish whether the nodes with item-name=i and their suffix are in the same itemset. The mps is the minimum possible support, which is equal to the minimum MIS value among this node and all nodes in its suffix tree. This value is used to be part of threshold in our candidate pattern growth algorithm to reduce the number of candidate patterns. The iname records which item this node represents. The icount records the number of sequences represented by the portion of the path reaching this node. The pcode is position code used to identify the ancestor and descendent of this node. The related node links, including plink, clink, slink and nlink, are linked to its parent node, first child node, first sibling node and the next node sharing the same item name, respectively.

We use an simple tree with three sequences, (ab)(a), (abc) and (ac), to explain our tree in Fig. 2 first. Node (a:3) is not the last item of an itemset, it?s iflag is false. Node (_c:1) is the last item of an itemset, it?s iflag is true. The mps value of node (a:3) is 30% because the minimum MIS value in it?s suffix tree is 30%. The represent tree is used to describe the sequence database and the data structure tree shows the related node links between nodes, in the following content, we will use the represent to describe our example.

Based on the definitions of PLMS-tree, we proposed a PLMS-tree construction algorithm. As shown in Fig. 3, and we  will use an example to explain the PLMS-tree constructing steps as follows.

Figure 2. Example tree  Input : sequence database S and the MIS vale of each item in I Output : PLMS-tree T, header table HT Method : tree_construction(S, MIS value of each item i in I) 1. Initial PLMS-tree T // create root 2. Initial header table HT based on MIS value for each item i in I 3. For each sequence s in S 4. Call insert_tree(s, T) //insert s into the tree 5. End loop 6. For each node n in T 7. If with supp(n.iname) < MIS(I) then 8.         Call prune(i , T) //prune items that cant reach the threshold 9. End if 10. End loop 11. For each node n in T 12.      Call merge(T) //merge nodes and give position code to each node 13.      If n is a leaf node of T 14.          Call thres_set(T) //set the threshold to each node 15.      End if 16. End loop 17. Call bulidLinkage(root of T) //build linkage with the same iname  Figure 3. Algorithm 2 ? Tree construction  In the following example, node n1 (iname1:icount1:pcode1) denotes n1?s item name, item count and position code, respectively. The path from n1 to its descendent nr is denoted as (iname1:icount1:pcode1) ? ?? (inamer:icountr:pcoder). Given a sequence database and assume the MIS value of each item is defined as follows: a:70%, b: 80%, c:40%, d:65%, e:30%, f:40%. Based on the MIS values above, we can sort each sequence shown in table I.

TABLE I. SORTED SEQUENCE DATABASE  SID Unsorted sequence Sorted sequence 10 <(ac)(ab)(bc)> <(ca)(ab)(cb)> 20 <(ac)(ab)(c)> <(ca)(ab)(c)> 30 <(a)(e)(c)(a)(b)> <(a)(e)(c)(a)(b)> 40 <(ac)(f)(ab)(d)> <(ca)(f)(ab)(d)> 50 <(a)(e)(c)(d)(b)> <(a)(e)(c)(d)(b)>  At the first step, we create a root node of tree T labeled as ?NULL?. A header table HT records all items? names and their MIS values. The first scan of database S inserts the sequences s into T by turns. We use the first two sequences to explain the insertion process as follows.

1. The first sequence s10 = <(ca)(ab)(cb)>. Since the child node of root is NULL, we create new nodes as Fig. 4(a). Noted that each sequences in the S is a combination of itemsets, we mark the last item i of each itemset as _i to distinguish among     itemsets, and the items in the same itemset are sorted by their MIS value in increasing order.

2. For the second sequence s20, it shares the same prefix (ca)(ab)(c) with the s10. But our study wants to discuss about the situation of itemset, the last item ?(c)? in s20 is the last item of one itemset, and the same item in s10 is not, the last item ?(c)? in s20 will be considered as a different item with item ?(c)? in s10. The icount in branch (c:1) ? (_a:1) ? (a:1) ? (_b:1) are increased by 1 because they shares the same prefix, but the next item of s20, i.e. (c), is not the same node with s10.

Item (c) in s20 cannot share node with the item c in s10, it will be a new child node (_c:1) of (_b:2) as Fig. 4(b).

The remaining sequences will be inserted in the same way, the insert algorithm is shown at Fig. 5 and the result after step 1 is shown at Fig. 6(a).

Figure 4(a)    Figure 4 (b)  Figure 4. Tree construction steps  Input : sequence s , PLMS-tree T Method : Insert_tree(s , T) 1. point to root 2. For each itemset t in s 3. Sort all items in t based on their MIS values in increasing order 4. If item i is the last item of t then 5.         lastitem = TRUE //distinguish between itemset 6. End if 7. For each item i in t 8.         If there exists a child node e which e.iname = i.iname and the  same lastitem value then 9.             e.icount ++ 10.             Point to e 11.         Else 12.             Create a new node e with e.iname = i.iname 13.             Set the related information about e 14.             Point to e 15.         End if 16. End loop 17. If i appears in s the first time 18.         supp(i) ++  //count item i?s support 19. End if 20. End loop  Figure 5. Method: insert tree ? insert S into tree  At the second step, since all items in S are inserted into T, it possibly has items with support less than MIS(I), i.e. these items are impossible to be part of sequential patterns. Follow the example above, the supports of each item is as follows: a:100%, b:80%, c:100%, d:40%, e:40%, f:20%. Since the MIS(I) is 30%, all nodes with iname=f will be removed because supp(f) is less than MIS(I). The pruning step is shown in Fig. 6. In this case, node (_f:1) is going to be removed and  the node links related to (_f:1) will be modified to keep the data structure right at the same time. Several links related to this node needed to be modified, including node (a:2)?s slink is linked to (_f:1) and node (a:1)?s plink is linked to (_f:1), Node (a:2)?s slink will be linked to (_f:1)?s child node (a:1) and node (a:1)?s parent link will linked to (_f:1)?s parent (_a:3). The complete pruning algorithm is shown in Fig. 7.

Figure 6(a).                               Figure 6(b)  Figure 6. PLMS-tree ? Step 2 ? pruning  Figure 7. Method: prune ? remove infrequent items  Since we prune some nodes from the tree, there might exist nodes sharing the same item name with their sibling node. The third step merges nodes with their sibling node if they share the same item name and assign position code to the nodes which do not need to be merged. Follow the example in Fig. 6, we find that two child nodes of (_a:3), i.e. (a:2) and (a:1), share the same iname=a. Moreover, both of them are not the last item of an itemset, so they will be merged together and the merging step is shown in Fig 8(b). At the same time, we modify the related node link to keep the data structure right. The node (_b:1)?s plink will be changed to the merged node (a:3) and node (_b:2)?s slink is changed to (_b:1) because they have the same parent node. The following two nodes (_b:2) and (_b:1) will do so in the same way as Fig. 8(c).

Figure 8(a)                    Figure 8 (b)             Figure 8 (c)  Figure 8. PLMS-tree ? Step 3 ? merging steps  The position code is assigned at the same time when we merge nodes by traversing T from root, if a given node n does  Input : node n with supp(n.iname) < MIS(I) Method : prune(e) 1. Modify the link linked to e?s child node and sibling node 2. Remove e     not need to be merged, it will be assigned with a position code.

Since the leftmost node of root in T is (c:3), we use (c:3) to be the root of equivalent binary tree, which means it is assigned with position code 1. As we motioned in section 2, its leftmost child (_a:3) is assigned with an additional 1 in the bottom and becomes 11, the first sibling node (_c:1) is assigned with an addition 0 in the bottom and becomes 10. The other nodes in T are assigned with unique position code in the same way. The result of PLMS-tree with position code is shown in Fig. 12, too and the merging and assigning algorithm shows in Fig. 9.

Input : Tree T Method : merge(T) 1. Form the root of T, trace each node e in T 2. If e?s slink != NULL then 3. For each sibling nodes of e 4.         If this node is appeared the first time then 5.          assign position code 6.         Else 7.             merge to the node with same node name 8.         End if 9. End loop 10. Else 11. assign position code 12. End if  Figure 9. Method: merge ? merge tree and assign position code  The fourth step is to set the mps value of each node e in T.

This value is used to record the minimal possible support in this node?s suffix tree and it will play an important rule in our mining process. The mps is equal to the minimum MIS value between e and all nodes in e?s suffix trees. We trace form the leaf nodes in T to assign their mps value. In this case, the mps of node (_b:1:1111111) is 80% and its parent node (c:1:11111) is 40%, so the mps value of (c:1:11111) does not to be modified, but (c:1:11111)?s parent node, (_b:3:1111)?s mps value is 80%, so the mps value of (_b:3:1111) needs to be modified to 40% which is equal to it?s child?s mps value. The other nodes in T are assigned with mps vales in the same way.

The result of PLMS-tree with mps value is shown in Fig. 12 and the mps setting algorithm is shown in Fig. 10.

Input : leaf node of T Method : thres_reset(T) 1. If e.plink.mps > e.mps then 2. e.plink.mps = e.mps 3. Else 4. break 5. End if  Figure 10. Method thres_reset(T)  The final step is building nlink between header table and the nodes, so we can trace all nodes with the same iname through header table. In this example, the nlink of item e in the header table is linked to (_e:1:11111001) first, and the nlink of node (_e:1:11111001) in linked to next node with the same item name (_e:2:101) in prefix order. The other nodes are also linked together by the same way. The algorithm of building nlink shows in Fig 11 and the complete PLMS-tree with part of node link value shows in Fig. 12.

B. MSCP-growth After the PLMS-tree is constructed, we develop the MSCP-  growth algorithm to generate a complete set of sequential  patterns with MMS. The complete pseudo code of MSCP- growth is shown in Fig. 13. Due to the limit of space, we briefly explain the main procedure of MSCP-growth below.

Input : node n Method : buildLinkage (n) 1. If n != NULL then 2.         If HT[n.iname].nlink == NULL then 3.             HT[n.iname].nlink = n; 4.         Else 5.             link the last node linked by HT[n.iname].nlink?s nlink to n 6.         End if 7.         buildLinkage(n.clink); 8.         buildLinkage(n.slink); 9. End if  Figure 11. Method: thres_set ? set mps to each node  Figure 12. Complete PLMS-tree with part of node link  Input : Root set RS, root Output : the complete set of sequential patterns Method : MSCP-growth(RS, ?, MIS(?)) 1. If RS is empty then return 2. For each item i with supp(i) > MIS(I) in HT 3. For each node n belongs to i in T 4.         If n is a descendent node of RS and n is not recounted then 5.             icount += n.icount 6.             Join n?s child into new root set nRS 7.           If n.mps < mps then 8.             mps = n.mps 9.             End if 10.         End if 11. End loop 12. If icount > MIS(?) || icount > mps then 13.         Append i to ? as ?? 14.         If icount > MIS(??) then 15.             Join ?? to sequential patterns set 16.     End if 17.        MSCP-growth(nRS, ??, MIS(??)) 18. End if 19. End loop  Figure 13. Algorithm 3: MSCP-growth ? generate sequential patterns set  In line 2 through line 11, firstly, the algorithm counts each item i?s support in ??s conditional PLMS-tree (the suffix tree of root set RS). In PLMS-tree, the nodes with the same iname in the same path cannot be counted more than once since each item in the same sequence can be counted once only. Here, the binary code can be used to quickly identify the relationships between any two nodes in PLMS-tree. While cumulating icount for node n, in line 4 through line 6, the algorithm records n?s child node in a new root set nRS. The nRS will be used to     locate (??)?s conditional PLMS-tree in the mining of sequence (??)?s super-patterns. In line 7 through 9, we examine the mps of each node n and record their minimum mps. This value will be used in next step.

In line 12 through line 18, secondly, we generate candidate patterns and output sequential patterns according to each item?s icount. Three conditions may occur here. The first is that the icount satisfies MIS(??). This sequence is directly appended to the sequential patterns set. The second is that the icount doesn?t satisfy MIS(??) but satisfies mps. That is, the sequence is currently not a sequential pattern, but its super-pattern (i.e.

appending suffix in the sequence) has possibility to be.

Therefore, both two conditions have to keep growing and recursively call procedure MSCP-growth for further examining their super-patterns. The third is that the count of sequence doesn?t satisfy both MIS(??) and mps. It means that any super- pattern of the sequence ?? will never be a sequential pattern.

The procedure will terminate while the root set becomes empty.

We use the constructed PLMS-tree in Fig.12 to explain the mining process in detail. At first, the RS is root and we trace all nodes belong to the first item e in HT. According to the nlink of item e from HT, we find (_e:1:11111001) n1 at first, the position code shows that n1 is a descent node of RS, and n1 is not recounted in the same branch. n1.icount will be add into icount, since n1 does not have any child node, no node will be joined into nRS. According to the nlink in (_e:1:11111001), the next node (_e:2:101) n2 will be found, n2 is a descent node of root and n2 is not recounted, n2.icount will be added into icount.

n2 also has a child node (_c:2:1011), that can be joined into the nRS to be a new root set if we need to growth this pattern. And the next node (_e:1:10111011) is recounted so the node count of this node will not be added into icount. Since the icount of item e is 60% which reaches the candidate growth condition we mentioned before, we use the nRS and e as a candidate pattern to generate new patterns. Pattern e also reaches the threshold of sequential pattern, i.e. MIS(e), and can be joined into the sequential patterns set. The second step uses the node in the nRS, (_c:2:1011), as the new root set of conditional tree to growth new pattern(s). According to HT, the icount of first item e rooted by (_c:2:1011) is 20%, which does not reach the candidate growth condition. The icount of second item c in HT rooted by (_c:2:1011) is 40%, which reaches the condition to growth new candidate patterns, so we use the child nodes of (_c:2:1011), (_a:1:10111) and (_b:1:10111), as nRS to generate new candidate patterns. This pattern also larger than MIS(ec) which means it can be joined into the sequential patterns set.

The complete sequential patterns set with beginning item e can be generated by repeating these steps.

The reason we use the mps value and candidate growth will show in following example. We trace the item b now, the icount of item b is 100% which reaches the condition to generate new candidate and the threshold to be joined into the sequential patterns set. As the tracing step goes, we have the icount of item d=60% in the suffix tree of pattern b, it reaches the mps value=30%, which is the second condition to candidate growth we mentioned before, and it means that this pattern still has possibility to be part of sequential pattern. Noted that it doesn?t reach MIS(bd)=65%, which means it is not a sequential pattern. In the next candidate growth step, we have item e with  icount=40% which reaches the MIS(bde) = 30%, pattern(bde) can be joined into the sequential patterns set. We can see that an infrequent pattern?s super set become frequent pattern in this case, that is why we need to consider about the mps value. The other sequential patterns which are started from other items are generated in the same way. Finally, we can have the complete sequential patterns with MMS by MSCP-growth algorithm.



V. CONCLUTION In this study, we first address the rare item problem in the  mining of sequential patterns. To generate more interesting patterns, we extend the concept of MMS on sequential pattern mining and propose an algorithm, named MSCP growth, to discover complete set of sequential pattern with MMS. In experimental study, we will compare three well-known traditional methods, including GSP, PrefixSpan and PLWAP- tree, with MSCP-growth using several synthetic datasets as well as real-life datasets. Some research directions are briefly discussed. First, users may have to tune items? supports many times while discovering useful patterns. Instead of restarting the whole mining process, it is valuable if a support tuning mechanism can be developed. Second, the concept of MMS can be further applied to constraint-based sequential pattern mining, such as time constraints or duration constraints.

