A New Improvement of Apriori Algorithm for Mining Association Rules

Abstract-among the many mining algorithms of association rules, Apriori Algorithm is a classical algorithm that has caused the most discussion; it can effectively carry out the mining association rules. However, based on Apriori Algorithm, most of the traditional algorithms existed "item sets generation bottleneck" problem, and are very time? consuming. An enhance algorithm associating which is based on the user interest and the importance of itemsets is put forward by the paper, incorporate item that user is interested in into the itemsets as a seed item, then scan the database, incorporate all other items which are in the same transaction into item sets, Construct user interest itemsets, reduce unnecessary itemsets; through the design of the support  functions algorithm not only considered the frequency of itemsets, but also consider different importance between different itemsets. The new algorithm reduces the storage space, improves the efficiency and accuracy of the algorithm.

Keywords- Association Rules; Apriori Algorithm;Importance of Frequent Itemsets; Improved Algorithm

I. INTRODUCTION  Association rules were presented by R. Agrawal and others in 1993, it is an important research issue in data mining. It is the most typical application is to discovery new useful information in the sales transaction database. For example, 85% of customers buy goods A and B, while C and o to buy goods, to find all the similar rules for determining sales strategy is useful. As the database is usually very large, therefore it requires an efficient algorithm to mining association rules. Apriori algorithm is the most classical algorithm for mining association rules, its basic idea is to use  the known (k -1) dimensional k frequent itemsets to generate the frequent itemsets, that is, using the known (k -1) dimensional k frequent itemsets to generate candidate itemsets, and then scan a database to determine whether candidate itemsets frequent itemsets. In this process, there are insufficient:  If the generation before the candidate itemsets can be judged by some of the non-candidate itemsets frequent itemsets, Then, it can not generate these can determine in advance the non-frequent item sets, this can prevent the connection of these candidate itemsets generation time overhead, and avoid the time when the cost of scanning the database.

In the linker to repeat the same item more too, if we can avoid duplication of comparison, can improve the efficiency of the algorithm.

Gao Y ongping East China Institute of Technology  Nanchang, China e-mail: ypgao@ecit.edu.cn  Each candidate itemsets generation later, they have to go back to scan the database to determine whether these candidate item sets are frequent itemsets, In the process of scanning the database, and some can be judged not go scanning the items or services was repeatedly scanned, If you can avoid these unnecessary scans, you can improve the efficiency of Apriori algorithm.



II. MINING ASSOCIA nON RULES  Set 1 =  {i1' iz" . " im} is a collection of items, set D = {I;, Tz, .. " z;,} is a collection of database transactions related to task, where each transaction is a collection of items made 1; ? I.

Set A is an itemsets, itemsets containing k items is called k - item sets. Transaction 1; contains A if and only if A ? Ii' The support degree of item sets A in transaction sets D that is to say, sup (A) is the percentage of the transaction with Au B in D, that is, sup( A) = P( A u B) .

If the sup(A) ? minsup , where minsup is the given minimum support is said to A for the frequent item sets.

Association rules is a containing type like A => B , in which A c I and A (\ B = ? . The confidence conf(A => B) of Association rules A=> B in the  transaction sets D is the percentage of the transaction, which includes A and B in D at the same time, that is conf(A => B) = PCB I A).

Therefore, the problem of mining association rules can be converted into two sub-issues:  ? To identifY all the frequent item sets, that are set to  meet the pre-defined minimum support ( minsup ).

? The frequent item sets generated by the strong  association rules, these rules must also meet the minimum support and minimum confidence. Issues (2) implementation is simple and easy. The performance of association rule mining algorithm focused on issues (1), in which the most algorithms are focused on how to efficiently discover frequent item sets.

At present, there have been a number of association rules mining algorithm, which first proposed by R. Agrawal, etc.

The Apriori Algorithm is a more efficient algorithm for mining frequent item sets. The basic idea of the algorithm is to use a hierarchical order to complete the search of the      iterative methods of mining frequent item sets, namely the use of k item sets to generate (k + 1) item sets using candidate set Ck to find frequent item sets Lk . First of all, to  find a collection of frequent 1- item sets, denoted L1, LI is  used to find a collection of frequent 2- item sets of L2, and  L2 is used to find L3, so go on until the frequent k - item sets can not be found. To find each Lk requires one database  scan. Once from the database D, the frequent item sets are found, the strong association rules can be directly produced from the minimum degree of confidence. The defect about the algorithm: It may need to generate a large number of candidate sets, Apriori may need to repeatedly scan the database, through the pattern matching checks a large collection of candidates.



III. ApRIORI ALGORITHM  A basic algorithm Apriori, designed by Agrawal and others in 1993, generated all frequent item sets, Apriori uses the recursive method, the core algorithm is:  LI =find_frequent_I-itemsets( D);  for (k=2;LH :;t?;k++){ Ck = apriori _gen(Lk-l,minsup). ,  for each transaction tED {//scan L = Uk Lk for counts  C1 = subset( Ck, t)  ;//get the subsets of t that are candidates  for each candidate C E C1 c.count+ +;  Lk = {CE Ck I c.count ? minsup} }  returnL = ukLk; Iiall of Lk; First, scan database once, resulting in frequent I item sets  of LI ; and then loop, in the first k cycles, the first the frequent k -1 item sets through self-connection and pruning, to generate the candidate frequent k item sets Ck, and then use the Hash function to store Ck in a tree, scanning the database for each transaction T to use the same Hash functions to calculate the candidate frequent k item sets the transaction T contains, and make the support number of the candidate frequent k item sets plus 1, If the candidate frequent k item sets support number is greater than or equal to the number of minimum support, then the candidate frequent k item sets are frequent k item sets; the loop ends until the candidate frequent k item sets are not generated any longer.

1) Apriori-gen Function Lk -1 is the parameter of function Apriori-gen, which is  the production function of candidate set. The function returns the superset of set which include all large k-item sets.

? when connecting to the database, connect Lk - 1 and Lk -1 to obtain a superset Ck which is the final set  of candidate items: Insert into Ck  select p[I],p[2], ... ,p[k -1],q[k -1] from Lk - lp,Lk - ,q where  p[l] = q[I], ... ,p[k -2] = q[k -2],p[k -1] < q[k -1] ? It will delete all the item sets C E Ck , when pruning  the candidate item set, if some (k -1) subset of C are not in Lk -1 , for any items which have minimum  support in Lk - 1 , each k -1 subset must have minimum support, in this way, it is easy to maintain the integrity of the process of generating candidate  items. So, expand each item sets in Lk -1 using all  possible item, then delete all (k -1) subset which is not in Lk -I, can obtain a superset of item sets in Lk . Merger operation is equivalent to using all items  of the database to expand Lk -I , if the k -1 item set which is obtained after removed the first k -1 items in expansion item set is not in Lk -1 , then delete this  expansions item set. p[ k -1] < q[ k -1] Condition ensures the expansion of entries will not be the same.

After merger operation, Lk ? Ck ; similarly, in the deletion operation, remove item set in Ck ,if its k -1 subset is not in Lk -1 , also haven't deleted item sets which are included in.

For example: set L3 as {{I, 2, 3}, {I, 2, 4}, {l, 3, 4}, {2,  3, 4} }. After connected to the database, C 4 is {{ 1, 2, 3, 4}, {I, 3, 4, 5}}. It will delete item set {I, 2, 4, 5} (item set {I, 4, 5} is not in L3 ) when pruning, then there only {I, 2, 3, 4} remains in C 4 ?  At the time of pruning, need to check a new (k -1) subset whether in Lk -1 ? In order to make the test of membership faster, large items are stored in a Hash table.

2) Subset/unction Candidate item set Ck is stored in a Hash tree, one node  of Hash tree includes a linked list of item set (one leaf node) or a Hash table (one internal node), on internal node, each Bucket of Hash table point to another node. The depth of a Hash tree root is defined as I, the node in depth of d points to the node which is in depth of d + 1 , item set is stored in leaf. When leading an item set C , start from the root to a leaf.

On a node which is in depth of d, need to determine which branch to choose, could use a Hash function on the d1h item of this item set, then follow the corresponding pointer in the Bucket; all nodes initially created as leaf nodes, When the number of items in a leaf node is more than a defined value, this leaf node is converted to an internal node.

V2-530     Begin with the root node, Subset function search all candidate items which is included in a transaction t , the procedure includes:  If at root node, search this leaf, find out these items which are included in t, and add references that point to the answer set to them. Else if at a internal node, and arrived to this node through the Hash item i , then do Hash on each item which is behind i in t ,  and recursively apply this process to the corresponding node in Buckset. For root node, do Hash on each item which is in t .

The disadvantages of Apriori Algorithm: ? The number of the candidate frequent k item sets  generated from frequent k - 1 item sets through self-connection is huge.

? To verify the candidate frequent k item sets is very time-consuming when scanning the entire database.



IV. IMPROVEMENT OF ApRIORI ALGORITHM  A. The generation of interesting itemsets The basic principle of resulting in a item set is: first,  items of interest to the user entry into the item set as a seed set, and then scan the database for the first time, all other items that appears with the seed item in the same transaction are incorporated into the item set, when the 1 st scan is complete, the project focused on all the items in tum as a seed key to the 2nd scan database, using the same approach to new items entry into item set, so the cycle continues until there are no new items can be incorporated into the item set.

Specific to algorithm design, can use a count structure to record the state of the items which is accessed in database, record numbers of all items in database constitute this kind of count structure. All record numbers of seed items are initialized to 1, rather than to O. In each database scan, if the retrieved record to a item record number in a transaction is not 0, then the record numbers of all items in transaction add 1, while the state of access to this transaction is marked as visited, in order to prevent for repeat visits at the next time that it scans the database. Check whether the count structure has been updated after one time of database scanning. If so, then continue to scan the database; If not, stop scanning, all the recorded value is not 0 items constitute a set of user interest item set.

B. Design of support functions In the process of generating frequent itemsets, need to  calculate the degree of support for each item set, and then compare them with the support threshold value given by the user, it is frequent item sets if it greater than the threshold of support threshold value. In most existing algorithms, the calculation of support only considered the frequency of itemsets appearances, without taking into account the different item sets have different importance. For example, shopping malls sell 10,000 towels one month, but only 300 TV sets, in the traditional association rule mining algorithm, the TV is likely to be ignored. However, from the view of the total sales amount, TV is far more than a towel, derived from the traditional association rules algorithm is obviously wrong.

Therefore, the same consideration of a towel and a TV is obviously unreasonable, need to take into account the frequency of item sets and importance of item sets when calculate the support degree of item set, in this way, association rules will be more scientific and reliability.

If only considering the frequency of item sets, then calculating item set support functions can be defined as: f(X) = numTids(tids(X))/ numTids (tids (rfJ)) (1)  C. Set the weight of the elements in the item set In order to consider the importance of item sets in  calculating support, need introduce a function which compute support degree after quantify the importance of item set, weight is a key quantitative indicators of measuring the importance of item set.

Because the weight of items is a quantitative indicators that is used to measure the importance of item, when establish a database, can set a individual column to store weight, also could make use of a existing column to measure the weight. For example, in a database that records the sales records of shopping mall, each item will have a price, suppose that the Px means the price of goods I , Pmax means the price of the most expensive goods, then the weight  of goods X can be calculated using the following formula: Wx = Px / Pmax (2)  This calculation may be not very appropriate, but it does reflect to some extent the importance of each item. For this algorithm, the most important consideration is that existing algorithms often ignore these items which have great importance but frequency less than other items. Therefore, such an idea can be used to assign weight to the item in the item set: increase these support degree of items that have great importance but less frequency. In order to do this, the following formula can be used to assign weight to the item:  Wx = (1- /3) x numTids (tids (x)) I max ,ci (numTids (tids ((=))) (3) I stands for item set, fJ is a parameter in the range of  [0,1] , which control the degree of correlation between the frequency and weight of item. If fJ = 0 , then Wx = 1, that means there is no correlation between the weight and the frequency of items ,it is always constant to 1, only considered the frequency when calculate item set and the support of item set.

D. The description and process of algorithm LI = init(1);  for (k = 2; Lk > k - 1; k + +) Lk = GenltemSet(LH,minsupp);  L=UkLk;  R=GenAssociationRule (Lk' mincorif) ; In which, obtain frequent item set through GenltemSet sub  function. Pseudo code as follows:  procedure LargeItemGen (Lk_l, minsupp) {  for X and Y is in LH {  V2-53J     if first k - 2 items of X and Yare same and any subset of XuY is in LH {  tids(X u Y) = tids(X) n tids(Y) If  (X u Y) = min(Wx' Wy)! numTids(tids(X n Y?;) and  f( X U Y ? minsupp insert Xu y, tids( X n Y ) and f1: Xu Y ) into  Lk; } return Lk; } And obtain association rule from frequent item set through  GenAssociationRule sub function, Pseudo code as follows:  procedure GenAssociationRule (Lk' mincorif) { for any X ELand any nonempty subset Y of X if f(X)! feY) ? minconf  insert Y ? (X - Y) into R; return R;}  Code shown in the above process, first, generate a set of items of user interest as mining object from the database, and then scan the database one time to represent item set with transaction identification number. After generating itemsets, assign weight to element in item set, then use the introduction function of the support of the weight to calculate the item set degree of support to generate frequent item set, finally, obtain association rule from these frequent item set.



V. CONCLUSIONS  Among the many mining algorithm of association rules, Apriori algorithm is a classical algorithm that has caused the most discussion. It can effectively carry out the mining association rules. However, most of the traditional algorithms based on Apriori algorithm have the problem of "item sets generation bottleneck" problem, and are time? consuming. This paper presents several ideas to improve the algorithm, which are basically consistent with the Apriori  algorithm. What they have in common is that the support given by date scanning can't be less than the frequent item sets of the minimum support given by the user. Meanwhile the algorithm put forward in this passage has its own advantages, That is the pruning is done prior to the database scan, after the pruning it is re-connected to scan the database, thus reducing the number of scans.

In the algorithm efficiency, through data compression the data that is mined can be scanned into memory once, avoiding duplication of disk I/O operations. While the date that isn't compressed can not be read into memory one-off, thereby increasing the computational efficiency; besides, data compression can reduce the character length of each one, especially when we compare whether the two are the same, the number of characters need comparing is much less, which can improve the operating speed. In a word, data compression can save memory and reduce the time of comparing the candidate sets. So the speed of generating frequent item sets will be faster, which can solve the problems of Apriori algorithm better.

