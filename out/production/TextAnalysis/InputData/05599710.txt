Text Knowledge Representation Model based on  Human Concept Learning

Abstract?The essential abilities of text knowledge representation, such as automatic construction, carrying abundant semantics and flexible reasoning, should be held due to the rapid growth of web resources and the requirements of the reasoning-based web services. However, current text knowledge representation models either lose many textual semantics or cannot be constructed automatically. To solve the above issues, text knowledge representation model is proposed based on the concept algebra of human concept learning. Then, the degree-2 power series hypothesis is developed and the reasoning ability of text representation is proposed. Finally, the results compared with current knowledge representation models show that our model performs better than other models in representing text knowledge.

Keywords-text representation; power series; human concept learning; machine understanding

I.  INTRODUCTION The essential abilities of text knowledge representation,  such as automatic construction, carrying abundant semantics and flexible reasoning, should be held due to the rapid growth of web resources (e.g. web pages) and the requirements of the reasoning-based web services.

Generally speaking, there are four main types of knowledge representation models. 1) Statistics model, such as vector space model (VSM), latent semantic analysis (LSA). 2) Cognition based model, such as element fuzzy cognitive map (EFCM), concept algebra based model [1] [2]. 3) Probability topic model, such as author topic mode (ATM) [3], author-recipient-topic model (ART) [4], correlated topic models (CTM) [5]. 4) Ontology based models, such as ontology inference layer (OIL) [6], ontology web language (OWL) [7].

From the analysis of the above models, we know that those models either are complicated computationally (e.g. CTM) or cannot be constructed automatically (e.g. OWL), even lose many textual semantics (e.g. VSM) and lack the ability of flexible reasoning. Herein, a text can be regarded as a concept and sentences contained in text can be regarded as objects belonging to the text, where the keywords are the attributes of these objects [8]. Based on concept algebra [9], power series representation of text knowledge is developed in this paper.

The proposed model not only can carry more abundant  semantics from text (e.g. web page) but also can be represented automatically, which adapts to the rapid growth of web resources and holds more abundant semantics than VSM and EFCM.

This paper is organized as follows. The idea of concept algebra of human concept learning will be introduced in section II. In section III, we propose our text representation model base on the concept algebra of human concept learning.

Then, the reasoning rules of text knowledge have been discussed in section IV. Finally, comparison results with other models and conclusions are given in section V and VI, respectively.



II. CONCEPT ALGEBRA OF HUMAN CONCEPT LEARNING  A. Concept Algebra Feldman pointed out that any set of data exhibits many  patterns, some of which may be specified with only a small number of features, while others may be described by a larger number of features and other patterns involve an intermediate number of features to specify [9]. The key issue is how the arbitrary patterns of data can be described by the combinations of component patterns. In order to solve this issue, in the following parts, we will introduce the related work by an example (Fig.1) cited from [9].

The "world" W consisting of the five amoeba-like objects corresponds to a concept of human concept learning. We choose some "language" in which to express the structure of this "world" W, which we think of as simply as a list # ? ba1,a2,e,adc of abstract property tags, called the property   Figure 1. A "world" W containing amoeba-like objects cited from [9]. This work is supported by the Shanghai Municipal Science and Technology  Commission (project no.09JC1406200), the National Science Foundation of China (project no. 90612010), and the Shanghai Leading Academic Discipline Project (J50103).

4?4  Proc. 9th  IEEE  Int. Conf. on Cognitive  Informatics  (ICCI?10) F. Sun, Y. Wang, J. Lu, B. Zhang, W. Kinsner & L.A. Zadeh (Eds.)    language [9]. For the "world" shown in Fig.1, an appropriate language might be # ? bblob_shaped, shaded, has_nucleus, has_dotted_membrane, largec, which is abbreviated to # ? ba, b, c, d, ec under the assignment:  a=blob_shaped; b=shaded; c=has_nucleus; d=has_dotted_membrane; e=large,  and the "world" W encoded into  # is denoted by W|#.

In [9], Feldman figured out an idea which is to pick out a certain atomic set of rules as the building blocks of "concepts" or "patterns". These building blocks, together with the rules for combining them, constitute the description language "concept algebra". Then, there are two basic and important definitions described in the following parts.

Definition 2.1: Implication  One very obvious kind of rule - implicit in the notion of "causality" - is when one property's value determines another's.

This distal causality will be reflected proximally in a pattern among observed variables, namely a logical implication:  a1 a2,  where a1 a2 means that a1 is never true unless a2 is also.

Definition 2.2: Affirmation  An even simpler type of consistent world structure is when one property holds consistently in all observed objects, that is a rule with the very simple form:  a.

Hence, the conceptual structure built up so far reduces to the following algebraic form. Write ? for the set of constant properties; ? for the set of pairwise implications; and @?for the rest of #, the unconstrained properties [9]. Every possible world that can be produced by these types of concepts alone can be expressed as the Cartesian product of the lattice for ??with the lattice for @, conjoined with the properties ?, that is [9]  ?d???@?, and W|# ??d???@?.

Based on empirical evidence, Feldman proposed the  linearity hypothesis. Human learners are biased towards linear concepts [9]. And cognitive scientists have done many researches so far. They find that the hypothesis accords with the cognitive behaviors of human beings.

Therefore, according to the above analysis, the example in Fig.1 can be described as follows:  ? = {a}, @ = {b}, ? = {e c, c d, d c}, W|#?{a}d [{e c, c d, d c} ?{b}].

Because of the similarity between the concept and the text [8], an example about text fragments is given as follows.

S1: That boy stands on the left, whose t-shirt is red.

S2: Two girls stand on the right, whose skirts are also red.

Referring to the above basic definitions, we find a property language #text which includes all the properties from the text fragment.

#text = {boy, left, red}, and  #text = {a, b, c} under the following assignments:  a = boy, b = left, c = red.

After analyzing the text fragment, we find that the clothes of all people including one boy and two girls all are red. In addition, we can deduce that one person who is boy must be left, and one person who is left must be a boy. So the above text fragment can be described as:  ? = {c}, @ = {}, ? = {a b, b a}, W|#Text ? {c}d [{a b, b a} ?{}].

B. Extended Concept Algebra of Human Concept Learning Based on definition 2.1 and 2.2, we model a "world" or  "concept" with affirmation and implication; that is to say, the "world" or "concept" is described by linear terms. However, there are many higher-order terms in reality, so Feldman extended the concept algebra to more general hierarchy.

Definition 2.3: Extensive implication  According to definition 2.1, the "implication" is linear and means the paired causality. However, there are higher-order causal regularities among more than two attributes in reality.

So the paired causality should be extended to a general form.

The general form of an implication is  a1a2eak a0.

The number k here, called the degree of the implication, is the number of properties that are antecedent to the implication, which serves as a measure of the complexity of the implication.

Obviously, the paired implication proposed in the previous section a1 a2 is the special form of the general case.

Definition 2.4: Power series expansion  After defining the extensive implication, the "world" or "concept" set can be described more fruitfully. So, Feldman proposed the power series expansion of concept. For an object set x, the set of implication of degree k that is contained in the series expansion of x will be denoted as . And the full power series for observations x (denoted as  k x? )(xE ) looks like:  )degree minimum(:0 0x?  4??    )degree maximum(:1  :  :1    ???  ?  ?  D x  k x  x  D  K ?  ?    Then, a theorem power series representation is proposed, that is: for any object set # defined over a language of size D, there is a minimal irredundant set of implications:  (x)  |    E??  ???  ?ddd???? ?  ?  ? #  k x  D  k  D xxxXW    Contrast to the lack that the simple algebra is only able to express linear concepts; the full power series can describe any discrete featured pattern. On the web, there are a huge number of web pages, which are discrete and hold all kinds of relationships, so it is an important task to distinguish these different web pages and represent the relationships among them. After studying the feature of concepts and that of web pages, we find that the power series representation of concepts can describe discrete featured pattern of concepts and the relationships among them. Considering that there are some similarities between the concepts and the web page, we have tried to study the representation of text knowledge (e g. web pages) based on human concept learning. Therefore, the power series representation of text knowledge will be discussed in the next section.



III. TEXT KNOWLEDGE REPRESENTATION BASED ON HUMAN CONCEPT LEARNING  Simply speaking, text knowledge is composed by sequential sentences, a sentence composed by some word?s combinations. When we regard the keywords as attributes and a sentence as an object in a text, the understanding of text can be regarded as the process of concept learning [8]. In this paper, we regard the keywords as attributes of describing text knowledge, a sentence or paragraph as an object, so a text which consists of many sentences and paragraphs can be regarded as a concept. Then, we can represent the text knowledge according to the concept algebra of human concept learning.

A. Machine-oriented Text Representation  According to the related thesis of human cognitive process [10], machine-oriented text understanding should aim at gaining text knowledge or relationships among keywords.

Referring to concept algebra of human concept learning, in this paper, different knowledge from text can be regard as different sets. There is a simple description as follows.

?: {} denotes text assertion, the common sense belonging to the text;  ?f?{} denotes text association rules, which contain the relationships among keywords from text.

@f?{} denotes isolated keywords, which are not related with other keywords in text, and affect the understanding difficulty of text.

We utilize above three sets to represent text knowledge, which can be represented as  ?d?@???,  where d denotes adding ? to every element in the set (?@???); ? denotes the Cartesian product of two sets (@ and ?).

If @ is O, @?? equals to ?, which means that no isolated keywords affect the relationships ? among keywords from text.

For the text fragment in section , if we regard the text fragment as a text T and suppose that {boy, girl, red, left, right} is the set of keywords of this text fragment, the knowledge of this text fragment can be represented as follows:  ?: {red}, ?: {boy left, girl right}, @:  {}.

So the text can be represented as: T = ?d?@??? = {red}d [{} ? {boy left, girl right}].

From the above simple and qualitative analysis, we know that the representation method of concept algebra can describe knowledge from text. Therefore, the next section will develop text knowledge representation model based on concept algebra of human concept learning.

B. Power Series Representation of Text Knowledge  In order to describe the power series representation of text knowledge conveniently, some terms will be given in advance.

Term 3.1: Text assertion  Text assertions are certain keywords that stand for the common sense knowledge, which is the simplest and the most easily-understood information in certain field.

For instance, WSDL, UDDI are common sense knowledge in the field of web service.

Term 3.2: Text association rules  Text association rules are association relationships among keywords, and these keywords have adjacent location and co- occurrence appearances, which are causations and reflect the latent semantic relationships of text.

According to human concept learning [9], different text association rules are represented as the following forms:  ba r1  ,  cab r2  ,  ? ,  4?C    mab irk   ddd  ??  ,  where are keywords in a text;  denote corresponding confidences of text association rules.

mcba ,,,, ddd irrr ,,, 21 ddd  Term 3.3: Degree of text association rule  Degree of text association rule is the number of its antecedent keywords.

For example, the degree of text association rule a b is 1; the degree of text association rule a,b c is 2. From the point of view of human concept learning, the degree of implication reflects the understanding difficulty of concepts and relationships among them. Similarly, the degree of text association rule measures the complexity of relationships among keywords. The greater the degree of text association rule is, the more complicated the text association rule is; vice verse. For instance, there are two text association rules in the field of web service:  WSDL?Service Description; WSDL, UDDI, SOAP?Service Framework.

The degree of the former is 1, and that of the latter is 3. It is obvious that the latter is more difficult to be understood, because there are more complicated associations among WSDL, UDDI and SOAP. Specially, the largest degree of a D-keyword text is D-1. Text assertion is a special text association rule whose degree is 0.

Term 3.4: Degree-k text association rule set  One text, containing keywords # ? bK1, K2, ?, KDc, is denoted as T|#. And the set of degree-k text association rules is denoted as , where x ? #. kx?  In previous section, the concept is expanded as the form of power series, so we expand similarly the text association rules in the following terms.

Term 3.5: Power series expansion of text association rules  In a text containing D keywords, power series expansion of text association rules is a set of text association rules whose degrees are from 0 to the D-1. Referring to the expansion of power series of human concepts, power series expansion of text association rules is represented as  degree)largest   theof rulesn associatio(text :1  :  :1  )assertions(text :0     ???  ?  ?  ?  D x  k x  x  x  D  k ?  ?   Term 3.6: Power series representation of text knowledge  Referring to concept algebra of human concept learning [9], there is a set of text association rules corresponding to any text T, so there is the formula:  k x  D  k  D xxxT  ???  ??ddd????? ?  ?  ? #       |   where #  denotes all keywords in text T; D denotes the number of keywords;  denotes a set of degree-k text association rules.

k x?  After analyzing the power series representation of text knowledge, we find it is complicated to represent text knowledge. Therefore, it is an important task to find the special features of web texts. So the simplicity of power series representation will be discussed in the following parts.

C. Simplicity of Power Series Representation According to the definition of the degree of text association  rule, text association rules are classified into different types.

After analyzing the distribution of text association rules in a huge amount of texts, we find that text association rules mainly consist of degree-1 and degree-2 text association rules. And degree-3 text association rules are few; text association rules of degree more than 2 are seldom. We calculate the average number of all degrees' text association rules from 500 web pages, and the results are showed in Fig. 2.

In Fig. 2, degree-1 text association rules reach to 78.5%, degree-2 text association rules cover 18.9%, degree-3 text association rules hold 1.2% and degree-4 text association rules is about 0.4%.

The distribution of text association rules supplies the important reference for machine extracting association rules automatically. Therefore, text association rules of degree-1 and degree-2 are the most important parts, which should be extracted by machines. So, we propose degree-2 hypothesis in machine-based text understanding.

Degree-2 hypothesis: If one text can be composed by text assertion, degree-1 text association rules and degree-2 text association rules, that is to say, if one text can be represented by , the most semantics of one text can be 210 xxx ?????   Figure 2. The distribution of all degrees' association rules of 500 texts  4?B    extracted by machines.

Base on the above hypothesis, power series representation of text knowledge is simplified as  210| xxxT ??????# .

Compared with linear hypothesis theory, degree-2 hypothesis is the expansion of linear hypothesis in human concept learning. Linear hypothesis reflects the cognitive behaviors of human concept learning. The complex relationships among the concept's attributes are hard to be obtained by human beings because of some physiological and psychological restrictions such as brain storage capacity, logical thinking ability, etc. In fact, for the majority of concept learning process, it is sufficient to obtain a concept's knowledge with degree-1 relationships between attributes because human beings have the associated ability and the prior knowledge to explain the concept's content with the help of degree-1 association rules [9]. However, to the machine, as there is no storage capacity limitation and logical thinking limitation, it is able to obtain more complex semantic relationships among keywords, making up of the lack of associated ability and prior knowledge like human beings.



IV. REASONING BASED ON DEGREE-2 HYPOTHESIS OF TEXT KNOWLEDGE  According to the above analysis, especially degree-2 hypothesis, any text can be represented as:  210| xxxT ??????# .

And this formula not only simplifies the representation of text knowledge, but also makes our model's semantic reasoning possible. Herein, we will introduce the basic structure of this model, and then analyze the essential features of power series representation of text knowledge based on human concept learning (PSR). Therefore, the reasoning rules of textual knowledge will be discussed in the following. Above of all, we choose two texts denoted as:  aaa xxxa  T ??????  bbb xxxb T ??????  Then we propose three types of basic reasoning rules, which are intersection, union and subtraction between two different texts in the same domain.

The common information can be reasoned by the intersection rule, the domain information can be collected by the union rule among certain texts belonging to the same domain, and the differences between two different texts can be extracted by the subtraction rule between two different texts.

In this point, our model performs better than other models.

Vector space model hasn?t the reasoning ability because the resources are just represented by keywords. Element fuzzy cognitive map just considers the degree-1 association rules, so the reasoning rules of this model is too simple. Ontology based  models can express rich semantics, but the reasoning rules of these models are too strict, which cannot meet with the diversity and irregularity of web resources. Probability topic models are based on probability and statistic, where many mathematical assumptions are given. However, these assumptions are not suitable, because the web resources are massive, dynamic and uncertain. To our model, text knowledge can be represented automatically, and can accord with the mass, dynamic and uncertainty of the web resources. At the same time, the reasoning process is easier to be implemented in our model, which also contains richer semantics.

Definition 4.1: Intersection reasoning of two texts Ta, Tb which stands for the special information that appears not only in Ta but also in Tb, is denoted as I (Ta, Tb).

0 1 2 0 1 2  0 0 1 1 2 2  0 1 2  0 1 2  ( , ) ( ) ( )  ( ) ( ) ( )  =    a a a b b b  a b a b a b  a b a b a b  a b x x x x x x  x x x x x x  x x x x x x  x x x  I T T ? ? ? ? ? ? ? ? ? ? ?  ? ? ? ? ? ? ? ? ?  ? ? ? ? ?  ? ? ? ? ? ?          where ax  is the set of keywords of text ; and aT bx  is the set of keywords of text ;bT a bx x x?   is the intersection of ax   and  bx .

Intersection of two texts contains the common information, including text assertions and text association rules.

Definition 4.2: Union reasoning of two texts Ta, Tb, which stands for all of information from two texts, is denoted as U (Ta, Tb).

0 1 2 0 1 2  0 0 1 1 2 2  0 1 2  0 1 2  ( , ) ( ) ( )  ( ) ( ) ( )      a a a b b b  a b a b a b  a b a b a b  a b x x x x x x  x x x x x x  x x x x x x  x x x  U T T ? ? ? ? ? ? ? ? ? ? ?  ? ? ? ? ? ? ? ? ?  ? ? ? ? ? ?  ? ? ? ? ? ? ? ? ?  ? ? ?  ?  ? ? ?   where  is the set of keywords of text ; and ax aT bx  is the set of keywords of text ; bT U a bx x x? ?  is the union of  ax  and bx .

When intersection reasoning between two texts isn?t O, union reasoning can extract comprehensive semantics from two texts.

Definition 4.3: Subtraction reasoning of Ta to Tb, which stands for the special information that appears in Ta but not in Tb, is denoted as Sa-b (Ta, Tb).

0 1 2 0 1 2  0 0 1 1 2 2  0 1 2  0 1 2  ( , ) ( ) ( )  ( ) ( ) ( )      a a a b b b  a b a b a b  a b a b a b  a b a b a b  a b a b x x x x x x  x x x x x x  x x x x x x  x x x  S T T  ? ? ?  ?  ? ? ?  ? ? ? ? ? ? ? ? ? ? ? ?  ? ? ? ? ? ? ? ? ? ? ? ?  ? ? ? ? ? ?  ? ? ? ? ? ?  a    where x  is the set of keywords of text ; and aT bx  is the set of keywords of text ; bT a b a bx x x? ? ?  is the subtraction of  ax  to  bx .

4??    TABLE I. COMPARISONS OF REASONING ABILITY, AUTOMATIC CONSTRUCTION ABILITY, COMPLEXITY COMPUTATION, AND SEMANTIC RICHNESS  Representation models VSM EFCM PSR PLSA, LDA OWL Intersection  ? ? ? ? ? Union ? ? ? ? ?   Reasoning  Subtraction ? ? ? ? ? Automatic construction ? ? ? Manual Semi-automatic Complexity Computation Not Easy Easy Difficult Not Semantic richness Poor Media Rich Rich Rich  Subtraction reasoning between two texts means that there are some differences between them.

The above three types of reasoning are the basic components of the reasoning rules of power series representation.

With , we are able to find all knowledge of the two texts; with the , we could find the shared information between two texts; with the , we can find the main section of two different texts. So, three types of basic reasoning rules play an important role in the reasoning of power series text representation.

( , )a bU T T I T( , )a bT  ( , ),   ( , )a b a b b a a bS T T S T T? ?  In addition, there are many other reasoning rules in our model, such as "exclusive or", "inclusive or". However, these two reasoning rules may be deduced by the above three types of basic reasoning rules. So, intersection, union and subtraction constitute the principal part of the reasoning rules of our model.



V. COMPARISON OF TEXT REPRESENTATION MODELS In section IV, we have studied the reasoning rules of our  model. Then, we will compare our model with other models in  the following parts, in order to verify that our model performs better than other models in representing text knowledge.

A. Summary comparasions among different kinds of models In this part, we will compare our model with current  knowledge representation models in reasoning ability, automatic construction ability, complexity computation, and semantic richness (Table I).

In Table I, we discuss vector space model, element fuzzy cognitive map, power series representation, probability topic models, and ontology representation models. From this table, we can find that PSR, PLSA, LDA, and OWL are all of semantic richness, but only PSR can be constructed automatically. Because of the rapid growth of web resources, the automatic construction ability is one of the most important properties. So, in the following part, we focus on these models, which can be constructed automatically.

B. Experiment Verification In the following part, six texts divided into two groups  (Table II, III) from web pages (Appendix 1) will be represented by the three different models, which are VSM, EFCM and PSR.

TABLE II. GROUP ONE CONTAINS THE SAME KEYWORDS {SUN, APPLE}, AND THESE ARE SEARCHED FROM GOOGLE  Webpage One (NSA Offers Security Guidelines for Sun, Apple and Red Hat, denoted as T1) VSM {sun, apple, Schaeffer, policymaker, warfighter, guideline, NSA, harden, assurance, infrastructure, baseline, spectrum,  subcommittee, institution, operator, homeland, Solaris, demand, commerce, situation} EFCM {sun?T1: 0.067,apple?T1: 0.067, Schaeffer?T1: 0.133, assurance?T1: 0.133, infrastructure?T1: 0.133, NSA?T1:  0.467}?{assurance?Schaeffer; Schaeffer?assurance; infrastructure? NSA; Schaeffer?NSA} PSR {sun, apple}?{assurance?Schaeffer; Schaeffer?assurance; infrastructure? NSA; Schaeffer?NSA}  Webpage Two (NSA helps Apple, Sun and Red Hat harden their systems, denoted as T2) VSM {sun, apple, NSA, SCAP, guideline, operating, hearing, automation, protocol, subcommittee, development, agency, effort,  homeland, assistance, Solaris, director, terrorism, door ,attack, range} EFCM {sun? T2: 0.042,apple? T2: 0.042, hearing? T2: 0.083, NSA? T2: 0.333, agency? T2: 0.083, director? T2: 0.125,  operating? T2: 0.125, development?T2: 0.083, attack? T2: 0.083}?{hearing?NSA; agency?NSA; director?NSA;operating?NSA;operating?development;hearing?operating;development?operating;development?NSA}  PSR {sun, apple}?{hearing?NSA; agency?NSA; director?NSA; operating?NSA; operating?development; hearing?operating; development?operating; development?NSA}?{NSA, hearing?director; development, operating?NSA; NSA, operating?development; NSA, development?operating; attack, NSA?operating; attack, NSA?director; attack, NSA?development} Webpage Three (Planting Apple Trees, T3)  VSM {sun, apple, planting, soil, root, fruit, diameter, grass, hole, pocket, graft, watering, nutrient, drainage, scion, stake, bloom, lawn, water, everybody, standing}  EFCM {sun? T3: 0.024, apple? T3: 0.195, root? T3: 0.146, soil? T3: 0.146, grass? T3: 0.049, planting? T3: 0.268, diameter? T3: 0.049, water? T3: 0.073, pocket? T3: 0.049}?{root?soil; soil?root; grass?planting; diameter?grass; grass?diameter; diameter?water; water?diameter}  PSR {sun, apple}?{root?soil;soil?root;grass?planting; diameter?grass; grass?diameter; diameter ?water; water?diameter}?{ pocket, soil?root}  4??    TABLE III. GROUP TWO CONTAINS THE SAME KEYWORDS {INTERPRETER}, AND THREE PAGES ARE SEARCHED FROM GOOGLE  Webpage One (Meaning of Computer Interpreter and Compiler, denoted as T4) VSM {interpreter, bcc32, binary, conversion, compilers, Borland, literate, task, package, programmer, editor,  person, string, rule,  operating, character} EFCM {interpreter? T4: 0.313, person? T4: 0.063, programmer? T4: 0.156, editor? T4: 0.094, conversion? T4: 0.063, binary?  T4: 0.313}?{person?programmer; person?editor; editor?person; conversion?binary; editor?programmer} PSR {interpreter}?{person?programmer;person?editor;editor?person;conversion?binary;editor?programmer}?{editor,  person?programmer; programmer, person?editor; programmer, editor?person} Webpage Two (Interpreter Vs. Compiler, denoted as T5)  VSM {interpreter, BYTECODE, Perl, assembler, processor, 3gl, assembly, PASCAL, platform, compilation, output, instruction, object, QBASIC, COBOL, advantage, programmer, visual, execution, Basic, manipulation}  FCM {interpreter? T5: 0.214, platform? T5: 0.143, bytecode? T5: 0.119, advantage? T5: 0.048, programmer? T5: 0.143, output? T5: 0.071, compilation? T5: 0.048, execution? T5: 0.048, processor? T5: 0.071, Perl? T5: 0.095}?{platform? bytecode; bytecode?platform; advantage?programmer; output?compilation; compilation?output; execution? bytecode; bytecode ?execution; processor?instruction; instruction?processor; execution?Perl}  PSR {interpreter}?{platform? bytecode; bytecode?platform; advantage?programmer; output?compilation; compilation?output; execution? bytecode; bytecode ?execution; processor?instruction; instruction?processor; execution?Perl} Webpage Three (Interpreters and Translator-- Job Outlook, denoted as T6)  VSM {interpreter, prospect, increasing, localization, specialty, translator, employment, population, growth, globalization, shortage, demand, relay, Spanish, employer, occupation, skill, decade, expansion}  EFCM {interpreter?T6: 0.268, employment?T6: 0.107, prospect?T6: 0.071, demand?T6: 0.125, translator?T6: 0.232, increasing?T6: 0.018, growth?T6: 0.036, population?T6: 0.036, localization?T6: 0.036, specialty?T6: 0.036, Spanish?T6: 0.036}?{employment?prospect; prospect?employment; demand?translator; increasing?translator; prospect?translator; employment?translator; growth?translator ;population?translator; localization?demand}  PSR {interpreter}?{employment?prospect; prospect?employment; demand?translator; increasing?translator; prospect?translator; employment?translator; growth?translator ;population?translator; localization?demand}?{increasing, translator?demand; translator, specialty?prospect; translator, Spanish?demand; translator, employment?prospect; prospect, employment?translator; prospect, translator?employment}  The titles of six pages are respectively denoted as T1, T2,e, T6, respectively.  In the two groups, there is ki?Tj: Hi, which means that contribution rate of the keyword ki to the page Tj is HI in a certain EFCM. At the same time, there is the support rate of keywords in the three types of models, and confidence of association rules in EFCM, PSR.

In Table II, although there are the common keywords {sun, apple} in three pages, the former two pages tell us the one similar event and the latter one tells us another. The former two pages tell that the National Security Agency (NSA) helps apple, sun and red hat harden their systems. Herein, sun and apple are different companies. But, the latter article tells something about planting apple tree. Just judging the content of keywords, these three pages are not easily distinguished. However, with the degree-1 and degree-2 association rules, the similarity and difference among them become obvious. In page one, there is {Schaeffer assurance; infrastructure NSA}, which is related with the assurance and the infrastructures supported by NSA.  In page two, there is {development operating; development NSA}, which denotes the development supported by NSA. The two pages have something to NSA, which aims at the computer system security. In page three, viewing {soil root; grass planting}, readers can infer that it is different from the former two pages, and there is something about soil and planting. So, PSR expresses the difference among them very well, which is generated readily automatically by machines.

As similar to group one, there are three pages which contain some common keywords (here, only one "interpreter").

So, how to distinguish these pages becomes more difficult because of the lack of clue information. However, with the help of text association rules, it can be determined whether interpreter is a human or machine. In page one, interpreter can be regarded as a computer program with the help of the association rules {conversion binary; editor programmer}.

And, in page two, there is {platform?bytecode; bytecode?platform; instruction?bytecode}, which distinguishes the meanings of interpreter more easily. So, in the former two pages, interpreter is a computer program. But, in page three, {translator, employment?prospect} poses interpreter as a certain person whose job is to exchange information from one language to another language. That is to say, it is further proved that some issues that cannot be solved by other knowledge representation models can be easily solved by PSR, which can be generated automatically.

In conclusion, there are summary comparisons and experiment verification given in above part. From summary comparisons of different models, PSR, PLSA, LDA, and OWL are all of semantic richness, but only PSR can be constructed automatically. VSM, EFCM, and PSR can be constructed automatically, but EFCM and PSR contain richer semantics than VSM. From experiment verification, we find that poor semantics can be extracted by VSM, but EFCM and PSR can extract more semantics from texts.  In this point, both EFCM  4?    and PSR perform better than other models in expressing semantics.

However, EFCM can only express the whole contribution of all keywords, which mixes the common knowledge and association rules included in texts. That is to say, PSR can distinguish these different semantics from texts, which represents the common knowledge as text assertion and text association rules holding different degrees, which have different understanding complexity. Therefore, PSR is better than EFCM.



VI. CONCLUSION In this paper, we have proposed power series representation  of text knowledge based on the human concept learning, which makes some contributions to the development of the knowledge representation.

1) Firstly, power series representation of text knowledge is proposed based on the latest research of human concept learning, and many related theories of cognitive psychology about text understanding are introduced.

2) Secondly, we propose the machine-oriented degree-2 hypothesis, in order that text knowledge can be represented automatically, efficiently and compactly.

3) Thirdly, based on the machine-oriented degree-2 hypothesis, we have studied the reasoning rules of our model, which perform better than other models. Through the implement process of the reasoning rules, we can find the common information, differences and the collected domain knowledge, which supply the theoretical foundation for the discovery of latent semantics.

