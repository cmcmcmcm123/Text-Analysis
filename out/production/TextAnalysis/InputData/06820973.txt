Modeling Energy Savings in Volunteers Clouds Congfeng Jiang?, Jian Wan?

Abstract?In this paper we propose different models for the energy consumption in a special existing cloud system named SlapOS. In this cloud, the data center comprises dedicated and volunteer machines; these latter ones are not always available.

Our objective is to state how to plan the run of applications for minimizing the global energy consumption; we propose two modelings. In the first model, we assume that we have a finite number of homogeneous volunteers nodes on which our applications can be run. The objective is to determine which application to run on each node in order to minimize the overhead in energy consumption caused by these runs. We show that the key computational challenge in this problem consists in finding a feasible solution when it exists. We propose for it a polynomial time algorithm. In the second model, we assume that the volunteers nodes are heterogeneous. In this case, we show again how to find a feasible solution in polynomial time. But in comparison to the homogeneous case, the key computational problem to solve here is NP-hard. We then propose an ILP (Integer Linear Programming) formulation for addressing and evaluate it throughout various simulations of the SlapOS system in a realistic volunteer computing context.

Index Terms?Energy efficiency, performance evaluation and benchmarking , cloud computing, Energy consumption, Volun- teer and desktop grid computing, Linear programming and optimization.



I. MOTIVATIONS  SlapOS [1] is an open source Cloud Operating system which was inspired by recent research in Grid Computing [2] and in particular by BonjourGrid [3], [4], a meta Desktop Grid middleware for the coordination of multiple instances of Desktop Grid middleware. Figure 1 shows the current architecture. SlapOS is based on two types of nodes: SlapOS Nodes and SlapOS Master. SlapOS is an hybrid cloud in the sense that each of its nodes can be dedicated (Data center in Figure 1) or dynamically provisioned from volunteers (Home cloud in Figure 1).

The Master?s role is to install applications and run processes on SlapOS nodes. It acts as a central directory of all SlapOS Nodes, knowing where each SlapOS Node is located and which software can be installed on each node. The role of SlapOS Master is to allocate processes to SlapOS Nodes. In comparison to the traditional cloud view, SlapOS innovates in considering the possibility to build data centers-like with  dedicated and volunteer nodes. The usage of volunteer nodes can improve the cloud elasticity at lower prices. Indeed, for managing temporal overhead in users requests or extra- storage demands, it might be cheaper for the cloud owners, to negotiate the subscription of volunteer nodes, instead of buying new machines. Moreover, we can benefit from the fact that volunteer machines might consume less energy.

Regarding fault tolerance issues, volunteer nodes imply that we maintain multiple active copies for each application in order to have a minimum of service availability. Doing so however, we increase the global energy consumption. The purpose of this paper is to optimize these excesses in energy consumption by adequate placements of cloud applications.

In this paper, we propose two models for optimizing the run of applications in our volunteer context. In the first model, we assume that we have a finite number of homogeneous volunteers nodes on which our applications can be run. Ho- mogeneous volunteers means that the computing nodes used in the cloud platform have the same specification. This implies for instance that each node can run the same maximal number of applications and that each machine is assumed to have similar (or identical) power consumption characteristics. Het- erogeneous volunteers means that the computing nodes may have different specifications e.g. different power consumption characteristics. Considering the homogeneous case first then the heterogeneous case is conventional in distributed comput- ing. Results obtained in the analysis of the homogeneous case are extended in the heterogeneous case. Our modeling in the homogeneous case also serve as introducing the discussion and the key concepts. This step by step methodology can also help understand the rationale of the modeling and the refinement process.

Whatever case we consider, the objective is to determine which application to run in each node, in order to minimize the total overhead in energy consumption caused by these runs.

In the homogeneous case, we show that the key computational challenge consists in finding a feasible solution when it exists.

In the heterogeneous case, we show again how to find a feasible solution in polynomial time. But in comparison to the homogeneous case, the key computational problem to solve   DOI 10.1109/CLOUDCOM-ASIA.2013.78    DOI 10.1109/CLOUDCOM-ASIA.2013.78     here is NP-hard. We then propose an ILP formulation for addressing it and evaluate it throughout various simulation of the SlapOS system in a realistic volunteer computing context.

We would like to assert that the paper is about mod- eling first and not about experiments on a real system.

However, we estimate that our modeling is a good com- promise between the parameters we consider impacting the phenomenon and the realism of current cloud technolo- gies. For instance, our parameter settings consider the Sla- pOS cloud system running in production and available at https://slapos.cloud.univ-paris13.fr.

The organization of the paper is as follows. In section 2, we introduce some applications of our work and we give further motivations. We formalize the energy consumption problem in our cloud system in section 3 and section 4, step by step and we conduct extensive experiments under different configurations. Section 5 is about related works. Section 6 concludes the paper.

Fig. 1. The SlapOS Architecture.



II. APPLICATIONS OF OUR WORK  SlapOS has been ported on a Nexus S mobile phone and under Android 4.0.4. Mobile phones are potential volunteers for hosting applications! The following experiment has been made by the SlapOS team. They deployed the Wordpress (which contains an Apache server) application on the Nexus and the operating system was a ?chrooted? Debian. The standby processor option (Wave Lock) was disabled and Wifi was enabled (Wifi Lock). A script was coded for writing the date in a file every 5 minutes. They measured the battery life when Wordpress was running and when it was not running.

The script was running in these two cases.

They noticed a battery life of 22 hours without Wordpress and 12 hours with Wordpress running. They also noticed that starting a new Wordpress instance took few seconds with Android. These observations reinforce our initial intuitions about the current technology: a) SlapOS applications may reduce drastically the battery life. This high penalty might not motivate volunteers to keep their nodes in SlapOS clouds b) the penalty in restarting an application when needed is not prohibitive. In another words, we could imagine at large scale to use volunteers nodes (Tablets, Smartphones, PCs) for  hosting Web applications under the condition that we know how to optimize the energy consumption.

This work is a first stage for improving the SlapOS scheduler and application manager in this direction. In its actual version, SlapOS implements the following behavior.

For joining a SlapOS cloud, any new volunteer must first register its machine to a SlapOS master node by declaring various parameters such as the memory space, disk space, CPU type, location of the machine. . . Then, the volunteer can choose from the catalog of SlapOS applications some that he accepts to install on his machine. Finally, the volunteer can let his machine opened for other users (the machine is registered as public) or not. For any user, SlapOS maintains a list of executable applications. These applications are those that the user accepted to install or those that are located on machines nodes registered as public.

Our study considers the restricted setting where SlapOS manages a finite set of client/server applications on volunteer nodes. We call such a system a volunteer cloud. At each time instant, we must run multiple instances of the server part of each application on volunteers nodes. This is both for managing the flow of user requests and preventing faults in execution. However these runs are energy consuming. For reducing the consumption, we propose at any time instant to only start on volunteer nodes, a subset of servers (we are referring to the server part of applications) among those that can be started. Naturally, these servers must be on available nodes and will be used for all client executions.

Since volunteers nodes can get disconnected at anytime, we need in this strategy to constantly adapt the subset of servers that are maintained active. In a technical viewpoint, this might require to implement mechanisms for data migration. We do not consider this issue in this paper.

In summary, given a set of applications, a set of volunteer nodes with their availability distribution, the objective of this work is to propose a strategy for making the servers running and available on each volunteer node in order to minimize the global energy consumption. In the mid term, we expect to include such solutions into the SlapOS system.



III. PROBLEM DESCRIPTION ? FIRST REFINEMENT  A. Initial Assumptions  We have N client/servers applications. At each time instant, we must maintain available k instances (or copies) of the server part of each application. In practice, a challenging question consists in determining the good value of k. This requires a large system analysis that we do not consider in this paper. We assume in our modeling that k is a known input.

The applications can be run on m volunteers machines. We assume that the time is discretized in periods of size ?t for which we have the availability of machines. In each period, this is given by the vector M of size m (m is the number of machines) such that Mi = 1 when the machine is available in the interval of time; Mi = 0 otherwise. We are interested in finding, for any time interval [t, t + ?t], the applications     to keep active on each machine. Let us remark that, in doing this, we do not consider the global optimization problem in the multiple time intervals and that, in practical cases, m is far larger than k.

We admit a piece-wise constant model for the energy consumption. At any time instant, if there is no application running on machine i, its power consumption is P basei . If however there is an application j running on machine i, we have an extra power consumption of P ei,j(t). We can relate this model to the one used in [5]. In this solution, the power consumption for a given computer h is given by Cminh + f ? (Cmaxh ?Cminh ). Cminh corresponds to the power used when the host is idle and Cmaxh is the power consumption when the host is fully used. Our model generalizes this formulation by not considering an explicit formulation of the extra consumption led by applications run.

We denote by Pi,j(t) the power consumption of machine i at time t running application j. From our previous assumption, we have Pi,j(t) = P basei +P  e i,j(t). On machine i, the induced  energy consumption in interval [t, t+?t] is given by:  Ei,j(t, t+?t) = ? t+?t t  Pi,j(t).dt  = ? t+?t t  (P basei + P e i,j(t)).dt  = ?t.P base i +  ? t+?t t  P ei,j(t).dt  B. First Modeling  We assume that machines are identical. This implies that the value of ?t.P basei is the same, whatever machine. On identical machines, it is also normal to expect a low variation in the energy consumed by a same application on different machines.

This means given an application j and two machines i, i? that  we have ? t+?t t  P ei,j(t)dt ? ? t+?t t  P ei?,j(t)dt. Therefore, we  propose to use a constant E+j = ? t+?t t  P ei,j(t).dt ?i, j.

Admitting that we have the constants E+j and the availabil-  ity matrix in a time interval [t, t + ?t], we are looking in this first modeling for an assignment of applications copies that will minimize the excess in energy consumption caused by applications run. For estimating this excess, we consider that the real-time power consumption is linear with respect to the number of application run on each machine. Therefore, let us admit that on each machine i, we assigned copies of the applications Zi ? {1, . . . , N}; the excess of energy consumption (that we want to minimize) is:  m? i=1  ? j?Zi  E+j .Mi  Let us remark that to assign an application copy on a machine means that we maintain this application active on it.

We include Mi in this formulation because there is no energy consumption led by machines that are not available. There are some additional constraints that must be fulfilled in order to ensure that any assignment Z1, . . . , Zm is valid. The Figure 2 presents them.

C1: Applications copies must be put on machines that are available; C2: k copies of each applications must be assigned on exactly k distinct machines; C3: On any machine i, there is a constant number q stating the maximal number of application copies that can be assigned to it.

Fig. 2. Constraints in the first modeling  The idea with constraint C2 is to ensure that we have enough replications executed for any application while ensuring that copies of the same applications are not put on the same machine. Let us remark that there is no interest in putting all applications copies in the same machine since an execution fault that occurs in this machine can cause the unavailability of the application. A parameter q is introduced in constraint C3 for keeping a reasonable CPU load on machines. Indeed, the more we assign application copies on a machine, the lower is their service response time. For this first modeling we have the following results:  Fact 3.1: Any feasible solution will lead to the same total  energy consumption. This cost is equal to N? j=1  E+j .k.

This simple observation suggests that we can find an optimal solution by looking just for a feasible solution.

Property 3.2: The conditions m? i=1  Mi ? k and  ( m? i=1  Mi).q ? N.k are necessary and sufficient for the existence of a feasible solution.

Proof: If k < m? i=1  Mi then, it will be impossible to put  applications copies on distinct machines. The constraint C1 will be violated. Given the availability matrix, the total number of copies that can be run in the time interval [t, t + ?t] is  ( m? i=1  Mi).q. There is a feasible solution only of this number  is at least equal to the number of application copies (N.k) Proposition 3.3: If a feasible solution exists, then it can be  found in O(N.k +m).

Proof: Indeed, a feasible solution can be obtained from  Algorithm 1 (see page 4).

The worst case complexity of this algorithm is in O(N.m).

However, if there exist a feasible solution, then we will always have M [L[i]] = 1 and |ZL[i]| < q in executing the while loop (because of the ordering). Since the ordered list can be created in O(m), we have the proof.



IV. INTRODUCING A FORM OF HETEROGENEITY ? SECOND  REFINEMENT  A. Second modeling  Let us now consider a setting where machine are het- erogeneous. Our initial assumption states that the energy consumption led by the run of application j in machine     Data: A vector T [1 . . . N ] of application copies; the availability matrix M [1 . . .m]; m empty sets Z1, . . . , Zm  Result: An assignment Zi for each machine i 1 Create and ordered list L of machine indices (1, . . . ,m)  from the available to the non available 2 for j ? {1, . . . , N} do 3 T [j] = k 4 end 5 for j ? {1, . . . , N} do 6 i = 0 7 while (T [j] ? 0 and i < m) do 8 if (M [L[i]] = 1) and (|ZL[i]| < q) then 9 ZL[i] = ZL[i] ? {j}  10 T [j] = T [j]? 1 11 end 12 i = i+ 1 13 end 14 end  Algorithm 1: Feasible solution in homogeneous case  i is ?t.P basei + ? t+?t t  P ei,j(t).dt. Since the machines are heterogeneous, both terms of this sum might differ for an application ran in distinct machines. Let us denote the first term by Ebasei and the second by E  + i,j .

In the first modeling, we stated that the minimization goal was to reduce the excess in energy consumption led application runs. This formulation is natural since either the machine will be available and not used or, it will be available and exploited in our assignment. It is not easy however in the heterogeneous case to find a good formulation for capturing the excess in energy consumption. Indeed, the machines here have different energy consumptions when they are idle. It is important to include this difference in our minimization objective. Therefore, in the heterogeneous case, what we are looking for is an assignment Z1, . . . Zm that minimizes  m? i=1,|Zi|>0  Ebasei + m? i=1  ? j?Zi  E+i,j  Any assignment here must again respect the constraints de- fined on Figure 2. We however consider here that the maximal threshold above which there is a performance degradation due to the execution of multiple application can differ per machines. We then reformulate the constraint C3 as follows in Figure 3.

C3: On any machine i, there is a constant number qi stating the maximal number of application copies that can be assigned to it.

Fig. 3. Constraints C3 in the second modeling  We now discuss about the feasibility and the complexity in the heterogeneous case.

B. Feasibility and complexity in the heterogeneous case  As for the homogeneous one, we cannot always ensure that there is a feasible solution in the heterogeneous case. We can also establish that such a solution however will exist if  the conditions m? i=1  Mi ? k and ( m? i=1  Mi.qi) ? N.k are met.

Finally, we have the following result:  Proposition 4.1: A feasible solution in the second modeling can be found in O(N.k +m2).

Such a solution can be obtained by adapting our previous algorithm for building a feasible solution in the homogeneous case. We propose Algorithm 2.

The initial ordering that we did ensure that we will always have (M [L[i]] = 1) and (|ZL[i]| < qL[i]). Since L can be created in O(m2), we have the result.

Proposition 4.2: The feasible solution returned by Algo- rithm 2 is not always optimal.

Proof: There is indeed a simple counter example. Let us consider the case where we have N application and one copy for each k = 1. Let us also assume that we have l machines available such that qi > N . Finally, let us assume that ?i, j, Ebaseim + E+im,j > Ebasei + E+i,j where im = arg{max{qu|u is available }  u?1,...,m }. It is clear in this setting  that the feasible solution returned by Algorithm 2 will not be optimal. One can criticize this proof since we chose k = 1.

However the argument used in it can easily be generalized even for k > 1.

Data: A vector T [1 . . . N ] of application copies; the availability matrix M [1 . . .m]; m empty sets Z1, . . . , Zm  Result: An assignment Zi for each machine i 1 Create and ordered list L of machine indices (1, . . . ,m)  in using as first criteria the availability and second one the value of qi (from greater to smaller)  2 for j ? {1, . . . , N} do 3 T [j] = k 4 end 5 for j ? {1, . . . , N} do 6 i = 0 7 while (T [j] ? 0 and i < m) do 8 if (M [L[i]] = 1) and (|ZL[i]| < qL[i]) then 9 ZL[i] = ZL[i] ? {j}  10 T [j] = T [j]? 1 11 end 12 i = i+ 1 13 end 14 end  Algorithm 2: Feasible solution in heterogeneous case  Since any feasible solution will not necessarily be optimal, we need to find another approach for obtaining the optimal solution in the heterogeneous case. The following result shows that even in feasible cases, it will not be simple to find an optimal solution.

Theorem 4.3: The computation of the optimal solution in the heterogeneous case is NP-hard  Proof: The proof is obtained by a reduction to the minimum knapsack problem. Let us indeed assume that ?i, j, E+i,j is the same. Then the problem can be reduced to the one of choosing a subset of machines with minimal base energy consumption and whose global capacities is enough for executing copies. More formally let us consider a variable xi defined as follows:  xi =  { 1 if at least one application is assigned on i;  0 otherwise.

Then the optimization problem to solve is the following:  Minimize m? i=1  (Ebasei ).xi  such that 1) xi ? {0, 1}, i = 1, . . . ,m 2)  m? i=1  xi.qi ? N.k  In this formulation we did not include the energy consump- tion induced by the execution of applications since its global cost will be the same whatever feasible solution we have.

This formulation corresponds exactly to the minimum knap- sack problem. Since this latter problem is NP-hard, we have the proof.

This NP-hardness reduction is based on the idea that it is hard to determine the right subset of machines that should be used in order to reduce energy consumption. In the case however where this subset is known, the problem is not necessarily hard. More formally we have the following result:  Theorem 4.4: If k = 1 and we have the subset of machines used in the optimal solution, then, the optimal feasible solution can be computed in pseudo-polynomial time.

Proof: The proof is obtained from a reduction to the assignment problem. Let us assume that in the optimal so- lution, applications must be deployed exactly on the machines 1, . . . , p. We propose to associate the heterogeneous case with the following assignment problem:  For each machine i, we create qi ?unit machines? i1, . . . , iqi such that on each of them, we can run at most one application at a given time instant. Given any application j, its energy consumption on a unit machine iv is E+iv,j = E  + i,j .

Let us consider that Q = ?p  i=1 qi. If Q > N , we also create Q?N virtual applications such that on any unit machine iv , the energy consumption of the virtual copy is 1.

The goals of this assignment problem are the following: 1) there is one application assigned to any unit machine; 2) any application is assigned to only one unit machine; 3) the sum of the energy consumed by applications on  assigned machine is minimized.

It straightforward to notice that the creation of this as-  signment problem from the initial heterogeneous problem can be done in pseudo polynomial time. Virtual applications in  this reduction serve for having a bijection as in the classical assignment problem.

From any solution of this assignment problem, we can derive a solution for the original heterogeneous problem. This can be done by putting the application j on the machine i if in the related assignment problem, j is assigned to a unit machine iv . Since this assignment problem can be solved in O(Q3) by the mean of the Hungarian algorithm, we have the proof.

Thus, in the case where k = 1, we can find an optimal solution by building 2m subset of machines and solving on each an assignment problem.

The algorithm proposed for k = 1 can be generalized for an arbitrary k. The idea is to consider successive allocations of one copy of each application until all copies are deployed.

This means that we solve k assignment problems. The solution that results in the lowest energy consumption is retained. In doing so, we obtain a local solution to the global optimization problem obtained however in exponential time. If this can be interesting for small values of k, we do believe that if we have to spend an exponential time, near exact or exact solution are preferable. Consequently, in what follows, we propose a mixed integer linear programming model that can be used in the general case.

C. An integer linear programming model for the second modeling  The proposed model are based on the following variables and constants:  ? For any machine i and application j, we have a decisional variable Ai,j . If Ai,j = 1, then the application j must be deployed on machine i. Otherwise, Ai,j = 0  ? For any machine i, we have a 0-1 variable xi that will comprise 1 if at least one application is assigned to it.

We derive the following model:  z =  m? i=1  N? j=1  E+i,j .Ai,j +  m? i=1  Ebasei .xi (1)  min z s.t: (2) Ai,j ? {0, 1} ?i, j (3) xi ? {0, 1} ?i (4)?m  i=1 Ai,j .Mi = k ?j (5)?N j=1 Ai,j .Mi ? qi.xi ?i (6)  Let us remark here that the equation 3 ensures both that the constraints C1 and C2 are met. The equation 6 ensures that we can assign at least one qi applications on the machine i.

Moreover, an assignment of at least one application implies to set xi to 1. The proposed formalization is a standard 0- 1 programming problem. The next section is devoted to our experiments.

D. Experiments  We performed some experiments in order to observe: ? How the ILP runtime behave with a huge number of  machines (because it is one of the key factors that usually have impact on the user experience);  ? How is the energy distributed in using our allocation modeling.

For observing the energy distribution, we will use two notions: the distribution of the energy and the service distribu- tion. The distribution of the energy captures the total energy consumption when doing experiments with a given number of nodes. The service distribution metric returns for each machine the total number of applications copies that were run on them during the experiments.

E. Experimental Settings  We created various configurations with different number of machines ( from 100 to 4000). Given a machine number we created an availability matrix for a period of 24 hours discretized in 1 hour. We assumed that the availability of our machines obey to a Weibull distribution with the parameters ? = 0.393, and k = 2.964 as in [6]. We assumed while using this distribution, that a machine is available if the probability returned by the implementation is respectively 0.2, 0.4, 0.6, or 0.8 according to the experiment. In this way we obtain the availability of machines during one hour.

We assumed that we have 20 applications with 3 copies.

The queue lengths and the energy values (E+, Ebase) were generated with a Gaussian law. The parameters used for generation are given in Figure 4. Finally let us recall that by the mean of our generation approach for the matrix availability, it might happen that some machines don?t be used. Thus, we reported in the same figure the mean effective number of machines used for each configuration.

F. Experimental Results  We used Matlab for implementing and generating optimal solutions of our Integer Linear Programming model.

In Figure 7 we present the evolution of the cumulative energy consumed due to application runs, when considering various number of machines. The accumulated energy dimin- ishes as we increase the number of machines. But, the reason for this is simple: the more we have machines, the more we have available ones, suitable for running our application. This also means that the consumed energy decreases here because we used a random setting generation. Finally let us precise that in this plot and in the others, it is the cumulative results observed for a period of 24 hours that are reported.

For the service distribution metrics, we plotted the figure where the x axis is machine node, y axis is hours (1 to 24), z axis is the accumulated application instances executed on the node for each hour.

Figures 5 and 6 are samples of our results. This kind of figure, more exactly the shapes, may help the applications provider to find some trends for application distribution and migrations. From the service distribution results, we can see  that the accumulated service instance execution decreases when the number of machines increases (from 100 to 500).

It is intuitive that for large scale cloud system, more nodes can reduce the workload on individual node and balance the workload. However, Figures 5 and 6 also show that more energy is consumed if few machines (acceptable probability: 0.4) are available for the same amount of application replica- tions in the heterogeneous volunteer cloud. One explanation is that with less machines we have less choice to allocate services on machines. Therefore, a larger system scale is not only important for load balancing but also important for application offloading; this is because we can find optimal application placement such that the energy consumption is also minimal (Fig. 7). Tabular 4 summarizes the above results and discussion.

If we extend the system scale to very large (i.e., 1 million nodes), it?s nearly impossible for the solver to find an optimal application placement solution in 1 minute due to the huge searching space. Actually in Matlab (version 2010b), it costs almost 28.3613 seconds to generate such solution with 20 applications and each application with 3 copies in a system with 3000 node(available node number is 2607). We can see in Fig.8 that the solution time increases almost linearly with the number of available nodes. In our simulation, when the number of available nodes is 3474 (accept prob. is 02. for 4000 total nodes in one case), then the size of the problem is too large for the current solver in Matlab without hacking into the optimization solver. Moreover, if we increase the scale of the problem in other dimension, i.e., the number of application types from 20 to 100, or increase the number of copies from 3 to upper value, it cost much more time to generate such solution. Therefore, we should ?tradeoff? between the real- time response of the application allocation and the optimal allocation with less energy consumption.

Fig. 5. Services distribution(100 nodes,acceptable probability=0.2)

V. RELATED WORK  The context of our work is related to volunteer computing and Desktop Grid Computing [2]. Desktop Grids (DG) have     Node acc.prob av.nodes ass.apps ?e ?e avg qi. stddev qi energy solu.time 100 0.2 93 60 0.4991 0.2868 5.4255 2.6940 1.1606 0.6335 100 0.4 40 60 0.5183 0.2912 5.2826 2.7862 2.9799 0.3763 500 0.2 431 60 0.5081 0.2873 5.4362 3.0287 0.3274 4.0052 500 0.4 168 60 0.5081 0.2891 5.4043 2.8258 0.6866 1.6589 1000 0.2 873 60 0.5014 0.2894 5.5487 2.8081 0.1192 6.6276 1000 0.4 362 60 0.5007 0.2893 5.5470 2.9084 0.3069 3.3893 2000 0.2 1749 60 0.4983 0.2884 5.5597 2.8579 0.0657 17.3444 2000 0.4 682 60 0.4968 0.2898 5.6012 2.8022 0.1651 5.5039 3000 0.2 2607 60 0.4980 0.2894 5.4956 2.8779 0.0387 28.3613 3000 0.4 1059 60 0.5005 0.2898 5.3654 2.8180 0.1003 8.2305 4000 0.4 1426 60 0.5000 0.2884 5.3212 2.8794 0.0715 13.0007  Fig. 4. Overall Statistics for the experiments (10 examples of one scheduling interval). Here acc.prob is the accept probability, av.nodes is the number of available nodes, ass.apps is the number of assigned application instances,solu.time is the time consumed by the Matlab solver for one solution, ?e and ?e are the mean and standard deviation value used for generating machines energy consumptions.

Fig. 6. Services distribution (3000 nodes,acceptable probability=0.4)  Fig. 7. Energy consumption of different allocations with same application instances on different available nodes (energy consumption is normalized)  been successfully used for solving scientific applications at low cost. DGs middleware such as Condor [7], BOINC [8], XtremWeb [9], OurGrid [10] provide researchers a wide range of high throughput computing systems by utilizing idle resources. However, we did not find in this context a work that aims at optimizing the energy consumption in a setting similar to the one considered in this paper.

In [11] authors introduce a synthesis of the usage of meth-  Fig. 8. Solution time(in seconds) for one scheduling interval (mean value of 20 measuring on the experimenting laptop)  ods and technologies used for energy-efficient operation of computer hardware and network infrastructure. They consider the ICT field in general and they focus on energy-aware scheduling in multiprocessor and grid systems, on the power minimization in clusters of servers, on the power minimization in wireless and wired networks. Clouds are not specifically considered in their work.

In [12] authors propose an efficient resource power manage- ment policy for virtualized Cloud data centers. The objective is to continuously consolidate virtual machines. They show that the dynamic reallocation of virtual machines brings substantial energy savings. They propose four criteria for migrating the virtual machines. Authors do not discuss about the quality of the solution and they recognize that despite the fact that they use heuristics, the algorithms provide good experimental results. We believe that our solution can be extended for consolidating virtual machines in a volunteer context.

In [13] Beloglazov and Buyya propose online algorithms for virtual machines placement with guaranty of performance.

They conduct competitive analysis and prove competitive ratios of optimal online deterministic algorithms for a single virtual machine migration and dynamic virtual consolidation problems. An interesting future work is to extend this result to volunteers clouds contexts.

Beaumont, Eyraud-Dubois and Larcheve?que consider in [14] the problems of reliable service allocation in clouds.

Among the different papers introduced in this section, it is a paper that cover similar problems than ours. They consider first that mapping virtual machines having heterogeneous com-     puting demands onto physical machines having heterogeneous capacities can be modeled as a multi-dimensional bin-packing problem. They assume that each virtual machine comes with its failure rate (i.e. the probability that the machine will fail during the next time period). But they do not consider that services should be duplicated on different machines to derive a robust solution.

The authors focus on formal specifications of the problems as we usually do before deriving complexity results, then they show and prove complexity results. For instance, to prove Theorem 5.1 they use a reduction to the 3-partition problem [15]. For the experiments they consider only n = 15 machines, far below our study, because they say that computing the reliability of an allocation takes time 2n.

In [5] authors study the problem of energy-aware resource allocation for hosting long-term services or on-demand com- pute jobs in clusters. They do not capture the possibility of copies but they have a constraint on the RAM available on each node. The objectives and constraints lead to greedy algorithms. This work is similar to our work but the main differences are that for each job, allocated to a machine, we must have to decide the fraction of CPU to use and we need also for an estimate of the RAM consumption of the job. The fraction of CPU is for studying a form of heterogeneity but it does not include all the cases that can be derived from our heterogeneous modeling.

We do not model the commonly-used Dynamic Voltage and Frequency Scaling (DVFS) power management technique [16], [17] as it is now available on most processors including processors for smartphones and tablets. DVFS is able to reduce the power consumption of a CMOS integrated circuit, by scaling the frequency at which it operates, and when the load varies dynamically.



VI. CONCLUSION  In this paper we explain how to optimize the energy consumption in a context where volunteers nodes are used for building a cloud. Our proposal consists in two models: one in homogeneous context and the other one in heterogeneous ones. We then propose various experiments using Matlab for solving the combinatorial problem in order to observe the system behavior suggested by our models. The simulation results show that our modeling is effective and suitable for real volunteer cloud environment.

For future, since we need to offload the copies of applica- tions to various machines with different availability patterns, we should also consider more realistic issues, including the performance and user experiences on the devices network latency, and data transportation and communication time.

We can also use a weighted objective function for energy consumption and other performance overheads like application migrations. The network-dependent factors have significant impact on user-perceived performance of the hosting appli- cations. The network communications also consumes lots of energy. The cellular connectivity is one of the biggest contributors of energy consumption in a smartphone [18]. The  task affinity and QoS guarantees are also important in real volunteer cloud environment.



VII. ACKNOWLEDGMENT  The funding supports of this work by Natural Science Fund of China (No. 61003077), Natural Science Fund of Zhejiang Province (Y1101092), and Research Fund of Department of Education of Zhejiang Province (No. GK100800010) are greatly appreciated. In France, this work is funded by the FUI- 12 Resilience project from the ministry of industry.

