Classification Rules Mining based on Intent Reduction

Abstract?Concept lattice is accurate and complete in knowledge representation and is an effective tool for data analysis and knowledge discovery. This paper focuses on classifcation rules mining based on concept lattices. By modifying incremental computation of intent reduction of concepts, it develops a method to mine affirmative classification rules and approximate classification rules using intent reduction.

Key words?- concept lattice; classification rule;intent reduction  ?.  INTRODUCTION There are many extraction algorithms for classification  rule based on concept lattice, but the number and form of extracted rules can not achieve satisfactory results. For example, Godin [1], Wang Zhi-Hai [2] and other scholars respectively proposed the algorithm to extract implication rules by the concept lattice, which are mostly deterministic rules. However, the actual database often contains a lot of errors or uncertain data, thus only a few or even no deterministic rules. And other algorithms may produce a large number of redundant rules of high confidence. This study, borrowing the idea from the references [3], carries out classification through determined and approximate classification rule bases, and proposes content-based synchronization and synchronization of pruning to reduce computing classification rule to extract these two types of classification rules, thus effectively reducing the number of classification rules without loss of useful information.

?.  RELATED DEFINITIONS  First give the related definitions of classification rules for intent reduction extraction [4]  Definition 1  The determined classification rule base is:  ER B={r:red? c|c?B?red?IRed(B)?c?red} Where, B is the frequent closed itemset containing the  classification property c (corresponding to the connotation of frequent concept), IRed(B) reduction for the B set.

The confidence of the determined classification rule r:red ? c is Conf(r)=1, namely, the Sup(red ? {c})=Sup(red), which means that the red and c in the antecedent appear in connotation of the same concept.

Therefore, only lattice node containing current classification c needs to be examined, to extract determined classification rules.

Definition 2  The approximate classification rule base is      ARB={r:red ? c | c?B?red?RED?f(g(red)) ? B} Where, B is the frequent closed itemset which  contains the current class c (that is, the connotation of frequent concept), RED is reduction set for all the frequent closed itemsets.

The confidence of the approximate classification rule r:red ? c is that Conf(r)<1, namely, Sup(red ? {c})<Sup(red), which means that the node containing the antecedent red should be the node that contains the precursor of type c node, which has been discussed in definition 1.Therefore, as for approximate classification rules, we only need to consider the precursor node without containing class c.

As reference [3] indicates, all valid determined classification rules, including its support and credibility, can be deduced from the class association and its support base. Therefore, we should find all determined and approximate classification rule bases which meet the user-specified minimum support and confidence, and then construct a classifier.

To extract the determined classification rules, only the lattice nodes containing the current class need to be examined; to extract approximate classification rules, just examine the relationship between nodes containing the current class and its precursor nodes. This study suggests generating a section node which includes nodes of the current class and its precursor nodes. On this basis, classification rules are extracted.

?.  IMPROVED SYNCHRONIZED ALGORITHM OF INTENT REDUCTION  In reference [5], the solution to intent reduction based on incremental construction method is put forward. This paper considers extracting determined and approximate classification rule bases. Only intent reduction of two types of nodes are needed. The first class is the node which satisfies the threshold value and contains class c.

The second class is the node which meets the threshold value and holds the first class node as super-concept node, without containing class c. In the process of solution to incremental intent reduction, redundant calculation may exist. In order to complete the extraction of classification rules, algorithm and its efficiency need to be further improved. It is necessary to study the change of these two classes of nodes in the incremental process.

Set the form background as?G?M?I?, record MinSup as minimum support threshold value, L as the corresponding concept lattice, and L~  as the     corresponding pruning section lattice, which is the part after deleting the nodes whose extension base is less than the minimum support threshold value MinSup in L. It is the the filter of L, and the supremum of sub-lattice; assume we add attribute m to the background, Gm=g(m) ? G  is a object set with attribute m, that is, the new background is (G?M ? {m}?I ? Gm?{m}). Set the concept lattices corresponding to the new background as  *L , *~L  as section lattice after inserting property m  in L~  and *L  as the pruning section lattice corresponding to *  ~L .

Theorem 1, LBA ~),( ? ?and 21 ~~),( LLBA ?? ,  Here, 1 ~L  is the first class node set in L~ ?and 2  ~L  is the second node set in L~ . Then, the update of ),( BA does not affect the reduction of node in 1  ~L ? 2 ~L .

Prove that the descendants of the nodes do not change after update of ),( BA . Discuss two circumstances: if  ),( BA  is the updated node, the conclusion is trivial; If it is the generator, mark nCon as the corresponding new node, and it satisfies the threshold value, obviously,  *~),( LBA ? . At this time, if pCon is the parent node in *~L . There are two cases: pCon is the new node, and then  pCon can not belong to the set 1 ~L . Since the extension  of pCon is more than A only by x, it can not belong to set  ~L . Otherwise, the super-concept of pCon is also the one  of ),( BA , thus ),( BA  being the second class node, which is contradictory to 21  ~~),( LLBA ?? , so the conclusion holds up; pCon is a updated node, and through similar deduction, we know that pCon does not belong to  ~~ LL ? , so the conclusion is established.

According to theorem 1, the node not belonging to  ~~ LL ?  in L~  does not affect calculation of node  reduction. If the node in 1 ~L  is the generator which  meets the threshold value or updated node, it?s easy to infer that corresponding new node and updated node still belong to the first class node; If the node in 2  ~L  is the generator which meets the threshold value?the father node will be considered.

If the corresponding new node is not the second class node, then by the Theorem 1, it is inferred that if the new node needs not be involved in next incremental process, it does not belong to the updated pruning lattice. Then, the nodes not belonging to 21  ~~ LL ?  in L~  can be directly deleted, and are not involved in the incremental process.

Then, mark 21  ~~:~ LLL ?= , *2 *  * ~~:~ LLL ?= , that is, the first and second class nodes are obtained after synchronized pruning, during which the dual case should considered. The algorithm is as follows: an improved  synchronized algorithm MIIR for intent reduction, based on incremental construction of grid. Figure 1 is a flow chart of algorithm MIIR, and figure 2 is a flow chart of the intent reduction algorithm COMPUTE.

Fig. 1   Flow chart of algorithm MIIR   Fig. 2   Flow chart of algorithm COMPUTE  Function JUDGE (Con) in the quotient semi-lattice determines whether Con is the first class node with super-node, if yes, then return true. The return of JUDGE decides whether a certain generator needs producing a new node, thus reducing the computation of the next-round incremental process.

Through the above-mentioned synchronized pruning algorithm, the section lattice containing only the first and second class nodes can be obtained. It is still an ideal concept lattice *L , so the calculation of node reduction does not change. The above algorithm MIIR realizes the synchronized calculation of intent reduction for the required nodes.

?. THE CLASSIFICATION RULE EXTRACTION BASED ON INTENT REDUCTION    With the pruning lattice obtained from the above format (the reduction of each node storing in its ERed field), the two classification rules are extracted according to definition 1 and 2. Give a classification rule base on pruning lattice to extract CRPL. First, through the above MIIR, we can obtain the section lattice containing only the first and second nodes and the related reduction set.

Next, save all the first class nodes with T1?and save the boundary nodes which have father nodes and belong to the second class nodes in T1 by use of T2? Call ERB for T1 (flow chart shown in Figure 4) to extract all the established rules-based classification and then on the T1, T2 call ARB (flow chart shown in Figure 5) to extract all of the approximate rule-based classification, thus obtaining the required set of classification rules.

The flow chart of the extraction of algorithm CRPL based on the rule-based classification of pruning lattice is shown in Figure 3. CRPL input / output: Input: Lattice[],G, categorie c, support threshold value MinSup, confidence threshold value MinConf; Output: classification rule set Rset.

Fig. 3   Flow chart of algorithm CRPL   Fig. 4   Flow chart of algorithm ERB    ?.  EXPERIMENTAL ANALYSIS  To verify the method of the classified association rule this study proposed, C + + is used to carry out the above algorithm. Classifier CSCR is constructed for the above-mentioned classified association rule set by use of  method in reference [6].  And, six base set in machine learning base UCI is tested, the information of which is shown in Table 1.

Fig. 5   Flow chart of algorithm ARB  Four cross-validation technique is adopted in this experiment. Minimum support value greatly affects accuracy of the classifier. In experiment, we find that when minimum support value is near 1%, we can obtain a moderate number of rules and the best classifier.

Therefore, with the minimum support of 1%, minimum confidence of 60%, contrast experiment between the above six data sets and the standard classification method C4.5 is carried out, the result of which is shown in Table 2. The first column is data set, the second, classification accuracy of C4.5, the third, classification accuracy of algorithm Classifier, the fourth, the time for extracting algorithm of association rules, and the fifth, the number of extracted classified association rules.

Table 1   Data sets used in experiments  Date ets Number of attributes number  of Category Number  of objects breast-w 10 2 699  diabetes 8 2 768  glass 9 7 214  heart 13 2 270  iris 4 3 150  pima 8 2 768    As can be seen from Table 2, classification accuracy in algorithm CSCR is higher than that of C4.5 in four of the six data sets under experiment, especially in data set glass, and it?s lower than that of C4.5 in only two data sets. In the six data sets the mean classification accuracy is: CSCR 83.8%, and C4.5 81.58%, that is, the classification method put forward in this study is better    than the standard C4.5 method. In addition, with regard to the time needed, four data sets are carried out fast, with the average time of 1.4 s. The needed time is a little longer, in the two data sets breast-w and heart, for the number of extracted rules is larger. However, the average time is still less than 8s, which is improved compared with the speed in reference [7].

Table 2   Results of algorithm CSCR and C4.5  Data sets C4.5 CSCR Time Number of rules  breast-w 95.0 96.3 28.7 307  diabetes 74.2 73.1 2.6 35  glass 68.7 81.1 1.3 49  heart 80.8 83.2 10.9 528  iris 95.3 96.1 0.4 19  pima 75.5 73.0 1.2 25 Average  value 81.58 83.8 7.52 160.5    ?.  CONCLUSION  To sum up, it is effective to propose a classified association rule method based on incremental lattice construction and sync calculations of intent reduction.

Compared with that in references [3] and [5], this algorithm is improved in the following two aspects. Firstly, the  range of nodes for computing intent reduction is restricted, namely, only nodes related to computation and classified rule extraction are needed. Secondly, since incremental computation was carried out ahead for the needed intent reduction, the extraction of these two classification rules can be given according to the foregoing definitions, thus improving the efficiency of rule extraction. In future, more data sets are to be tested to further improve the proposed method in this study..

