Int?l Conf. on Computer & Communication Technology??????????

Abstract? Feature Selection is a preprocessing step that has optimization effect in data mining. The feature set of a dataset generally contains redundant or irrelevant features in order to avoid the risk of incomplete description of instances and to provide utility to different purposes of the dataset. This may lead to an inefficient Classification rule mining process that bears with memory and time overhead. Recently developed Associative Classifiers like CBA, CMAR and CPAR are almost equal in accuracy and have outperformed traditional classifiers. CPAR has been found to be most consistently generating results with good average accuracy. So, it is selected to compare the suitability of four popular feature selection methods: GGA, SSGA, LVW and MIFS for classification of data.  The Genetic algorithm is found to be the most suitable.

Keywords- feature selection; associative classification; CBA; CMAR; CPAR; GGA; SSGA; LVW; MIFS

I. INTRODUCTION Classification is a Machine Learning technique that  requires datasets containing many instances. Classification approach uses a dataset in two parts as ? training set and test set. In training set, each instance is defined with many features (attributes) and a class label. It is used for building the classifier. The test set is used to test performance of the classifier. A dataset may contain many impurities that affect the efficiency of a classifier. Hence, it requires preprocessing like filtering, transforming, summarizing, etc.

Feature Selection is method of filtering a dataset in which a subset of features or attributes are selected out of original feature set for reducing the dataset to make it more relevant to the concept of classification. While creating a dataset, relevant features are generally unknown a priori. There may be many candidate features that represent the domain. Hence, the feature set may contain many partially or completely irrelevant attributes and redundant attributes. Such features increase the size of dataset and interfere in the efficiency and accuracy of classifier. It also leads to processing cycle and time overhead. It also compels generation of large sized test sets.

Feature Selection is important both to speedup learning and to improve concept quality. Irrelevant features degrade the performance of concept learners both in speed (due to high dimensionality) and predictive accuracy (due to irrelevant information), especially in large dataset. Feature selection methods use various approaches to select a feature and test its relevancy to the target concept. Generally a  features is taken as irrelevant if on removing it, the classification accuracy does not decrease significantly or if the resulting class distribution is nearly same as the original class distribution in the test set. A general Feature Selection process is shown in figure 1.

Original Feature set  Subset                    Feature subset           Evaluation for Generation                                                    irrelevance or redundancy  Goodness of the subset   Stopping No                                      Criteria   Yes   Validation w.r.t.

other methods  Figure 1.    Feature Selection process  Broadly, Feature Selection approaches can be categorized as follows:  Feature Selection Approach  Complete                   Heuristic                        Random   Filter                             Wrapper  A Feature Selection approach is called complete if the search space is completely searched for an optimal subset. It may be an exhaustive search. Heuristic approach searches for a nearly best solution in incremental manner based on    Int?l Conf. on Computer & Communication Technology??????????   some simple criteria. Hence they are fast in producing results. Random approach selects candidates based on some parameter values and its optimality depends on availability of the resources. Filter approaches obtain feature subset by maximizing or minimizing a criteria function. They do not take support of any induction algorithm. Wrapper approaches are feedback oriented and evaluate feature selection using a learning algorithm in terms of predictive accuracy. The time required depends on complexity of learning algorithm for fitness evaluation.

Feature Selection methods have been studied and compared with traditional classification systems, but still they have not been tested on higher ended Associative Classification systems. In this work, Associative Classification approaches CBA [6], CMAR [7] and CPAR [8] have been studied and CPAR has been chosen to test the accuracy of popular Feature Selection methods GGA [2], SSGA [3], LVW [4] and MIFS [5].  Section II describes the feature selection methods and the Associative Classifiers are described in section III. Section IV gives the comparative study of the feature selection methods and section V concludes this work.



II. METHODS OF FEATURE SELECTION Feature selection methods taken into this work are as  follows:  A. Las Vegas with Probabilistic Wrapper(LVW) Model This algorithm [4] is a probabilistic and wrapper  approach version of Las Vegas algorithms.  Las Vegas algorithms make use of probabilistic choices to guide them more quickly to a correct solution. Las Vegas algorithms, use heuristic methods, take long time to reach to decision. LVW adopted a modified approach by adopting estimation of error rate for each subset. The smallest subset with lowest error rate is chosen as the most relevant feature subset for the target concept. It computes error of a subset both with training set and test set. And then compares with the current minimum error rate. LVW outputs every current best. LVW produces good solution or an optimal solution if number of runs is chosen to be large.

B. Mutual Information for Feature Selection (MIFS) This method [5] investigates the application of Mutual  Information criterion to evaluate a set of candidate features and to select an information subset for neural network classifiers. It uses the problem ? given an initial set F with n features, find the subset S of set F with k features that minimizes conditional entropy H(C/S) i.e. that maximizes the mutual information I(C;S).  Shannon?s information theory is adopted for suitable formalism of this target. It starts with original set of features. It computes mutual information I(C;S) for each feature of the selected subset.

Then it adds the feature f with maximum I(C;S) into the subset S. It sets F= F\{f} and S ={f}. Then iteratively MIFS selects greedily feature s from F by computing I(f;s), chooses f with maximum I(C;S)- ??  C. Generational Genetic Algorithm (GGA)  I(f;s)  and sets  F= F\{f} and S =SU{f}. S is the selected feature set.

Genetic algorithms (GA) [2] are adaptive search techniques based on biological evolution of natural selection.

Sometimes they may take hours to obtain good feature subset on large datasets. To improve this state, GGA adopted filter method that selects based on some criteria and reduced significantly the   computation time. It used inconsistency rate as the filter criteria for a subset of features. It uses binary coding that show the presence or absence of features in the subset and implements standard genetic operators, crossover and mutation.

D. Steady State Genetic Algorithm (SSGA) This method[3] has been developed for fuzzy rule based  classification system (FRBCS). FRBCS have a disadvantage that there is exponential growth of fuzzy rule search space with increase in number of features in training process.

Hence it has big efficiency degradation with datasets. To counter this overhead, this method uses a precision measure provided by the k-NN considering only those features that are included in the candidate feature subset . Candidate subdset is selected with any  GA. It uses integer coding scheme with a fixed cardinality. In fact, it combines both filter and wrapper approaches in two steps. First it uses filter approach to find cardinality for the feature subset with minimum number of inconsistencies.Second, the cardinality is used as chromosome length for a wrapper based feature selection process. It provides a variable subset. It used k- nearest neighbour estimation which is very sensitive to irrelevant variables.



III. ASSOCIATIVE CLASSIFICATION TECHNIQUES  Associative Classification is the integration of Association Rule Mining and Classification Rule Mining by focusing on a special subset of association rules whose right-hand-side are restricted to the classification class attribute. Associative classifiers had been found [8] [9] to be better than C4.5 and other traditional classifiers in terms of accuracy and dataset coverage. Associative Classification consists of three steps: 1. Generation of frequent item sets or association rules.

2. Selection of all the class association rules (CARs), 3. Building a classifier based on the generated CARs.

Among the associative classifiers: CBA [6], CMAR [7] and CPAR [8], CPAR had been found better and have been used in this work. But Associative Classifiers are more memory and time consuming. The techniques are described as follows:  A. Classification Based on Associations (CBA) This approach [6] gives the first algorithm that integrates Association rule mining and Classification. It uses the apriori approach to discover the association rules. Best association rules having any of targeted rules are selected based on confidence, support and size of antecedent. These rules are pruned using ?pessimistic error rate?. Finally the rule list is generated using a variation of ?cover? principle.

Int?l Conf. on Computer & Communication Technology??????????   For prediction of a new case, it uses a set of related rules by evaluating the correlation among them.

B. Classification Based onMultiple Association Rules (CMAR) CMAR [7] algorithm uses the FP-growth approach to  find association rules. The classification rules are stored in a prefix tree data structure, known as a CR-tree. CR-tree makes effective rule storage and speed retrieval of rules in the classifier.  For classifying a new object, the subset of classification rules matching the new object is observed at their class labels. In the case where all rules have a common class, CMAR simply assigns that class to the test object. In cases the classes of the accumulated rules are not identical, the rules are divided into separate groups based on their class values and the effects of every group are compared to identify the strongest one. The strength of the groups is measured by the weighted ?2. The class of the strongest group is assigned to the test object.

C. Classification Based on Predictive Association Rules (CPAR) It [8] is a greedy associative classification approach.

The best rule condition is measured by FOILgain [10] of the rules generated among the available ones in the dataset.

FOILgain is used to measure the information gained from adding a condition to the current rule. Once the condition is identified, the weights of the positive examples associated with it are reduced by a multiplying factor, and the process repeats until all positive examples in the training data set are covered. CPAR generates rules directly from training data instead of generating CARs. It generates and tests more rules to avoid missing important rules. Whereas other approaches select the best rule for prediction, it uses best k rules in prediction in order to avoid over fitting. In the rule- generation process, CPAR is capable of deriving not only the best condition but also all similar ones. It includes the rules with similar gains.



IV. EXPERIMENT In order to study empirically the performance of various  Feature Selection approaches on Associative Classification, some easily available Feature Selection methods are selected. KEEL [11], a data mining software tool was used for discretizing the datasets. The datasets are also available with this tool. The datasets1 we used are as shown in Table 1. The methods LVM and SSGA can be directly applied on undiscretized dataset whereas GGA and MIFS require discretized dataset for their application. Number of features to be selected is user-defined in MIFS.

1In each dataset, the data-10-3tra.dat file of data is used in the experiment  TABLE I. THE DATASETS  S.no. Datasets Size Classes Attributes 1 Iris 150 3 4 2 Wine 143 3 13 3 Glass 191 6 10 4 cleve 268 5 13 5 Breast 614 2 10 6 Pima 692 2 8  Associative Classifiers CBA, CMAR and CPAR can be applied only on discretized data. MDLP [12] discretization approach was adopted on the datasets, either before or after feature selection of the datasets depending on the requirements of the feature selection methods. The performances of the three associative classifiers on different datasets with the MDLP discretizer was empirically tested [13] with the result obtained as shown in table2.

TABLE II. PERFORMANCE WITH MDLP  Figure 2.   Accuracy with MDLP  It can be observed that CBA and CMAR are failing to generate any rule in some cases. According to current observation and [14], the best accuracy seems to swing among CBA, CMAR and CPAR with change of dataset.

CPAR is consistently giving relatively good performance in terms of accuracy. Also the number of generated rules by  Datas ets  CBA CMAR CPAR  Accurac y  No.

of Rule  Accurac y  No. of Rules  Accurac y  No. of Rules  Iris 33.33 2 93.33 16 33.33 4  Wine 0 2 74.29 79 47.89 3  Glass 0 0 0 0 36.84 4  cleve 55.97 14 53.14 168 52.24 11 haber man 54.01 2 53.46 4 73.72 2  Breast 96.42 17 94.46 112 95.77 11  Pima 75.43 10 44.94 28 66.47 11 Aver age  45.023 59.089 58.037    Int?l Conf. on Computer & Communication Technology??????????   CPAR is neither too large nor too small in comparison to the other two. CPAR seems to be much balanced with respect to number of rules and accuracy. Hence CPAR is adopted for our target test.

On applying the feature selection methods on the datasets in table 1, the number of features obtained in reduced dataset is shown in table 3.

TABLE III. NO OF FEATURES IN REDUCED SET  Application of CPAR on the above dataset produced classification accuracy as shown in table 4.

TABLE IV. ACCURACIES OF CPAR W.R.T. DISCRETIZERS  Figure 3: Accuracies of CPAR w.r.t. Discretizers From the above table and corresponding graph, it can be observed that GGA is giving best performance with CPAR.

MIFS is also showing relatively good results.



V. CONCLUSION AND DISCUSSION This is an investigative work aimed at finding a suitable  combination of feature selection and classification methods in order to get high performance in classification applications. Associative classifiers had been found to be better than C4.5 and other traditional classifiers in terms of accuracy and dataset coverage. Though CBA, CMAR and CPAR have been found similar in accuracy and one shows superiority from the others with respect to datasets, CPAR have been found give little better average accuracy. This work also finds a similar trend. Also CPAR is found to be more reliable to churn out rules. Along with accuracy, the number of rules generated by CPAR is more balanced for being applied in problem solutions. But Associative Classifiers are more memory and time consuming.

Feature Selection provides a way to remove irrelevant features and reduce the memory and time disadvantage considerably. It can also be concluded that in order to choose a high performance classifier for obtaining good results on some target application dataset, CPAR along with Generational Genetic Algorithm (GGA) is providing a promising combination for an application oriented project.

Dataset was applied with four types of feature selection approaches: Probabilistic wrapper, Heuristic, Genetic filter, Genetic filter-wrapper. Genetic filter method (GGA) is showing highest performance (even improved performance) with CPAR. It can also be observed that MIFS is also providing competitive performance to GGA. But number of features to be selected by MIFS is user-defined, so its reliability to produce a proper subset of relevant features gets questioned.

As Genetic algorithms are generally slow and Heuristic methods are much faster. For a static application, one time application of GGA is acceptable. For dynamic applications that may deal with classification of many datasets, faster method of MIFS may be more suitable for feature selection.

