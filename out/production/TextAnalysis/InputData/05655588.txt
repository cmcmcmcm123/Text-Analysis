An Adaptive Method for Discovering Maximal Frequent Itemsets  To large databases

Abstract   A novel adaptive method included two phases for discovering maximal frequent itemsets is roposed. A flexible hybrid search method is given, which exploits key advantages of both the top-down strategy and the bottomup strategy. Information gathered in the bottom- up can be used to prune in the other top-down direction. Some efficient decomposition and pruning strategies are implied, which can reduce the original search space rapidly in the iterations. The compressed bitmap technique is employed in the counting of itemsets support. According to the big space requirement for the saving of intact bitmap, each bit vector is partitioned into some blocks, and hence every bit block is encoded as a shorter symbol. Therefore the original bitmap is impacted efficiently. Experimental and analytical results are presented in the end.

Key words: frequent items, apriori, support count.

Association rule mining, Datamining,  1.Introduction   Frequent itemsets mining was first proposed by  Agrawal[1] for market basket analysis in the form of association rule mining. A major challenge in mining frequent itemsets from a large data set is the fact that such mining often generates a huge number of patterns satisfying the min_sup threshold, especially when min_sup is set low. This is because if a pattern is frequent, each of its subsets is frequent as well. A large pattern will contain an exponential number of smaller, frequent subpatterns. To overcome this problem, closed frequent itemsets(CFI) and maximal frequent itemsets(MFI) mining were proposed[2]. An itemset _ is a maximal frequent itemset (or max-itemset) in set D if _ is frequent, and there exists no super-pattern _ such that _ ? ? _ and _ is frequent in D. For the same min_sup threshold, the set of max-itemsets, which is more compact, contains the complete information regarding to its corresponding frequent itemsets.

1.1 Existing methods   In this paper first discussed on association rule mining, compared some algorithms on frequent itemsets mining. Frequent itemsets mining is first proposed by Agrawal[1] for market basket analysis in the form of association rule mining. A major challenge in mining frequent itemsets from a large data set is the fact that such mining often generates a huge number of patterns satisfying the min_sup threshold, especially when min_sup is set low. This is because if a pattern is frequent, each of its subsets is frequent as well. A large pattern will contain an exponential number of smaller, frequent sub patterns.

To achieve this problem, closed frequent itemsets   CFI) and maximal frequent itemsets (MFI) mining ere proposed [2]. An itemset ? is a maximal requent itemset (or max-itemset) in set D if ? is frequent, and there exists no super-pattern ? such that ? ? ? ? and ? is frequent in D. For the same min_sup threshold, the set of max-itemsets, which is more compact, contains the complete information regarding to its corresponding frequent itemsets. Mining max- itemsets was first studied by Bayardo[3], where an Apriori-based, level-wise, breadth first search method was proposed to find max-itemset by performing superset frequency pruning and subset infrequency pruning for search space reduction.

The above  zlgorithms approach is bottom up,  but the Pincer Search algorithms approach is both the bottom up and top down method. Pincer- Search[4] uses horizontal data format. It not only constructs the candidates in a bottom-up manner like Apriori, but also starts a top-down search at the same time, maintaining a candidate set of maximal patterns. Depth-Project [5] finds long itemsets using a depth first search of a lexicographic tree of itemsets, and uses a counting method based on transaction projections along its branches.

DOI 10.1109/ARTCom.2010.56       Mafia [6] is another efficient method formining the MFI, which uses three pruningstrategies to remove non-maximal sets and a vertical bit-vector data format to improve performance. Both Depth Project and Mafia mine a superset of the MFI, and require a post pruning toeliminate non-maximal patterns this algorithm isespecially efficient when the itemsets in the database are very long..

FP-Growth[7] uses the novel frequent pattern tree (FP-tree)structure, which is a compressed representation of all the transactions in the database, and a recursive divide-and conquers and database projection approach to mine long patterns. A portioning-based, divide and conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space.

The complexity of enumerating maximal itemsets  is shown to be NP-hard. Ramesh et al.[9]characterized the length distribution of frequent and maximal frequent itemset collections.

1.2 Proposed System   A modern adaptive approach method proposed included two phases for discovering maximal frequent itemsets is proposed. A flexible hybrid search method is given, which exploits key advantages of both the top- down strategy and the bottom up strategy. Information gathered in the bottom-up can be used to prune in the other topdown direction. Some efficient decomposition and pruning strategies are implied, which can reduce the original search space rapidly in the iterations. The compressed bitmap technique is employed in the counting of itemsets support. According to the big space requirement for the saving of intact bitmap, each bit vector is partitioned into some blocks, and hence every bit block is encoded as a shorter symbol.

Therefore the original bitmap is impacted efficiently..

1.2 Contributions  Firstly, support counting is a confessed bottleneck in the association rules mining, which requires a great I/O and computing cost. A novel compressed bitmap index technique to speed up the counting process is employed.

The presented algorithm could reduce both the number oftimes the database is scanned and the search space rapidly. Secondly, a flexible hybrid search method is given, which exploits key advantages of both top-down and bottom-up strategies.  The rest of this paper is organized as follows. Section 2 discusses the problem of itemset support counting based on compressed bitmap technology. Section 3 introduces the description and decomposition strategies of search space. Section 4  presents the algorithm and some pruning strategies. The feasible optimizations and experimental results are presented Section 5. Finally, we conclude this paper in Section 6.

2. Itemset Support Counting  The bitmap technique was proposed in the 1960?s, and has been used by a variety of products. A typical context is the modern relational DBMS[10]. It also has been applied in association mining. The key idea of the approach is to use a bitmap index to determine which transactions contain which itemsets. Each transaction has one unique offset position in the bitmap. A bit vector ) X,bit=(b1,b2 bn)  is associated to each itemset X.

In X.Bit . the ith bit i bi is set to 1 if the transaction I contains the itemset X, and otherwise i b is set to 0. It should be noted that i b is set to null if itemsets or transaction does not exist. The ordered collection of these bit vectors composes the bitmap. In a bitmap, a line represents a transaction, while a column corresponds to a given k-itemset. During the supports counting, Intensively manipulates bit vectors requires a lot of disk space and main memory. So, it is necessary to compress the bitmap in advance. A new block strategy is proposed to encode and decode the bitmap, which is similar to the pagination technology in operating systems. In this approach, every bit vector is partitioned into fractions, called blocks, that can be encoded respectively, so that a bitmap is divided into granules. Each block should have an appropriate size, if the size is too small, the impact is not remarkable; otherwise the encoding is not straightforward. In order to take full advantage of Logical Calculation Units, the block size should be an exponential to 2. Each block is represented as  (p:W), p is the number of the block, and W is the block bit vector. Let l be the block size, m the number of transactions in database D. In this way each bit vector B=(b1,b2,?bi?bm) can be partitioned into p= INT( mll)  blocks ( INT is the integer function). The kth block vector  Wk=(w1,w2?.wj),j=MOD(I,l)=i-i*(k-l) (MOD is the mode function)  Encoding each block as a shorter code can reduce the space demanding. The encoding principle which should be conformed is that each block can be represented uniquely. As a part of the initial bit vector B, each block vector W is also a binary bit vector. The conversion between binary, octal, decimal and hexadecimal can be implemented conveniently, hereby every block can be represented a binary, octal, decimal or hexadecimal code. We use a hexadecimal code, i.e. every four bits in a block encode a hexadecimal code. As each itemset is associated to a binary bit vector, the support of a given itemset is the total number of 1 in the vector. For the sake of efficient counting the number of 1, we previously store the binary block in a bit array Bit[1..l] (l is the block size), and the hexadecimal       blocks in an array ABit[1..p] (p is the number of blocks). The value in  ABit[i] is the hexadecimal code of the ith block. Implementation of this support counting algorithm follows.

2.1 Proposed algorithm to Itemset support Counting  Algorithm 1 Itemsets Support Counting Algorithm Countsupport(X1, X2) Begin Support=0; For (i=1; i=p; i++) do If X1.ABit[i]<>0 and X2.ABit[i]<>0 then For (j=1; j=l; j++) do X1.Bit[j]<>= X2. Bit[j]& X2. Bit[j]; Support+= X.Bit[j]; Endfor;

X.ABit[i]= X1. ABit[i]& X2. ABit[i]; Else

X.ABit[i]=0; endif;   end;  3. Description of the Search Space   With only a limited amount of main memory in practice, we should decompose the original search space into some smaller pieces, such that each one can be solved independently in main memory. Following the above description and partition mechanisms, the original enormous search space could be partitioned into some little ones as flexibly as possible.

Furthermore, theorem1 can be used to prune the search space. The search space for item set I={a,b,c,d} is S=[:I]. It can be partitioned into some little ones step by step. The iterative results of a hierarchy of search space are shown in figure 1.

For a given item set I in database D and the minimum support threshold min_sup, the task of mining FI or MFI is in the follow: finding the set (x\x?[:l] and Supp(x)>=minsup) in the search space S=[:I].

4. Discovering Max-itemsets  4.1 Search and Pruning Strategy In general, it is possible to search for the maxitemsets either top-down or bottom-up. The bottom-up approach is good for the case when all max-itemsets areshort, and the top-down approach is good when all maxitemsetsare long. If some max-itemsets are long and some are short in the mining database, then both search approaches will not be efficient. A key idea of our hybrid approach is the use of information gathered in the search in the bottom-up to prune search space during the top down search. It uses infrequent itemsets found in the search in the bottom-up to prune search space during the top- down search For search space S=[X:Y], if the number of infrequent itemsets containing X is large, the scale of S will be reduced rapidly.

4.2 The Hybrid Max-Itemsets Search Algorithm There are two phases in this hybrid approach: the search in bottom-up direction and the other in top- downdirection for every pass. Max-itemsets are enumerated in both bottom-up and top-down directions.

Consider a pass k, the set of frequent k-itemsets Lk and the set of infrequent k-itemsets ~Lk are to be classified in the bottom-up direction. This procedure repeatedly uses Apriori-gen algorithm to generatecandidates like the Apriori[1]. During the kth pass, every search space S? ?[X:Y] where X is an itemset of size k-1 can be decomposed into some little pieces, whose ancestors are k-itemsets. For search space S? [X:Y], the top-down procedure check whether the border element (i.e.  XUY ) of S is frequent firstly, if not, S is decomposed.

Implementation of this hybrid approach is shown in algorithm2.

Algorithm2. Algorithm for Max-itemsets Mining Procedure: MFI Search (Transaction Set: D, Item               5. Experimental and Analytical Results   In this algorithm bitmap technology is used to count the support of every itemset instead of scanning the entire transaction database. Figure 2 shows the relative times at varying numbers of transactions for databases where the average size of transactions is 10 and the average size of potential max-itemsets is 4.

When the average size of transactions or the average size of max-itemsets increase, there has much more itemsets (or search spaces) to be tested. therefore the  total time will increase. Figure 3 shows the relative times of this hybrid algorithm at varying minimal supports on the datasets of T15.I8.D10K.

To illustrate expandability of this algorithm, we performed an experiment varying the database size from 5K to 20K. The average size of transactions is 10, and the average size of potential max-itemsets is 6. For the experiment we fixed a minimum support of 4%.

Figure 4 shows the result for the datasets.

As we have shown, there are three search strategies for discovery MFI. Figure 5 shows the relative times of the three approaches for the tests at varying minimal supports on T10.I6.D10K. From the experiments, we could see that when minimal support is greater than 2%, the performances of the bottom-up is little better than the hybrid. The main reason is that the number of itemsets generated is small with the increasing of minimal support. As the minimal support decreases, MFI becomes longer, which results in an increase in the number of counting itemsets. In such a case, the hybrid has performances. We can also see performance of the bottom-up approach is lower than the others. The most primary factor is almost all the max-itemsets are expected to not be long in this T10.I6.D10K dataset.

The experiment illustrates the fact that top-down search might be efficient for the long maxi temsets.

6. Conclusions  We use an improved compacting bitmaps database format. Support of itemset can be counted by means of binary bit vectors intersections, which minimizes the I/O and computing cost. To reduce the disk and main memory space demanding, we break the bitmap down into some little blocks, which can be encoded as a shorter code. The blocks of bitmaps are fairly adaptable.

Hence the additional space decreases rapidly. The hybrid approach exploits key advantages of both the top-down strategy and the bottom-up ones, which can discovery both longer max-itemsets and the shorter ones in earlier passes. And the infrequent (or frequent) itemsets discovered in the bottom-up can also be used to prune the search space in the other top-down direction. Furthermore, this algorithm can be parallelized easily on this hierarchical search space organization. We note that using ~Lk to prune the search space is not the only technique. If  it would be more efficient to decompose and prune the search space using Lk rather than ~Lk.

Too many candidate itemsets and a large database would create a performance bottleneck,  if we apply the pipeline methodology using hardware.  So we effectively reduce the frequency of loading the database into the hardware.

Acknowledgements   Thnks very much to prof Fu-zan Chen,and prof Min-qiang Li published lot of research work in this rea.

