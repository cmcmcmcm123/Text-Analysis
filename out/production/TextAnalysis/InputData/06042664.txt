A Comorbidity-based Recommendation Engine for Disease Prediction

Abstract  A recommendation engine for disease prediction that combines clustering and association analysis techniques is proposed. The system produces local prediction models, specialized on subgroups of similar patients by using the past patient medical history, to determine the set of possi- ble illnesses an individual could develop. Each model is generated by using the set of frequent diseases that contem- porarily appear in the same patient. The illnesses a patient could likely be affected in the future are obtained by consid- ering the items induced by high confidence rules generated by the frequent diseases. Experimental results show that the proposed approach is a feasible way to diagnose diseases.

1. Introduction  In the last few years we are witnessing to an increasing  interest in the application of computational science methods  to health care information and management systems. The  utilization of information technologies that could signifi-  cantly improve efficiency and effectiveness of health care  strategies are very important because of the implications  they could have in every day life of individuals.

An emerging viewpoint aims at identifying prospective  health care models to determine the risk for individuals to  develop specific diseases [17]. In fact, prevention or inter-  vention at the disease?s earliest onsets allow advantages for  both the patient, in terms of life quality, and the medicare  system, in terms of costs. However, recognizing the origin  of an illness is not an easy task because it can be generated  by multiple causes. Physicians prescribe laboratory tests  only after the appearance of patient?s complains, and use  family and health history to assess the hypothesized prob-  lem. The approach is thus reactive, i.e. a medical treatment  is undertaken only after the patient has already developed  the disease, rather than proactive.

Hospitals and physicians, however, collect thousands of  patient clinical histories that include valuable information  regarding illness correlations and development. The patient  medical records contain important enlightenment regarding  the co-occurrences of diseases affecting the same individ-  ual. A comorbidity relationship between two illnesses ex-  ists whenever they appear simultaneously in a patient more  than chance alone [11]. Although comorbidity is very com-  mon in the population and its extension increases with age,  few investigations have been conducted on patient?s comor-  bid conditions [5]. The comorbidity relationships between  diseases, however, could be exploited to build a model that  predicts the diseases a patient could have in the future.

Advanced risk assessment tools are currently at disposal,  mainly based on statistical techniques [7, 8]. Another ap-  proach for addressing the problem, which is gaining in-  creasing interest, is the use of methodologies coming from  the fields of knowledge discovery [19].

Among the most recent proposals coming from this re-  search field, Davis et al. [6, 3] have been the first that used  patient clinical history for disease prediction. They built  a collaborative assessment and recommendation engine,  based on the ICD-9-CM codes, to predict future diseases.

The engine relies on the collaborative filtering methodol-  ogy [16] used for producing recommendations to people by  collecting preferences from users having similar behaviors.

A patient is characterized by a vector of diagnosed diseases  and a prediction is made on the base of other similar pa-  tients. The similarity function adopted includes the inverse  frequency of diseases to reduce the weights of very com-  mon sicknesses. In order to apply the collaborative filtering  technique, the training set of patients is reduced by remov-  ing all those patients having one or no disease in common  with the active patient.

Steinhaeuser and Chawla [18] used a hybrid technique  based on collaborative filtering and nearest neighbor classi-  fication. The similarity between two patients is computed  with the Jaccard coefficient [1, 12], which is the normaliza-  978-1-4244-9166-7/10/ $26.00 c?2010 IEEE 6    tion of common diseases that two patients have, with respect to their union. Given a patient, the k most similar patients are selected to make a prediction. They found that almost the 42% of diseases were predicted as expected. A disease network is also built and their structural properties studied.

In this paper we propose a recommendation engine for disease prediction that combines clustering and asso- ciation analysis techniques. The system, named CORE (COmorbidity-based Recommendation Engine), extends the approach proposed in [9] by introducing a clustering phase on the data set of patient records that allows the generation of local, more specialized and accurate prediction models, instead of a general, global model. CORE uses the past patient medical history for generating models able to de- termine the risk of individuals to develop future diseases.

Analogously to Davis et al. [3], a patient is represented by the set of ICD-9-CM codes of diagnosed diseases, and a disease is predicted by comparing a patient with individuals having a similar clinical history. However, differently from their approach, we use association analysis [19] to generate a disease predictive model composed by more models, each specific to a particular patient profile. The model is built by using the set of frequent diseases that contemporarily appear in the same patient. The diseases the patient could likely be affected in the future are obtained by consider- ing the items induced by high confidence rules generated by recurring disease patterns. The medical record of a pa- tient is then compared with the patterns discovered by the model, and a set of illnesses is predicted. The approach is similar to that used in recommendation systems from web usage data, where given the pages visited by a user during a web session, a recommendation value for the next page the user will probably visit is computed on the base of be- havioral profiles induced on groups of users sharing similar navigational habits [14, 15]. Experimental results show that the approach is a promising method to predict individual diseases by taking into account only the illnesses a patient had in the past, and that the specialization of the models on group of similar patients increases the prediction accuracy.

The paper is organized as follows. The next section briefly describes the data set used. In section 3 the CORE system is described. Section 4, finally, reports the evalua- tion of the proposed approach on a data set of patient medi- cal records.

2. Data description  The data set consists of medical records of 1462 patients of a small town in the south of Italy. Each record contains a unique patient identifiers, date of birth, the gender, and the list of disease codes with the date of the visit in which that disease has been diagnosed. The disease codes are those de- fined by the International Classification of Diseases, Ninth  Figure 1: An overview of the CORE system.

Revision, Clinical Modification (ICD-9-CM). The Interna- tional Classification of Diseases (ICD) and Related Health Problems supplies codes to classify diseases and a wide va- riety of signs. Every health condition is associated with a unique category and given a code, up to five digits long. The first three digits constitute the principal diagnosis, while the other two identify secondary diagnoses. The ICD is pub- lished by the World Health Organization and used world- wide for morbidity and mortality statistics, reimbursement systems and automated decision support in medicine. The data is completely anonymized, thus there is no way to iden- tify the patients. In our database the number of diagnoses are 8768 spanning from 1990 to 2009. From an analysis of the patient records, we found that the raw data contained some disease not informative for our study. These diagnoses have thus been eliminated. Some patients had no or only one diagnosis. These patients have been discarded because not useful. After this preprocessing phase, the database re- duced to 1105 patients and the number of diseases was 972.

However, the number of diseases was still too high. As de- scribed above, the first three digits of a code denote the gen- eral diagnosis. Even if some details can be missed, these three digits are sufficiently informative to study the disease correlations. Thus, the five digits ICD-9-CM codes have been collapsed to these first three digits, in such a way the number of diseases was reduced to 330.

In the next section we first give an overview of the sys- tem architecture proposed to perform disease prediction, then a description of each embedded module is reported.

3 A Framework for Disease Prediction  We first give an overview of the system, then a descrip- tion of each module is reported. The CORE prediction sys- tem, as depicted in Figure 1, consists of two main com- ponents: an off-line component for the model generation, and an on-line component for the disease prediction. The     model generation component involves a preprocessing step  to transform the raw data to a transactional set of patient  records constituted by a sequence of ICD-9-CM codes. i.e.

the list of diseases a patient had. Then, clustering is per-  formed to group patients on the base of the diseases they  share, and a representative is generated for each cluster  found. The representative computation is a very important  step since it is used by the prediction module to decide the  right model to apply for foreseeing next diseases of the cur-  rent patient. After that, frequent patterns for each cluster  are computed and a model is generated for each group. Per-  taining the on-line component, it first assigns a patient to a  cluster by matching him against each cluster representative,  then the prediction model associated with this cluster is se-  lected, and finally the next expected diseases are released  by applying this model.

It is worth to notice that the system in Figure 1 is a  general predictive architecture, parametric with respect to  both the clustering algorithm and the prediction model used.

Therefore, by suitably customizing the algorithms for the  kind of data at disposal, this architecture could be profitably  exploited also in different scenarios. In the following a de-  tailed description of each single component in the architec-  ture is provided.

3.1. Model Generation  Data Preparation. Let m be the number of patients contained in the original data set of patient histories. The  data preparation module transforms the original data set  into a new data set T = {t1, . . . , tm}, where each ti is a patient medical record of variable size constituted by a  sequence of ICD-9-CM disease codes. Thus T summarizes the medical histories relative to all the m patients.

Clustering. The main motivation for grouping the set T of patients sharing most of their disease history, it that it is an  effective way of improving the accuracy of the prediction  model, as experimental results will show. Performing clus-  tering is not an easy task at all, since its performances are  tightly related to the kind of method used. For the purposes  of this paper, we decided to exploit a straightforward vari-  ant of the traditional k-means clustering [10, 13] able to deal with categorical tuples of variable size, like those present in  the dataset T .

For a given parameter k, this algorithm partitions T into  k clusters C = {C1, . . . ,Ck} in a way that high intra-cluster similarity and low inter-cluster similarity are guaranteed. C is a partitioning of T , i.e.,  T i=1..k Ci = /0 and  S i=1..k Ci = T .

Each record ti ? T is assigned to a cluster Cj according to its distance d(ti,r j) from a vector r j that represents the cluster at hand, and is called the representative of the cluster. For- mally, the clustering algorithm finds a partition C such that:  1. for each Ci the representative ri is computed  2. ti ?Cj iff d(ti,r j) < d(ti,rl) for 1? l ? k, j ?= l  3. C minimizes the cost function Qk = ?ki=1 ?t j?Ci d(t j,ri)  Essentially, the algorithm works as follows. Firstly, k records are selected from T randomly. They represent the initial cluster centers, and each other ti ? T is assigned to a cluster on the base of condition 2). Then, the algorithm up-  dates the representative of each cluster and re-assigns each  record consequently. The iterations terminate when the rep-  resentatives do not change any more, i.e., the condition 3)  holds.

It is worth to note that the schema above is paramet-  ric w.r.t. the definitions of distance d and representative r.

Since in our scenario we deal with categorical data, we used  a kind of distance that proved to work very well in this set-  ting: the Jaccard distance. This measure is derived by the Jaccard coefficient [1, 12] which is based on the idea that the similarity between two itemsets is directly proportional  to the number of their common items and inversely propor-  tional to the number of different ones. Therefore, given two  records ti and t j ? T , the Jaccard distance can be defined as:  d(ti, t j) = 1? |ti? t j| |ti? t j|  The next step pertains a suitable definition for the cluster  representative. Intuitively, the representative should model  the content of the cluster in order to make trivial the inter-  pretation of the cluster itself. Among various possibilities,  an easy and effective way for building the representative  consists in using the frequent items belonging to the clus-  ter. The frequency degree can be controlled by introducing  a user-defined threshold value ? representing the minimum percentage of occurrences an item must have for being in-  serted into the cluster representative. More formally, given  TCi = {t1, . . . , tq} the set of records belonging to the cluster Ci, DCi =  S i ti = {d1, . . . ,dp} the set of items of Ci, i.e. the  disease codes, and ? ? [0,1], then the representative rCi for Ci can be computed as follows:  rCi = {d ? DCi | f (d,TCi)/q? ?} (1)  where f (d,TCi) = |{ti ? TCi |d ? ti}| is the number of medical records of cluster Ci in which d appears.

Clearly, the clustering algorithm assumes that the  number of clusters k has to be fixed at the beginning. Thus, another open issue is how to set k in order to obtain the best partitioning. Ideally, the best partitioning is achieved for  the value k? in correspondence of which the cost function Qk has its global minimum. However, finding k? could be unfeasible in practice. Therefore, we pragmatically     recurred to a sub-optimal solution: we iterated the cluster- ing algorithm by ranging k in [1, |T |] until the first, local minimum for Qk is reached.

Prediction Model Generation. A disease prediction model DPM for the dataset T can be defined as a couple DPM = ?C,M?, where C = {C1, . . .Ck} is a clustering of T and M = {M1, . . .Mk}, where each Mi is the prediction model built on top of Ci.

In order to build a prediction model for each cluster Ci, we follow the same approach introduced in [9], that exploits association analysis [19] for inducing a pattern-based pre- diction schema able to generate predictions about the dis- eases a patient can incur in the future, given the past his- tory of his health conditions. The main difference with the previous approach is that, in this paper, we deal with ?lo- cal? models instead of a unique, global model built on the whole dataset. Intuitively, and as validated by experimental results, local models tend to produce better prediction accu- racy because the predictions are generated by considering the most similar individuals of the patient under examina- tion.

Employing association analysis for prediction purposes is not new in the data mining literature. It relies on the concept of frequent itemsets to extract strong correlations among the items constituting the data set to study. Origi- nally, association analysis has been applied to market bas- ket data, where each item represents the purchase done by a customer. However, it can be easily transposed into the medical context by associating an item with a disease, and by considering an itemset as the set of diseases a patient had along his life until the present. For extracting patterns, we apply the well-known Apriori algorithm [2] that effi- ciently searches for frequent itemsets by cutting the expo- nential search space of candidate itemsets. The concept of frequency is formalized through the concept of support.

Given a set ICi = {i1, . . . , il} of frequent itemsets induced on a cluster Ci = {t1, . . . , tp}, the support of an itemset i j ? ICi , ?(i j), is defined as:  ?(i j) = | {ti | i j ? ti, ti ?Ci} |  |Ci |  where | . | denotes the number of elements in a certain set.

The support, thus, determines how often a group of diseases appear together. It is a very important measure because very low support discriminates those groups of items occurring only by chance. Thus a frequent itemset, to be considered interesting, must have a support greater than a fixed thresh- old value minsup.

An association rule is an implication expression of the form X ? Y , where X and Y are disjoint itemsets. The importance of an association rule is measured by both its support and con f idence values. The support of a rule  is computed as the support of the X ? Y and tells how often a rule is applicable. The confidence is defined as ?(X ?Y )/?(X), and determines how frequently items in Y appear in transactions that contain X . Frequent item- sets having a support value above a minimum threshold are used to extract high confidence rules that can be exploited to build a prediction model by matching the medical record of a patient against the patterns discovered by the model.

In our scenario, the support determines how often a group of diseases appears together, while a rule like X ? {d} (where X is a set of frequent diseases and d is new dis- ease) having a high confidence, allows to reliably infer that d will appear along with the diseases contained in X .

3.2. Disease Prediction  Once the Mi models are built for each discovered cluster Ci, performing the prediction is rather straightforward. The next likely diseases are computed by the disease prediction component (see the CORE architecture in Figure 1) at the time a new patient arrives. The prediction phase encom- passes three main tasks:  ? Cluster Assignment, where the patient is recognized as member of a cluster by matching him against each cluster representative;  ? Model Selection, where the model Mi (relative to the corresponding cluster) is selected;  ? Prediction, where the proper prediction is performed by exploiting Mi.

Actually, the Prediction step works in this way. We set a sliding window of fixed size w over the medical records for capturing the patient history depth used for the prediction.

A sliding window of size w means that only the last (in time order) w diseases appearing in the record influence the com- putation of possible forthcoming illnesses. Thus, fixed w, we consider the frequent itemsets of size w + 1 induced on Ci that contain the w items appearing in the current medi- cal patient record ti ?Ci. The prediction of the next disease is based on the confidence of the corresponding association rule whose antecedent are the w frequent items of ti, and the consequent is exactly the disease to be predicted. If this rule has a confidence value greater than a fixed threshold, its consequent is added to the set of predicted illnesses.

For the sake of clarity, let us perform a prediction on a medical patient record tw  i ? Ci of size w. We match twi  againsts all the frequent itemsets Iw+1 Ci  of size w+1 induced on Ci. Each itemset iw+1i ? I  w+1 Ci  containing tw i  contributes to the set of the candidate diseases with a prediction di. It is easy to note that iw+1  i = tw  i ?{di}. Finally, if the confidence     of the rule t w  i ? {di} (i.e., ?(twi ? {di})/?(twi )) is greater  than a fixed threshold ?, the disease di is considered reliable, and it is added to the set of predicted diseases.

Example. In order to explain the way our prediction ap- proach works in practice, let us consider the set T of patient  medical records reported in Figure 2.

t1 401 715 722 723  t2 401 721 715 722 723  t3 401 721 715 722  t4 241 255 595 780  t5 241 255 272 595 780  Figure 2: Set T of patient records involving some common dis-  eases.

Let us suppose k = 2 be the number of clusters that min- imizes the cost function Qk (see the discussion on clus-  tering in Section 3.1), and ? = 0.5 be the minimum per- centage of occurrences a disease must have for being in-  serted into the cluster representative (see Equation 1). On  the base of the above parameters, it is easily verifiable  that the clustering algorithm (Section 3.1) finds the clus-  ters C1 and C2, as reported in Figures 3(a) and 3(b), re-  spectively. Furthermore, the clusters are equipped with  their representatives: rC1 = {401,721,715,722,723} and rC2 = {241,255,272,595,780}. After the clusters have been built, a disease prediction model is carried out for each  cluster found.

t1 401 715 722 723  t2 401 721 715 722 723  t3 401 721 715 722  (a)  t4 241 255 595 780  t5 241 255 272 595 780  (b)  Figure 3: Cluster C1 (a) and Cluster C2 (b).

Now, let t = {401,721,715,733} be a new patient dis- ease record. Since the distance d(t,rC1) = 1?3/6 = 0.5 is lower than d(t,rC2) = 1?0/9 = 1, t is recognized belong- ing to C1, thus the model built upon C1 is exploited to per-  form the predictions. By fixing ? = 0.8, the model shown in Figure 4 is obtained.

I  I  I  I  721 (2) 721, 715 (2) 721, 715, 722 (2) 401, 721, 715, 722 (2)  715 (3) 721, 722 (2) 715, 722, 723 (2) 401, 715, 722, 723 (2)  723 (2) 715, 723 (2) 401, 721, 715 (2)  722 (3) 715, 722 (3) 401, 721, 722 (2)  401 (3) 722, 723 (2) 401, 715, 723 (2)  401, 721 (2) 401, 715, 722 (3)  401, 715 (3) 401, 722, 723 (2)  401, 723 (2)  401, 722 (3)  Figure 4: Disease risk prediction model built upon cluster C1.

If the window size w is set to 3, this means that  only the first three diseases of t are used to generate  the predictions, i.e., t 3 = {401,721,715}. By match-  ing t  against the 4-frequent itemsets I , the disease with  code 722 is candidate for being the likely, next disease  the patient t may incur in. As previously stated, the  disease 722 changes its status from candidate to pre-  dicted only if the confidence of the association rule r:  {401,721,715}? {722} is greater than the minimum con- fidence threshold ?. If we set ? = 0.8, since the confi- dence ?({401,721,715,722})/?({401,721,722}) = 1, the disease 722 is definitively added to the set of predicted ill-  nesses. Therefore, by means of the rule r, we foresee that  a patient presenting hypertension (401), spondylosis (721),  and osteoarthrosis (715), he is very likely to develop also  intervertebral disc disorders (722). ?  In the next section we show that CORE is effective in pre-  dicting diseases.

4. Experimental Results  In this section we first define the measures used to test  the effectiveness of our approach. Next, we present the re-  sults and evaluate them on the base of the introduced met-  rics. As discussed in Section 2, the dataset T we used for  the experiments consists of 1105 patient records involving  330 distinct diseases. In order to perform a fair evaluation  we applied the well-known 10-fold cross validation method  [4], i.e., the original dataset is split in 10 equal-sized parti-  tions. During each of the 10 runs, one of the partitions is  chosen for testing, while the rest of them are used for train-  ing the prediction model. The cumulative error is found by  summing up the errors for all the 10 runs. The strategy we  followed for testing our approach is detailed in the follow-  ing.

First of all, the records in the training set Ttrain are parti-  tioned in k clusters, and for each group, a distinct prediction  model Mi is built upon. Relatively to the dataset at hand, we  empirically found that k = 10 and ? = 0.5 is the setting that ensures the best possible partitioning for the dataset at hand.

A record t in the test set Ttest is first assigned to one of the  k cluster, then it is divided in two subsets of diseases. The  first subset, called headt , is used for generating predictions,  while the remaining one, referred as tailt , is used to eval-  uate the prediction. Actually, the length of headt is tightly  related to the maximum window size w allowable for each  cluster, and, intuitively, must be lower than the maximal  length of frequent itemsets mined in each cluster. For in-  stance, in this very specific case, since we verified that the  prediction models M built on clusters (also for low values  of support ?) produce frequent patterns of size at most 5, the maximum length of headt can?t exceed 4. More in gen-  eral, given a window size w, we select the first w diseases  as headt and the remaining |t|?w as tailt . If the record t belongs to the cluster Ci, the relative prediction model Mi  matches headt against all frequent patterns I w+1 Ci  for gener-  ating the candidate predictions.

Fixed the minimum confidence threshold ?, P(headt ,?) is the set containing all the candidate predictions whose confidence is greater than ?. Subsequently, the set P(headt ,?) is compared with tailt . The comparison of these sets is done by using two different metrics, namely preci- sion and recall [19]. Precision and recall are two widely used statistical measures in the data mining field. In partic- ular, precision is seen as a measure of exactness, whereas recall is a measure of completeness.

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9   ?  P re  c is  io n      w = 4  w = 3  w = 2  (a)  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9   ?  R e c a ll      w = 4  w = 3  w = 2  (b)  Figure 5: Impact of w on precision and recall measures when ? = 0.1.

By customizing these definitions to our scenario, we ex- ploited precision for assessing how accurate the provided predictions are (i.e., the proportion of relevant predictions to the total number of predictions) and recall for testing if we predicted all the diseases the patients are likely to be affected in the future (i.e, the proportion of relevant predic- tions to all diseases that should be predicted). Formally, the precision of P(headt ,?) is defined as:  precision(P(headt ,?)) = |P(headt ,?)? tailt |  |P(headt ,?)|  and the recall of P(headt ,?) as:  recall(P(headt ,?)) = |P(headt ,?)? tailt |  |tailt | The cumulative precision (recall) scores drawn in Figure  5 are computed as the mean of the precision (recall) val- ues achieved by each single record t ? Ttest over the size of Ttest . More in detail, we measured both precision and re- call by varying the threshold ? from 0.1 to 1. Moreover, in order to evaluate the impact of window size w on the qual- ity of predictions, we ranged w from 2 to 4, by considering the predictions done on just one disease unreliable. The re- sults has been obtained by fixing the overall support ? for the frequent patterns to 0.1. Notice that a low support value is necessary for ensuring an adequate length for the mined patterns also in the case of poor cluster homogeneity. As expected, the results in Figure 5(a) clearly reveal that the precision increases as a larger portions of patient medical history, i.e. an increasing number of diseases are used to compute predictions. Conversely, the recall is negatively bi- ased by larger window sizes, as pointed out by Figure 5(b).

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9   ?  F ?  m e a s u re      CORE  FPV  Figure 6: F-measure when w = 4, ? ? [0.1,1], ? = 0.1 and ? = 0.01 for CORE and FPV , respectively.

After that, for the sake of comparison, we want to show that the overall prediction performances of CORE are better than those obtained by the approach in [9] (henceforth re- ferred as FPV ), where a unique, global prediction model M built upon the overall training set Ttrain, is employed. In or- der to perform a fair comparison, we recur to a well-known metric, the F-measure [19], which is the harmonic mean be- tween precision and recall, and it is often used to examine the tradeoff between them:  F?measure = 2? precision? recall recall + precision  For this experiment we fixed, for both CORE and FPV , w = 4 and ? varying from 0.1 to 1. As regards the support value ?, it can be noted that each local model contain, on average, |Ttrain|/k records, where |Ttrain| is the size of the training set and k is the number of clusters in which Ttrain has been partitioned. Thus, since the approaches deal with     different sizes of Ttrain, in order to have a comparable num- ber of frequent itemsets mined by both, we suitably set ? to 0.1 for CORE and 0.01 for FPV . Figure 6 clearly shows the better overall performances of CORE w.r.t. FPV . This definitively proves that the specialization of the prediction  models by means of clustering is meaningful.

5 Conclusions  We presented a recommendation engine based on the  combination of clustering and association rules to generate  a predictive disease model. The system uses the past medi-  cal history of patients to determine the diseases an individ-  ual could incur in the future. Experimental results showed  that the technique can be a viable approach to disease pre-  diction. Future works aims to compare our method with  other proposals in literature, and to perform a more exten-  sive evaluation on large medical history data sets.

Acknowledgements. This work has been partially sup- ported by the project Infrastruttura tecnologica del fasci- colo sanitario elettronico, funded by Technological Inno- vation Department, Presidenza del Consiglio dei Ministri,  Italy.

