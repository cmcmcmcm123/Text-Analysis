Support for data-intensive computing with  CloudMan

Abstract - Infrastructure-as-a-Service (IaaS) compute infrastructure model has showcased its ability to transform how access to compute resources is realized; it delivered on the notion of Infrastructure-as-Code and enabled a new wave of compute adaptability. In previous work we have been developing CloudMan (usecloudman.org) as a versatile solution for enabling and managing compute clusters in cloud environments via a simple web interface or an API.

However, CloudMan only supported batch processing workloads. As the magnitude of the data produced and processed in digital form grows, the need to support big data applications in clusters in the cloud becomes more evident. In this paper, we have extended the batch processing capability of CloudMan and presenteda novel architecture for supporting big data analysis workloads in cluster-in-the-cloud environment.We also implemented the details through CloudMan using established big data platforms.



I. INTRODUCTION As the information age continues, the rate at which  data is produced is continuing its exponential growth; it is fueled by everything from news services and social media activities to sensor networks and research-driven devices.

Recent advances in biology and the advent of Next Generation Sequencing (NGS) systems is a prime example of such developments. This data growth has led to what is often referred to as the data deluge and has posed a shift in the research problems at hand[1]. Several years ago, the rate at which, for examplebiological, data was produced was a limiting research factor. Namely, biologists would generate genomic sequences one nucleotide at a time and this allowed the data to be easily stored and analyzed even on a researcher?s personal computer.

With the availability of NGS systems those days have been surpassed. Today, it is possible to sequence an entire human genome, consisting of ~3 billion nucleotides, in just a few days, at better quality, and at a fraction of cost compared to just a few years ago. This ability to generate so much data has, however, led to major challenges when dealing with storage and analytics from the data.

Although often primarily described as a challenge and an obstacle, the reality is that the availability of the increasing data volume presents enormous opportunities.

The real power of the data will not come just from the sheer volume, but from the ability to analyze it. It is thus vital to provide flexible yet accessible solutions that enable researchers and companies alike to move beyond  the data collection and step into the world of data analytics.

Although IaaS is an established approach for providing computing facilities on the cloud, many workloads still execute only in a more structured and traditional cluster computing environment where jobs are handed off to a job manager and possibly executed in parallel.As a step in this direction, we have been developing a cloud resource manager called CloudMan [2]that facilitates creation of a compute platform [3] providing clusters in the cloud environment. CloudMan iscapable of handling a range of workloads, including biological analyses (see [4]). Cloud computing in general allows compute and storage resources to be requested, provisioned, and utilized to handle the necessary scaling of a computational problem at hand. However, those resources are often provisioned as bare virtual machines and disks without application context or coordination. CloudMan helps in this regard by orchestrating all the steps required to provision a functional compute and application environment by establishing a traditional batch queue job manager atop flexible cloud resources.This setup provides a user with an accessible and functional data analysis platform that is rooted in familiar cluster-based concepts but leverages cloud resources instead.

In this paper, we describe recent advances in CloudMan where support for data intensive workloads has been added. Until now, CloudMan provisioned a functional and scalable batch queue processing system through Sun Grid Engine (SGE) compute cluster in the cloud as well as a range of bioinformatics tools as part of CloudBioLinux [5]and Galaxy[6], [7], [8].Extending the CloudMan capabilities,we have now  introduced the big- data-platform-on-demand concept by presenting a new architecture through adopting and utilizinga well established big data platform component;Hadoop [9]. In addition we have addedsupport for federated computing concept into CloudManusingHTCondor [10]. These advances empower the next wave of data analysis workloads to be seamlessly executed in the cloud environment and easily integrated with the already existing and more traditional pipelines. Support for these types of workloads also facilitates easier development of Hadoop-based tools (due to the accessibility of the cloud and the functional execution/development environment) and provision groundwork for federation of clusters across multiple clouds and/or data centers. It is worth explicitly stating that CloudMan is not limited to biological  ??????? ???????	?????? ???????????????     workloads and can just as readily be utilized for any other workload where a compute cluster is necessary.

This paper is organized as follows. In chapter II we provide an overview on the two main components namely, Hadoop and HTCodor, used in the presented architecture and provide details on how CloudMan benefits from this architecture with a brief overview of other existing solutions.  In chapter III we present our architecture and its implementation details. In chapter IV test case studies and usage have been provided. Finally, in chapter V the conclusion and future direction have been stated.



II. BACKGROUND  A. Big Data: Hadoop  Big data is a concept introduced as a result of recent explosion of available data in various fields. This also applies to sciences that were traditionally not considered data-intensive and is having a ripple effect on how research is being transformed. One possible approach for tackling big data problems comes in the form of distributed and cloud computing where use of commodity computers enables feasible and accessible solution for a variety of problem domains. These approaches are providing opportunities for researchers and companies alike to store, retrieve and interpret their data withina reasonable amount of time and budget. Hadoop is an example of such a solution; it is a framework for analyzing big data across clusters of computers using theMapReduce programming model. In this model a problem is divided into several independent smaller problemswhere each sub-problem can be solved over a single node of a cluster (the map phase). The process of solving the problem continues by collecting and merging the results from each sub-problem (thereduce phase) to shape a single output. Programs written in this style can be automatically parallelized and executed on a large cluster of commodity machines[9]. Combining this computational model with the readily available computing infrastructure and making it all accessible will go a long way toward tackling the big data problem.

B. Federated Computing: HTCondor  Complementing the notion of distributed computing is the notion of federated computing. In this model, the aim is to assemble distributed computing resources that are possibly geographically and administratively disparate into a cooperating unit. Joining of such resources makes it possible to achieve higher workload throughput and resource utilization; the aim is to utilize all the known and available resources available over a period of time without assuming high availability of those resources [15]. Joining a shared pool of resources brings other complexities such as enforcing each organization?s policy and maintaining service layer agreements. In these scenarios, flexibility is a key for managing complex system.To achieve this goal, HTCondor has introduced twoconcepts: flocking and gliding[16], [17]. In flocking scenarios, the overflow workload will be sent to other, off-site HTCondor resources to keep the runtime promises. Flocking was first introduced by the HTCondor team as a solution to submit  the work overflow to other HTCondor remote resources.

In this scenario, remote resources managed by a HTCondor component can be acquired when necessary, given they are not currently being utilized. Although flocking can fulfill off-site resource rental, it will not apply when remote resources are not managed by a HTCondor component. To solve this problem, gliding was introduced. The gliding scenario is a dynamic, on-demand deployment of HTCondor?s necessary daemons over remote resources that are otherwise not controlled by a HTCondor component. In this solution, use of resources managed by other job manager (such as SGE or PBS) can be achieved. This is accomplished through a Globus Toolkit [18] connection string, which is shared between remote resources. Although traditionally utilized across a dedicated set of resources, it is possible to envision use of federated computing across multiple clouds and/or clouds and dedicated resources. Enabling application execution environments and consequently applications to run in such environments would help minimize vendor lock-in, increase code portability, and create an equilibrium market.

C. Beyond the state of the art  Several projects exist in the field of data-intensive and/or federated computing atop cloud resources; the ones most relevant to the work being presented in this paper are presented in Table 1.

This work was, in part, supported by the Genomics Virtual Laboratory (GVL) grant from the National eResearch Collaboration Tools and Resources (NeCTAR).

TABLE I. LIST AND OVERVIEW OF PROJECTS RELATED TO THE WORK PRESENTED HERE  StarCluster [11]  StarCluster is an open source project created by STAR group at MIT that allows anyone to easily create and manage their own cluster computing environments hosted on Amazon's Elastic Compute Cloud (EC2) without needing to be a cloud expert.

The project also provides support for Hadoop based workloads.

Amazon Elastic  MapReduce [12]  Amazon?s Elastic MapReduce (EMR) is a general- purpose web service that utilizes a hosted Hadoop framework running on Amazon EC2 and Amazon Simple Storage Service (S3) allowing anyone to process data-intensive workloads on Amazon Web Services (AWS) cloud.

ConPaaS [13]  This projects provides an open source system in which resources that belong to different operators are integrated into a single homogeneous federated cloud that users can access seamlessly. ConPaaS is a PaaS layer providing a set of elastic high-level services and runtime environments, including support for MapReduce jobs, Java and PHP applications, as well as as TaskFarming service. ConPasS is compatible with AWS and OpenNebula clouds.

HTCondor [10]  HTCondor is a workload management system for compute-intensive jobs. It provides a job management mechanism that automatically chooses when and where to run the jobs. In addition to being a traditional job manager, HTCondor scavenges CPU resources from potentially federated computers and makes those available via a single job queue.

Standalone, HTCondor can also submit jobs to Amazon?s EC2 resources.

Crossbow [14]  Crossbow is a domain-specific tool that focuses on whole genome resequencing analysis. It combines Bowtie and SoapSNP with Hadoop in a fixed data analysis software pipeline to enable fast data analysis on AWS or a local workstation.

CloudMan brings forward parts of each of the projects listed in Table 1 and yet delivers a solution that caters to users not supported by any of the existing projects. At its core, CloudMan is an accessible cloud manager that establishes functional application execution environments.

It operates on a range of cloud providers (namely, AWS, OpenStack, OpenNebula, and Eucalyptus), thus making it the most versatile cloud manager available. Built with CloudBioLinux and Galaxy, it provides ready access to over 100 bioinformatics tools and hundreds of gigabytes of biological reference datasets that can be utilized via Galaxy?s web interface, Linux remote desktop, or the command line interface. Nonetheless, CloudMan is not limited to biological workloads and can also be utilized for any workload where a compute cluster is necessary.

CloudMan can be installed (via automation; see CloudBioLinux) on any Ubuntu machine image, thus instantly turning that machine image into a functional cloud cluster. Alternatively, prebuilt CloudMan images can be started without any installation via a single web- form (http://biocloudcentral.org). Once launched, CloudMan provides a web interface for managing cloud and cluster resources. Our new architecture extends the already extensive set of provided features and provides support for the new wave of applications and use cases of big data problems.



III. IMPLEMENTATION The CloudMan platform is currently available for  immediate instantiation on the AWS public cloud and is also compatible with private or academic clouds based on OpenStack, OpenNebula, or Eucalyptus middleware (visit biocloudcentral.org). Deploying CloudMan on a private cloud requires creation of a custom machine image, configuration of which is automated via CloudBioLinux configuration scripts (see http://github.com/chapmanb/ cloudbiolinux). An example of such a deployment is available on Australia?s national cloud, NeCTAR, which is based on OpenStack middleware.

CloudMan itself is written in Python as a standalone web application and is open source licensed under the MIT license. Internally, CloudMan is based on service- oriented architecture; each of the components within CloudMan (e.g., file systems, job managers, applications) are defined as self-contained services. Services implement a standard interface and control all actions related to the given service. The master control thread manages and coordinates all of the running services. Instructions on how to use all of the features of CloudMan are available at (http://usecloudman.org) while the source code is available at (http://bitbucket.org/galaxy/cloudman).

A. Service Implementation: Hadoop  Along with the cloud resource management, resource allocation and management in a distributed system is a task, which requires a lot of effort. Various systems have been implemented to fulfill this need. Among these systems, Sun Grid Engine (SGE) has gained reputation for being a stable, efficient and reliable system. On the other hand, having the necessary infrastructure to ease the development of solutions for big data problems by using the computing power from distributed systems is quite demanding and Hadoop has been introduced as a solution  for this task. Integrating Hadoop with SGE (i.e., providing an available and ready to use distributed architecture to the end user), gives us the capability of having the grid engine manage distributed resources while having Hadoop as a platform service available on demand. This approach was pioneered by [19], where Hadoop systems was pre- installed over cluster?s nodes while the Sun Grid Engine allocated the necessary resources to its users on the fly.

Although this solution is satisfactory for systems where cluster?s nodes are pre-configured and Hadoop is the most demanding sub-system used, running Hadoop daemons over a cluster can be seen as waste of resource as it might not be in use over the entire lifespan of given cluster node.

Furthermore, the available solution has separated Hadoop jobs from other jobs in the system, which would decrease job throughput by reducing the system?s coherency.

We reengineered the available solution to increase system throughput by having Hadoop sub-system be provided by the grid engine on-demand (vs. static). For this purpose, a grid engine integration script will inspect jobs submitted to SGE and whenever a Hadoop job is found, requested computing resources will be acquired from SGE and a Hadoop environment setup dynamically.

In other words, before assigning a job to the resources, Hadoop will be installed on the acquired subset of the SGE cluster. The process of inspecting the job is realized in a constant amount of time, ????, and is handled by a job submission template script, which is made available to the end users. Each time a Hadoop job is detected, the script will read the allocated resources from SGE and dispatch the required binaries to each resource. This process happens in parallel across all the nodes so the time complexity of deploying Hadoop over the allocated nodes is minimum and reduced to the amount of time required to dispatch Hadoop onto a single system. After Hadoop installation is completed, the job is sent to the Hadoop master node to run. Once the job completes, grid engine integration script cleans all the resources by removing any unnecessary files dispatched to each node and killing all the daemons. This process is necessary as CloudMan is providing a hybrid platform to be used by different cluster tools and techniques. Therefore it is desirable to remove the Hadoop?s files and daemons from every single node and get the system ready for any other non- Hadoop/Hadoopuses to keep the system layer agreement.

On the other hand, as mentioned the process of deploying Hadoop gets only a constant amount of time, around 4 minutes (please see figure 2 for details) which in comparison with the actual job?s running time can be ignored; as big data analysis are often computing intensive tasks which take a while to run.  For a visual representation of this process please see figure 1.

B. Service implementation: HTCondor  Using private or community resources while having on- demand access to additional processing units from the cloud, to spread the workload when necessary or desired, is the promise that a hybrid IaaS solution offers. In the presented architect we have adopted flocking and gliding concepts from HTCondor (Please refer to section IIB for their definition) as a solution for providing a hybrid IaaS solution through CloudMan. In this approach we have targeted scenarios where multiple clusters with in a cloud     a o a m t w j r r r b a r  r p c i p H c u C r p  r c  are managed b other organiza among users minimizing the the scenarios whilstother clu jobs will be resource is m remote resourc resources. The beneficial is w a limited com resource can b sending the resource.Using promises with costs.For this t is necessary provide the c HTCondor. Al connection fil until the resou CloudMan us remote resour privileges to r send the job remote resourc connection stri  Figure 1:  CloudM  by an organiza ations and the to maximi e cost. The thr where one o  usters are heav flocked (esp  managed by C ce to have load e other scenario when a huge po mputing resou e acquired or r job pool?s  g this feature, C h the minimum to be happenin for the remo  connectionstrin lthough access e, gliding sce  urce provider g ser to run ne rce. After hav run the daemo to the remote  ce IP or DNS a ing.

Man architecture ap federated comp  ation or in coll ses resources ize the thro roughput can b or multiple clu vily loaded. In pecially if the CloudMan) or d balancing sys o where our hy ool of jobs has urce. In this c rented by the o  overflow to CloudMan will m amount of  n the context o te organizatio  ng to the Clou s will be grant nario will not  grants enough p ecessary daem ving been gran ons, the local H e resource by and the path to  pproach for suppo puting fashion.

laboration with can be shared  oughput while be optimized in usters are free these cases the e remote free glidedinto the  stem across the ybrid solution is  to be taken by case a remote  organization fo o the remote l keep the SLA  f overflow job f CloudMan, i  on resource to udMan?s loca ted through the t be functiona privilege to the  mons over the nted sufficien HTCondor can  y receiving the the Globus file  orting big data in a  h d e n e e e e e s y e r e  A b it o al e  al e e  nt n e e   Integr  our soluti as a solut HTCondo settings in sets up th to the sys node to th shared po flocking (resource they shou when an CloudMa resource CloudMa DNS and provider? for which web inte configura cluster?s section fo  Subm parameter by Cloud job requir submittin SGE job(  The ch both SGE each othe balanced.

Hadoop w for Hadoo this point system un completio  Figure environm the given  Figure 2  rating these tw ion represents tion for hybrid or is achieved n CloudMan a  he HTCondor m stem, CloudMa he established ool. To share scenario, a ne provider and r  uld first come nd what resou an. After reac  provider shou an?s HTCondor d HTCondor? s DNS or IP h a form has erface. CloudM ation, letting H  resources as or an example).



IV. U mitting a Had  rs in a templat dMan and spec res and a path  ng this job is e (Please see figu  hallenge in this E and Hadoop er while keep . This goal is a where SGE pr op and then ha t on Hadoop h ntil the job has on and cleans th  e 2 captures th ment within SGE  process.

2. The Hadoop jo s  wo concepts fr a first step in  d IaaS. The act by pre configu  and, whenever master node. A an automatical condor pool,  e the created egotiation betw resource consu into a level of urce can be  ching a level uld grant suff r. This is achiev s configuratio should be pas been integrate  Man will then TCondor nego required (see .

USAGE AND RES doop job req e script file. Th  cifies the numb h to the job ex equivalent to s ure 3 for a samp  s approach was p to work with ing the resour achieved by de repares the nec ands over the jo handles the job  done. SGE is t he environmen  he process of c E and also sho  ob preparation proc sample timeline  rom HTCondo enabling Clou tual integration uring the HTC CloudMan sta  As workers are ly adds each w resulting in a resource pool  ween the two p umer) is require f agreement on  used through of agreemen  ficient privileg ved via CloudM  on file. Lastly sed into Cloud ed into CloudM n set the nece otiate and use r  Usage and R  SULTS quires setting he script is pro ber of CPU slo xecutable. From submitting any ple job details)  s to find a way hout interfering rce utilization ecoupling SGE cessary environ ob to Hadoop.

b as a self-con then informed  nt.

creating the H ows sample tim   cess through SGE  or into udMan n with  Condor arts, it added  worker single l in a parties ed i.e.

n how, h the  nt, the ges to Man?s y, the dMan, Man?s essary emote  Results  g two ovided ots the m here  other .

to get g with  well- E from nment From tained of job  adoop mes for  and a     Submitting HTCondor jobs in a CloudMan cluster requires a HTCondor job submission script to be created.

Once composed, the script is submitted to HTCondor as shown in figure 4.

Although employing HTCondor as an engine for the federated cloud resource management gives us the  capability of utilizing the disparate resources, the big data migration challenge remains an issue to be taken care of.

Job files can be broadly grouped into two categories: the systematic ones, in which the system should take care of, and the user input data. In the context of CloudMan, a simple but effective solution for handling systematic data has been adopted; namely, any cluster managed by CloudMan will have the same copies of files in their local directories. This includes installed applications, application configuration files, as well as reference data, which is often required by a variety of bioinformatics tools and reaches multiple terabytes. Therefore, jobs requiring access to these data can be used without any data copying from one CloudMan cluster to another.

Whilst this is an efficient solution for systematic files, the user input data is a bottleneck in job dispatching. This is because flocking a job into a remote cloud or cluster requires transferring the input data next to the job, which means carrying the required amount of data over the network. It is worth mentioning that although tackling this bottleneck is an interesting question going under research, in this paper, our focus is on introducing the necessary architecture and its implementation details to provide an accessible platform for enabling further research in this field. Currently, CloudMan is relying on HTCondor job scheduler?s decision for submitting jobs into the remote resources. We are exploring alternative scheduling approaches to utilize data locality between clusters with the aim of submitting data intensive tasks local to the data while sending compute intensive tasks to the available remote resources.



V. CONCLUSION Infrastructure-as-a-service (IaaS) is the delivery of  hardware and its associated software as a service[16].

CloudMan was first introduced to facilitate management of such a service with a focus on easing the process of preparing a basis for providing bioinformatics tools on the cloud. Successful projects, such as Galaxy, the Genomics Virtual Laboratory [20], CloudBioLinux, and SARA Cloud [21], are using or providing cloud services to the end users based on CloudMan.

In this paper we extended the capabilities of CloudMan to accommodate managing cloud resource in a more versatile and federated environment. This was achieved by integrating Hadoop platform into CloudMan and thus providing Hadoop-as-a-Service. With Hadoop?s ability to process big data, availability of such a platform enables easier access to a new wave of (bioinformatics) tools for both, development and usage. In addition, we have integrated HTCondor into CloudMan. With HTCondor integrated, it is possible to easily span boundaries of a single compute cluster and extend the capacity of any administratively single resource, regardless of whether it is in the cloud or not. This enables overflowing excess workloads to a different set of computational resources (e.g., local cluster to a cloud or from one cloud provider to a different cloud provider).

This also opens a new door towards federated job execution and data-locality based job execution. The architecture and the developed platform discussed here is a step towards further research in the space of big data movement and job scheduling to exploit data locality. This  Figure 3. HTCondor flocking example. A is the script to be run by HTCondor in this exmaple it is named python_random_lines.sh. B is the HTCondor script which should be submitted to HTCondor named remote_job.submit. Here we submit these scripts multiple times into AWS cluster (From Figure 3) using multiple different input files. It is worth noting that the Exit 42 at the end of python_random_lines.sh  will let HTCondor detect when the job is finished. Then afterwards it can transfer the related files back to the local HTCondor.Part C shows  how to sumit a Hadoop job into Sun Grid Engine.

Figure 4. The CloudMan cluster setup for HTCondor flocking scenario. In this picture two CloudMan cluster has been created: one on the Amazon cloud (AWS) and one on the NeCTAR cloud (NeCTAR).

The flocking happen from AWS to NeCTAR.

is due to the fact that by using CloudMan it has become feasible to have a federated cluster computing over the cloud; something that otherwise requiresconsiderable effort on behalf of researchers or users.

Adding these features to CloudMan significantly extends the feature set of CloudMan and confirms its role as an accessible cloud management console and a provider of big-data-platform-on-demand.Based on our knowledge and supported by the relevant works described in Section II, this is the first solution that delivers an efficient approach for providing ready to use big-data-analysis- platform over a federated cluster on the cloud environment; our solution utilizes existing technologies to deliver a novel architecture and functionality to cloud environments, minimizing required investment and yielding an innovative approach.

