EVALUATING TECHNOLOGIES BASED ROUGH SET THEORY    VU THANH NGUYEN1, NGUYEN THI LY SA2

Abstract: Most data mining algorithms usually generate combining  large rules, finding useful rules from rules set necessary and important. There have been several techniques proposed to assess the rule as useful measure which is based on rough set theory as the RIM, ERIM measure. On rough set theory has been studied, the article proposed AWERIM measure which is improved from  ERIM measure and applied this specific measure to an application of the data mining problem about bank loans.

Keywords: RIM measure; ERIM measure; AWERIM measure  1. Introduction.

Discovered the combining rule is key areas in data mining and is used widely in many fields. However, these techniques often generate a large combined rules, how to choose the the rules which are really important? One solution to this problem is to use the interesting measures ([3]) to organize and evaluate rules by these measures. The useful measures are divided into two main directions: objective measure and the subjective measure. The subjective measure depends on the support of the people to evaluate the rule, the objective measure bases on statistics or the structure of the patterns. The Jiye Li proposed a measure based on rough set theory:  - The rule importance measure - RIM ([2]), the objective measure, is defined similarly useful measure, is used to evaluate the importance of rule;  - The enhanced rule importance measure - ERIM) ([3]) is a combination of two measure subjective and objective, is defined based on the weight of condition attributes.

In this article, the authors evaluate the AWERIM (Average of Weight based Rule Importance Measure) proposed measure and measure ERIM, comparing the advantages of the suggested measure to the Jiye Li?s  measure by proposed a practical application of data on bank loans.

2. Introduction about rough set theory.

The rough set theory was developed by Zdzisrule Pawlak ([5]) in the early 1980s to be seen as a new approach to discover knowledge, and it forms a solid foundation for data mining applications. The highlights of issue of the rough set theory are making the idea to solve ambiguity and uncertainty of information systems.

The decision Table.

In rough set theory, data set is presented as a decision table with rows in the table called objects and the columns in a table called attributes. Attributes in decision table is divided into two categories: condition attributes and decision attributes; the condition attributes are used to describe the conditions and decision attributes are used to describe the results when there are some certain satisfied conditions. A decision table T = (U,C,D) with the symbol U is the set of objects in decision table, C is the set of condition attributes and D is the set of decision attributes.

The reduct and core.

The reduct and the core are two important concepts in rough set theory, the reduct set of attributes typical conditions likely to classify the entire board decision. A decision table may have reducts and their intersection is said the core, but it is a set of attributes needed for the whole data. Because reduct don?t contains redundant attributes therefore it is often used in the attributes selection process.

A decision table may have more than one reduct, the determination of all reducts is this problem which has complexity to be NP-hard ([6]) and are interested by many researchers.

3. Association rule.

Mining association rules to help find the relationship between the components in the data. Mining field of association rule so far has been studied and developed in several different directions, in general problems of mining association rule are divided into two main stages: (1) find all frequent itemsets, (2) mining association rules from these frequent itemsets  Definition.

With the transaction data set D, the association rule is implied operator X ? Y, in which X, Y are the item sets.

The X ? Y Rule has the s support if s% of transactions in D  which contains YX ? ( D  YX s  ? = ), and the c reliability if  c% of transactions containing Y in transactions containing X  ( X  YX c  ? = ). Mining association rule is to find  combinations of all rules X ? Y which have s support greater than the threshold support of minSup and the c reliability greater than the threshold of minConf.

The conversion decision table to bitmap table.

The methods of mining association rules have been proposed to apply to transaction data with the value domain of the properties of 0 and 1 and these properties are considered as the item. To apply this method to data defined in rough set theory, data in the decision table with multiple values attributes to need to convert decision table to bitmap table with attributes to be the name of the attribute in decision table associated with every its possible value.

When the objects in decision table is converted to bitmap table which has transactions with values associated for each attribute to be 0 or 1.

Mining association rules from bitmap table.

There are several methods for mining association rules.

In this article, the authors use the IT-tree search ([4]) to mine the frequent item sets which have the threshold of minSup support after converting decision table to bitmap table. From these frequent item sets, the association rules set X ? Y is arisen with minConf threshold of reliability and X is the subset of C condition attributes set and Y is a subset of the D set of decision attributes.

Eliminate redundant rules.

Results of the process of mining association rules usually contain a lot of redundant rules; need to remove redundant rules from the association rules set. A rule is  considered redundant rule if it satisfies one of the following conditions:  (1) If there exists two rules of the form as ZYrZXr ?=?= 21 , . I say the rule r2 is the  redundant rule if X subset of Y (X?Y).

(2) If there exists two rules of the form as ZXrYXr ?=?= 21 , . I say the rule r2 is the redundant rule if Z subset of Y (Z?Y).

4. Technical for evaluated rules.

Discovered the association rule is one of the main approaches of data mining, but the number of rules discovered usually large while a small number of rules is actually useful from the user view. There are many proposed methods to choose the useful rules by using the utility measure as the interesting measure ([3]), the measure which is based on rough set theory is proposed by the Jiye Li author  to include the rule important measure - RIM ([2]), the enhance rule important measure - ERIM ([1]).

The interesting measure.

The interesting measure of a rule is a technique for evaluating the benefits, the usefulness of this rule. There are many useful measures as Support, Confidence, Lift, Laplace, Conviction, Interest, Cosine and Jacquard ... However, no one of measures can give the best results in all applications.

The authors V. Kumar ([3]) compared and evaluated 21 measures based on experimental tests and proposed some suitable areas.

RIM and ERIM measures.

Two these measures applied from rough set theory, particularly the reducts, for evaluating the arisen association rules.

RIM measure.

The reduct is a subset of the typical conditions attributes can describe the full meaning of the data set, so the association rules which are arisen from a reduct are typical knowledge. A decision table often have more than one reduct, the rule arising from the various reduct may contain different typical information, if we only used a reduct to generate rule may to miss the other important information. Therefore, we should use all reducts to generate rules, then some rules will appear more frequently in the files of rule rules; and we can say that the rule often appears to be viewed is more important than the rule appear regularly. Based on this idea, the Li Jiye author ([2])        proposed RIM measure to assess the importance of the rule.

This measure is defined as follows:  n rulesetruleRuleSetsruleset  RIM jiji ??  = |   Where n is the number of reducts, RIMi is the  importance of rule rulei, rulesetj is the j-th rule set j arising from a j-th reduct and Rule Sets is the rule sets arising from reducts  Can say that RIM measure is quite simple and easily calculated, providing a clear and direct view of the importance of a association rule. The measure is objective measure. However, the disadvantage of RIM is that when only one reduct from the decision table is found, then the value of RIM of all rules are found to be 100% (that all rules are important as together) so should not be able to evaluate the importance of rules based on the RIM measure.

ERIM measure.

The ERIM measure is proposed to efficiently measure of RIM, and this is a subjective measure which is defined based on the weight of condition attribute in decision table, and the weight is evaluated by experts of this field.

According to experts, the attributes have the higher weighted more necessary so the rule which has greater weight is considered more important. The ERIM measure is defined as follows:  ? =  = in  k kii wERIM  ,    Where ERIMi is the ERIM measure of the i-th rule (rulei), ni the number of condition attributes in the rulei  rules and wi,k is  weight of k-th attribute of rulei rule.

The ERIM of rule is the total value of the weights of condition attribute in the rule; these rules which have higher ERIM measure are considered important. For convenience to compare the rules by the ERIM measure, instead of using the ERIM value the percentage of ERIM value is used to compare the maximal ERIM value in rule set.

The approach of ERIM measure includes three steps: The first step is generated set of rules which use the RIM; the second step is calculating the ERIM measure value for each rule in the obtained rule set in first step. Finally a combination of both RIM and ERIM measure to evaluate the rule. Advantages of this measure is combined subjective measure with  objective measure in evaluated rule process so its result can be better more than using other measures.

However, this measure depends on the key element of the evaluation which is received by experts. This process is  spent many times on statistics process and sometime difficultly to implement.

The AWERIM measure.

As described above, the ERIM measure has disadvantages as follows: if two rules have equal RIM measure and weights of the condition attributes in the decision table are the same, then by definition of ERIM measure, then the rule that has more number of condition attributes on the left side than the others will be  more important; this means that ERIM measure depends on the number of attributes on the left sides of the rule, the higher number of conditions attributes on the left sides is, the more important it will be. This is quite non reasonable because the condition attributes in both rules have equally weighted.

To solve this problem, the article proposed AWERIM measure (Average of Weight based Enhanced Rule Importance Measure) as alternative measure to overcome the disadvantages of ERIM measure. The AWERIM measure is defined as follows:  i  n  k ki  i n  w AWERIM  i  ? == 1  ,   Where the AWERIM measure is AWERIM of i-th rule (rulei), in  the number of attributes in the conditions attributes in rulei and kiw ,  weight of k-th attribute of rulei.

The AWERIM measure of rule is the average value of weight of the condition attributes in the rule; these rules have higher AWERIM measure to be considered important.

For convenience to compare the AWERIM measure of rules, instead of using the AWERIM value, we use a percentage of AWERIM value compared with the value AWERIM maximal value in rule set. ERIM similar measure, approach of AWERIM measure also includes three steps: The first step generated rule set using the RIM measure; the second step calculated the value of AWERIM measure for each rule in the rule set obtained in the first step. Finally a combination of both RIM and AWERIM measure to evaluate the rule.

5. Practical application.

The authors use data mining for bank loans to evaluate the advantages and the rule measure AWERIM that article suggested to RIM measure and ERIM measure.

Description of data set.

The application on the mining data set for bank loans to predict customers can be approved for a bank loan or not based on some information from customers. Data source which is mined has the 14 attributes in which 13 conditional attributes (2 continuous valuable attributes, 11 category valuable attributes) and 1 decision attribute are included.

List of attributes is presented in Table 1.

TABLE 1. DESCRIPTION OF BANK LOAN DATA  Attribute Type  Attribute Name Interpretation  Value Domain  Name Amount customers want to borrow  {0?N} Continuous V alue  Age Customer Age {0?N}  Interest rate Interest rate of loan (unit %)  {0.9, 0.98, 1, 1.05, 1.1, 1.12, 1.15, 1.18, 1.2, 1.3}  Duration Period of Loan (unit: month)  {12, 24, 36, 48, 60, 72, 84, 96}  Repayment Repayment Schedule  {Month, end of repayment}  Pay Interest Schedule for interest payment  {Every year, Every month}  Prestige Prestige of customer in the previous loans  {Yet, Yes , No}  Number of previous  loan  Number of previous loans  {0, 1, 2, 3}  Marriage Marital status of Customer  {Single, married, Divorce}  Number of dependents  Number of dependents of customer  {0, 1, 2, 3}  C onditional  attribute  C ategories  Ensured debt  Guaranteed loan ratio (value of loans against collateral)  {>=1, <1}  Income Stable level of income of customers  {Stable, Relatively Stable, Non Stable}  Afford Repayment ability of customer, calculated = loan period*monthly income net + other assets) / (interest + loans a month * term loan). Values of attributes:> = 1, <1  {>=1, <1}  D ecision  A ttribute  To loan with  interest  Customer information on loan or not  {Yes, No}  Building applications Model.

Application is built by model in Pic.1. The first step, in the first phase inconsistent and data will be processed, after that data is divided into two parts by the random method, 70% data for Training and 30% data for to test. Because the data source of Bank Loan has two attributes whose values are continuous values so training data needs to carry out to discrete values by algorithm which is proposed by HungSon authors ([8]). The results of phase of arising rule is association rule set after eliminating the redundant rules (applied algorithm to exploit the rules by IT-tree search for with minSup = 3%, minConf = 70%). From obtained rule set, calculating the RIM, ERIM, AWERIM measures for each rule and in build subclass for each measure. In the last step test data is used data to test the effectiveness of the subclasses that was built based on test accurate results. In this article, the authors use algorithm for building subclasses ([9]), the input of this algorithm is rule set to be ordered by using measure, the output is the layer with this measure.

Figure 1. Building applications Model  Weights of the condition attributes of Bank Loan are presented in Table 2. ERIM measure is defined ([2]) based on the weight of attributes, and these weights were determined to depend on the concept hierarchy, the attributes of the same hierarchy are same weight and equal weight of that hierarchy. But in this article the author is not set up the concept hierarchy because data source of Bank Loan has a few attributes.

TABLE 2. WEIGHTS OF CONDITION ATTRIBUTES OF THE BANK LOAN  Number  Condition attribute  Weight  1 Money 7  2 Interest 3  3 Period 3  4 Original payment  5 Interest payment  6 Prestige 7  7 Number of previous loans  8 Age 7  9 Marital Status 4  10 Number of dependent  11 Ensuring debit 8  12 Income 8  13 Afford to debit 8  Experimental result.

Table 3 is presented the results of 15 times for the experiment with three RIM, ERIM and AWERIM measures.

Fig.2 is represented by drawings about accuracy in cases for each measure from the data in Table 3.

Discussion.

Although the number of rules using three RIM, ERIM and AWERIM measures lower than entire set of rules if the rule set does not use any these measures but subclasses that are built by three measures give test results which are the more correct than when subclasses are not built by three these measures, especially when the AWERIM measure is used (see table 3). ERIM measure is a advanced RIM measure but result of the ERIM measure is not the better result of RIM measure. Compared with the 2 measures proposed by Jiye Li (RIM, ERIM), the AWERIM measure of this article almost gives the better results.

TABLE 3. RESULTS OF 15 TIMES FOR EXPERIMENTS WITH RIM, ERIM AND AWERIM MEASURES ON DATA SOURCE OF BANK LOAN  All rules RIM  ERIM AWERIM Number ? of  rules Accuracy ? of rules Accuracy  ? of rules Accuracy  ? of rules Accuracy  1 185 71.32 % 131 87.84% 131 87.84% 131 93.24%  2 152 68.70%  144 84.21% 144 77.63% 144 88.16%  3 163 70.67 % 86 86.67% 86 81.33% 86 89.33%  4 196 70.37 % 131 87.65% 131 87.65% 131 91.36%  5 174 65.85%  164 75.61% 164 78.05% 164 82.93%  6 176 76.62%  128 85.71% 128 87.01% 128 87.01%  7 183 62.82% 162 78.21% 162 78.21% 162 78.21%  8 198 64.79% 123 80.28% 123 80.28% 123 81.69%  9 167 70.51% 98 83.33% 98 83.33% 98 88.60%  10 177 69.62% 138 80.52% 138 80.52% 138 80.52%  11 198 72.37% 152 77.63% 152 80.26% 152 85.53%  12 191 67.90% 140 82.72% 140 82.72% 140 87.65%  13 178 64.38% 118 82.19% 118 82.19% 118 90.41%  14 200 63.85%  164  79.49% 164  79.49% 164  82.19%  15 183 70.51%  150 93.59% 150 91.03% 150 93.59%          Figure  2. The Chart compares the accuracy of the subclasses that are used RIM, ERIM, and AWERIM measures  6. Conclusion.

In the article, the authors present technical evaluation based on the rough set theory of Li Jiye, RIM and ERIM measures, propose a AWERIM measure. Experimental results on data source of Bank Loan prove effectiveness of rule set used AWERIM measure and compare them with ERIM and RIM measures.

At the present the authors intend to build applications with multiple attributes of data source to test the AWERIM measure performance used concept hierarchy ([2]), and develop other measure by combining two kind of subjective and objective measures to evaluate the rule.

Reference.

[1] Jiye Li, Nick Cercone, W. H . Wong, Lisa Jing Yan (2009). Enhancing Rule Importance Measure Using Concept Hierarchy. Faculty of Computer Science and Engineering, York University.

[2] Jiye Li, Nick Cercone (2005). Discovering and Ranking Important Rules. Granular Computing, IEEE  [3] P. Tan, V. Kumar, J. Sivastava (2002). Selecting the Right Interestingness Measure for Association Patterns, in SIGKDD?02 ACM  [4] M. J. Zaki, C. J. Hsiao (2005). Efficient Algorithms for Mining Closed Itemsets and Their Lattice Data Engineering.

[5] Z. Pawlak (1991). Rough Sets ? Theoretical Aspects of Reasoning about Data. Kluwer Academic Publishers, Dordrecht.

[6] A. Skowron, C. Rauszer (1991). The Discernibility Matrices and Functions in  Information Systems.

[7] Aleksander Ohrn (1999). Discernibility and Rough Sets in Medicine: Tools and Applications. PhD thesis, Department of Computer and Information and Science, Norwegian University of Science and Technology, Trondheim Norway. The ROSETTA Homepage, http://www.idi.ntnu.no/~aleks/rosetta  [8] N. H. Son (1997), Discretization of Real Value attributes. A Boolean Reasoning approach. Thesis for Doctor of Philosophy.

