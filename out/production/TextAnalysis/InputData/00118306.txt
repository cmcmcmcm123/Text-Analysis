S*dIUCHILD: A FAULT T O L P  ;?Tm  WXJRAL ?.JET?!OFK

Abstract  Bra inchi ld  i s  a neura l  network designed t o  br idge t h e  gap between currer l t  neura l models and t h e  brain.  It nodels  t h e  phys ica l o rganiza t ion  of  neurons by using both feed- forward and l a t e r a l  connections. It a l s o  has a high depree of f a u l t  t o l e r a n c e  i n  keeping with neura l  connections. A s e r i e s  of t e s t s were run on both Bra inchi ld  and a Hopfield model network t o  compare f a u l t  to le rance .

Both hard and s o f t  f a u l t s  were used, a s  wel l as combinations o f  t h e  two. Bra inchi ld  proved t o  be t h e  more f a u l t  t o l e r a n t  of t h e  two.

m R A L  ASSOCIATIVE MEMORIES USING CASCADE & RING ARCHITECTURES  H o n  hung KIM and Chi Kin LEE  Departmnt of E l e c t r i c a l  E n g i n e e r i n g U n i v e r s i t y  of Windsor  Windsor, Ontario Canada NQB 3 P 4  Abstract  Three associative vectors, are  methods to store temporal patterns, which are ordered presented. The first one is to  use a single Bidirectional Associative Memories ( B A N ) ,  the second one is to use a Cascade Bidirectional Associative Memories (CBAM) and the third one is to use a Ring Unidirectional Associative Memories (RUAM).

Their performances are compared by simulations which use words of four letters as the temporal associative data. These words can be recalled after one letter is input. Finally, the application o f  the RUAM to store autoassociative data is also discussed.

ERROR CORRECTING NETWORK  Jason M. Kinser Teledyne Brown Engineering Cummings Research Park  Mail Stop 60 Huntsville, AL 35807  and University of Alabama in Huntsville  Department of Physics Huntsville, AL 35899  H. John Caulfield University of Alabama in Huntsville  Center for Applied Optics Huntsville, AL 35899  Abstract  For many applications a simple neural network model with a high storage capacity would be sufficient.

There have been networks proposed that are easy to implement, but have a low storage capacity. There are other networks that have high capacities, but are more tedious to implement. We propose a network that uses the simplest learning rules but exhibit a high storage capacity. This increase in storage capacity is achieved by introducing error correction on the output of traditional low capacity networks. Experimental error corrected recall indicated the Error Correcting Network had the ability to recall very nearly all of the available information produced by a simple Hopfield-type network long after the Hopfield-type net- work had failed to recall any association by itself.

RECALL IN SATURATED ASSOCIATIVE NEURAL NETWORKS  by Oscar Martlner and Cralg Harston Computer Appllcatlons Servlce  6207 Forest TralI Slgnal Mountaln. Tn. 37377  Abstract  An assoc i at i ve neura i network was enhanced to provide both primary and secondary associative relationships on recall. input patterns, which were conceptually related, established extra relationships in the modified associative network. Not only did this network recall the original associated pattern when presented with a glven input, but it could identify the conceptually related input patterns. The output was separated into two groups. The primary output was the expected pattern associated with the input during training. The secondary output consisted o f the patterns associated with other inputs. This secondary information was limited to output patterns which were conceptually related to the training input.

