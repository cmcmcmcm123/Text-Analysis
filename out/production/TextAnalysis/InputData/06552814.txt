Leveraging Applied Materials TechEdge Prizm? for Advanced  Lithography Process Control

Abstract? Presently the CDSEM metrology tool continues to be the ?ruler of the fab? providing metrology at over 100 steps along the way to building a modern semiconductor product. We explore the history of data volume generated with Fab CDSEM fleets. As we quantify the size of the fleet dataset, it becomes clear why historically only a subset of the rich dataset is ever utilized.

We review what the treatment of the data using modern ?big data? analytics and enterprise level server hardware does to accelerate development learning, increase engineering turns per day and raise the velocity of accurate manufacturing feedback.

This paper further explores ways to leverage TechEdge Prizm to extract more value out of metrology for Lithography process control.  The paper will describe some of the challenges facing the changing Lithography metrology landscape and how Prizm delivers tailored analysis for these challenges.  Results at a customer site demonstrate that various types of analysis of CDSEM images and data are 10 times more efficient and thorough than traditional methods.

Keywords? CDSEM, Metrology, Data mining, Lithography

I. INTRODUCTION Circa late 1980s CDSEM data were printed or written into  logbooks. At the time digital imaging was new and images were not stored to a disk. Each tool was an ?island? and comparing tool results and matching tools was labor intensive and not well characterized. Tools had to be dedicated to specific measurement use cases. Host control through the SECS/GEM interface was becoming common around 1995 and allowed automated collection of data from the CDSEM tool fleet.   Real-time data collection led to closed loop feedback control of the exposure tool based on CDSEM metrology. This development placed intense pressure on the quality of the CDSEM data. Incorrect placement of the measurement on the wrong structure or incorrect analysis of the measurement signal would drive an incorrect exposure at lithography steps on the next product lot. Early innovators deployed this automated closed loop control scheme by the late 1990s.

Many CDSEM measurement issues would be identified and improved over the next ten years. Quality metrics were added to the basic Bottom CD data. A Macro CD measurement capability was developed allowing averaging over many features, optionally reporting results for the average and each individual feature. Total Measurement Precision, Fleet  Measurement Precision and Total Measurement Uncertainty metrics were developed [1]. Improved and increasingly powerful signal analysis algorithms would be developed in order to meet required measurement precisions.

SEM images overlaid with measurement results and capturing the subtlety of process variation became more valuable. A re-measurable lossless image could also be saved driving the need for more storage on and off the tool. This image or waveform contained the raw numeric data needed for the algorithm to measure. The waveform allowed for re- measurement without a new SEM scan for cases of low precision, failed or incorrect measurements. Powerful off-line utilities were developed to optimize measurement strategies based on waveforms.  As the variety and complexity of measured features grew through shrinking technology nodes capturing waveform data at scale became more valuable.  The volume of the data set however was often too large for the system to save continuously.  If there is a measurement issue a lot is placed on hold or a subsequent lot is stopped. If the waveforms are available and saved in real-time costly tooltime might be avoided.  It is not good practice to store this data on the tool for longer than a few days.  A full tool hard drive will make a tool unstable and ultimately crash.  Such incidents serve well as a detractor to deploying the full-on continuous saving of the waveform dataset locally.



II. NEW TECHNOLOGY STEADILY DEPLOYED Eventually CDSEM toolsets were connected to fab  networks allowing enhanced capability to collect data.

Furthermore tool event logs containing information about tool and recipe flow were collected and analysis tools created for fleet management.  FTP clients to tools opened access to the image dataset in a limited but distributed way. Prior to remote access users had to use costly tool time to collect data.

Additional parameters were then added to the measurement analyses.  Parameters such as edge width measurement, contact hole major and minor axes, pattern recognition scores, standard deviations, and roughness could now be used to complement the measurement and widen the process control palette. Most of these additional metrics were captured only locally on the tool in the form of report files.  Simple dis-positioning did not require these metrics so the additional parameters were typically not sent to the host. The SECS/GEM interface was not equipped to handle image data or keep up with the rapidly     changing capability of what CDSEMs could output. While the CDSEM capability grew it became more difficult to effectively make use of the data.  Users were mired in finding, transferring and preparing of the data for detailed analysis. The labor intensive aspect often meant that data was not reviewed, new metrics were undiscovered or underutilized and often metrology quality languished unnecessarily.

Over the last two decades the cost of ownership of CDSEMs has gone down mainly due to increased throughput.  This increase in measurement capacity has been consumed by increased sampling and the number of metrology steps per technology node due to increased complexity of processes. Fig 1 shows the increase in percentage of steps taken up by metrology over 4 technology nodes. [2]    Fig. 1. Percentage of steps in route taken by metrology/defect and process as a function of IBM technology nodes.

As the density of the features has doubled every two years [4] it is common to see more structures are being measured for better sampling.  As the number of measurements increases we also increase the number of images we save.  In parallel the number of entries in event or error logs and trace files also accumulates. All of the data collected in a medium sized fab across a fleet of 20 tools in one year amounts to around 84TB as shown in Fig. 3. This is Big Data.



III. DATA VOLUME The chosen approach to sizing of the CDSEM dataset is to include all user oriented data useful to the fab engineering staff.  Fig. 2 shows size calculations of data volume of a medium size fab. [5]            Fab Size # CDSEMs DataVolume (TB/Yr)  Small 10 42 Medium 20 84 Large 60 253  Fig. 2. Data Volume Estimates for different sized fabs assuming a mixed fleet of older and newer CDSEM generations.

Data is reported by the tool on a per wafer basis. This includes the critical feature measurements and associated quality metrics.  Additionally recipe and target build descriptors and statistical analysis output data are all captured in a file with 260 columns of data for each measurement.

On a per measurement level the data set would include one or more images.  The stored size varies depending on pixel density, scan area, and compression options. The re- measurable waveform image is included as one of the optional images to save.  In the sizing we include three images by priority; waveform, measurement display and pattern recognition field-of-view.

Medium Fab Annual Data Volume per CDSEM Generation (GB)  Data Type Prior Gen Current Next Gen  Wafers per hour 18 30 50  Images 50244 83740 139567 PR FOV 2115 3524 5874  Meas Display 5836 9727 16212 Waveform 42293 70488 117481  Report Files 56 680 1460  Events 2 33 49  Configuration 2 8 9 Totals (GB) 50305 84461 141085  Fig. 3. CDSEM Data volume estimated annually comparing different tool generations of a medium sized fab or 20 tools.  Assuming 20 sites per wafer, 90% Tool Utilization and 80% Fab Capacity.

Additional data to be included is event log data which provides contextual detail in and around all measurements. This data set has been shown to reveal insight into tool issues, recipe errors, alignment problems, software issues, calibration matching and other scenarios. The data allows temporal granularity to the second around a given tool event and also shows telling trends when rolled up over time.  The final data type captured is tool global configurations.  These contain parameters which vary regularly such as fine adjustments of column alignment settings and those which are open to user configuration preferences.

Figure 3 summarizes the data types and volume estimates for different generations of CDSEM. The summary captures the increase in data volumes due to tool generational throughput differences.  The annual data volume estimates for a small, medium and large sized modern semiconductor fab are shown  257 ASMC 2013    in Figure 3. These estimates assume a given throughput, utilization and fab capacity.  The volume is dominated by the image dataset.

Reduction of this dataset with a long time horizon is possible by deploying a modern enterprise scale big data solution. We now describe a system and a set of analysis capabilities turning a large rich dataset into actionable insights.



IV. TECHEDGE PRIZM AND BIG DATA ?Big data is all about better analytics on a broader spectrum of data.? [3]  Applied Materials TechEdge Prizm is a web based CDSEM data mining application released in Fall 2012.  It provides advanced data mining capability of CDSEM metrology at scale.  This approach of leveraging the vast differentiated data in Prizm gives customers the ability to tease out tool, recipe and process excursions from the noise of production and attain new levels of Lithography process control.  The upcoming challenges of shrinking design rules, 3D, EUV and other new process techniques prompted the development of Prizm.  Fabs rely more than ever on the images and deeper analysis than legacy host systems can provide.  For example at IBM with each technology node the number of CDSEM steps has increased by 40%. [1] Today?s 22nm node requires CDSEM measurement at over twice the number of process steps required for the 65nm process.

CDSEM accounts for the largest number of metrology steps and is the workhorse for high volume manufacturing process control.  What keeps this measurement technique the go-to solution is its ability to capture high resolution images of in- chip device features.  With the increasing complexity of measurement strategies and the growing number of products running in foundries and IDMs the need to audit recipes and ensure metrology integrity is critical.  The goal of Prizm with respect to Lithography process control is to better utilize the full data and high resolution images to provide new analysis modalities in order to streamline the workflow of process engineering stakeholders.  Streamlining workflow is critical in order to accelerate learning and shorten development cycle time.

Fig. 4. Workflow improvements seen with Prizm over traditional methods  Where there are issues with metrology integrity Prizm provides the tools to distinguish between tool, recipe or process issues.

Outlined here are several key innovations in Prizm that reduce the friction of data post processing for Lithography process engineers to answer questions like these and finally exploit CDSEM Big Data. See Fig. 4.  Prizm productivity improvements and insights are illustrated for each case below.



V. CDSEM METROLOGY CHALLENGES The following are workflow challenges that Lithography  process stakeholders face and the different modalities that Prizm provides to meet these needs.

Meas Errors Root Cause:  A critical lithography activity is to investigate a CDSEM recipe that is reported to have low Cp/Cpk.  These are the steps required to find a solution:  1. Confirm issue - Pull Wafer-to-Wafer Report  2. Troubleshoot - Identify root cause, use quality metrics  3. Action - Fix Recipe  4. Verify - Report and Track  Confirm Issue:  Using a Wafer-to-Wafer report an engineer can quickly understand that this recipe is running poorly with respect to other layers at the 32nm process.  In order to do this a live search query is made that identifies all targets at the same level across the process and then a comparison can be made.

Fig. 5. Live search for all similar targets.

258 ASMC 2013        Fig. 6. Wafer-to-Wafer chart showing flyers and missed measurements  The engineer can troubleshoot using all targets and all wafers in context reviewing bottom CD and many other quality metrics, in this case pattern recognition score (Pr Score).  Pr Score, measured charge, focus offset, fit quality, and other metrics are all examples of data which is not sent to the fab host but put at the fingertips of the Prizm user.

Troubleshoot: Subsequently the engineer clicks on the interactive chart to pull the wafer map and images.

Fig. 7. Wafer map and image review in Prizm  Upon review of wafer map and images the pattern recognition step is found to be non-unique resulting in flyers and missed measurements.  This detailed query on Prizm took only seconds to make ~100 wafers and associated images available.

If the same data and images were to be gathered manually from a tool set or repository, the time to find and collect the data could take hours.

Take Action:  A recipe writer selects a unique feature for pattern recognition based on the knowledge of the root cause.

Fig. 8. Selecting a unique pattern on the CDSEM tool  Verify - Report and Track:  The engineer can generate a Roll Up report to verify the initial recipe correction and regularly thereafter to track continuing performance of ALL 32nm target measurements.  This report aggregates thousands of wafer statistics in seconds, again saving multiple hours compared to traditional methods.

Fig. 9. Rollup report showing all 32nm target performance  After the target has been fixed the data looks much better with tight sigma, no flyers and no missed measurements. See Fig.

10.

Fig. 10. Another Wafer-to-Wafer pull is done showing clean data    Fig. 11. Metrology integrity root causes  Process Centering:  The following example shows the wafer- to-wafer mean chart with an FEM wafer running in context of production.  The process centering is confirmed rapidly as the data and all images are at hand after dose correction.  No need to find the tool the data was run on or manually collect and  259 ASMC 2013    chart the results. The process can be back under control with improved Cpk 2 hours sooner.

Fig. 12. FEM wafer in-line with production measurements  Rework Verification:  Fig. 13 illustrates the ability to visualize through-lot history on one chart.  The data points are colored by Layer showing reworked lots or flyer data at a glance.

Fig. 13. Through lot History  Furthermore Fig. 14 demonstrates the unique ability of the Verity4i tool to report localized wafer charge.  In this instance Prizm can provide crucial early learning about the impact of new resist materials or track  recipe changes driving rework.

Fig. 14. Wafer-toWafer chart of measured charge across multiple wafers    Raw Files Search:  Prizm provides a way for users to quickly find and download all the native files that are generated from the tools for use in excel or other offline utility.  Traditionally users access the measurements and images via the native files.

(Fig. 15).  The identification of specific data takes only seconds while downloads are 10 times faster by compressing all the data into one file prior to sending.

Fig. 15. Raw Files download  Development Images Review: Directly from the Wafer-to- Wafer chart a link is provided allowing the engineer to review the entire set of images captured from the wafer in a single click. Included are all measurement target images as well as images captured during wafer alignment. This makes the job of looking at a full wafer run a simple scan of thumbnails.

Figure 16 depicts the flow from wafer-to-wafer chart to full images gallery.

260 ASMC 2013        Fig. 16. Development Images Review in a Gallery by single mouse click including wafer alignment images, pattern recognition images and measurement display images across the wafer.



VI. RESULTS Utilizing the capability in Prizm the efficiency across several workflow categories captured in Fig. 4 has improved by 10x or more in many cases.  In cases where metrology engineers were manually gathering data from tools in unorganized folders the process of finding, transferring and preparing data for detailed analyses is done in minutes rather than hours. The speed with which data can be reviewed and the resulting improvements in metrology integrity are seen. Better and more utilization of the big dataset generated by a fleet of CDSEM tools by automatically collecting, organizing, visualizing and cross linking through state of the art analytics are demonstrated.



VII. CONCLUSIONS The introduction of Prizm in 2012 has demonstrated new  ways to extract value from the ever growing metrology data set.  Prizm proves to be far simpler and effective than  traditional solutions.  It is accessible on any browser enabled device as opposed to rigid platform dependent client applications.  Traditionally tools stored data and images for less than a month.  Users are drawn to the Prizm web application since it stores data for up to 7 years making it the go to solution to review long term historical data.  The ease of use of the wafer-to-wafer chart engages Lithography stakeholders with images and trends of their process flow.

Prizm exploits an underutilized dataset to enhance the Verity4i CDSEM tool offering and Applied fab software services to enable the next generation of advanced Lithography process control.  At a customer location productivity improvements have been shown to accelerate troubleshooting and time to corrective action.

