Civil Transportation Event Extraction from Chinese  Microblog

Abstract?People produce hundreds of millions of microblogs  everyday. With its 140-character message, Microblog has yielded an enormous corpus of information, which is noisy but informative in some way. However, previous work with standard NLP tools of event extraction performs poorly on Microblog. In this paper, we adopt a series of methods to extract events from Chinese microblogs. In particular, we grab the chatters from Sina Weibo to extract civil transportation information. We eliminate buzz in Weibo, use CRF methods to filter microblogs so as to focus on transportation, and we also use CRF to recognize named entities and to extract events.

Keywords?Event Extraction; Chinese Microblog; NLP; NER; CRF;

I.  INTRODUCTION Social Networking has developed dramatically in recent  years. By exploring sites as Facebook and Twitter, Renren and Weibo likewise for Chinese, users publish their thoughts, share their thoughts, comment on others?, at an unprecedented rate.

Sina Weibo, launched in august 2009, is an extremely popular online microblog service consisting of more than 500 millions of users. Yet the amount of microblogs people produce everyday through Weibo has exceeded 100 million1.

Not surprisingly, the enormity of information presents high opportunities to exploit this corpus of data to obtain real-time information, as well as to make certain prediction. In this case, it is quite demanding that we extract events effectively through these microblogs. The first step is to grab microblogs. Rather than Weibo API, here we have chosen python tools Scrapy to fulfill the task.

Event extraction for Chinese characters is composed of several modules: Word segmenting, Part-of-Speech tagging (POS tag), Named Entity Recognition (NER) and Event tagging. Compared with foreign language as English, Chinese phrase doesn?t contain space which serves as natural segmenting tag. Therefore, word segmenting is a thorny point in Chinese event-extraction. We have tried several tools for this task, and eventually we have chosen Language Technology Platform (LTP).

Event extraction for Chinese microblog is even harder than traditional text. Previous research in event extraction has focus  TABLE I.  PRECISION AND RECALL OF NER USING MODEL TRAINED ON THE CORPORA OF LIBERATION DAILY. WE CAN SEE A 30% DECREASE IN F1  SCORE WHEN WE APPLY THE MODEL ON WEIBO CORPORA  NE Type Corpora Precision Recall F1 Name Liberation Daily 96.65% 97.86% 96.87% Name Weibo 61.05% 62.33% 61.85%  Toponymy Liberation Daily 92.89% 93.36% 92.97% Toponymy Weibo 72.04% 77.35% 75.47%    mainly on news article. Trained on this genre of text, traditional NLP tools performed well and steadily in similar text. Unfortunately, with the development of social media, the performances of these ?off the shelf? tools are weak on microblog corpora, as we can see in Table 1. There are several reasons for this:  Firstly, due to the popularization of the user community of microblog, unlike those news articles written by professional journalist which are strict, structured and conformist, the content of microblog is unstructured, occupied by large amount of casual expressions, and even contain some emoticons like ?ORZ? and ( ) . Secondly, since there is word limit of 140, users tend to minimize their expressions, which makes it difficult to extract whole essential factors of one event from a single microblog. In addition, this kind of microblog is filled with user?s personal preference, rendering it more confusing to extract event equitably. Thirdly, today?s hot issue may well be in tomorrow?s landfill, especially when it comes to such vogue case as cyber-style of writing.

?Zhenhuan Style?, originated from a hit drama in early 2012, have once occupied Weibo for several months, yet seldom mentioned after one year. Models trained on these pop elements will soon lose its effectiveness and become redundant.

Last but not least, microblog users frequently mention their mundane events in daily life, such as what they eat and where they have been, which are only informative to their close social network relationships. Therefore, these redundancies of information will have to be removed in order to better extract event of general interest.

After close scrutiny of the existing NER and Event tagging tools, we have chosen CRF++, a simple, customizable, and open source implementation of Conditional Random Fields  1, http://baike.baidu.com/view/2762127.htm   DOI 10.1109/CLOUDCOM-ASIA.2013.48    DOI 10.1109/CLOUDCOM-ASIA.2013.48     (CRFs) for segmenting/labeling sequential data. Meanwhile, we wrote a series of scripts to eliminate the interference caused by casual expressions in Weibo.

In this paper, we focus on a specific task: to extract civil transportation information using chatter from Sina Weibo. For one thing, even though there are several Chinese microblog networks, Weibo possess the biggest user community and therefore yield the biggest dataset. For another, we can get a relatively more precise character dataset and avoid certain buzz since we have confined the topic in an isolated circle civil transportation in this case. In order to focus on this one topic, we also use method of CRF to filter the microblogs so that our basic corpora are solely consisting of clean microblogs concerning transportation.

The rest of the paper is organized as follows. Next we introduce the overview of our microblog processing system and the proposed approaches. From Section 4 to 9, we will describe each procedure of our system respectively. Then in section 10 we describe the testing standards and present our analysis on the result we obtained. We cite related work in section 11, and we conclude in section 12.



II. SYSTEM OVERVIEW An overview of the several components of our system for  extracting civil transportation information from Weibo is presented in Fig.1.

In the first place we virtually log in Sina Weibo, and grab microblogs from certain account who regularly issue contents concerning civil transportation. A raw dataset is obtained in the end.

In the second place, we use a series of scripts to remove noise such as emoticons and unrelated links. Next, we introduce method of CRF to filter unrelated topics, and we obtain our basic corpora free of noise.

In the third place, microblogs are segmented and POS tagged. We then annotate the data in order to make training set and test set. By adopting the method of CRF again, we obtain a model through training on the dataset.

Finally, we use this model on the testing set and get the precision and recall rate. We add at last a script to calculate the F score.



III. PROPOSED APPROACH As mentioned above, our system are composed of three  modules: python tools Scrapy to grab microblog, LTP tools to segment words and CRF++ to do NER and Event Tag.

A. Scrapy To Virtually log in and grab microblog Sina Weibo provides official APIs for grabbing microblogs.

However, after experimentation and examination, we found that official APIs have several limitations for the number of microblog grabbed, the IP for log in and the authority of grabbing, so that they cannot satisfy our needs in the research.

So we chose a web crawling tools Scrapy.

Fig. 1.

System Overview  Scrapy is a fast high-level screen scraping and web crawling framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing2.

B. LTP To segment sentence and to POS tag There are dozens of Chinese natural language processing  tools. To find one with highest efficiency we have chosen three most famous and widely used tools: ICTCLAS3 (Institute of Computing Technology, Chinese Lexical Analysis), LTP4 (Language Technology Platform), and Stanford Word Segmenter5.

2, http://scrapy.org 3, http://www.ictclas.org 4, http://www.ltp-cloud.com/ 5, http://nlp.stanford.edu/software/segmenter.shtml  Weibo  Raw corpora  Test Set  Train Set  Grabbing Weibo  Filter  Segmenting& POStag  NER Event Tag  Annotation  Result  Model  Standardization  Basic corpora  Display  Corpora Expansion     TABLE II.  ACCURACY OF THE THREE TOOLS FOR PROCESSING WEIBO TEXT  ICTCLAS 83.2% 75.38% LTP 83.44% 74.1%  Stanford Seg 81.5% /   To evaluate the effects of the three tools on Weibo text, we select randomly 100 Chinese microblogs, segment and tag them manually, then we tried the three tools on the same sample. The results are shown in Table 2.  Given the result of comparison, we finally chose LTP as our tools for sentence segmenting and POS tagging. LTP can achieve a relatively high accuracy in NER, so we also use it to do this task. The result of NER by LTP will be listed as a reference for later use.

C. CRF To filter microblogs, to do NER and to tag event Conditional Random Fields (CRFs) are a class of statistical  modeling method often used in pattern recognition and machine learning to make structured prediction. As an ordinary classifier predicts a label for a single sample regardless of the "neighboring" samples, a CRF can take into account the context; For example, the linear chain CRF popular in NLP predicts sequences of labels for sequences of input samples.

Principle: Conditional Random Fields can be defined as followed [13]:  Let G = (V, E) be a graph such that Y= (Yv) v V, so that Y is indexed by the vertices of G. Then (X,  Y) is a conditional random field in case, when conditioned on X, the random variables Yv obey the Markov property with respect to the graph P (Yv|X, Yw,  w? v) =  p (Yv|X,  Yw,  w~v), where w~v means that w and v are neighbors in G?.

Here X may range over natural language sentences and Y stand for the label sequence.

CRF is an undirected graphical model. Its nodes are divided into two disjoint sets X and Y, X stands for the observed variables, Y stands for the output variables. Then the conditional distribution p(Y|X) is modeled. The aim of the CRF is to find out the label sequence y Y, who maximize the conditional probability for a sequence X.

y=argmax p(Y|X)                         (1)  Therefore, Named Entity task can be regarded as a sequence labeling task. Thus CRF can be used to do NER.

Specifically, CRFs are often applied in shallow parsing, named entity recognition and gene finding, among other tasks, being an alternative to the related hidden Markov models. In computer vision, CRFs are often used for object recognition and image segmentation.

Tool: CRF++ is a simple, customizable, and open source implementation of  CRFs  for segmenting and labeling sequential data. CRF++ is designed for generic purpose and will be applied to a variety of NLP tasks, such as Named Entity Recognition, Information Extraction and Text Chunking6.

To use CRF++, a training file, a test file and a template file are necessary. We need to write a specific template file in  which we describe which features are used in training and testing. Fig.2 reveals the format of training file and the specification of the signs in a template file.



IV. GRABBING MICROBLOGS To successfully grab microblog, our approach consists of  two modules:  The first one is responsible for virtually login. By analysis the URL for first login, we obtain four values: ?servertime? time of server, ?nonce? random number of login, ?RSA Pubkey? RSA public key, and ?rsakv?. After encryption, we encapsulate these information along with user name and password into a form demanded by Sina Weibo, post it to server, and then we will receive the response and complete the virtual login. Later we submit request repetitively to crawl microblogs.

From the grabbed data, we obtain the user account, content of a microblog, time of issue, number of comment, number of forward, etc. In our case, we are only interested in the content of microblogs, while others will become a sort of noise during later analysis. Therefore, our second module analyzes the grabbed html data by using regular expression, solely extracting the content of each microblogs.

After these operations, we obtain raw corpora of microblogs.



V. STANDARDDIZING MICROBLOGS Obviously, the raw corpora of microblogs contain huge  amounts of noise, which are useless in the processing of microblogs, thus to be eliminated in standardization. Our standardization module has functions as follows:  ? Removing meaningless signs, such as ?#topic name#?, ?[Emoticon]?, ?@username?, some web links like ?http://t.cn/z8ne0w2?, etc.

? Discarding microblogs shorter than 7 words.

? Transforming complex Chinese into simple type, in order to avoid word repetition in later analysis  We may delete some useful information during the process of this module, but we do reduce the redundancies in later process.  We test the performance of NER in both unstandardized dataset and standardized dataset in order to see the effect of our standardization.



VI. FILTING MICROBLOGS Since we grabbed microblogs from the accounts who  frequently issue microblogs concerning transportation, the majority of these microblogs indicate the real-time traffic jam, traffic accident, construction information, etc. Not surprisingly, most of these accounts, especially for those ordinary users (compared with some official accounts who focus on reporting transportation information), may issue some noisy unrelated microblogs, such as forecasting the weather, saying good night, or reporting their location. Fig.2 demonstrates two microblogs from a same Weibo uid. The first one is about traffic condition while the second has nothing to do with transportation.

6, http://CRFpp.googlecode.com/svn/trunk/doc/index.html?source=navbar         Fig. 2. Example of two microblogs obtained from the same uid ?207073197?, the first one is useful transportation information telling which road has a  traffic jam in Shanghai, while the second one is just an advertisement of a musical program, that is to say, noise.

In order to discern those microblogs who really talk about transportation, we first scan them to identify some features. For example, if a microblog contains words like ? ?(road) or ?  ?(traffic jam), it is very likely to have something to do with transportation. On the other hand, if a microblog contains words like ? ?(weather), ? ?, it is most probably noise. In this way, we define the first feature as ?Whether this microblog contains the word ? ?(road)?. If so, we mark a ?1? at the end, otherwise we mark a ?0?. Similarly, we define other 6 features.

At the end, we annotate manually 900 microblogs whether they are about transportation.

With the seven features and the manually annotation result, we train 470 microblogs and obtain a CRFs model. We then use the rest 430 microblogs to test our model. We find out that our model achieves a high precision of 86.95%. With this model, we can easily annotate following grabbed microblogs, and filter those noisy topics unrelated to transportation.



VII. SEGMENTING MICROBLOGS AND POS TAGGING LTP is a language platform based on XML presentation.

Thus our basic corpora in form of TXT will first be transformed into XML. Then we invoke module IRLAS, which is responsible for segmenting and POS tagging.

In our basic corpora, each microblog is stocked in one line, so that LTP can read and process the input file line by line.

After processing, we got our result file also in format TXT.

Each line lay one segmented word, along with its POS tag and NER result.



VIII.  EXTRACTING NAMED ENTITIES Our origin dataset consists of 3 columns: segmented word,  POS Tag, NER by LTP. To extracting named entities, we have to first annotate manually each line with correct named entity tags.  Assuming that the longest named entity is composed of 7 Chinese characters, in the template file, we define the window size as 7. In other words, when we processed each segmented Chinese character, we examined three characters before it, itself and three characters after it. We also consider the relations between the three columns.

In this way, our template contains 70 macro template structures.  By using this template to train our origin dataset, we obtain a CRF model that can be used to tag named entities.

TABLE III.  EVENT TAGS SYMBOLS SET  Tag Symbol Implication B-E Begin of Event I-E Internal of Event E-E End of Event O OTHER  K-E Keyword of Event D-E Description of Event T-E Time of Event L-E Location of Event

IX. EXTRACTING EVENTS We use our CRF model obtained during the process of  NER to tag named entity of another basic dataset (result of LTP), and get a new dataset consist of 4 columns, of which the first (segmented words), the second (POS tag) and the fourth (our NER result) are useful in this module.

Similarly, we annotate each line manually with event tags.

Table 3 lists the possible symbols of event tags.

As for the training template, we adopt seven different templates so as to select the one with best performance:  In the first one, we just considered the relationship between two interfacing line. The window size is 5. In template 2, since some named entities and event body are longer than five words, we adjust the window size to 7. In template 3 we considered the relationship between two columns, that is to say, between words and POS tags, POS tags and NER references, and words and NER references. We also considered relationships between three interfacing lines. Template 4 abandons POS tags and focuses on relationships between words and NER references.

While template 5 abandons NER references and focus on relationships between words and POS tags. In template 6 we neglect the relations between columns and focus solely on relations between lines in one column. In the last template, we considered all the relationships. There are 70 template structures in this template.

We obtain 7 CRF models for Event tagging with different performance. We choose the best one amongst as our model to tag event.



X. EXPERIMENT To prepare for the experiment, we first establish our  corpora of experiment. Then we do the segmenting and POS tag. We annotate the dataset, and then we do the NER and Event tagging.

A. Corpora of Experiment In order to get real-time transportation information, we first  chose 12 Weibo accounts who officially issue microblogs concerning transportation information of Shanghai. We also selected 50 accounts of ordinary users who occasionally report traffic condition when they actually meet some specific traffic condition. After virtually log in Sina Weibo, we encapsulated the uids of these accounts into a list and transmit it to Scrapy as a reference. Then we ran Scrapy and crawled microblogs from     these accounts. Our original dataset of 2000 microblogs were obtained from 1st Aug 2013 to 7th Aug 2013.

In order to improve and perfect our corpora during the process of NER, we also added corpora obtained from Revolution Daily about transportation from 13th Aug 2013 to 17th Aug 2013.

B. Evaluation Criteria To evaluate the performance of CRF++ on named entity  recognition and event tag, we use three parameters: P (precision), R (recall rate), and F (F-measure).

Precision measures the percentage of correct named entities tagged by CRF++ over the total number of named entities tagged by CRF++. Recall measures the percentage of named entities tagged by CRF++ over the total number of named entities in the file. F-measure, a measure that combines precision and recall, is the harmonic mean of precision and recall.

rp rpmeasureF  +?? ??+?=  )( )1(-  ?? ??  (2)  In equation 2, ? stands for the relative weight of P in R.

Normally, ? equals 1, we get an F1 score.

C. Result and Analysis of NER We use our CRF model described in section 8 to test on the  two set of testing data: the unstandardized dataset (with noise like ?@username?) and the standardized dataset. Their performances are in table 4.

An increase of 17.3% of precision can be observed from the NER result, which means by eliminating informal expressions, our efficiency of extracting named entities increases.

D. Result and Analysis of Event Tagging Our seven templates produce seven models during the  training. Table 5 lists the training result of the seven templates, including their number of features, number of iteration, training time and the size of each model file.

From the result of training in table 5, we can see that as we aggrandize the window size, considering more relations between columns and lines, it will cost longer time to train a model, and the model size will become larger.

By using these seven template files, we obtain seven models. Table 5 and Fig. 3 illustrate the performance of the seven models. From Fig 4, we can see that for model 1, 2 and 3, as we expand the scope of relationships, both P and R augment, which means in certain scope, the more features we consider, the better result we will obtain.

TABLE IV.  NER RESULTS  P R F1 Unstandardized 73.59% 57.85% 64.99%  Standardized 86.33% 66.85% 75.35%   TABLE V.  TRAINING RESULTS OF SEVEN TEMPLATES FOR EVENT TAGGING  Template No. Feature Iteration training time(s)  Size Model(kb)  1 3508246 109 606.70 22262 2 9742306 111 763.59 69526 3 10113045 113 787.62 72133 4 13263404 124 960.45 106647 5 14446950 76 667.04 113030 6 14335048 112 921.01 112238 7 14901880 117 1011.96 116453  TABLE VI.  TESTING RESULTS OF SEVEN MODELS FOR EVENT TAGGING  Models P R F-1 F-0.1 1 37.24% 79.05% 50.63% 78.18% 2 37.62% 81.45% 51.46% 80.52% 3 38.18% 81.67% 52.03% 80.76% 4 16.76% 74.38% 27.36% 71.93% 5 27.00% 84.80% 40.96% 83.04% 6 35.01% 83.56% 49.34% 82.42% 7 36.49% 82.01% 50.52% 81.01%    Fig. 3. Example of input and output data for LTP  Model 4 neglect the result of POStag, and it receive the lowest precision rate. In other words, simply considering the relation between words and its NER tags cannot effectively identify its event tag. Compared with model 4, model 5 achieves a better performance in both precision and F-measure, which means that POS tags can affect the result of event tagging more deeply than NER tags do.

As we compare the performance between model 6 and 7, we found that the consideration of tags in one lines will increase the recall rate while reduce the precision. In other words, the relationships between segmented words, POS tags and NER tags as a whole can help improve the recognition of events, but may affect the judgment of event type in a bad way.

We can see that the performance of model 7 is worse than that of model 3, which means, with more thorough consideration of all the relationships, there will be more limitation in recognition of named entity and event tag, thus reduce the recall rate.

Overall, we chose model 3 as our CRF model for event tagging.

TABLE VII.  EXAMPLE OF EVENT TAG ABOUT A CIVIL FLOOD IN CERTAIN PLACES, AND CALLS ATTENTION OF ALL THE DRIVERS. AFTER PROCESSING, WE  IDENTIFY THE KEY WORD ?ROAD?, EVENT?FLOOD?,LOCATION OF EVENT?CENTURY AVERNUE, ZHOUHAI, ETC?, AND TIME OF EVENT ?17:05?.

Origin Microblog  K-E B-E L-E T-E  E. Demonstration of Events.

Finally, to demonstrate the civil transportation event we  extract from the microblog, we designed a script to find out all the segmented words whose event tag is K-E(keyword of event), B-E(begin of event), L-E(location of event) or T-E(time of event), according to the tag set in table 3. With these four elements, we can easily confirm the event information. Table 7 illustrates an example of our Event Tag.

XI. RELATED WORK There has been relatively little previous work on building  NLP tools for Chinese Microblog. Yet for common named entity recognition and research on English Microblog as Twitter, there are several:  There has been considerable published result on general Named Entity Recognition. The methods can be divided into three categories: method based on rules, method based on statistics and method based on the mixture of the two. For method based on statistics, there are mainly three models: Maximum Entropy, Hidden Markov and Conditional Random Fields.  Della Pietra and Mercer R L were the first to induce maximum entropy model into natural language processing [2].

Chen A, Peng F and Shan R built up conditional random fields model and maximum entropy model, and obtained an F value up to 86.2%[3].

As for the research on information extraction within microblog, by using distant supervision, Benson et al. trained a relation extractor which identifies artist and venue mentioned in twitter of users who list their location as New York [8]. T.Sakaki M.Okazaki and Y.Matsuo designed a system proved to be capable of recognizing almost all earthquakes reported by the Japan Meteorological Agency.

They realize this system by training a classifier to recognize tweets reporting earthquakes in Japan [9]. Ritter A and Clark S have designed a novel part-of-speech tagger module and word segmenting method; they have also introduce entity classification so that the precision of named entity recognition were higher than that of traditional methods [4].

XII. CONCLUSION In this paper we have shown an event extraction system  which can extract civil transportation event from microblog text like Weibo. We have used Scrapy to crawl microblogs, CRF methods to standardize and filter microblogs, LTP to segment each blog and tag it with POS, and CRF methods  again to do named entity recognition and event tag. At last we obtained a model trained on our corpus. By using this model, we can tag event from newly grabbed microblogs and extract their event information. The precision of our NER reaches xxx, and the precision of event tagging reaches xxx. At last we can obtain from each microblog the keyword of event, the event phrase, its location, and its time.

The civil management can benefit greatly from our system.

Event extracted from social media will help the city managers to know what's going on in the city and they can be more prepared for those coming events. For example, city administration can allocate adequate public transportation service for a big event which involves several thousand people.

Another example is that city administration can allocate enough police force to the place that a crime prone event is going to happen.  In this way, event extraction can make the smart city even smarter.

