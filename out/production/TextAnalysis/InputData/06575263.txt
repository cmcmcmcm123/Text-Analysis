Modeling of System of Systems via Data Analytics ? Case  for ?Big Data? in SoS1

Abstract - Large data has been accumulating in all aspects of our lives for quite some time. Advances in sensor technology, the Internet, wireless communication, and inexpensive memory have all contributed to an explosion of ?Big Data?. System of Systems (SoS) integrate independently operating, non-homogeneous systems to achieve a higher goal than the sum of the parts. Today?s SoS are also contributing to the existence of unmanageable ?Big Data?. Recent efforts have developed a promising approach, called ?Data Analytics?, which uses statistical and computational intelligence (CI) tools such as principal component analysis (PCA), clustering, fuzzy logic, neuro- computing, evolutionary computation, Bayesian networks, etc. to reduce the size of ?Big Data? to a manageable size and apply these tools to a) extract information, b) build a knowledge base using the derived data, and c) eventually develop a non-parametric model for the ?Big Data?. This paper attempts to construct a bridge between SoS and Data Analytics to develop reliable models for such systems.  A photovoltaic energy forecasting problem of a micro grid SoS will be offered here for a case study of this modeling relation.

Keywords: Data Analytics, Big Data, Solar Energy, Clustering, Micro-Grid, Neural Networks, Fuzzy Inference Systems, Fuzzy C-Means, PCA.

1 Introduction System of Systems (SoS) are integrated, independently operating systems working in a cooperative mode to achieve a higher performance. A detailed literature survey on definitions to applications of SoS can be found in recent texts by Jamshidi [1, 2]. Application areas of SoS are vast indeed. They are software systems like the Internet, cloud computing, health care, and cyber-physical systems all the way to such hardware dominated cases like military, energy, transportation, etc. Data analytics and its statistical and intelligent tools including clustering, fuzzy logic, neuro-computing, data mining, pattern recognition and  post-processing such as evolutionary computations have their own applications in forecasting, marketing, politics, and all domains of SoS.

A typical example of SoS is the future Smart Grid, destined to replace conventional electric grid. A small-scale version of this SoS is a micro-grid designed to provide electric power to a local community. A Micro-Grid is an aggregation of multiple distributed generators (DGs) such as renewable energy sources, conventional generators, in association with energy storage units which work together as a power supply networked in order to provide both electric power and thermal energy for small communities which may vary from one common building to a smart house or even a set of complicated loads consisting of a mixture of different structures such as buildings, factories, etc [2]. Typically, a micro-grid operates synchronously in parallel with the main grid. However, there are cases in which a Micro-Grid operates in islanded mode, or in a disconnected state [3]. Accurate predictions of received solar power can reduce operating costs by influencing decisions regarding buying or selling power from the main grid or utilizing non-renewable energy generation sources.

The object of this paper is to use a massive amount of data on solar irradiance as an integral system of the micro- grid SoS to extract relevant information for available solar energy in an attempt to derive an unconventional model.

Section II first describes the micro-grid that will be used as the SoS of interest for this paper. Section III then describes the set of environmental data to be used in this paper as the input to the different data analytics tools.

Section IV then discusses the application and effectiveness of different data analytics tools in the generation of models and relations that could be leveraged to better optimize the operation of the micro-grid.  Finally, Section V summarizes the exercises discussed in this paper and draws conclusions based on their findings.

2 System Model The micro-grid SoS is shown in Figure 1.  Shown here are solar array, battery storage, DC-AC inverter, load  1This work is partially funded by Lutcher Brown Endowed Chair, ACE Laboratory, University of Texas at San Antonio.

and a controller to manage the entire system. Ultimately, we want to forecast received solar power as a model based on real-time environmental measurements to be used in an energy management system [3] to minimize operating costs.

This micro-grid represents a facility scale Cyber-Physical System (CPS) or a SoS consisting of a building with:  ? A fixed (or with tracking system) solar photovoltaic system  ? A load demand in the form of overall energy consumption, HVAC and lighting, with bi- directional communications (e.g. bi-directional inverter)  ? A reconfigurable control and acquisition system (i.e. with open I/O modules, embedded controller for communication, processing and a user- programmable FPGA)  ? A local, off-site or cloud-based computing infrastructure for simulation/computational analysis.

Figure 1.  A PV Forecasting System as a constituent  member of a MicroGrid SoS [4]  3 PV Data Description To ensure the Photovoltaic (PV) input data for the different data analysis tools is comprehensive, data from different sources was combined to form the full dataset.

This was possible because of the solar research projects occurring in Golden, CO, where the National Renewable Energy Laboratory (NREL) is conducting long term research and data recording to support the growing renewable energy industry.

The first source was the data recorded by the Solar Radiation Research Laboratory (SRRL), which employs over 70 instruments to measure solar conditions and environmental parameters [5].  Also, this data set includes 180? images of the sky that are used to determine current  cloud conditions directly.  An example of this is shown in Figure 2.

Figure 2. Sample Sky Image  The second source of data was the SOLPOS data, made available by the Measurement and Instrumentation Data Center (MIDC), which has stations throughout North America to capture information on solar position and available solar energy [6].  Luckily, the MIDC has a station near NREL, so their data can be used in conjunction with the SRRL data.

The final set of data originates from the Iowa Environmental Mesonet (IEM) [7].  Their Automated Surface Observing System (ASOS) station near the Golden, CO site was also included to have current weather data in the set.

Data from the month of October 2012 was combined from the different sources of data.  This final set includes one sample for each minute of the month and incorporates measured values for approximately 250 different variables at each data point. The data set was sanitized to only include data points containing valid sensor data prior to the analysis.

4 Data Analytics of PV Data In this section, the analysis steps are described, and the results from the different techniques are compared.  The goal is to use data analytics tools to generate a useful model from the dataset without needing to resort to parametric analysis and the use of subject matter experts.

4.1 Objective Identification Since the micro-grid would benefit from predicted values of solar irradiance, it was decided that the output of the data analytics should be 60 minute predicted values of three key irradiance parameters (Global Horizontal Irradiance (GHI), Direct Horizontal Irradiance (DHI), and Direct Normal Irradiance (DNI)).

4.2 Input Variable Downselection The input variables were down selected from the full data set to only include cloud levels, humidity, temperature, wind speed, and current irradiance levels.  If this exercise was conducted using ?Cloud? computing, the number of variables might not need to be down-selected; however, since this effort took place on a single PC, the number of variables was reduced.

4.3 Cleanup of the Raw Dataset Next, the data set was further reduced by removing data points in which GHI, DHI, and DNI levels were very low.  The primary reason for this second step was to reduce the amount of time and memory necessary for analysis.

Figure 3 is a graph containing the measurements of GHI, DHI, and DNI over one day in the cleaned dataset.

Figure 3. Three Key Irradiance Parameter Plot for a Clear  Day  4.4 Non-Parametric Model Generation Tools After cleaning took place, the data could be fed into either of the two non-parametric model generating tools, the Fuzzy Inference System Generator and Back- Propagation Neural Network training tools included in the Matlab Fuzzy Logic Toolbox and the Neural Network Toolbox.

4.4.1 Non-Parametric Model Generation Tools  The Fuzzy Logic Toolbox function used in this exercise, genfis3 uses Fuzzy C-Means clustering to cluster values for each variable which produces fuzzy membership functions for each of the variables in the input matrix and output matrix.  It then determines the rules necessary to map each of the fuzzy inputs to the outputs to best match the training data set.  These membership functions and rules can be viewed using the Matlab FIS GUI tools such as  ruleview.  When run with default parameters, the genfis3 function ran significantly slower and performed worse than Matlab?s Neural Network fitting function.

Figure 4. Data Generated Using GENFIS3 Based on 13  Input Variables  Note in Figure 4, differences in the observed and predicted data points generally corresponds to the presence of clouds or other anomalies that could not be predicted an hour in advance using the variables input to the function.

4.4.2 Neural Network Fitting Tool The second model generating method was the Matlab Neural Network Training tool.  By default, this tool uses the Levenberg-Marquardt back propagation method to train the network to minimize its mean squared error performance.  Results from training one sample set are shown in Figure 5, Figure 6, and Figure 7.

Figure 5. Back-propagation Performance Curve      Figure 6. Post Training Network Regression Performance   Figure 7. Data Generated Using NFTOOL Based on 13  Input Variables and 10 Hidden Neurons  4.5 Additional Pre-Processing Discussion Once the initial performance of these two tools was evaluated, it was decided that further effort should go into including a greater number of original input variables and including additional preprocessed parameters in the training data in an effort to enhance the performance of the derived model.  This effort took three paths, the calculation of nonlinear input parameters, the inclusion of a greater number of input parameters, and the reduction of input data dimension when necessary in order to support the execution requirements of the two model generation tools.

4.5.1 Nonlinear Data Set Expansion In an effort to derive additional useful input parameters from the existing dataset, each variable included in the dataset generated several additional variables based on nonlinear functions and past values of  the variable itself.  Inclusion of these parameters in the training data set greatly improved the performance of the training tools.  A subject matter expert would be useful in this step to identify useful derived parameters such as these to add to the training data set.

4.5.2 Large Data Sets and Principal Component Analysis  Models were generated using different sets of input variables to try to assess the impact of incorporating increasing numbers of variables in the training data set.  In general, the trained model performed better when more variables were included in the training data set; however, as the number of variables increased, the training execution time became excessive and out-of-memory errors occurred when the data sets became too large.

In order to combat this issue, the dimension of the training data set was reduced to a manageable size using PCA.  PCA can be used to compress the information from a large number of variables to a smaller dataset while minimizing the information lost during this process [8, 9].

This can be performed directly on a dataset using the princomp function in Matlab.

The columns of the SCORE matrix returned by the princomp function represent the columns of the input data transformed to place the majority of the information in the data set in the first few principal components.  The information distribution among the principal components is illustrated in Figure 8.  The higher eigenvalues represent the principal components with the most information.

Incorporating principal components past 10 provides minimal additional information.

Figure 8. Principal Component Information Graph  Figure 9 below shows the quality of information recovery if transforming back to the original basis using only information from the first 50 principal components.

Figure 9. Data Recovery Demonstration Using First 50  Principal Components  In this application PCA was primarily useful because it allowed the reduction of very high dimension data sets to smaller, more manageable data sets that could be used as training inputs to the model generation tools.

4.6 Results In order to generate the best non-parametric model possible, different combinations of data inputs to the GENFIS3 and NFTOOL were considered.  Different implementations of the options discussed above were evaluated during this analysis.

The best performing NFTOOL generated model used all 244 original variables, which were then expanded the dimension to 1945 using the nonlinear variable derivation calculations.  Next, the dimension of the data was shrunk to 150 so that the training function had sufficient memory to train the network.  The resulting network was the best of all the generated models.

The best performing GENFIS3 generated model evaluated during this effort used the same input data set as mentioned in the paragraph above with the exception that the dimension was shrunk down to 50 using PCA.  It was observed during this effort that effectiveness of the GENFIS3 tool appears to be less tolerant of high dimension training data sets than the NFTOOL.

Figure 10. Best Neural Network Linear Regression  Performance   Figure 11. Best Neural Network GHI Error   Figure 12. Best Neural Network DHI Error      Figure 13. Best Neural Network DNI Error  Table 1 and 2 describe the performance of the models generated using these tools.  Note that these performance numbers should be compared qualitatively since the different input parameter configurations can yield different numbers of training data points.

Table 1. Performance Comparison of the Generated Non- Parametric Models (GENFIS3)    Table 2. Performance Comparison of the Generated Non- Parametric Models (NN10)    A sub-optimal predictor was constructed in order to show its performance relative to that of the non-parametric models.  This predictor was based on the average GHI, DHI, and DNI values for each time bin in the data set.

Table 3 shows the improvement of the non-parametric models when compared to this sub-optimal predictor, named ?Time Bin Mean? in the table below.

Table 3. Performance of Best Non-Parametric to Mean Time Bin Sub-Optimal Predictor    During this analysis, the aspect of the scalability of the GENFIS3 and NFTOOL tools was evaluated.  The model generation time for NFTOOL was always shorter than GENFIS3 for the same data sets.  The relationship of NFTOOL execution time to dataset length and dimension was generally linear for the test cases evaluated.  The relationship of GENFIS3 execution time to dataset length was also linear; however, its relationship between dataset dimension and execution time was a function of the dataset dimension squared.  This is shown in Figure 14 and Figure 15.

Figure 14. Model Generation Execution Time Relationship  with Dataset Dimension         Figure 15. Model Generation Execution Time Relationship with Dataset Length  5 Conclusion This paper presents a high level look at some of the tools available in the Matlab toolset that enable the user to extract information from ?Big Data? sources in order to draw useful conclusions.  As described in Section II, the specific application discussed in this paper is the prediction of the amount of solar power generated by a micro-grid.

Section III then discusses the data that was gathered to support this exercise.  Section IV discusses the steps and techniques considered while trying to generate the best solar irradiance prediction model.  Techniques discussed included dataset sanitation, training input parameter selection, model generation via Fuzzy C-Means Clustering and Rule Inference (GENFIS3), Neural Network training using back propagation (NFTOOL), Pre-Processing nonlinear variables to add to the training data set, and the use of PCA to reduce the dimension of the training data while maximizing the information retained in the data set.

It was observed in the results presented in Section IV that the best model predicting solar irradiance was one utilizing the maximum number of original and preprocessed variables, which was then reduced to a manageable dimension using PCA prior to use in training the model.  The results in this section also showed that the non-parametric model generation methods discussed in this paper performed significantly better than a sub-optimal predictor.  Finally, the results describing the model generation times for the two techniques showed that NFTOOL provides significantly better training times, especially when the dimension of the dataset is high.

Future work on this topic is planned to address the benefits of using Genetic Programming to optimally reduce the dimension of the dataset, the use of cloud computing to generate models for larger data sets, and the design and evaluation of a controller to buy or sell money from the grid based on demand and predictions of received solar energy.

