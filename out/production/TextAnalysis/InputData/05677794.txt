A Growing Evolutionary Algorithm for Data Mining

Abstract?an unsuitable representation will make the task of mining class association rules very hard for a traditional genetic algorithm (GA). But for a given dataset, it is difficult to decide which one is the best representation used in the mining progress.

In this paper, we analyses the effects of different representations for a traditional GA and proposed a growing evolutionary algorithm which was robust for mining class association rules in different datasets. Experiments showed that the proposed algorithm is effective in dealing with problems of deception, epistasis and multimodality in the mining task.

Keywords-association rule; genetic algorithm; representation;

I.  INTRODUCTION Genetic algorithms (GAs) are robust adaptive systems  that have been applied successfully to hard optimization problems. There are two famous approaches commonly used by genetic algorithms as classifier systems, the Michigan approach and the Pittsburgh approach. The Michigan approach maintains a population in which each individual is a rule and the Pittsburgh approach maintains a population in which each individual is a rule set and is encoded as a variable-length string [1,2,3,4].

Both of the two approaches run under the standard framework of GA and are based on the assumption that the fitness function value can be used to estimate the distance to the nearest global maximum. However, a lot of factors existed in the process of association rule mining largely affect the result of optimization, for example: epistasis, multimodality and deception. In this paper, we propose a growing evolutionary algorithm for data mining (GEA-DM) which decrease the affects coming from deception, epistasis and multimodality.



II. REALTED WORK  A. GA Difficulty The search for factors affecting the ability of the GA to  solve optimization problems has been a major focus within the theoretical GA community. One approach to the question has been the study of deceptive problems proposed by Goldber (1987), based on the work of Bethke [5]. Das [6] claimed that deception is the only thing that is important in making a problem hard for a GA. However, Grefenstette [7] believed  that deception is neither necessary nor sufficient for a problem to be hard for a GA. Now, it is clear that all of the facts, epistasis, multimodality, noise, deception and spurious correlations, can cause difficulty for a GA.

A model of fitness landscapes developed by Jones [8] suggests that there are strong connections between GA search and heuristic search. A general principle of heuristic functions is that they should correlate well with the distance to the goal of the search. If the fitness function of GA is viewed as a heuristic function, the value of fitness function is the estimate distance to the nearest goal of the search. FDC is a method of quantifying the relationship between fitness and distance.

Given a set F={f1, f2, ?,  fn} of n individual fitness values and a corresponding set D={d1, d2, ?, dn} of the n distances to nearest global maximum, we compute the correlation coefficient r, as  r=cFD/sFsD,               where  cFD= ? =  ?? n  i ddiffi  n 1 ))(1  is the covariance of F and D, and sF, sD, f  and d  are the standard deviations and means of F and D respectively.

B. Assciation Rule Let ?={i1,i2,?,im} be a set of literals, called items. Let D  be a set of transaction, where each transaction T is a set of items such that T ? ?.  Formally, an association rule R is an implication X??Y, where X and Y are sets of items or itemsets in a given dataset. The confidence of the rule Conf(R) is the percentage of transactions that contain Y amongst the transactions containing X. The support of the rule Supp(R) is the percentage of transactions containing X and Y with respect to the number of all transactions. Given a set of transactions D, the problem of mining association rules is to generate all association rules that have support and confidence greater than the user-specified minimum support ( called minSuppport ) and minimum confidence (called minConfidence) respectively [9].

Supp(R)>=minSupport                                                   (1) Conf(R)>=minConfidence                                             (2)  This work was supported by the National Natural Science Foundation of China (Grant nos. 60873035)     If Supp(X)>=minSupport, we say that X is a frequent itemset. Generally, association classification approach consists of three major processes, namely rule discovery, rule selection and classification. In rule selection, we always select the subset which gives the best accuracy to form a classifier and we believe that rules with higher confidence would probably give higher prediction accuracy [10].  So, finding the best confident frequent itemset is considered as a worthwhile thing.



III. GA DIFFICULTY IN ASSOCIATION RULE MINING  A. The Different Representations of Rules in Rule Space In this section, we describe a dataset D which consist of  four condition attributes A={a1,a2,a3,a4}, B={b1,b2,b3,b4}, C={c1,c2,c3,c4}, D={d1,d2,d3,d4} and a class attribute E={e1,e2}. Each attribute contains four values and we represent them with five variables X, Y, Z, M and N as follows:  x=1?a1, x=2?a2, x=3?a3, x=4?a4 y=1?b1, y=2?b2, y=3?b3, y=4?b4 z=1?c1, z=2?c2, z=3?c3, z=4?c4 m=1 ?d1, m=2?d2, m=3?d3, m=4?d4, n=1?e1, n=2?e2  As can be seen in Fig.1-A, we describe a record of the dataset D with a circle. The yellow circles are records in class e1 and the green ones are records in class e2.

However, if we change the representation of the record, we can obtain another distribution of the records in record space, which can be seen in Fig.1-B.

x=1?a1, x=2?a4, x=3?a3, x=4?a2 y=1?b1, y=2?b3, y=3?b4, y=4?b2 z=1?c4, z=2?c2, z=3?c3, z=4?c1, m=1?d2, m=2?d4, m=3?d1, m=4?d3, n=1?e1, n=2?e2.

a1 a2 a3 a4  b1  b2  b3  b4  c1  c2  c3  c4  d4 d3 d2 d1  a1 a2a3a4  b1  b2  b3  b4  c1  c2  c3  c4  d4d3 d2d1  A B  Figure 1.  Records in Record Space  B. The Effectiveness of Different Representation The association rule space S consists of all the possible  frequent itemsets. Each possible frequent itemset is a point in this space. If a point in the rule space whose support is greater than support threshold, we call the point a frequent itemset.

The objective of the association rule mining in space S is to extract all the frequent itemsets whose confidence is great than confidence threshold.

Comparing with Fig.1, we showed the association rules whose support and confidence values are great than the support threshold 5% and confidence threshold 50% in Fig.3-A. The blue circles are the found rules.

Figure 2.  Rules in Rule Space  Comparing with Fig.2, after changing the representation, we showed the association rules whose support and confidence values are great than the two thresholds in Fig.2-B. The blue circles are the found rules.

Figure 3.  Fitness Values of  The Founded Association Rules  In traditional GA, the confidence value of a rule is always seen as a value of a fitness function. If an appropriate expression manner was chosen and the problem is an easy one to solve, the fitness function should be used to estimate the distance to the nearest global maximum. But if some factors exist in the process which affect the mining greatly or an unsuitable expression was selected, the fitness function will not drive the optimal process effectively. We can see this from Fig.3, A is in an appropriate expression manner and B is in an unsuitable expression manner.

In fact, for a giving dataset D, it is very difficult to decide to which representations is suitable and at the same time, deception, epistasis and multimodality always existed in the optimal progress. These will make the traditional GA hard for mining association rules.



IV.  GROWING EVOLUTIONARY ALGORITHM  A. Growing Evolutionary Algorithm Mining the association rule with greatest confidence value  can be considered as an optimization process in the rule space.

The association rules can be considered as individuals in a population. The different representations will have a dramatic impaction on how easily the problem will be solved.

Different with traditional EA, we represent individuals with different number of items which can be seen as different age. Every time a generation was produced, the ages of the individuals in it were set to 1 and began to grow. As the increasing of the age, the support values of them become more and more small. The confidence values were used for selecting good individuals within the population in which individuals are at the same age. Since the selection was implemented within a limited area (at the same age), it prevented the misleading of the fitness function.

Step 0. Count the support of the all individuals with only one item and choose the one whose support value is greater than support thresholds to form the items set I, I={item1, item2, ?, itemn}.

Step 1. Randomly initialize a rule population PN, PN =p1+p2++pN. Each rule pi (i=1,2,?,N) in it has a class label and one item coming from set I. Calculate the support and confidence value of the rules.

Step 2.  Select rules in population Pr,kN and form the population Pn,kM, Pn,kM= Ts (Pr,kN). The selection operation Ts is defined by the following. (1) Suppress the rules in the population Pr,kN whose support value are less than the support threshold ?minSupport? and form the population P*r,kN. (2) Sort the individuals in the population P*r,kN according to their fitness and choose the first M highest confidence rules to form the population Pn,kM .

Step 3.  Proliferate Ni clones for each rule pn,i(i=1,2,?,M) in the population Pn,kM and form the population CkQ, CkQ=Tc(Pn,kM)={c1k, c2k, ?, cQk}. The number Ni is decided by the following equation, Q is the size of population CkQ.

( ) / ( )  M  i ii i  N Q conf R conf R =  = ? ? Step 4.  Bring up each clone in population CkQ and delete  the same one, then suppress the rules whose support value are less than support threshold and form the population C*kQ, C*kQ=Ts(Tm(CkQ)) ={c*1k, c*2k, ?, c*Qk}. The bring-up operation Tm is defined by the following: (1) Randomly choose an itemi from set I. If there is an itemk in the rule whose attribute is the same with itemi, randomly choose another one until no itemk in the rule whose attribute is the same with itemi. If no such an item can be found, the rule gets its maximal lifetime (the number of items of the a is equal to the number of attributes of a record) and is deleted. (2) If not be deleted, an itemi is added to it to form a new one.

Step 5. If the population C*kQ is not empty, go setp 2.

Otherwise, compares the best individual with the individual in memory pool, preserves the bigger one. If termination condition is met, the rule in memory pool is the best rule we searched.

B.  The Effectiveness of Different Representation The drawback of the traditional GA used in mining  association rule is the inefficiency in dealing with deception, epistasis and multimodality. However, with the proposed algorithm, the effect of the fitness function was restricted in a certain area in which all the individuals were at the same age.

With different representation, the areas of different age are  fixed. So efficiency of the proposed algorithm is independent of representation. We can see that from Fig.4 in which the rules with 1, 2, 3 and 4 items were described by red, pea green, pink and green circle. The rules whose confidence and support values are great than the two thresholds were shown by circle with a blue edge. The founded rules were shown in Fig.4-A, when we represent the individuals in the manner 1. In Fig.4-b, another representation manner was shown. If we look on the number of the circles between two rules as distance, we will find the distance between two rules are fixed with the changing of the representation.



V. EXPERIMENTS In this section, we examine how such an algorithm can be  used to search association rules. We have conducted an experiment on a 3.0 GHz Pentium PC with 512MB of memory running with Microsoft Windows XP to measure the performance of the proposed approach. The datasets used in the experiment are obtained from the UCI Maching Learing Repository [11].

We compared the proposed GEA-DM algorithm with GA algorithm in Table.1, which illustrates the statistic results of computation time, the threshold of confidence and support, the number of instances and attributes of the dataset. When the confidence threshold of dataset Balance Scale was set to 100%, we could almost not find a validated solution. So, we set it to 80%. One important thing we must emphasize is that the complexity of a dataset is decided not only by the number of instances and attributes, but also by the configuration of instances and the number of the items and classes in it. So, we should not simply compare computation times between different datasets. From table.1, we can see that the average performance of the proposed algorithm GEA-DM is better than the traditional GA in mining association rules.

TABLE I.  COMPARISION IN DIFFERENT DATASET  Dataset Attribute s  Instances Supp(R) Conf(R) GA(s) GEA- DM(s)  Balance Scale 4 625 >3.8% >80% 12 8  Hayes-Roth 5 132 >9.8% 100% 2.2 2  Chess 6 28056 >0.039% 100% 979 768  Car Evaluation  6 1728 >33.3% 100% 16 9  Nursery 8 12960 >33.3% 100% 166 132  Contraceptive Method Choice  9 1473 >1.4% 100% 91 69  tic-tac-toe Endgame  9 958 >9.5% 100% 59 42  Poker Hand 11 25010 >0..56% 100% 7296 5402  Adult 14 32561 >2.4% 100% 2434 1844  Congressional Voting  16 435 >50.57% 100% 174 132  Mushroom 22 8124 >21.66% 100% 350 280

VI. DISCUSSION AND CONLUSION Inspired by the natural evolutionary process, we proposed  an evolutionary algorithm, GEA-DM, for dealing with problems of mining association rules. In GEA-DM, each individual has a flexible life span during which individuals can grows and reproduces. The performance evaluation results have shown that the proposed GEA-DM approach has achieved good performance in comparison with conventional G algorithms.

Being different from GA, the performance of the proposed algorithm is stable with different representations of individuals.

This is very useful for dealing problems of deception and epistasis. Next, we will study on how to pass a message from parent individual to son individuals.

