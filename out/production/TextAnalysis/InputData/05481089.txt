A new robust classification method for CRM.

Abstract: ?Customer classification is a key step in customer  relationship management(CRM), and there are many methods  used for it, such as Neural Net, association rules, SOM model,  etc. However, most existing methods don?t take noise which is  very common in reality into consideration. In this paper , we  combine Croup Method of Data Handling(GMDH) with  Takagi and Sugeno fuzzy model(TS) to form a new  classification method TS-GMDH. The experimental result  shows that TS-GMDH outperforms the benchmark classifiers  when the noise level is high.

Keywords-TS-GMDH, experiment, noise, customer  classification

I. INTRODUCTION,  In recent years, customer relationship management (CRM) becomes an important tool by which companies can face the challenges brought by the changing market.

To cater to the customers customer analysis plays an important role, for companies hope to improve customer repurchasing rate, promote sales and win reputation with limited resources and ability. In fact, data used in CRM is always affected by noise, while the commonly used CRM customer classification methods are generally sensitive to noise, misleading decision.

Group Method of Data Handling (GMDH) which is proposed by Ivakhnenko [1].  GMDH has been recognized as a noise-immunity approach[2] and successfully applied to many real-world data mining applications[3]. Fuzzy model is a powerful model in  system modeling. There are mainly two kinds of rule-based fuzzy model: Mamdani fuzzy model and TS fuzzy model  Reference [3] proposes Self-Organizing Fuzzy Rule Induction (FRI), which combines advantages of Mamdani fuzzy model and GMDH techniques. When applied to the market survey data analysis, it automatically and objectively extracts the consumer segment market characteristics to make up for the deficiency of existing customers analytical methods.

TS model is a rule-based fuzzy model proposed by Takagi and Sugeno in 1985[4]. Compared with Mandani model, TS model can achieve higher accuracy with fewer rules.

Therefore, in recent years, many identification methods of TS model have been proposed. In this paper , we combine TS model and GMDH technique forming TS-GMDH model, which makes the TS model more useful and shows the strong noise-immunity.



II. TS-GMDH MODEL,  The modeling of TS-GMDH is very similar to FRI, the only differences is that the partial functions becomes TS fuzzy models instead of Mamdani fuzzy rules and the sample data set N is divided to a training set A and a testing set B( UN = A B ).The main steps of TS-GMDH modeling are as follows: 1) Fuzzification of variables. The initial variables are  transformed into fuzzy linguistic variables. If we define L fuzzy sets in every variables, there will be  2010 Asia-Pacific Conference on Wearable Computing Systems  DOI 10.1109/APWCS.2010.26   2010 Asia-Pacific Conference on Wearable Computing Systems  DOI 10.1109/APWCS.2010.26   2010 Asia-Pacific Conference on Wearable Computing Systems  DOI 10.1109/APWCS.2010.26   2010 Asia-Pacific Conference on Wearable Computing Systems  DOI 10.1109/APWCS.2010.161   2010 Asia-Pacific Conference on Wearable Computing Systems  DOI 10.1109/APWCS.2010.25     mL inputs in the first layer in the network (see [2] and Fig.1).

2) Forming of the first generation TS models. All the input fuzzy sets are combined in pairs to form the first generation TS models of. For example, if two  fuzzy sets isiA and j s jA defined in the space of input  variable ix and jx ( , 1,2, , )i j m? ? are  combined ,we get following TS model: 1 10 1  1 2 1 10 1  21 2 1 2  i  j  s i i i il  s j j j j  mn  if x is A then y a a x R :  if x is A then y b b x  i, j , ,...,n,i j;l , ,...,C  ? ? ? ?? ?? ??? ? ?? ?  ? ? ? ?? ??? ? ??? ? 	 ?  (1)  where 2mnC is the number of combination the  parameters in the consequents parts are estimated by Ordinary Least Square(OLS)in the training set A.

3) Selecting of  Models. 1F best TS models are  selected in the testing set B by external criterion(see the selecting of the first generation TS models in Fig.1). Regularity criterion in following form is applied in our algorithm[1]:  21 2 1 2 1 11 2 1 2   B  i  G G? ?y ( y y ) G G G G?   ? ? ? ?? ?? ?  ?        (2)  where y is the real output , B denotes the number of  data in the set B, 11y? and 1y? represent the outputs of the  two rules in the model,  1 is i iG A ( x )? and 2 j  s j jG A ( x )? are firing strength of the  two rules.

4) Rules fusion. The two rules of 1 lR are merged into  one rule which is still denoted by 1 lR :   1 10 1 1 11  ji ssl i i j j  l i i j j  R : if x is A and x is A  then y a a x a x ,l ,...,F  ?? ?? ?? ? ?? ?? ?  ??? ? ? ? ? (3)  where fuzzy set isiA j s jA will do the conjunction  operation the coefficients of the consequent linear function remain to be decided 5) Generating the second generation TS models. After  merging, the 1F rules are combined in pairs to form  following second generation TS models denoted  as 2 lR :  2 20 2 2   2 20 2 2  ji  k h  ss i i j j  i i j jl  s s k k h h  k k h h  if x is A and x is A  then y a a x a x R :  if x is A and x is A then y b b x b x  ? ? ? ? ? ? ? ??? ? ? ?? ?? ? ? ? ?  ? ? ? ? ? ? ??? ? ?? ?? ? ? ??  (4)  where the parameters in the consequents are still estimated in the training set A.

6) Generating and merging of the second generation TS  models. For the second generation TS models obtained in step 5), step 3) and step 4) are repeated to  get  2F second generation fuzzy rules as follows   2 20 2 2 2 2  ji  k h  ssl i i j j  s s k k h h  l i i j j k k h h  R : if x is A and x is A and  x is A and x is A then y a a x a x a x a x  ? ? ? ? ? ?  ? ? ?? ? ? ?? ?  ?? ? ? ? ? ?  (5)  7) Forming of the final rules. Step 5) and step 6) are repeated to generate TS models of the 3th 4th ?generation until the external criterion begin to increase. If the increasing happens in the kth layer,  kF best TS fuzzy models are chosen and then the  kF TS models are integrated into kF fuzzy rules  according to step 4). These kF fuzzy rules make up  of the final TS model, and parameters in the consequent part are estimated by OLS.

8) Calculation of the final output. The output y is computed by taking the weighted average of the  outputs of  the kF rules in the final model.

Fuzzification  1x      ix   nx   1x  mx1 ix   m ix nx   m nx  TS  TS  TS  TS  TS  TS  TS  TS  TS  TS  TS  TS  Selecting of 1th generation  Selecting of 2th generation  Selecting of kth generation  Weighted Average  y  1th generation TSmodel  2th generation TSmodel  kth generation TSmodel  Initial input  Figure1 Network of TS-GMDH modeling

III. EXPERIMENT  A. Experimental setting  The experiment starts with comparison between TS-GMDH and FRI in Matlab, for TS-GMDH is proposed based on FRI. Next, we compare TS-GMDH with four classification algorithms: Na?ve Bayes(NB), Support Vector Machine(SVM), C4.5 decision tree(C4.5) and K Nearest Neighbour(KNN) under four levels of noise, which  automatically run in Weka, for they are used for classification in our daily life and work for a long time and perform generally well.

We use classification accuracy to measure classifiers? performance and introduce 10-fold cross-validation to finish algorithm comparison on 9 UCI data sets. Data sets used in the experiment come from UCI database, basic information of which is listed on TABLE1  TABLE1 Data sets used in experiment Data set Abbr. #instance #attribute  Credit Approval credit-a 690 15  Echocardiogram Data Echo 132 12  German Credit credit-g 1000 20  Haberman's Survival Haberman 306 3  MAGIC gamma  telescope Magic 19020 10  Mammographic Mass  Data Mammo 961 6  Mammographic Mass  Data Mammo 961 6  Wisconsin Breast  Cancer Breast 699 10  Wisconsin Diagnosis  Breast Cancer WDBC 699 10  B. Result  Comparison between TS-GMDH and FRI To test whether TS-GMDH model makes some  improvement on FRI model, we compare the two models, and TABLE2shows the result.

TABLE 2 demonstrates that TS-GMDH model achieves higher accuracy on 8 sets out of 9 data sets in comparison with FRI. We do paired t-test on the result and set confidence level as 95%. It shows that the difference on all sets except ? Haberman? and ?Credit-g? is significant  TABLE2 Mean and Variance Data set FRI TS-GMDH  Breast 93.15?0.60 93.95?0.62  Credit ?a 85.51?0.10 84.91?0.59  Credit-g 71.06?0.62 71.06?0.96  Echo 95.22?1.82 96.68?0.88  Haberman' 73.12?1.64 74.14?0.39  Magic 72.39?5.8E-15 79.8?0.13  Mammo 79.52?0.44 81.47?0.85  Pima 73.94?0.45 75.04?0.60  WDBC 92.02?0.63 93.69?0.77  To test the overall performance of TS-GMDH, we treat the average results in 10-fold cross-validation of TS-GMDH and FRI on each data set as a group, forming 9 groups, then tested by Wilcoxon-Signed-Rank Test. It shows that there is significant difference between the two algorithms on the whole.

Therefore we come to the conclusion that TS-GMDH can get higher classification accuracy than FRI.

Comparison between TS-GMDH and benchmark classifiers  In this subsection, we compare TS-GMDH with NB, SVM, C4.5 and KNN. It demonstrates that the average rank of TS-GMDH is 3 ranking 4th among them without noise. However, it ranks 1st when there is 5% noise and keeps ahead when the level of noise gets higher showing that TS-GMDH outperforms other four classifiers under high level of noise.

We record the accuracy of each algorithm and draw line chart to show the trend directly. TABLE3andFig.2     show the result on credit-g The line charts on other 8 sets are similar to that on credit-g.

TABLE3 Result on credit-g  Algorithm Noise level  0.00% 5% 10% 20%  TS-GMDH 71.06% 68.81% 69.25% 66.87%  C4.5 70.70% 65.10% 66.10% 55.40%  SVM 75.20% 72.40% 71.30% 65.30%  NB 74.60% 71.20% 67.60% 63.10%  Figure2 Performance of each algorithm on credit-g  And then, we test the result under no and 20% noise with Friedman test, which is often used to test the performance of multiple algorithms. It turns out that there is no significant difference between each algorithm without noise ,while there is significant difference with 20% noise.

Next, we do post-hoc test on the result of 20% noise with Bonferroni-Dunn test. The result shows that TS-GMDH outperforms C4.5 and KNN, while we can?t tell whether there is significant difference between SVM and TS-GMDH, or TS-GMDH and NB.Therefore, we adopt Wilcoxon test, and it shows that there is significant difference, that is TS-GMDH outperforms SVM and NB.

In summary, we come to another conclusion that, TS-GMDH gets similar accuracy as others, however, when noise level gets higher, it performs outstanding.



IV. CONCLUSIONS  In this paper, we firstly make comparison between TS-GMDH model and FRI model with 10-fold cross validation in Matlab. The following conclusion can be drawn, TS-GMDH model is better than FRI model on classification accuracy, showing the improvement of TS-GMDH model.

And then, we compare TS-GMDH model with popular classification algorithms :SVM, C4.5,NB and KNN. We introduce 3 different level of noise: 5%,10% and 20% to how the TS-GMDH performs in comparison with other classification algorithms with noise. We draw the second conclusion that, as noise level gets higher, the performance of TS-GMDH is far superior to others, showing extra capability of anti-interference.

As we all know, the real life and work is filled with noise, we can?t identify the true information when making decision, easily causing serious consequence. In view of strong noise immunity of TS-GMDH, we recommend a new classification method in customer analysis, help companies to get useful information.

