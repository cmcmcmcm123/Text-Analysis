Tree Projection-based Frequent Itemset Mining on multi-core CPUs and GPUs?

Abstract  Frequent itemset mining (FIM) is a core opera- tion for several data mining applications as asso- ciation rules computation, correlations, document classification, and many others, which has been extensively studied over the last decades. More- over, databases are becoming increasingly larger, thus requiring a higher computing power to mine them in reasonable time. At the same time, the ad- vances in high performance computing platforms are transforming them into hierarchical parallel environments equipped with multi-core processors and many-core accelerators, such as GPUs. Thus, fully exploiting these systems to perform FIM tasks poses as a challenging and critical problem that we address in this paper. We present efficient multi-core and GPU accelerated parallelizations of the Tree Projection, one of the most competitive FIM algorithms. The experimental results show that our Tree Projection implementation scales al- most linearly in a CPU shared-memory environ- ment after careful optimizations, while the GPU versions are up to 173 times faster than standard the CPU version.

1 Introduction  Frequent itemset mining (FIM) is a core op- eration for various data mining applications, at- tracting attention of the research and commercial communities in the recent years [7, 20]. Given a database of transactions, where each transaction contains a set of items, it consists of finding all itemsets that occurs more than a given user thresh- old (support).

?This work is partially supported by CNPq, CAPES, FINEP, FAPEMIG, and INWEB- the Brazilian National Institute of Science and Technology for the Web.

Due to its importance, a number of works have focused on this problem and several FIM algo- rithms have been proposed [2, 3, 11, 20]. These algorithms may be divided into two classes. The first class includes those algorithms which are an Apriori [3] variant [20] and find frequent item- sets based on a candidate-generation approach that is levelwise and followed by the frequency count- ing of the candidates. The second class of algo- rithms, which includes Tree Projection [2] and FP- Growth [11], is based on projection techniques that employ a tree construction method based on prefix trees [1] and perform mining recursively on pro- gressively smaller databases. This second class of algorithms has been shown to outperform the first class, being an order of magnitude faster in several scenarios.

The current advances in computer architecture are transforming traditional high performance dis- tributed platforms into multi-core and many-core hierarchical environments. Thus, creating effi- cient FIM algorithms that fully exploit this paral- lelism is a very important contribution. A num- ber of works have also developed parallel formu- lations for finding frequent itemsets based on the candidate-generation approach (Apriori ? like), both for shared- and distributed-memory comput- ers [10, 14, 18, 19], including a GPU single node based implementation of Apriori [8]. However, the projection-based algorithms have received rel- atively little attention [5, 13], while they have been shown to be an order of magnitude faster than the candidate-generation based. Moreover, to the best of our knowledge, it does not exist a projection- based FIM algorithm for GPUs.

In this paper we propose the first projection- based FIM implementation for GPUs. We par- allelize the Tree Projection algorithm for multi- core CPUs and GPUs. The Tree Projection was chosen as the target of our implementation be-  2010 22nd International Symposium on Computer Architecture and High Performance Computing  DOI 10.1109/SBAC-PAD.2010.15     cause its most compute intensive step is suitable for data parallelism using a GPU or multi-core pro- cessors. After careful analysis our implementation has achieved almost linear speedups in a multi- core shared memory machine, while its GPU- version is up to 173 times faster than the CPU- based sequential code.

2 Tree projection  The tree projection-based frequent itemset min- ing algorithm (Tree Projection) employs a lexico- graphic tree as its core data structure, which is used to guide mining operations. Nodes in this tree are the frequent itemsets and it assumes a lexico- graphic ordering among items in a database. In this tree, a node on depth (or level) k corresponds to an itemset of size k, while the root node has no itemset.

Figure 1. The lexicographic tree with absolute support value of 2.

In order to explain the algorithm we first de- fine some concepts: The 1-extensions of a node X is the set of itemset extensions that are gener- ated from it, i.e. nodes whose itemsets have node X itemset as prefix and so are called frequent lexicographic tree extensions. In our example, Figure 1, the frequent lexicographic extensions of node a are c and h; the active extensions of a given node is the subset of its tree extensions that can still generate frequent itemsets; a node is in- active when it has no child that can generate more frequent itemsets; the candidate branches refers to the set composed by the last item of the item-  sets of brothers on the right of a given node, and a node that is singleton has no candidate branches; the set of active items are the same as its candi- date branches if it has been just created, thus be- ing at the tree boundary, or it is recursively defined as the union of the itemset of its active extensions with the lists of active items of all nodes included in these active extensions.

The projected transaction of a node is the in- tersection of the transaction?s items with the active items of this node. For example, if a transaction is abcdef and the active items of a given node are (b,c,e), the resulting projected transaction is bce. If the itemset of the node is not in the transaction, the corresponding projected transaction is null. For further details please refer to [2].

Algorithm 1: BreadthFirst Data: minsupport : s,Database : ? Result: Frequent itemsets freqOnesAtts = firstScan(support, T ); buildTopLevelTree(freqOnesAtts); k = 1; while kth level 6= ? do  createTriangularMatrices(kth? 1); foreach transaction T ? ? do  addCounts(T,RootNode); checkSupport(kth? 1); createNewNodes(kth+ 1); deleteTriangularMatrices(kth? 1); pruneTree(kth+ 1); k++;  The Tree Projection algorithm extended in this work is based on a breadth-first search on the lex- icographic tree, where all nodes at level k are cre- ated before nodes at level k+1. In Algorithm 1 we present its pseudocode. The first step of this algo- rithm is a scan in the database computing the fre- quency of each attribute to determine the frequent 1-itemsets (firstScan). This information is then used to create the top level of the lexicographic tree (buildTopLevelTree), which consists of the root and its frequent 1-itemsets children nodes.

After this initial phase, the algorithm will expand the tree levelwise, when it creates counting matrices in nodes at level k ? 1 (createTriangularMatrices(kth ? 1)), which are then used to count the frequency of nodes at level k + 1. Each node P at level k ? 1 maintains a matrix of dimensions |active extensions(P )| ? |active extensions(P )|, where a row and col- umn exists in this matrix for each item i ? active extensions(P ), and each entry that ex- ists in this matrix indicates the counting of itemset     P ? i, j (Since the matrix is symmetric, only one lower or upper triangular part is maintained).

The algorithm then proceeds by incrementing the appropriate entries in the existing counting ma- trices at level k ? 1, executing the addCounts for each transaction. During this phase, each trans- action is projected from the root node to nodes at level k ? 1, and, if the projected transaction is not null, its items are combined to increment the ma- trix entries as shown in Algorithm 2.

After processing all transactions, the matrix of each node at level k ? 1 is verified re- garding its support (checkSupport(kth ? 1)), and a new node in the tree is then created at level k + 1 for each entry with adequate sup- port (createNewNodes(kth + 1)). Then, the matrices of nodes at level k ? 1 are deleted (deleteTriangularMatrices(kth? 1)).

Algorithm 2: AddCounts Data: transaction : T,Node : P,Level : k if P ? levelk ? 1 then  foreach (i, j) ? 2? ascent? combination(T ) do  P.matrix[i, j] + +;  else foreach active extension i ? P do  T ? =: Project transaction T onto i; Call addCounts(T ?);  The last step of the algorithm prunes the tree (pruneTree(kth + 1)), which is done by travers- ing it in a bottom-up fashion, removing the in- active nodes and updating the remaining nodes.

First, the singleton nodes created at level k+1 are removed, since they have no one to combine with.

It then continues pruning the tree and updating the active items in each node. After removing the in- active nodes of a level, the remaining nodes have their active items lists updated following the re- cursive definition already discussed. The program finally ends when the newest level created has no active nodes.

In Figure 1 we show an example lexicographic tree (using absolute support value of 2) that has been generated and examined up to level 2, but the nodes at the level 3 have only been created.

3 Parallel Tree Projection  This section describes the Tree Projection im- plementations for multi-core processors and accel-  erators. First, in Section 3.1 we discuss the par- allelization opportunities of the algorithm and the adopted strategy. Section 3.2 then details our im- plementation to multi-core shared-memory envi- ronments, where several approaches to avoid race conditions when updating shared objects are stud- ied. In Section 3.3 the GPU accelerated version of Tree Projection is presented.

3.1 Parallelization strategy  As discussed in Section 2, the Tree Projection algorithm performs the frequent itemsets count- ing based on a lexicographic tree, which is built at run-time and may be constructed through var- ious strategies. In this paper, as in the original Tree Projection implementation [2], we employed a breadth-first strategy which, as shown in Algo- rithm 1, builds each level of tree in consecutive iterations of the algorithm.

This tree building method embeds two paral- lelization opportunities: (i) the transactions pro- cessing parallelism consists of processing the transactions in parallel on the available hardware, updating each counting matrix of the nodes at level k ? 1; (ii) the node level parallelism, where each node being expanded is assigned to a different pro- cessor, which processes all transactions updating its node matrix.

While both strategies may have advantages in different scenarios, we chose the transactions processing parallelism due to the following rea- sons: (i) the execution time is dominated by the addCounts method that processes the transac- tions, making it a very interesting candidate for parallelization; (ii) if the node parallelism was em- ployed, on the other hand, there would be a load imbalance and the number of nodes may not be enough to fully utilize all the processors during the entire execution; (iii) the node level parallelism also demands more memory, because the matrices of all nodes being concurrently updated need to be in memory, which may be unfeasible in many sce- narios.

Although the transactions processing paral- lelism has several advantages over node level par- allelism, it shares the tree nodes that are being ex- panded among the parallel processes, which may try to concurrently update the same cell of a given node matrix, requiring synchronized accesses. The section below explains different methodologies to deal with this problem on the CPU and GPU im- plementations.

3.2 CPU parallel implementation  This section presents the CPU shared-memory parallel Tree Projection. The proposed solution employs transaction processing parallelism, dis- cussed in the last section, dividing the transac- tions among different CPU threads that perform the addCounts operations independently. Other phases of the algorithm, which were shown to be inexpensive, are kept sequential.

The application?s parallel section is built based on a work queue (transactions) that is shared among threads (AdderThreads) [6], providing a dynamic assignment that reduces load imbalance among them. Our implementation also assigns blocks of transactions in order to minimize the overhead.

The addCount operation, shown in Algo- rithm 2, is applied to each input transac- tion updating the proper matrices. However, when processing transactions in parallel, different AdderThreads may need to update the same cell (i, j) of a given node (P ) matrix (P.matrix[i, j]+ +), demanding synchronized accesses. This prob- lem, although easily solved in many scenarios, is a critical aspect in achieving scalability in Tree Pro- jection, as this shared object is at the core of the algorithm.

A simple way to eliminate the overhead associ- ated with avoiding these race conditions is Data replication, where each process asynchronously updates its local copy with a reduce at the end of the parallel section. Unfortunately, in the Tree Pro- jection it is an unacceptable solution as the size of data to be replicated (matrices for each tree node) may be very large and could then create an unfea- sible memory requirement. Another way to avoid such undesirable conditions is by creating critical sections to prevent threads from updating matri- ces? cells concurrently. However, when solving this problem it is important to analyze the size of the critical sections, and the type of synchroniza- tion primitive. For instance, in [14] the authors assess the impact of the size of critical sections to the performance of data mining tree based appli- cations, which has a similar structure to Tree Pro- jection, based on software synchronization prim- itives. Next we discuss three different software- based lock schemes and a new lock-free imple- mentation using hardware based atomic instruc- tions.

Tree level locking creates a critical section that is associated with each level of the tree, thus only one thread can increment matrices at a time. This approach is simple and inexpensive in terms of overhead to create locks, but it imposes a barrier  among threads accessing different nodes.

Node level locking employs a lock for each  Node at level k ? 1 being updated. Despite its higher lock management overhead, locking at the node level will allow different threads to update matrices of different nodes in parallel.

Cell level locking: employs a lock for each cell, which may generate high memory require- ments as the memory used by locks is larger than the needed by nodes? matrices. Moreover, each cell update may generate extra cache misses when reading its lock structure.

In some conditions, where the critical section is very small and frequently executed by the par- allel threads, using traditional software based syn- chronization primitives may not be a good choice as it may become a bottleneck to the application scalability. Thus, in this paper, we evaluate the use of Wait-free [12] instructions, which are im- plemented at the hardware level and are avail- able in most of the modern multi-core proces- sors, to update such objects shared among differ- ent threads. The Wait-free instructions guarantees atomicity when updating memory and eliminates the need for critical sections, as provided by mu- texes and semaphores. Using a wait-free opera- tion as sync add and fetch(address, value), included as a gcc built-in, a memory address is guaranteed to be incremented by value even when address is a shared object. In order to provide the atomic add it loads the old value pointed by ad- dress, adds value to it, and updates the address with a new value if the old is still the same. To avoid the ABA problem, one solution is to keep a counter to make sure that the threads are updating the correct data generation [4].

3.3 GPU implementation  The Tree Projection is a very computate- intensive application which makes it an interest- ing candidate for GPU acceleration. As other al- gorithms for FIM [11, 20], however, it is irregular in several dimensions, for instance, because of the different costs for processing each transaction or node matrix. This challenging behavior imposes a barrier for the development of efficient FIM algo- rithms for GPU, as these processors are more suit- able for very regular and parallel computations.

The proposed parallelization of Tree Projection is implemented using CUDA [16], and, as in the multi-core shared-memory implementation, it ex- ploits the transactions processing parallelism. The addCount function was then implemented as a GPU kernel that process several transactions con- currently.

Algorithm 3 shows the pseudo-code of the modified breadth first tree building strategy. As discussed, the addCounts is now a kernel func- tion that is executed on the GPU and processes a set of TS transactions each time, exploiting the GPU parallelism by assigning different transac- tions to different GPU threads. Since transactions are organized in lines, one dimension of blocks and grids is enough.

In the addCounts step, a set of nodes is deliv- ered to GPU, always respecting its memory con- straints. In the GPU, transaction blocks are pro- cessed for each node, where several transactions of one block are projected in the active items of the nodes and various entries of the node matrices are updated simultaneously. After all transaction blocks are processed, the nodes are brought back to CPU and their matrices are used to build the next tree level.

It is important to note that in this GPU version, the projection of transactions are made directly in the k ? 1 level nodes instead of projecting them all along from the root in a top-down fashion like the original implementation. Although this strat- egy goes against the benefits of eliminating trans- actions that will not generate frequent itemsets as soon as possible, it is easier in the GPU.

Another critical problem we address is how to efficiently represent a set of transactions on GPUs.

In the existing FIM algorithm for GPUs [8], the authors use a m ? n transaction-bitmap approach to represent the database, where n is the number of transactions and m is the number of items, and bit (i,j) is set to 1 if item i occurs in transaction j. Although this representation is interesting due to its regularity and easy manipulation on GPUs, it may not be acceptable to store sparse databases due to the memory requirements.

Thus, we propose a novel compact and simple representation of the database of transactions for GPUs, which is not only useful to FIM algorithms but may also, for example, be used to compute in- verted indexes on these processors. The proposed structure, shown in Figure 2, stores the transac- tions using a vector, adding the size of each trans- action just before its items. In addition, it creates an auxiliary index to identify the beginning of each transaction on the vector where they are stored.

Besides being a very simple and intuitive way to represent sparse database, this approach provides a very efficient access pattern, as data elements are successively located in the memory, a compact data representation for sparse datasets.

Algorithm 3: BreadthFirstGrid (CPU) Data: minsupport : s,Database : ? Result: Frequent itemsets freqOnesAtts = firstScan(support, T ); buildTopLevelTree(freqOnesAtts); k = 1; while kth level 6= ? do  createTriangularMatrices(kth? 1); foreach transactionsSet TS ? ? do  grid := (TS/Th, 1, 1); threads := (Th, 1, 1); addCounts <<< grid, threads >>> (TS, kth? 1);  checkSupport(kth+ 1); createNewNodes(kth+ 1); deleteTriangularMatrices(kth? 1); pruneTree(kth+ 1); k ++;  Figure 2. Sparse data representation.

4 Empirical results  The experiments were performed using two machines: (i) a dual quad-core AMD Opteron 2.00GHz processor, 16GB of main memory, an NVidia GeForce GTX260 GPU, and Linux oper- ating system, and (ii) a second machine equipped with an NVidia GeForce GTX470 GPU. We tested the algorithm implementations using the three databases presented in Table 1. The synthetic datasets were generated using a data generator ob- tained from IBM Almaden, and the procedure de- scribed in [3]. In order to provide a fair compar- ison among different versions of the application, the programs were compiled using the flag ?-O3?.

The results shown in this section are an average of 5 executions, the highest standard deviation for CPU and GPU execution are, respectively: 0.75% and 0.5%.

4.1 CPU multi-core analysis  This section analyzes the Tree Projection im- plementations to multi-core shared-memory ma- chines, as a function of the input datasets charac- teristics. In Figure 3, we present the performance of the proposed approaches to update shared ob-     Dataset #Items Avg. Length Transactions Density Characteristics Sparse repr. size Bitmap size T25.I20.D1000K 10,000 25 1,000,000 0.25% Synthetic ? 49 MB ? 1,192 MB T35.I20.D1000K 10,000 35 1,000,000 0.35% Synthetic ? 68 MB ? 1,192 MB T45.I20.D1000K 10,000 45 1,000,000 0.45% Synthetic ? 87 MB ? 1,192 MB  Table 1. Dataset characteristics.

jects: Tree level locking (Level), Node level lock- ing (Node), Cell level locking (Cell), and Wait- free based (W-F).

In our first evaluation the average transaction length is varied, for a fixed support value of 1%, as shown in Figures 3(a), 3(b), and 3(c). As a reference, the sequential execution times for the databases: T25.I20.D100K, T35.I20.D1000K, and T45.I20.D1000K, are respectively 68.96, 105.41, and 296.82 seconds. For the first dataset, Fig- ure 3(a), all proposed solutions achieved good scalability, being the W-F scheme slightly better than the others. When using the second dataset: T35.I20.D1000K, on the other hand, Level and Node lockings strongly failed to exploit the avail- able parallelism. It occurred because as the av- erage transaction length (T) increases there is a higher pressure over the critical section, as the pro- jected transactions are also larger, thus only fine grained locks could scale. These results are inter- esting because although the synchronization sec- tion refers to a very small piece of code (just in- crement a variable), it is executed very frequently and will scale poorly for coarse grained critical sections.

When analyzing the T45.I20.D1000K dataset (See Figure 3(c)), an even higher degradation of coarse grained lock based solutions is observed, while W-F and Node schemes still having good speedups. For this database, however, the W-F speedups is superlinear. In order to explain this behavior we measured the number of L2 cache misses for this scheme, Figure 3(d), which shows a reduction in misses as the number of threads in- creases. The observed behavior is due to sharing of read only objects among threads when projec- tion transactions in nodes. The cache misses where measured using oprofile [15] and event counter setted to 10,000.

Finally, in Figure 3(e), we evaluate the impact of support to the application performance as it as- sumes the following values: 0.5%, 0.75%, and 1%.

First, as shown, all schemes suffer with reduction of support, which is due to larger resulting pro- jected transactions at the tree nodes and more in- crements on the critical section. For the shown support, although, the W-F scheme is the most ef- ficient for all configuration, achieving near 20% higher performance over the Cell locking, on aver- age.

4.2 Tree Projection on GPUs  The Tree Projection implementation to GPUs is evaluated in this section. Firstly, Table 1 presents the size in MB for 3 databases when using our proposed sparse representation and the Bitmap based currently used by other FIM algorithms on GPUs [9]. As shown, the amount of memory used by Bitmap representation is much higher than our proposed strategy, making it unfeasible to repre- sent low density and very large databases. This problem is worsened for GPU accelerated appli- cations, as data transfers among CPU and GPU is one of the critical problems for achieving high per- formance in such accelerators [17].

We also evaluate the impact of the support and the GPU type for the Tree Projection performance, comparing its accelerated version to the sequen- tial CPU based implementation. In Figure 4 we present GPUs speedup over the sequential CPU version as the support is varied, while Table 2 shows the absolute execution times for each pro- cessor and the dataset T45.I20.D1000K.

Figure 4. GPUs speedup.

These results show that our proposed GPU based Tree Projection is very efficient when com- pared to the CPU version, and is also capable of scaling up the support decreases. The GTX470 and GTX260 achieved maximum speedup over the sequential Tree Projection of 173? and 95?.

Moreover, the GTX470 near doubles the GTX260 performance for all values of support, what was expected, as it represents a newer generation con- taining more stream processors, bandwidth, etc.

Although gains of the GPUs version of Tree Projection over the sequential CPU still high as the     (a) Speedup - T25.I20.D1000K (b) Speedup - T35.I20.D1000K (c) Speedup - T45.I20.D1000K  (d) W-F cache misses (L2) - T45.I20.D1000K (e) Support variation - T45.I20.D1000K  Figure 3. Tree Projection on multi-cores.

Support CPU GTX260 GTX470 1.5% 164.068 1.730 0.945  1.25% 203.293 2.509 1.405 1% 296.827 3.906 2.221  0.75% 332.128 6.179 3.543 0.5% 379.314 10.003 5.865  Table 2. T45.I20.D1000K: execution times (secs.) per processor.

support decreases, the relative performance among them is reduced. This difference in performance is the result of the CPU version?s ability to eliminate the transactions which are not useful at lower lev- els of the tree, while the GPU evaluates all trans- actions of the candidates in each level, and conse- quently is not capable of reducing the computation cost for smaller supports as less transactions are used at the lower levels.

The Tree Projection on GTX470 GPU relative performance, over the sequential CPU-based ver- sion, as the average transaction length varies for 25, 35 and 45 is presented in Figure 5. As shown, the GPU relative performance is higher as the av- erage transaction length increases. This behavior is due to higher processing costs of larger trans- actions and consequently more pressure over the parallel section, boosting the GPU Tree Projection as its parallel threads have more work per kernel call.

Finally, in Figure 6, we present the relative ex- ecution times as the database is scaled, having the T45.I20.D1000K as the base case. As shown, the CPU and GPU execution times increase almost linearly according to the number of transactions,  Figure 5. TX.I20.D1000K: GPU speedup varying avg. trans. length.

showing a very good scalability of the algorithm in both implementations.

Figure 6. Scaling the database.

5 Conclusions  In this paper we presented a multi-core shared memory and a GPU-based parallelizations of the     Tree Projection algorithm, which is, to the best of our knowledge, the first projection-based frequent itemset mining implementation for GPUs. We also conducted a detailed analysis of the algorithm?s performance on multi-core processors, providing a study of several strategies to mitigate potential race conditions when processing transactions in paral- lel.

The experimental results showed that we could achieve almost linear speedup on multi-core shared memory platforms through the use of low level wait-free atomic instructions, while tradi- tional software based locking strategies failed to provide scalability. Our GPU-based implementa- tion of the Tree Projection, on the other hand, pro- posed a new simple sparse data representation and achieved speedups of up to 173 when compared to sequential code compiled with ?-O3? flag. These results are very relevant, as the sequential Tree Projection is already an order of magnitude faster thanApriori based implementation in several sce- narios. As future work, we intend to develop a distributed GPU accelerated Tree Projection, and utilize CPU and GPU to concurrently execute its most compute-intensive and parallel section.

