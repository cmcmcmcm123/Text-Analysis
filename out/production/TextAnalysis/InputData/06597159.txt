Data Mining Approaches for Packaging Yield Prediction in the Post-Fabrication Process

Abstract?In the post-fabrication process for semiconductors, it is critical to predict the yield. This process consists of a series of electrical and physical tests following semiconductor fabrication, tests that generate a significant volume of parametric data. While past research has investigated yield prediction using parametric test data, most studies have difficulty correctly predicting the low and high yield because of the wide range of variables and the large data set. Also, in the case of the packaging yield, prediction is inaccurate as this yield does not directly correlate with the parametric test data.

Therefore, this study proposes a framework in which the packaging yield is classified using the parametric test data of the previous step of the packaging test. This study involves three stages. In the first, data preprocessing is conducted due to the large data set. To learn a data mining model using much more data, parametric test data generated in the die level need to be changed into the wafer level. In the second stage, a random forest algorithm is used to select significant variables affecting the packaging yield. Finally, the third stage uses a nonlinear support vector machine (SVM) to classify the low and high yield. Through the three stages, this study demonstrates that this proposed algorithm has a superior performance.

Keywords-Random Forests; Ensemble Support Vector Machine; Packaging Yield Classification; Semiconductor Manufacturing Process

I. INTRODUCTION The semiconductor manufacturing process produces core  components such as memory and central processing units (CPU). Because of the high demand for the newest electronic devices, such as smart phones and tablets, it is important to consistently improve the manufacturing process through state-of-the-art equipment or process management techniques. In particular, accurate yield prediction is imperative in managing the process. Yield prediction is conducted by classifying the low and high yield groups in the final test step of the post-fabrication process. However, this step in semiconductor manufacturing is complicated because there are hundreds of manufacturing processes  which take several months to complete. In addition, to accurately manage the yield, semiconductor manufacturing companies require a super-clean environment, regular equipment maintenance and comprehensive worker training [1]. But above all yield management through the analysis of data generated during the post-fabrication process needs to be conducted. Therefore, yield classification is generally performed using parametric test data. In the post-fabrication process, the packaging yield is more important than other kinds of yield. Fig. 1 represents the sequence of steps and the yield in the post-fabrication process.

Figure 1. Flow chart of the semiconductor manufacturing process  The semiconductor manufacturing process consists of about 300 to 400 units and it takes 3 to 4 months to complete a finished chip [1]. First, wafers go through the fabrication process for 2 to 3 months before moving on to the post- fabrication stages. The post-fabrication process consists of a wafer test, the assembly, and the final test [2]. In conjunction with each of these steps, the fabrication yield, the wafer test yield, the assembly yield and the packaging yield are all calculated. The fabrication yield refers to the ratio of wafers in to wafers out. The wafer test yield is calculated as the ratio of whole chips of wafer out and the number of non-defective chips as decided by the wafer test. The assembly and  Fab Wafer Test Assembly FinalTest  Prediction Model  Parametric data  Classifying high/low yieldWafer  Wafer test yield  Packaging yield  Assembly yield  2013 IEEE International Congress on Big Data  DOI 10.1109/BigData.Congress.2013.55     packaging yields are the ratio of the input chips after assembly and the input chips after the final test respectively.

This study focuses primarily on the packaging yield, in particular the classification of the low and high yield based on the probe test data.

In order to best predict the yield in the semiconductor manufacturing process, a great deal of research has been performed. However, most research has focused on the wafer test yield prediction, rather than the packaging yield prediction. The yield prediction that uses a markov chain is conducted by calculating the probability that a chip is acceptable given ? defects [3]. However, this prediction is limited due to the univariate analysis on which it is based. So, as the number of variables from the probe test is enormous, multivariate analysis is essential. A classic method for multivariate data analysis is multiple discriminant analysis.

However, this method of analysis is limited by the assumption that variables are independent, identical and normally distributed. For more advanced multivariate analysis, the hybrid machine learning method is applied in the semiconductor manufacturing process, using an inductive decision tree, a neural network with a back-propagation algorithm, and a self-organizing map (SOM) [4]. Also, the yield prediction is conducted using a fuzzy-based neural network from the multivariate parametric test data [5].

However, this prior research has not focused on the packaging yield. To predict the packaging yield, a stepwise support vector machine (S-SVM) algorithm is applied to the wafer test data [1]. S-SVM classifies the low and high yield of the final test by screening for potential defects in the final test process. However, problems with parameter selection, step selection, and classification accuracy exist in this algorithm. Therefore, this paper proposes a data mining approach that selects significant parameters and improves classification accuracy.

The rest of this paper is organized as follows. Section II and Section III introduce the basic concept of the random forest algorithm and the ensemble support vector machine (E-SVM) algorithm respectively. Section IV describes the proposed algorithm based on Sections II and III. In Section V, the experimental results are explained after the proposed algorithm is applied to actual industrial data. Finally, a conclusion and further studies are described in Section VI.



II. RANDOM FORESTS ALGORITHM Widely known and efficient, the random forests  algorithm is used for both classification and regression.

Similar to the ensemble method, this algorithm is based on bagging (bootstrap aggregating) and is introduced by Breiman [6]. Bagging is a type of ensemble algorithm and is generally applied in the decision tree method [7]. The advantage of random forests is that there is no need for pruning trees and it automatically generates the importance and accuracy of variables. Also, this algorithm is not sensitive to outliers in training data.

The basic principle of random forests is to combine many binary decision trees built by using several bootstrap samples and choosing a subset of predictors randomly at each node  [8]. The basic flow of random forests is represented by using the following algorithm [9].

? Step 1: Generate ? trees from the bootstrap sample data. The number of trees means the training cases; let the number of variables for classification be ?.

? Step 2: Input ? variables to be used to find a random split point at a node of the tree unlike an optimal split point; m should be far smaller than ?.

? Step 3: Determine a training subset for this tree by choosing ? times with replacements from all ? available training cases. To estimate the error of the tree, the remaining cases are used by predicting their classes.

? Step 4: For each node of the tree, randomly choose ? variables on which to base the decision at that node.

? Step 5: Find an optimal split point based on ? variables in the training set.

? Step 6: Grow each tree completely; do not prune them  The random forests algorithm is used for the selection of variables as well as classification and regression. Many selection procedures are based on the importance of the variable to ranking and model estimation so that a family of models can be generated, evaluated and compared [8].



III. SUPPORT VECTOR MACHINE The SVM is a supervised learning method that classifies  data by using the hyperplane maximizing margin and is introduced by Vapnik [10]. The SVM performs exceptionally in document classification, image recognition and text recognition [11]. The training data set for the SVM can be represented as (1) and it consists of predictors and a class label.

? = ?(?	, )|?	 ? ?, ? {?1, +1}?	?? ?  (1)  The training data set ? includes ? elements, pairs of ?- dimensional vectors ? known as a predictor and class  corresponding to  ? . The hyperplane is defined as ? ? ? ? ? = 0 and it is known as the SVM classifier. As shown in Fig. 2, the margin 2/??? should be maximized for more accurate classification. The objective function that maximizes 2/??? is reformulated to minimize ???? .

However, as shown in Fig. 2, a hyperplane that can totally classify two class data does not exist. Therefore, a slack variable (?) is used for inseparable cases. The slack variable adjusts the degree of misclassification and is adopted for the entire training data set.

Figure 2. The concept of the SVM classifier  In (2), which represents a modified objective function, the multiplication of the penalty cost and summation of all (?) are added to the existing objective function. Based on the objective function, the optimization problem is formulated by adding constraints as shown in (2).

???????? ????  ? + ? ? ! ?	?	??                                        (2)  "#?$? % %& (? ? ?	 ? ?) ' 1 ? ?	,   1 * ? * ?  Also, this problem can be extended to the dual problem as shown in (3).

?-?????? ? ? ! .	?	?? ? ?  ? ! .	.3  3?	3?? 4?  5?36 (3)  "#?$? % %&    0 * .	 * ? (i=1,?,n),   ! .

= 0?	??  The function of the SVM linear classifier is formulated as (4), where "7?(?) represents the function that returns +1 when ? is less than 0, otherwise ?1 [12].

8(?) = "7?(?5? + ?) = "7?(! .

4?	5?36 + ?)?	??      (4)  Also, the kernel trick should be applied for the SVM non- linear classifier. Equation (5) is transformed by replacing variable ? in (3) with the kernel function 9. Finally, in (6), the non-linear classifier using the kernel trick is calculated.

?-??????    ! .	?	?? ? ?  ? ! .	.3  3?	3?? 94?  5?36          (5)  "#?$? % %&    0 * .	 * ? (i=1,?,n),   ! .

= 0?	??  8(?) = "7?(! .

94?	5?36 + ?)?	??                (6)

IV. THE PROPOSED ALGORITHM This section describes the proposed algorithm, which  consists of three parts. First, data preprocessing is performed to cover large amounts of data. Second, the random forests algorithm is applied to the selection of variables and third, the E-SVM is conducted in order to classify low and high yield in the final test process. The outline of the proposed algorithm is shown in Fig. 3.

Figure 3. Overall framework of the proposed algorithm  More detailed description with regards to the proposed algorithm is as in the following.

A. Data Preprocessing In the semiconductor manufacturing process, it generally  takes 3 to 4 months to complete a finished product. During this time, a large amount of data is generated. In particular, data at the chip level incorporates many observations, making analysis difficult in establishing relation to the packaging yield. As a result, we focus on data at the wafer level. Using statistics such as the mean and the standard deviation of the parametric test data, we extract a new feature representing wafers. Finally, a single wafer represented as just one observation unlike including about 1500 observations in one wafer.

B. Variable Selection Using Random Forests Even after data preprocessing, a large number of  variables remain. Because the presence of many variables reduces the learning performance, it is necessary to select the significant variables. Therefore, this study proposes a selection method using the random forests algorithm. The objective of this algorithm is to choose variables closely correlated with the response variable, for the purpose of identification using decision trees. To identify the significant variables, four measures to assess the importance of the  margin : ? ?	 ? ? = 1 ? ?  ?  DATA PREPROCESSINGStep 1:  RANDOM FORESTS  ENSEMBLE SVM Or  OTHER ALGORITHMS  Raw Data  Wafer Summary Data  Significant Variables  Classification of High and Low Yield  Step 2:  Step 3:     variables are calculated [13]. Four measures are as in the following: (1) Measure 1 is calculated by comparing total classification error rate with the new classification error rate using ; -th variable. (2) Measure 2 is defined as the proportion of correct classifications exceeds the proportion of the most voted misclassifications.(3) Measure 3 is calculated by the difference between the number of lowered and raised margins as described framework in Measure 2. (4) Measure 4 is calculated through the Gini index or the Shannon entropy [13]. These measures are considered as points in a four-dimensional space. Fig. 4 shows the process of variable selection.

Figure 4. Procedure for variable selection  Consequently, the input variables of the classification model are selected using the procedure above.

C. Yield Classification Using E-SVM E-SVM refers to ensemble learning using a SVM. The basic concept of ensemble learning is to employ multiple learners and combine their predictions [14]. The SVM is a widely known classifier on the diverse domain. However, there are limitations in learning the algorithm due to the non-linearity and complexity of the data. Therefore, this study proposes an ensemble SVM to increase the classification rate. Fig. 5 outlines the proposed ensemble SVM. In Step 1, the given data set is separated into a training set A and a test set < in the ratio of 4 to 1. Training set A is used to build an ensemble learning model using the SVM. Step 2 selects random samples for learning recursively. Generally, bagging, boosting, disjunct partitioning and fold partitioning are considered when composing a sample data set [15]. In this study, disjunct partitioning that randomly splits the training data is used. Following the selection of random samples, Step 3 is performed: the building and classification of the model. The SVM model ?	(? = 1, ? , ?)using the  random sample ?	 is constructed and the prediction of the test set < is performed using the model ?	 . Steps 2 and 3 are repeated ? times and ? prediction results are produced.

The average of the prediction values is then regarded as the final predicted class for test set <.

Figure 5. Flow chart combining the ensemble technique and SVM

V. EXPERIMENTAL RESULTS For the experiment, an actual industrial data set is used.

The original data set is large, consisting of observations at the chip level. Therefore, the data set is reduced to the wafer level and each wafer is labeled as a high or low yield using specified criteria. In this study, we have labeled the high yield in situation where the yield is greater than the average yield and the low yield in situation where the yield is less than the average yield. Also, 500 wafers are separated into a training set and a test set in order to evaluate the performance of the model.

This experiment consists of two parts. The first represents the results of variable selection using the random forests algorithm and the second evaluates the performance of the E-SVM. The results are compared to those of widely used classification algorithms in the data mining domain: K- Nearest Neighbor (KNN), Random Forests, Decision Tree, Neural Network and SVM.

To evaluate the performance of the algorithms, we compared the low yield classification accuracy (LCA),  the high yield classification accuracy (HCA) and total classification accuracy (TCA) before and after applying the significant variables obtained from the random forests algorithm. The LCA represents the ratio of classifying correctly the actual low yield class as the low yield class and  Step 1:  Get a four-dimensional center vector with coordinates using an average of four  measures.

Calculate a distance between each point and the center vector and sort the distances in  decreasing order.

Select the variable whose the distance exceeds a given threshold.

Step 2:  Step 3:  Step 1:  Split a training set     and test set  Get randomly samples from  Learning of SVM using Get the SVM Model Prediction of     through  Step 2:  Step 3:  Calculate the average of prediction results  Step 4:  A B  A ),...1( niSi ?  B iM  iS ),...,1( niM i ?  Repeat times  n  n     HCA means the ratio of classifying correctly the actual high yield class as the high yield class. TCA is calculated by averaging the LCA and the HCA. In addition, as shown in Table I, the classification accuracy increment (CAI) is calculated to identify any differences before and after the application of the selected variables. The low yield classification accuracy increment (LCAI), the high yield classification accuracy increment (HCAI) and the total classification accuracy increment (TCRI) represent increments corresponding to LCA, HCA and TCA respectively. When the CAI is positive, the selected variables are significant. In the opposite case, the selected variables are not significant.

Table I is to compare LCAI, HCAI and TCAI in order to identify the performance of variable selection. As a result, the SVM algorithm demonstrates a powerful performance increment. In the case of the remaining algorithms, LCAI produces a positive number because the HCAI is greater than the absolute value of the LCAI, in spite of the negative LCAI.

Therefore, the variables selected by using the random forests algorithm are significant.

TABLE I. VARIABLE SELECTION PERFORMANCE  Table II shows the performance of the algorithms, including our proposed algorithm E-SVM. First, the KNN algorithm has a difficulty in learning due to the decision of > and its weakness caused by multivariate data. Also, the poor performance of the neural network algorithm occurs because of the structure of network layers or the sensitivity in setting the parameters. The SVM using a kernel trick proves suitable for classifying the non-linear data set. However, learning performance is reduced due to the small data set. Therefore, the ensemble technique is applied to the classification algorithms, such as decision tree and SVM. The proposed algorithm applying the ensemble technique produces the highest LCA, HCA and TCA both before and after the  application of variable selection. Also, the classification accuracy increases after using the proposed variable selection algorithm.

TABLE II. THE PERFORMANCE OF THE ALGORITHMS  Consequently, the proposed algorithm combining the ensemble technique with SVM demonstrates significance in variable selection using the random forests algorithm, and performs better than the other methods tested.



VI. CONCLUSION The post-fabrication process consists of many test stages  and each stage generates a large amount of parametric test data. Therefore, the variable selection technique proposed in this study is applied to real industrial data and its performance is verified. Also, the ensemble learning using SVM is a highly efficient method for predicting the packaging yield. Consequently, this study offers two data-  Algorithms  Classification Accuracy Increment  LCAI HCAI TCAI  Random Forests -6.38%p 8.21%p 0.91%p  Decision Tree 0.00%p 8.46%p 4.23%p  KNN -8.51%p 11.15%p 1.32%p  Nerural Network -29.79%p 24.39%p -2.70%p  SVM 25.53%p 14.22%p 19.87%p  Average -3.83%p 13.29%p 4.73%p  Algorithms Classification Accuracy  LCA HCA TCA  N o V  ariable Selection  Random Forests 68.09% 64.71% 66.40%  Decision Tree 55.32% 68.63% 61.97%  KNN 65.96% 45.10% 55.53%  Neural Network 78.72% 23.53% 51.13%  SVM 38.30% 60.78% 49.54%  Proposed Algorithms (E-SVM) 76.60% 78.43% 77.51%  Average 63.83% 56.86% 60.35%  V ariable Selection  Random Forests 61.70% 72.92% 67.31%  Decision Tree 55.32% 77.08% 66.20%  KNN 57.45% 56.25% 56.85%  Nerural Network 48.94% 47.92% 48.43%  SVM 63.83% 75.00% 69.41%  Proposed Algorithms (E-SVM) 74.47% 83.33% 78.90%  Average 57.45% 65.83% 61.64%     mining approaches to manage yield efficiently. In furtherance of these results, the algorithm integrating the two data-mining approaches needs to be revised given that variable selection and ensemble learning offer superior performance in semiconductor manufacturing. Also, many more data sets need to be applied to this proposed algorithm to test the robustness of these results.

