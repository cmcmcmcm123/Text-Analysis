How Data Partitioning Strategies and Subset Size Influence the Performance of an Ensemble?

Abstract?When dealing with big data, ?divide and conquer? is the most commonly used strategy in practice to partition a big dataset into such smaller subsets that each subset can be handled by a computer or a node of cluster or cloud computing systems. However, among many existing partitioning or sampling techniques, it is not clear which one is suitable and how the size of subset may affect the performance of further analysis.

In this paper, after presenting a generic framework of ensemble approach for learning from big data, we focus our investigations on systematically evaluating the effect of partitioning strategies and subset size on ensemble performance. The experimental results have demonstrated that three investigated partitioning / sampling strategies behaved statistically similar but the subset size may affect the performance of the ensemble in very drasti- cally different ways, which are grouped into three patterns, rather than just one default perception - the bigger the better.

Keywords?Big data, partitioning, subset size, ensemble learn- ing.



I. INTRODUCTION  Nowadays, the quantity of data collected through various digital media and activities can quickly become so large that any existing computers alone cannot load the data into their memory all together for analysis. Therefore, a big dataset has to be divided into smaller and manageable subsets to overcome the limit of memory. However, although there exist many data partitioning and sampling strategies, there is no systematic study on them examining firstly how these techniques perform when dealing with big data, and secondly how the size of subset may influence the performance of machine learning and data mining methods. In practice, the second aspect is more important as machine learning methods may have quite different performance in dealing with smaller datasets and big datasets. Studies such as, distributed data mining [1], parallel data mining [2], incremental data mining [3] and ensemble methods [4] have shown the ability to deal with small datasets, but most of them do not show the same performance when dealing with single massive dataset. A very plausible reason is that the learning algorithms employed by these methods are developed under the assumption of having the whole data loaded into the main memory, and obviously this assumption cannot be upheld when dealing with large datasets that cannot be loaded into the main memory all together.

In this paper, we firstly propose a generic ensemble framework that consists of three phases for partitioning big data, generating models from the partitioned data subsets, and combining the generated models to produce a final answer.

Then we focus our empirical investigations on the effect of diffident partitioning methods and subset size on ensemble accuracy in dealing with big datasets.

The rest of the paper is organized as follows. Section 2 briefly describes some related works. Section 3 presents an ensemble framework. The empirical investigation design and experimental results are presented in sections 4 and 5 respectively. Then the discussion is given in section 6 and conclusions in Section 7.



II. RELATED WORK  As previously mentioned, partitioning a big dataset into smaller subsets that are manageable is a basic strategy used by different data mining methods. Some researchers, such as Oates and Jensen [5] and [6], have shown that increasing the size of a training set does not greatly increase classification accuracy. Another research proposed by Hall and Chawla [7] showed some promising results in relatively small datasets.

Their proposed method was to build a single decision system after learning is done independently on N disjoint subsets of data in parallel computing. In addition, Nittaya and Kittisak Kerdprasop [3] proposed an algorithm to partition the training dataset into manageable and learning effective subsets and tested the algorithm using the incremental data mining method.

They found that the size of a subset should be between 10% and 50% of the whole data set. Moreover, Tsang et al. [4] proposed a SVM ensemble and sampling technique that was used to partition the dataset and showed that the use of orthogonal constraints in the SVM ensemble leads to better performance than bagging. An algorithm proposed by Patil and Bichkar [8] using a round robin partitioning method to classify large data sets, showed that the proposed algorithm achieved equivalent performance to the performance on a complete dataset. COMET is the MapReduce algorithm [9] for learning from large datasets. COMET builds multiple ensembles on distributed blocks of data and merges them into a mega ensemble. Their study showed that COMET compares favourably to subsampling Random Forests run serially on a single block of data. However their study fixed the subset size to 100K which was chosen by running IVoting algorithm for 1000 iteration. It is not clear whether different subset size affects the performance of the classification and in what ways if it does. So there is still need to investigate the effect of different subset sizes on the ensemble accuracy and find out how to choose the best subset size when partitioning a big dataset.

Fig. 1. Proposed Ensemble Framework

III. PROPOSED ENSEMBLE FRAMEWORK  In this research, an ensemble method framework is pro- posed as shown in Figure 1 to deal with very large datasets. It consists of three phases partitioning, modelling and combining which will be described in detail below. The operations of the framework are described in Algorithm 1.

Algorithm 1: Proposed Ensemble Algorithm Inputs : Training Dataset Tr.

validation dataset V Testing dataset Ts.

Partitioning Method P.

Learning algorithm L.

Fusion strategy F .

The relative size of a subset Rt as a percentage of  the whole size of the Tr, number of subset N = ?  Rt  ?  Outputs: Ensemble ?.

Ensemble accuracy Acc(?)).

The mean accuracy of individual models  (Acc(mi)).

Ensemble time (TM?) = Partitioning Time  (TMp)+ Learning Time (TMln) + Classification Time (TMc).

Declare counter i=0 Call P ( Tr , Rt ) , and receive a set of N training subsets SUB= {t0, t1, t2, ..., tN?1} while i less than N do  Call L (ti ) and receive a base classifier mi.

add mi to the ensemble ?.

end Evaluate ? with V and Ts using F and receive Acc(?, V ),Acc(?, T s), (Acc(mi)), (TM?), (TMp), (TMln) and (TMc).

End  A. Partitioning Phase  In this phase a given large dataset will be divided into smaller subsets. However, there is no study to show which partitioning method should be used, and how they may affect the results of mining data as a whole.

In our experiments, three commonly used data partitioning methods, Sequential non-overlapping partitioning, Round robin partitioning and Sampling without replacement are examined.

1) Sequential non-overlapping partitioning: This method simply, divides the data instances of a training dataset (Tr) into a given number of subsets (N) in sequential manner. The size of each subset is equal to  ? |Tr| N  ? . except the last subset which might  take up to ? |Tr| N  ? instances, where |Tr| is the size  of Tr . After calculating the size of subsets, data instances will be read from Tr sequentially, so the first  ? |Tr| N  ? instances goes to the first subset and  so on until the end of the file of Tr. Algorithm 2 illustrate the procedure for this partitioning method.

Algorithm 2: Sequential non-overlapping partitioning Input : Training Dataset Tr.

The relative size of a subset Rt as a percentage of the whole size of the Tr  Output: a Set of partitions, SUB = {t0, t1, t2, ..., tN?1}  Start: Declare counter i =0, as counter to keep track of created file.

Calculate number of subset N =  ? Rt  ? .

Calculate number of instances in a subset |t| = ? |Tr| N  ? ,  where |Tr| is number of instances in Tr.

while not reach the end of Tr do  Create a subset ti if (i = (N ? 1)) then  Read the rest of the instances.

Save them into subset ti  end Read the first |t| unseen instances from Tr.

save them into ti  end End  2) Round robin partitioning method: In round robin partitioning, the first instance of a training dataset goes to the first subset, the second to the second subset, and so on until the last subset. If the last subset is reached, then the method starts over from the first subset [10]. Algorithm 3 shows how to round robin partitioning method works.

3) Sampling without replacement: Sampling is a technique used to create subsets by selecting a random instance from a dataset. Random selection means that all the data instances have the same prob- ability of being selected at any time. In sampling without replacement, if an instance was chosen to be     Algorithm 3: Round robin partitioning method.

Input : Training Dataset Tr.

The relative size of a subset Rt as a percentage of the whole size of the Tr  Output: A set of partitions, SUB = {t0, t1, t2, ..., tN?1}  Start: Declare counter i =0, as counter to keep track of created file.

Declare counter j =0, as counter to keep track of training instances.

Calculate number of subset N =  ? Rt  ? .

Calculate noInstances = |Tr| , where |Tr| is number of instances in Tr.

while j < noInstances do  reset counter i.

while i < N do  xj = Read data instance (j) from Tr.

Save xj into subset ti.

end end End  a member of a subset, this instance cannot be chosen to be a member of any other subset during the sampling process. Algorithm 4 describes sampling without replacement method.

B. Modelling Phase  The modelling phase is the second stage of constructing an ensemble. In this phase a learning algorithm is chosen to learn from each subset of the data generated in the partitioning phase and then to create a data model. In principle, any learning algorithm can be used in this phase.

In this study a core vector machine (CVM) [11] is used as the base classifier because it performs very well with large datasets and its source code is available for free from the authors website for research purposes [12].

Core Vector Machine (CVM) Algorithm Tsang et al. proposed CVM as an enhanced version of the stan- dard SVM to accelerate the learning process. A standard sup- port vector machine (SVM) has O(|Tr|3) time and O(|Tr|2) space complexities where |Tr| is the size of the training dataset. While CVM has a linear time complexity in |Tr| and space complexity independent of |Tr|, this enhancement was achieved by formulating ?a kernel (including the soft-margin one-class and two-class SVMs) as the equivalent minimum enclosing ball (MEB) problem and then obtain approximately optimal solutions efficiently with the use of core sets?[11].

Tsang and his colleagues have proven experimentally that their proposed CVM algorithm achieves a similar performance to standard SVM but is much faster especially with large datasets.

In 2007, BVM with enclosing balls was introduced [13]as a faster version of CVM.

As mentioned earlier, other types of learning algorithms can be used, and the base learners choice depends on the  Algorithm 4: Sampling without replacement Input : Training Dataset Tr.

The relative size of a subset Rt as a percentage of the whole size of the Tr  Output: a Set of partitions, SUB = {t0, t1, t2, ..., tN?1}  Start: Declare counter i =0, as counter to keep track of created file.

Declare availableSubSets , as array that contains pointers to available subsets.

Calculate number of subset N =  ? Rt  ? .

Calculate number of instances in a subset |t| = ? |Tr| N  ? ,  where |Tr| is number of instances in Tr.

while i < N do  create subset ti size of ti, |ti| = 0 add ti into availableSubSets  end while not reach the end of Tr do  x = Read data instance from Tr.

select a random subset t form availableSubSets save x into subset t |t| = |t|+ 1 if |t| = N then  Remove t from availableSubSets.

end  end End  suitability for the data. This choice and other related issues in this phase will not be discussed in this paper as they are not the focus of the study at this stage.

C. Combining Phase  The combining phase is the last phase of the ensemble?s operation. The aim of this phase is to produce the final result of the ensemble by combining the predictions of individual models. Majority voting will be used as a fusion strategy in this experiment. Similar to the modelling phase, other types of fusion strategies can be used, their effect on the ensemble performance will not be discussed in this paper, as the focus of this paper is on partitioning phase.



IV. EXPERIMENTAL DESIGN  A. The aim of the experiments  In this paper, the experiments are designed and carried out with an aim to examine the effect of three partitioning strategies on the ensemble performance when mining large datasets using the proposed ensemble method. In addition, the influence of the subset?s size on the ensemble performance will be examined too.

B. Experimental Design:  1) Datasets: Five datasets of different sizes were used to evaluate the performance of our ensemble. The main char- acteristics of these datasets are listed in Table I. The Cover     type dataset is available from the UCI Machine Learning Repository. In our experiments the used adult dataset is a preprocessed version of the original adult dataset which is available from the UCI Repository. The details of the prepro- cessing process for the adult dataset can be found in [14]. The Web and IJCNN01 datasets are available on Libsvm website 1. In addition, IJCNN01 is the preprocessed version of the original IJCNN01; full information about the preprocessing process can be found in [15]. The KDDCUP-99 dataset was downloaded from [12], and the description of the dataset is available from the KDD website 2.

All 5 datasets come with separate testing datasets. For evaluation purposes, a validation set was created from each training set. The size of validation dataset is 30% of the training set. Table II shows the characteristics of used datasets after the creation of the validation dataset.

TABLE I. DATASETS USED  Dataset Training Testing No. of Numeric categorical Instances Instances Attributes Attributes Attributes  Adult 32,561 16,281 123 0 123 Web 49,749 14,951 300 0 300 IJCNN01 49,990 91,701 22 2 20 Cover type 522,910 58,102 54 51 3 KDDCUP-99 4,898,431 311,029 127 120 7  TABLE II. DATASETS USED AFTER THE CREATION OF VALIDATION DATASETS  Dataset Training Validation Testing No. of Instances Instances Instances Attributes  Adult 22,793 9,768 16,281 123 Web 34,825 14,924 14,951 300 IJCNN01 34,993 14,997 91,701 22 Cover type 366,037 156,873 58,102 54 KDDCUP-99 3,428,902 1,469,529 311 127  2) Experiment procedure and setups: :  1) Three series of experiments were conducted, one for each partitioning method. Figure 2 illustrates how each experiment is performed.

2) For each experiment, five datasets will be used.

3) Although for each of the five datasets a separate test-  ing dataset is provided for testing the performance of our ensemble system, a validation dataset will be created from each training dataset to be used to estimate the learning performance of our ensemble before testing.

4) For a given dataset, each experiment will be repeated several times by varying the relative size of the subset Rt in a systematic manner as shown in the following points.

5) In all the experiments, the relative size of the subset should be chosen to create an odd number of partitions to avoid a potential tie situation when using majority voting as a fusion strategy.

6) The relative size of a subset Rt, is varied from a given minimum Rtmin = 1%, to maximum Rtmax = 49% subject to the above requirements in the 5th and 6th steps. The value of Rt varied as follow Rt = 4%,6%,7%, 8%, 12%, 15%, 16%, 20%, 21%, 22%, 23%, 24%, 34%, 35%, 36%, 37% 38%,  1http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/ 2http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html  39%, 40%, 41%, 42%, 43,% 44%, 45%, 46%, 47% and 48%.

7) CVM and majority voting will be used as the base classifier and the fusion strategy in each experiment respectively.

8) For each experiment, the results of the ensemble and individual models in training, validation, and testing datasets will be calculated. Also, the ensemble time will be recorded.

9) Statistical significance test will be carried out to analyse the results of experiments.

Fig. 2. The experiment procedure  3) Evaluation Methods:  1) Accuracy and Efficiency The accuracy is used as a performance measure in this study. In addition partitioning time is recorded as a measure of efficiency but it is not discussed in this stage of the research due to the three used partitioning strategies have very similar time.

2) Statistical significance analysis McNemar?s [16] test is used in this experiment to compare between the classification errors of two classifiers (ensembles) to find out if they are sta- tistically different or not. In addition, Friedman test [17], will be used later to compare between more than two classifiers over multiple datasets.



V. EXPERIMENTAL RESULTS  To evaluate the impact of subset size and partitioning strategy on the Acc(?), We run our proposed ensemble on each dataset for different runs by varying the Rt and the partitioning strategy. The results for each dataset are presented by three charts, one for each partitioning strategy. For each strategy     we monitored the ensemble accuracy Acc(?), and the mean accuracy of individual models of the ensemble ((Acc(mi))) on testing and validation datasets. The experimental results section is divided into thee subsections. Firstly the effect of Rt on Acc(?), then comparison of the accuracy of the three partitioning methods and finally statistical analysis section.

A. The effect of Rt on Acc(?)  Fig. 3. Adult dataset experimental results, accuracy of ensemble, the mean accuracy of individual models within the ensemble on validation and testing datasets, against the size of subset  1) Result of Adult dataset:  ? Relationship between Acc(?) and Rt On the adult dataset, the highest Acc(?) was achieved in the three partitioning methods when a small Rt was used as a size for the subsets. Figure 3 shows the Acc(?) on testing and validation datasets for adult dataset when the three partitioning methods were used. The left chart in Figure 3 illustrates the result when the sequential non-overlapping partitioning was used, and it shows that the Acc(?) on the testing dataset fell from 84.82% to 82.77% when the subset size was increased from 4% to 48% respectively. In the same manner, the middle graph and the right shows that the Acc(?) decreased when the subset size increased and round robin partitioning and sampling without replacement were used, respectively.

? Relationship between Acc(?) and (Acc(mi)) Another important result that can be observed from Figure 3 is that the Acc(?) achieved better perfor- mance than the mean of the accuracy of individual models of the ensemble ((Acc(mi))) in all the exam- ined cases.

Fig. 4. Web dataset experimental results, accuracy of ensemble, the mean accuracy of individual models within the ensemble on validation and testing datasets, against the size of subset  2) Results of web dataset:  ? Relationship between Acc(?) and Rt The effect of the size of subset on Acc(?) in the web  dataset was completely different from its effect on the adult dataset. Figure 4 illustrates the relationship between Acc(?) and Rt for the three used partitioning strategies.

When round robin and sampling without replacement were used, the Acc(?) had a proportional relationship to Rt. The only difference between the behaviour of the two partitioning methods on this dataset is that in round robin partitioning, the Acc(?) increased as a result of increasing Rt up to Rt = 34%. Then after that point, (Rt = 34%) the Acc(?) became steady and no improvement was achieved by the growth of Rt.

When sampling without replacement was used, the Acc(?) continued to increase by the increase of Rt.

The big difference occurred when the sequential non- overlapping partitioning was used, which shows that Acc(?) was not affect by Rt at all and that the Acc(?) was steady when the value of Rt varied from 4% to 48%.

? Relationship between Acc(?) and (Acc(mi)) Figure 4 also shows that when round robin and sampling without replacement were used, the Acc(?) achieves still better performance than the mean of the accuracy of individual models ((Acc(mi))). While in sequential non-overlapping partitioning, although the Acc(?) was steady and did not improve during the experiment, the ((Acc(mi))) has a direct relationship to Rt and ((Acc(mi))) where (Acc(mi)) = 95.47% when Rt =4% up to 97.62% when Rt=48%. In ad- dition the ((Acc(mi))) achieved better performance than the Acc(?) between Rt = 21% and Rt=48%.

Fig. 5. IJCNN dataset experimental results, accuracy of ensemble, the mean accuracy of individual models within the ensemble on validation and testing datasets, against the size of subset  3) Results of IJCNN dataset:  ? Relationship between Acc(?) and Rt Figure 5 shows that when the sequential non- overlapping partitioning was used, the Acc(?) on the testing dataset increased from 94% to 98% when Rt also increased from 4% to 48%. When round robin partitioning was used, Acc(?) on testing and validation datasets varied up and down, with no obvious pattern or trend, starting from Rt = 4% up to 34%; after 34%, the Acc(?) become steady and was not affected by the Rt, whatever its size. When sampling without replacement was applied, there was an improvement to Acc(?) on validation and testing datasets by increasing of Rt, as illustrated by the right chart in Figure 5.

? Relationship between Acc(?) and (Acc(mi))     By focusing on the relationship between Acc(?) and mean of the accuracy of individual models ((Acc(mi))), Figure 8 shows that our ensemble per- forms better than ((Acc(mi))) on the sequential non- overlapping and sampling methods. When round robin was used, the Acc(?) achieved nearly similar per- formance to ((Acc(mi))) in almost all cases except between Rt = 21% and 34% where ((Acc(mi))) showed better performance than Acc(?).

Fig. 6. Cover type dataset experimental results, accuracy of ensemble, the mean accuracy of individual models within the ensemble on validation and testing datasets, against the size of subset  4) Results of Cover type dataset:  ? Relationship between Acc(?) and Rt The relationship between Acc(?) and Rt for this dataset is very similar to the relationship between Acc(?) and Rt on adult dataset. The highest Acc(?) was achieved when a small Rt was used. Figure 6 shows the Acc(?) on testing datasets fell dramatically from 78.86% to 62.94% when Rt varied from 4% to 48% when the sequential non-overlapping partition- ing was used. The same relationship pattern appears when round robin partitioning and sampling without replacement were used.

? Relationship between Acc(?) and (Acc(mi)) Figure 6 also illustrates that Acc(?) achieves better performance than the mean of the accuracy of indi- vidual models ((Acc(mi))) in all the examined cases in the three partitioning strategies.

Fig. 7. KDDCUP-99 intrusion dataset experimental results, accuracy of ensemble, the mean accuracy of individual models within the ensemble on validation and testing datasets, against the size of subset  5) Results of KDDCUP-99 intrusion dataset:  ? Relationship between Acc(?) and Rt In this dataset, the Acc(?) for the testing and vali- dation dataset in all three partitioning strategies has not been affected by the relative size of the subset Rt.

The Acc(?) was steady during the whole experiment as Figure 7 shows.

? Relationship between Acc(?) and (Acc(mi)) Figure 7 also illustrates that Acc(?) achieved bet- ter performance than the mean of the accuracy of individual models ((Acc(mi))) in all the examined cases in which the round robin and sampling with- out replacement were used. While sequential non- overlapping partitioning was used on testing and val- idation datasets, the Acc(?) achieved better perfor- mance than (Acc(mi)) from Rt=4% up to Rt=40%.

The (Acc(mi)) achieved better performance from Rt=42% than the Acc(?).

B. Comparing the accuracy of the three partitioning methods  1) Adult dataset: Figure 8 shows that the sequential non- overlapping partitioning and round robin partitioning yield similar pattern on the testing dataset, while sampling without replacement achieves lowest accuracy compared to the previ- ous two partitioning strategies.

2) Web dataset: By comparing the performance of the three partitioning methods depending on testing accuracy, it is clear from Figure 8 that round robin and sampling without replace- ment have very similar performance, whereas the sequential non-overlapping partitioning yields lowest performance and has no effect on accuracy of ensemble on the test data.

3) IJCNN dataset: Figure 8 shows that the round robin partitioning achieved the worst accuracy in general, compared to the other two strategies. Sampling without replacement achieved better accuracy in most cases.

4) Cover type dataset: By comparing the accuracy of the three partitioning methods depending on testing accuracy, it is clear from Figure 8 that the sequential non-overlapping partitioning and sampling without replacement have very sim- ilar accuracy, while the round robin partitioning achieves the lowest accuracy when a small subset size is used, and the highest accuracy when large subsets are used compared to the other two partitioning strategies.

5) KDDCUP-99 intrusion dataset: By comparing the ac- curacy of the three partitioning methods depending on testing accuracy, it is clear from Figure 8 that round robin and sampling without replacement have very similar accuracy and much better accuracy than that achieved by the sequential non- overlapping partitioning.

C. Statistical analysis  We statistically evaluated our ensembles using two non- parametric approaches: the McNemars and Friedman tests.

1) McNemar?s Test: McNemars test [16] is suitable to test two classifiers on a single domain. In this set of experiments McNemars test was used to determine if the classification errors of two ensembles with different Rt are statistically different.

For all datasets and for each partition , we compared the ensemble created from the smallest Rt of 4% denoted by ensemble4 and the largest Rt of 48% denoted by ensmble48.

In most of the 15 examined cases the results of McNemars test provide strong evidence to reject the null hypothesis with     Fig. 8. Compare the effect of different partitioning methods on Adult,Web,IJCNN, Cover Type and KDDCUP-99 datasets  a P value less than 0.0005, which means that the ensemble created from the training datasets with Rt = 4% is significantly different than the ensemble created from the training datasets with Rt = 48%.

Four cases did not present enough evidence to reject the null hypothesis: the Web dataset when the sequential non- overlapping partitioning was used and in the intrusion dataset when all the three partitioning methods were used.

2) Friedman Test: The Friedman test can be used to evaluate multiple classifiers for multiple datasets [17]. In this set of experiments the Friedman test was used to determine if the three partitioning strategies used in this experiment are statically different.

The results show that there is not a statistical difference in the performance of the three partitioning methods on the five datasets with p=0.819.



VI. DISCUSSION  Our results confirm that subset size affects the ensemble accuracy Acc(?) in different ways, which can be roughly classified into three groups of patterns as shown in Figure9.

In P1, Acc(?) decreased when increasing the subset size (Rt).

In P2, the Acc(?) increases with the growth of Rt. In P3, the Acc(?) does not improve or decrease with the increase of Rt.

Fig. 9. Possible conceptual relationship patterns between Acc(?) and Rt  Figure 10 show the true relationship patterns between the relative subset size Rt and Acc(?)in the three partitioning  Fig. 10. Relationship patterns between Acc(?) and Rt which were observed in our experiments  strategies examined in this experiment for all the used datasets.

It is clear from Figure 10 that the relationship between Rt and Acc(?)in Adult and Cover type datasets is similar to pattern P1, while in web and IJCNN datasets their relationship is similar to pattern P2 and the relationship in the intrusion dataset is similar to P3.

In this experiment we intended to find out possible relation between the pattern group and dataset. It seems that the number of instances and number of attributes are not the only factors that determine the relationship pattern. Adult and Cover types belong to the same pattern, while they are very different in terms of number of instances and number of attributes. Also, Web and IJCNN belong to the same pattern; although they have a similar number of instances, but a huge difference between their numbers of attributes. For these reasons, there is a need for further investigation to find how to determine the pattern of the relationship and how to find the suitable subset size.

In addition, our results show that sampling without replace- ment and round robin partitioning achieves better accuracy in 80% of the examined datasets. Table III shows the best par- titioning strategy for each dataset depending on their Acc(?) on testing datasets.

TABLE III. BEST PARTITIONING STRATEGIES FOR EACH DATASET  Dataset Best partitioning strategies Adult sequential non-overlapping and round robin Web round robin and sampling without replacement IJCNN01 round robin and sampling without replacement Cover type sequential sampling without replacement KDDCUP-99 round robin and sampling without replacement

VII. CONCLUSIONS  In this paper, we explored the relationship between parti- tioning methods and Acc(?). For this task we examined three partitioning strategies: sequential non-overlapping partitioning, round robin partitioning and sampling without replacement.

Although sampling without replacement and round robin par- titioning achieved better accuracy in 80% of the examined datasets, as shown in Table III, the results of the statistical test do not present enough evidence to conclude if there is a significant difference in the performance of the three partition- ing strategies on the five examined datasets. Nevertheless in practise, sampling should be considered in the first place if there is no specific knowledge about the other two.

We also investigated the relationship between the relative size of the subset (Rt) and the ensemble accuracy (Acc(?)).

We found that Rt can have effect on Acc(?). As illustrated in Figure 9, this effect can be represented by one of three     relationship patterns between Rt and Acc(?). Our results show that the well accepted idea that having more training data instances generally leads to the best classification performance is not always true, as the best accuracy for in Adult and Cover Type datasets were achieved by having small subset sizes.

The relationship patterns of a dataset cannot be detected by the number of its instances and attributes because it is found they are not the only factors that affect the behaviour of the relationship between Acc(?) and Rt for a dataset.

Further study should include more investigation to discover the relationship type between ensemble accuracy (Acc(?)) and the relative size of a subset (Rt), as the number of instances and number of attributes do not provide an interpretation for this relationship. For that reason, it is necessary to repeat the experiment on larger datasets and apply some vertical petitioning techniques and conduct an in-depth and detailed analysis to find out how to determine the best size of a subset and what type of partitioning strategies is suitable for a dataset.

