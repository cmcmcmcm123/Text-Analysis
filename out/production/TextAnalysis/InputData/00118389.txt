A NFllBAL -0RK MODEL  USING INTER-PAITHU? ASSOCIATION (IPA)

Abstract This paper investigates a neural Field Transfornation network model - IPA model, in which a basic logical operation is used to determine the Truly Massive inter-pattern association (i.e., association between the reference patterns), and a simple rule is applied to construct the tri-state interconnections in the network. Computer simulations of the reconstruction of similar English letters in the random noise have shown a better performance by the IPA neural network over the Hopfield model.

A 2-D hybrid optical neural network is used to show the usefulness of this new model. Since there are only three gray levels in the Interconnection Weight Matrix (IWM) , the requirement for the dynamic range of the Spatial Light Modulators (SLMs) is easily satisfied, and the interconnections in the proposed network are much simpler than those in other models.

AI is moving into a new phase characterized by a broadened understanding of the nature of knowledge, and by the use of new computational paradigms. A sign of this transition is the growing interest in neurocomputers, optical computers, molecular cotnput- ers and other massively parallel analog computers. We have argued elsewhere [l, 2, 3, 41 that the new AI will augment the traditional deep, narrow computation with shallow, wide computa- tion. That is, the new AI will exploit massive parallelism Now, massive parallelism means different things to different people; massive parallelism may begin with a hundred, a thousand, or a million processors. On the other hand, biological evidence sug- gests that skillful behavior requires a very large number of proces- sors, so many in fact that it is infeasible to treat them individually; they must be treated en masse. This has motivated us to propose [l] the following definition of massive parallelism: A conlputational system is massively parallel if the number of processing elements is so large that it m y  conveniently be considered a contmuous quam.

That is, a system is massively parallel if the processing elements can be considered a continuous mass rather than a &ete ensedle.

How large a number is large enough to be considered a con- tinuous quantity? That depends on the purpose at hand. A hun- dred is probably never large enough; a million is probably always large enough; a thousand or ten thousand may be enough. One of the determining factors will be whether the number is large enough to permit the application of continuous mathematics, which is generally more tractable than discrete mathematics.

We propose fhis definition of massive parallelism for a number of reasons. First, skillful behavior seems to require significant neural mass. Second, we are interested in computers, such as opt- ical computers and molecular computers, for which the number of processing elements is effectively continuous. Third, continuous mathematics is generally easier than discrete mathematics. And fourth, we want to encourage a new styIe of thinking about paral- lelism. Currently, we try to apply to parallel machines the thought habits we have acquired from thinking about sequential machines.

This strategy works fairly well when the degree of parallelism is low, but it will not scale up. One cannot think individually about the 10" processors of a molecular computer. Rather than post- pone the inevitable, we think that it's time to develop a theoreti- cal framework for understanding massively pardel analog com- puters. The principal goal of this paper is to outline such a theory.

