

WPI2 - 3:30 Decilsonmaking with th Boftmunn Machine  by k og C  * Departmet of ec and Compu Engieern Drexel Universiy, Philadelph PA 19104  Department of En ring Princeton University, Priceton NJ 08544  The Boltzmann Machine has been hirduced as a means to perform global oibMiztin for obj functions using the principles of simulated armealng. In this paper we consider its utility as a s -ee content-addressable memory and provide busontjIoracre in this context: a) We show how to explbit racne's abity to escape bcal minima, in order to use i, at a constant tenperature, for unambiguous associative pattern-retval in nroly enverts.

An association rule, which creates a dwher ktksre around each stored patten, is used aong with the MachNe's dynamics to match the machine's noisy inpu with one of the pre-stored pattems. Spurious fixed poins, whose regions of attraion are not recognized by the rule, are skipped, due to the Macine's fnite prbabiliy to escape from any state.

b) We propose the use of the incremental Hebbian rule as a leaming scheme for the Boltzmann-Machine-based content addressable rnmeory. This learnn rule is difterent frorn the probabity-distbutio-matchinig *&u* uls which are used at present for the Machine's weigts in the global-minimizer appliction. We interpret the Incremenal Hebbian rule as a steepest-deset algortm, maximking the probablity of pattem stabilzation durin leann.

c) We descbe tea n dOne from a sored patte using a birth-and-death Makvcvtin, and find bous on t retrieval probabilties. Our bwxixsslw asasessment of the Machie's efficency as a content-ad le nemor. SpefFaly, we can compare differernt learning rles used to instal pattems in the Machine's state space, as well as 'emperatures' o opemrtn, length of learning periods, length of 'prductI' periods and coding schemes for information to be stred and retrved by te network.

The resuts apply to the Botmann machiine and to the asynchronous net of binary threshold elements ('Hopfield model). They provide the network designer wih worst-cm and best-case bounds for the network's performance, and allow a polynomial-tine tradoff studie of design parameters.

The su o tht artfi nera network be utled in classification, pate r i a ssociive real has be the main theme of numerus studies which appeared In the last decade (e.g. [10], [261 and their references.) Among the most popular fardlies of neural networks are fly conneded networks of binary threshold elemnrts ( [2], [3], [4], [5], [6], [12], [13], [171, [18], [21], [23]' [25D1) Thee structures, and the reiated family of fully conneced networks of sigmoidal threshold elenmnts ([8I, [14], [22D have been used as error-corcting decoders in many applications, among which were interesting applications in optinization ([71,[151,[191,[28D. A cwomon drawback of the many studied schemes is the abundance: of 'spurious' local minima, which 'trap' the decoder in undesirable, and often non-interpretable, states during the process of input / stored-pattem association. The resuk of numerous studies of this issue (e.g. [2], [51, [131, [21], 122], [231, [251) can be roughly summarized as follows: while the nunber of arbitrary binary patterns that can be stored in a fuly-connected newo is of the order of magitude of N (where N is the number of neurons), the number of created loca attractors in the network's state space is exponential in N.

It was.propose (3W2)tafully-connected ina neural networks, which update their states on the basis of aI b state-rassessment ruls, could be used for gkbaI optimization when the objetve function is multi-modal. The suggested archiltecture, fth Boluzrnam machine, is bsdon fth principle of simied anagln ( [1J, [91. [20,. 1271) and ha been shown to perform interesting tasks of decision making and opimizaton.

However, the leaming algorihm that was proposed for the Machine, along with itRs cooling procedures, do not lend themselves to real-timne operation. In addiion we note that most studies so far have concentrated on the properi-es of the Machine in global optimization and only few studies have mentioned possible utilization of the Machine (at constant 'emperature) as a cortert-addressable memory (e.g. for loca optimization.)  In the present paper we propose to use the Bolzmann machine for assoiative retrieval, and develop bounds on its perfomance as a cortert-addressable memory. We iruroduce a learning algorthm for the Machine, which locafly maximizes the stabilzation probabity of leaned patterns. We then proceed to calculate (in polynomil time) upper and lower bounds on the probabithat a tple at an kinW H I distanoe frm a skd pattern wil get attracted to that pattem. A proposed assiaton rule creates a sphere of /rtence. arund each stored pattem, and is indifferent to 'spurio' atactors. Due to the fact that the Machine has a nonzero prbabilty of escape from ary state, the spurious' atflctors are ignored. The obtand bounds allw the assessment of retrial p ls, diffenrentlearn algoIts and necessary leaming periods, network 'emperatures' and coding schem for kems to be stored.

After introdcng the notation, we presert the procedure of using the Boltzmann Machine as a coternt addressable memrory, along wth the association rule (section 11); we then develop an icemental learning rule for th Machi (section IIQ; In section IV we develop the one-step convergence/divergence probabiles of the MacNne from a given Hanuning distance towards a stored pattem. These probabities are then used to find bounds on the convergence performarce, using Markov birth-and-death processes (setion V). The application of th bounds is Nhjsted, using a forty-neuron network example with two dcfferent siored pattem sets (section VI)  IL The Bofznann Mmchlrn as a Contn Add sble Menoiary  The Boftznann Macie is a nui-wconnected network of N sirle processors caNed probabitic binary neurons. The gh neuron is chwacterized by N-1 real -numbers represeting the synapti weqlits (w ij j-l,2.i-1,i,..N; wl is assumwd to be zero foraal ),arealthresll (rijaabinaryakyvyol(ue B =(-1,1,)Iwhich we shl also refer to as the neuron's state. The binary N-tuple U = [ui,u2,. . . uN] is caled the network's state.

We distigsh between two phases of the network's opration: a) Th leaning hase - when the network parameters wi1 andri are determined. This determination could be achieved through autonomous leaming of the binary pattem environrment by the network (unsupervied learning); through leaming of the environment with the help of a 'eacher which supplies evaluattve reiriorcement signas (suprised leaming); or by an exlernal fixed assignment of paraieter vaus.

b) The aLoduction ghase - when the network's state U is     determined. This determination could be performed sycrnuly by al neurons at the sarre predeermined ftie nor asynchronous - each neuron reassesses is state  independently of the other neurons at random times. The decisions of the neurons regarding their states during reassesnwret can be arrived at determinirsitaly (the set of neuron ipus detemies the neuron's state) or stochasticaly ( the set of neuron iuts shapes a prbaliy cisibuton for the  stae- won lw.) We shal descrb f t (ah and sohasic)  production rube which we sAid here. Al random times during the production phase, asynchronously and Kndpenderntly of all other neurons, the i, neuo deckdes tpn is next state, using the p c decision "JIB  1 wth probably 1+@6'r  j4  -1 wt pobabqety 1+e .

N  E= XWM-.4 (1 b) Is caled th %fh nerW pap, and Te is a predeterined real -ub caked I fln.

The ae-updatg rule (1) is related to the network's snyw ( [13118J114D whch is descrbed by  ES2 [Ei UiCE; m (2)  f the retwork is toW a lOcal nirmm of (2), thn the ih neuron, when chsn (at random) for state udag, ShOUId choosedra  (1 a)  mnimum in a short tmw [9]427J.) However, the network may also be used as a selcie conten * nwnvry which does not suffer frominadvertey-inale spuriou loa ninira.

We consider the folowing appcation of the Bottzmann machine as a scheme for pattem classfication under noisy communication conditons: let an ermder emit a sequence of NXI binary code vector from a set of 0 codewords, each haft a prbailt of occurence of IIm (mm 1,2,.. . ,0). The bradcas word encourers noise and disto n, resuling insome of its bits being reversed. The Boltzmann Machie at the receiMng end accepts this distorted pattem as its initial state (U(O)), and observes the consequent fme evoluion of the network's state U. Ata certain me isant no, th Machie wideclare afte irpid patm U(O) is to be a d with pattem Bm i U at hat instart is 'close encoug to Bmn. For thi purpoe we define Dletin 2: The dma arn of ktfence of pattern Bm. a( Bam 4maxJ is On dmj = Ue d': HD(U 8m): dn}- (5) dmax is d LtoZw)=uodmj d IdobW%h mulinprachts ffi aX  U Thenseboefno s :Iasex tUPwitattm ri,flL#vShhtmwoh*ed torn U1slISsc t/"G OaCJ,d,) Due to th finte of escape from any ntum, th usually-shallow energ mnimalicrsocIg to spuriouxS tbced poit, are qucky skd by th network on is way to th deeper vaeys' dud by th des e fied vfl (ie. Bi,..

-B.)  AL ProbOINy fS o ud the hebblu  We conside the case U = Bm and caihcue the pot of s on brBmwihafixedarvhles (flxedw1Nridq.) D n 3: V bWAtydaaxrbr apn Bm is the one-sep p  Pw Pr "v'=B U'V=BJ.

n[ ] (3)  We rUe tha rule (3) cr be otdie from rule (1) - Te - OlThis determdnistic choice of ui guarantees, under symmetry coitD on the weght (wi=w), that the ntw s state wil stalze at a fixed point in the 2N-tuple state space of the ntwrk (13D,where DL : Astate Uft BNiscald afredpoint in the state  Saace of the N-reuron network I  PJU0'CZ=U, IUV=ud=1. (4)  A fixed p- found through appliations of (3) may not be te gbbal nimum of the energy in (2). A n I WhiCh seeks the gbW minimum should aid local-mrinimum traps by allowing 'uphil dcitng with respect to the value of E. The decision scheme (1) is devised for hat purpose, alwing an increase in E with nonzero probablity. This provision for Progress in the ocaly Wrong' direction in ordo reaP a'flal' advantage ler, is in accordce with the puIcles of si/ated annealing techniques ([9],[20J,[27J), which are used in urOdalotimit. In our ce, the pb i dosing  the cally 'rigt decision (equation (3)) 'and t lcaly 'rn decision ardetermined by t ratio of the ergWy gap AE to the terw@ratre' shap*ig constart To.

The Bokzmann Machine has been proposed for glb minkriz*atl[,[31,[12D and a considerab amoru of effor has been im sted in devising a good cooling schenm, namely a mneans to contol Te in order to a te f of a global  N LEa P. 7, piU 4(bl + [ 1 -U1(,3 97 6  1.1 AEs  whwe N  AEnt SIW"' 1.11.4  (6)  (7a)  (7b)  U1 isthe unit stp fundcon,  and pt= probablitfattfiT rwnuron ese8 d f upaing.

To take into acout the cifferert b s oft (cdword) paerns, we de fntbnA4: The average yof ab on frthe  patterns Bm, mn-1,2,. . . is the one-step average prbabity 0 .N LEa,  P,=XHXpr U-Ab+[1-U(bm)J]e ?&Ea1rn-1 '- 1. +CeV (8)  Incremental Leaning Ruls Using (7) we can develop the increental ocal learnn nrle wtich will maxime the stablon probabilities. Incremental local learning wig have the same stochastic form that the state assessment had (during production): at each tme instari, during leamrig, a neuron (say the kth) is mndomly selcted for weight update. At tha instarn the environment presents the network with one of the patters, say Bm. The randomly-selted neuron k chane its weiits and threholds in the direction which wE locall maximize the probait of stazatibo 8m. The nework txrore chooses     the cal steepest-descert direcion, aong the rade d Psm Let the k' neuron be seleted be larn Of the patern Bm at8 cerain thie nstnt. The deriatis of Psm with repec to st weigh 4wwenS 9wtheso k NO  6-7- bad bf*ar  (1 +et7  ap 1pk b,^ (9b) (1 +crf  The steepes descen dlans belw ocAth w a the kth tfshold at the presc d S are re b d respeivy, ad a graft-Ised lwa ng e wE be:  wfV)= w4J +&bwb +4"!)-qY-t  (10O) (itO)  whre SW ad 8r are sp si chhin sraMed non ar otinizatbwould beo Ibid uS* wthsbd ath(or  consitetl urdhtdin order to g i in the objecive 1funto S each lwrni step.) The ru (10)  cowlswiththe celeb d H Iwhyphsb [11L) aid in 9w icrsnw form (10) b use in sevru Schemes of adaptve blarni be rNu d teshdsmfl (e.g. [4,1173, 18J.) The HNboin hypthesi ca for mi on of a connection between eto elemets in p t the pr ct of their acivaMtons; eutn (1tO) ir a die rlizatin of tha statement, and so I Ob) ( 1za of the theshold Is t gh an (t NO 1 with uLNj- -1 aNd q  Using 9w rule (5), the 6yof . t  P,(HDUPT,hBd,W. 1 140 [Um,BJ=d }m,Z) .. (1I)  ber ed_ter*en add0 Inorder.hoS (1t)we hd first caulathe9w folOiWoae-SpS'Mbpbs.

[ 1+eST~ 1+e7  P .6d.,l)=  N-d U-twL U1-40W 1+ Q .+To'  1 eCrT@ 1+e -rr ad the besmE e S : pF(8, d I1)-  Wd pULI1e 14-e7 j  r 1 +,-? ++- J  where boecae PkBnd,O)n 1 -Pw9M d, 1)-P*6d, 1).

(iSa)  (It)  OICe)  (17b)  (17  For the woeS- (rsevl,best-) co promIss, we tuwo used the extreme values of AEmI(d) to underestimate (overestimate) convergence and oesinse (uesMa) divee,gin that thr b a ikagrere in d of the N  obetw netwod at dBm; we -swtherrs -suytlky Saoe tflbb.

W~~~~~~~~~~~~~~~~wm am eqwb (* at oe ofqth b an

V. Estiadion Roer P U To estimate the reti p we shas the  Hringda of the networis state fronta sored pat The evoktion of the netwrk fro Mast stAte, - diance, from a sue patten, can be knerpeedhin tens of a blth-and-dnth Markov poces ([16D (figure 1) wih the  ~~ -  PRB.dA) Pd[HDA+"),Bj=d+ IHPM),BwJ-d) (12) wher 5-1,P, I Fora-1 weoWSainthepIbtofWoWUme ForS- +1 we obtan te p A d e; For 8 we 0*ain t  p t-a oWf stal mate.

An exac cawlton of the attractionFitots (1.2)Is  thne-xponerle ad we sha therefore seek lower ad uppe bounds can be cabtated In lM.We Sh re-order thew of each neuron ao gto their cortxtn to the -A for each pattt ug on  WI(wj- ,w,b w ... w  DC=max( Wt"-{<f4S'I ',Q .,i3ra- I(13)  LMAEj%d)= &E, -.2x0 pa1  (14a)  5f dAEd=)"&E. - 2twDug*i (14b)  Thes values r tw maxwzm andrrmrdn values ta the h enrgy gap could asmun when d network is at O of d fron Bm . Using these extema, we Ca find we wost case  oI-% A. 0 0 0  O 1 p.O0 o 0    0 Fe 1 e  0 o N, Vs4t bc  *0P g' *  where th bit oty pU i the ciegec_Pr ofd irreaski t HD fromn i to W, ad the death t pg is t _oortrcipn d deos g the H1D kiom Ibl1.

Pw 1 P6cP tqP qP ttd4  P41 P~ PCPA P4 d .

: _ _l_bk     Givn ta an ipt pattern was at HO of do frOm Bi, the probt*yt Stern pethenetwowrk assat f with Bm is P, {u0'?- IHD rUP'WB =do} 4-m  Z Pr[ HD(U@J),BB= r 1 HDV, B,,BO=d0] (19) r.O  were we can use he one-ep bonds foundin secto III in order to calculat the Wst-case and best-case prbilties of associton. Usin eqr. (15) and (16) we deiremmat siesbromhpnB.susme Svfix,4 aids bustcn maMz;IddreXmmsSbpknS; otc 1 dabestcmS\4: %= P4,0j o) P=O( ii,t1) Pd= Fwo.,-1) P t(Bb.i1, Using hse matrIes, k is now possl to cal e wer and upper bounds for the associaon probabUkies needed In equation (19):  [z4(YJS Pr [ HDet),B) r HU"D,B.=YoI sp (na) diem [xIirnabs ,srds*Mentftt* cbr X.aNd x4IsltwurSt Izn+lwC  IX 1= do 10 iO_6  The bluos of o 20(a) cian be uwed to bound the aso_an _y---f in (19). The boun at th  twlm in (19) by thei 4J Yll@:  r-o The bWM bod cWX be mea skr W* shoe ki osA #Wsat i d of w prx to thpes rafp (n) Om  I koi ta8 alead asoiae sae U wk on f h pattM. we SW Oww"" a the kwor bouii on the womgic 11 ineqdo (19):  DXkno(2 : PrfLP,{-W I HD P")AY (2 1 b) r-o whe fth undefd matrix is Ome bith-xklth fff (18)  ,iVI,,+l)sbri12,.L1  1 for ib,.

PJ,Bi,-t) f i=1t,2?.1for Pd -  7o for i ,Pr-1..N..N  mid (21 c)  p.=mkiHD(BB-d. j=l,...,R,j,i (21d) Equation (21c) and (21d) assume that the network wanders it the dmar sphere of influence of a pattem oer than Bi, whenver is dtance from BqIs pi or more. ThIs assulTqlis ve _cowevive, sne pW spee the shorte Stnc to a competin dmaxr sphere of wdtence, and the network ooukd actuy wander to d s ta pi and sti corwerge eventuay It the dm -sphere Virfencer*vB .

VL Exmles pelannanc assessmentm and copNtsofl id,lt-ssg.et algorthn  Our examVes are desige to demonstrate the utW of our approach in comparing weight asiomerts for a netwok.

Simlarly, the seldctn of a temperature (Te), th selection of a ceword set for repesertation of iormation, and the leaing and production periods th ar necessary to attain required pefm ace could be studied.

Weight anlgnnwnts We shall compare the utility of two (static) weight sn ris for wq and . The fis is th popular sun-f-outer-prodfsassignmrt (eg. [51, [6]. [13D:o 0 wi=X bkb,j andt=-X1:tb (22)  k-I h-i The secd Is based on th steady sWte aerag values of weights calculaed according to the incremental Hettian larning rule ([117D. This learr*igrle is a derivative of the updat schm t we h ed in equation (10):  4I)+4 ffbbv= I and L e)+t, 1d= 1uadf T ?t= 4O4 f%b1-ld jW)-L il= f-4 iX-%rn- T  4' t4=- IwwwN= L n?i= Imwfh)= i  qj iwb= lm siwtu L 9 ffb =-1 ddd- T  In (23) we have itrduced lwer and upper bounds for the weiWS (-L and L) amd for ftstlveKfs (-T ad T). We sia use the steady sae average vues of the weights and the trsold (17D as psmp d Ve ntwok (for )  % I (- t+ IXafO7 2L(0m&lLb  WI ~~~~~~~~~~~~~~(24a)  0 for ~~~arQgs (1-02+ XCt T-']T* m~~-I(-Q I4  | %[ 1-+ 1Xo+2T- _- -}T fora* 1  01uls2T+1xT+2T0.'hT =(24b) OfrOq=1  Th parameters Qi and Oq we dlt ratio related t t p lntiilsfIrursBo_sdebnsftd 0-1 - 1-1,I) andof82 ={-, 1-)  (1,), (-1-1 In the wnedkns a'17: n r nXIlr  4= _ _ r--I a2 o= (24c) r IEnr  ri bb.-i rlbg-1 Whe nr Is the MIblt of dWe rth pam in th environment, during the tain period. The main between the aim-of er-products assimetaw d t assi n (24) Is the fact that, -whle both take Ino acoouint th correlaion between pattems, the tter takes irno acount also the probability distributions of the pattems in the network's envirnent. RuLe (24) is fthrore expected to perform beter than rule (23), even when we equate the bounds on their parameter vae by sett L - T = R (R - Xrtnumber of paffemrs.) Pattern sts we study examples of networks. with forty neuros which have eamed (using the sInmets (22) and (24)) dlferert sAeets othe fooi pattem set.

I 1 2+ 1 1 1 I1 1 4t+f1- --- -  B2- - ++++-H- - + - + - + - ++++-t+ + - - - - + --t134-~ ~~~~~~~~~~~~4H  B5se-+ - -+-+- -+-+- - - --+ +- - -  +  B7- +a+++++*.++--+ - - ---     Case 1: RetiaWprobtli -o n We study the set ( B4, B5, Be, B7) of four mutualy orthogonal pattems. In figure 2 we show the p ity of convergence from Hamming ditances of 1 and 2 to B4, versus the number of production steps. Here dmaxvs. The sum-of-outer-prducts weigt assig*mnt (eq. (22)) was used. The method gives tight bounds for i itial Hamng distan of one. For reeal from an iitial Hamming distace of 2, the upper ard bwer bounds separate. The same paterns are exanined again in figure 4 where the steady-state Hebbian assignment (eq. 24) is used.

The performance with this weight assigment gave better steady-ste bounds for lar eia Hammng d c: we show the bounds for Hamming d tancs of 2 and 5.

We have stuxid the retrieval of pattern B-I, when the set of arbitrariy-seled paerns fB1,B2,B3) was stored in the 40-euron netwrk - using (22) and (24) for wet.

Figures 3 and 5 depict the retrieval probabilties for the two assignments with dmax - 0. Again, we have observed an advanaoge to the steady-state Hebblan snent for laW iiial Hamming distances. We show the bounds for Hamming distances I and 2 for the sum-of- ter-products assignment (figure 3) and for Hamming distances of 2 and 5 for the steady-Wate Hebbana (fu 5).

Man We have preserted how h Bolmann Machin can be used as a cortent-addressable memory, exploit the stochstc nwe of its sate-scion p r n escape undesirable mirima. An asso n rd in tern of pater' her of irdnluece is- used in order to match an p wh one of th predetermined stored patterns. The system Is therefore Ifferent to 'spurious' sates, whose spheres of Influnc are not recognized in the retrieval process. We have detald a technique to calculae the upper and loww bounds on retrival probabities of each stred pattern. These bourds are functiu of the network's parameters (i.e. or larnin res, and the pattem sets); the inta Haing distance from the stored pattern; the asociation rule; and the number of production steps. They aNow a ponymitme assessfent of the networks cpab e as ana e memory for a - set of paterw ; a copnarison of different codg schemes for pafttr to be strdand retieved; an assmen of the ngh of the leamrnng period necessary in order to gartee a presb d oilty rof ievl; ard a r o o iffe leamgassignmnnst rules for the ntwork paramets.

A-I. A-l1!

