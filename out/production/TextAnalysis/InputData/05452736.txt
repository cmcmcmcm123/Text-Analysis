Constrained Frequent Itemset Mining from Uncertain Data Streams

Abstract?Frequent itemset mining is a common data min- ing task for many real-life applications. The mined frequent itemsets can be served as building blocks for various patterns including association rules and frequent sequences. Many existing algorithms mine for frequent itemsets from traditional static transaction databases, in which the contents of each transaction (namely, items) are definitely known and precise. However, there are many situations in which ones are uncertain about the contents of transactions. This calls for the mining of uncertain data. Moreover, there are also situations in which users are interested in only some portions of the mined frequent itemsets (i.e., itemsets satisfying user-specified constraints, which express the user interest). This leads to constrained mining. Furthermore, due to advances in technology, a flood of data can be produced in many situations. This calls for the mining of data streams. To deal with all these situations, we propose tree-based algorithms to efficiently mine streams of uncertain data for frequent itemsets that satisfy user-specified constraints.



I. INTRODUCTION  Frequent itemset mining [3], [18], [23] aims to search for implicit, previously unknown, and potentially useful sets of items (aka itemsets) that are frequently co-occurring. The mined frequent itemsets (FIs) can be used in as building blocks of various patterns (e.g., association rules, correlation, sequences, episodes, maximal patterns, closed patterns) for many real-life applications. This explains why the problem of mining FIs has been the subject of numerous studies since its introduction [2]. In the early days, many of the algorithms proposed were Apriori-based. They depend on a generate-and-test paradigm: They find FIs from the transaction database (DB) by first generating candidates and then checking their support (i.e., their occurrences) against the DB. Han et al. [14] improved efficiency of the mining process by proposing the FP-growth algorithm, which uses a restricted test-only approach (i.e., does not generate candidates, and only tests for support). This algorithm constructs an extended prefix-tree, called Frequent Pattern tree (FP-tree), to capture the contents of the DB.

The above algorithms mine traditional static DBs (e.g., DBs of market basket transactions, Web logs) containing precise data. When mining these precise data, users definitely know whether an item (or an event) is present in, or is absent from, a transaction in the static DBs. However, there are situations (e.g., medical diagnosis, environmental surveillance, quantitative economics/survey research) in which users are uncertain about the presence or absence of some items or  events [6], [15], [16], [27]. To elaborate, let us consider a situation where a physician highly suspects (but cannot guarantee) that a patient suffers from the H1N1 flu. The uncertainty of such suspicion can be expressed in terms of existential probability. In this uncertain DB of patient records, each transaction ti represents a patient' s visit to a physician' s office. Each item within ti represents a potential disease, and is associated with an existential probability expressing the likelihood of a patient having that disease in ti. For instance, in ti, the patient has a 90% likelihood of having the H1N1 flu, and a 30% likelihood of having a seasonal flu regardless of catching the H1N1 or not. With this notion, each item in a transaction ti in traditional DBs containing precise data can be viewed as an item with a 100% likelihood of being present in ti. As mining of uncertain data [1], [5], [29] is in demand, some algorithms (e.g., U-Apriori [7], UF-growth [22]) have recently been designed to find all FIs from uncertain data.

Note that there are real-life situations in which users may have some particular phenomena in mind on which to focus the mining. For example, a store manager may want to find groups of customers having an average age older than 65 or sets of popular merchandise items having a total price exceeds $500. Without user focus, the user often needs to wait for a long period of time for numerous FIs, out of which only a tiny fraction may be interesting to users. This leads to constrained frequent itemset mining [4], [13], which aims to find only the FIs that satisfy the user-specified constraints. DCF [17], CAP [24] and FIC [25] are some algorithms that mine static DBs of precise data for constrained FIs.

Constrained frequent itemset mining is not confined to pre- cise data; it can also be applied to uncertain data. To elaborate, having some particular phenomena in mind on which to focus the mining while users are uncertain about the presence or absence of items in a DB is not unusual. For instance, when analyzing laboratory test data, there are some known factors (e.g., human reaction time, measuring errors) contributing to the uncertainty of the data. Analysts may be interested in only the data belonging to patients observed to have abnormal blood cell counts (rather than all the patient data). Hence, algorithms (e.g., U-FPS [19]) for mining constrained FIs from uncertain data are in need.

The automation of measurements and data collection has produced tremendously huge amounts of data in many real- life application areas. The recent development and increasing     TABLE I  OUR PROPOSED ALGORITHMS VERSUS THEIR MOST RELEVANT ALGORITHMS  Our proposed CAP [24], U-Apriori [7], U-FPS FP-streaming UF-streaming UF-streaming+, DCF [17], UF-growth [22] [19] [12] [20] UF-streaming?, FIC [25] CUF-streaming  Constrained mining ? ? ?  Mining uncertain data ? ? ? ?  Stream mining ? ? ?  use of a large number of sensors has added to this situation.

Consequently, these advances in technology have led to a flood of data. We are now drowning in streams of (precise or uncertain) data but starving for knowledge [26], [28]. In order to make sense of these dynamic streaming data, algorithms for mining data streams [8], [9], [11], [21] are in demand.

FP-streaming [12] and UF-streaming [20] are some examples of algorithms that mine FIs from data streams.

Among the above algorithms (see Table I), the CAP, DCF and FIC algorithms all find constrained FIs, but they mine precise data. Both U-Apriori and UF-growth mine uncertain data, but they find all unconstrained FIs. The U-FPS algorithm mines uncertain data for FIs satisfying constraints, but it does not handle data streams. The FP-streaming algorithm was designed for mining streams of precise data; the UF-streaming algorithm was designed for mining streams of uncertain data.

However, both find all FIs instead of only those satisfying the constraints. Hence, a natural question to ask is: Is it possible to mine streams of uncertain data for only those FIs that satisfy user-specified constraints? In response to this question, we propose three tree-based algorithms for mining streams of uncertain data for FIs that satisfy user-specified constraints.

Here, our key contribution of this paper is the non-trivial in- tegration of (i) mining uncertain data, (ii) constrained mining, and (iii) stream mining. The resulting tree-based algorithms avoid the candidate generate-and-test paradigm when mining constrained FIs from uncertain data streams.

On the surface, the three proposed algorithms may appear to handle a very specialized situation as they were designed for mining constrained FIs from uncertain data streams. However, it is important to realize that these algorithms are not confined to this situation. They can be serve as alternatives to algorithms for mining constrained or unconstrained FIs, from precise or uncertain data, in the traditional DB or data streams (e.g., mining unconstrained FIs from traditional DBs containing uncertain data). Thus, in terms of applicability, the proposed algorithms can be used in a suitably broad range of real-life applications.

This paper is organized as follows. The next section pro- vides background and related work. In Sections III?V, we describe our algorithms for mining constrained FIs from streams of uncertain data. Section VI shows experimental results. Finally, conclusions are presented in Section VII.



II. BACKGROUND AND RELATED WORK  In this section, we provide background on topics related to mining data streams of uncertain data for FIs that satisfy user-specified constraints.

A. Uncertain Data  When compared with precise data, each uncertain data trans- action contains items and their existential probabilities. The existential probability P (x, ti) of an item x in a transaction ti indicates the likelihood of x being present in ti. Using the ?possible world? interpretation of uncertain data [7], [22], there are two possible worlds for an item x and a transaction ti: (i) the possible world W1 where x ? ti and (ii) the possible world W2 where x ?? ti. Although it is uncertain which of these two worlds be the true world, the probability of W1 be the true world is P (x, ti) and that of W2 is 1 ? P (x, ti).

In general, there are many items in each of the n transactions in a TDB. Hence, the expected support of an itemset X in the TDB can be computed by summing the support of X in possible world Wj (while taking in account the probability of Wj to be the true world) over all possible worlds:  expSup(X) = ?  j  [sup(X) in Wj ? prob(Wj)] , (1)  where sup(X) in Wj , denoting the support of X in possible world Wj , can be computed by counting the number of transactions contain X in Wj . The probability of Wj to be the true world, denoted by prob(Wj), can be computed by ?n  i=1  (? x?ti in WjP (x, ti) ?  ? y ??ti in Wj[1 ? P (y, ti)]  ) .

When items within X are independent, Equation (1) can be simplified [10] to become the following:  expSup(X) = n?  i=1  (? x?X  P (x, ti)  ) . (2)  With this setting, an itemset X is considered frequent if its expected support equals or exceeds the user-specified support threshold minsup.

B. Constraints  To express their interest in finding FIs that satisfy cer- tain criteria, users can specify SQL-style constraints [17], [24] such as aggregate constraints C1 ? min(X.WBC ) ? 10000/?L, C2 ? max(X.RBC ) ? 6.1 ? 106/?L, C3 ? avg(X.Temperature) ? ?5?C, and C4 ? sum(X.Rainfall ) ? 200mm. Here, for medical laboratory test results, constraint C1 says that the minimum white blood cell count of all patients in X is at least 10,000 cells per microlitre of blood and C2 says that the maximum red blood cell count of all patients in X is at least 6.1 million cells per microlitre of blood. For another domain (meteorological records collected by wireless sensors), C3 says that the average hourly temperature in X is at least ?5?C, and C4 says that the total rainfall of all records in X is at least 200mm.

In order to efficiently find FIs that satisfy these constraints, we better explore properties (e.g., anti-monotonicity, mono- tonicity [18]) that are possessed by these constraints. Note that C1 is anti-monotone because whenever X violates C1 (i.e., the minimum WBC count < 10000/?L), all supersets of X also violate C1 (as adding more patients to X will not increase the minimum WBC count). C2 is monotone because whenever X satisfies C2 (i.e., the maximum RBC count < 6.1? 106/?L), all supersets of X also satisfy C2 (as adding more patients to X will not lower the maximum RBC count).

C. Data Streams  When compared with traditional static data, streams are continuous and unbounded. Moreover, data in the streams are not necessarily uniformly distributed. Their distributions are usually changing with time. Hence, we no longer have the luxury of performing multiple data scans. Once the streams flow through, we lose them. Moreover, a currently infrequent itemset may become frequent in the future, and vice versa. We have to be careful not to prune infrequent itemsets too early; otherwise, we may not be able to get complete information such as the support values of some itemsets (as it is impossible to recall the pruned itemsets).

To mine FIs from data streams, Giannella et al. [12] de- signed the FP-streaming algorithm. For an incoming batch of transactions in a data stream, FP-streaming calls the FP-growth algorithm [14] with a threshold that is lower than the usual minimum support threshold minsup to find ?frequent? itemsets.

Let us call this lower threshold preMinsup. Then, an itemset is ?frequent? if its actual support is no less than preMinsup. Note that, although we are interested in truly frequent itemsets (i.e., itemsets with support ? minsup > preMinsup), FP-streaming uses preMinsup in attempt to avoid pruning an itemset too early. An itemset X having preMinsup ? sup(X) < minsup is currently infrequent but may become frequent later; so, X is not pruned by FP-streaming. Once the ?frequent? itemsets are found, the FP-streaming algorithm stores and maintains these itemsets in another tree structure called FP-stream before handling the next batch of streaming transactions. It is important to note that, while the FP-streaming algorithm finds ?frequent? itemsets from dynamic streams of precise data, it neither handles uncertain data nor does it explore constraints.



III. UF-STREAMING+ : MINING CONSTRAINED FREQUENT ITEMSETS FROM UNCERTAIN DATA STREAMS  Inspired by FP-streaming, we propose in this section an algorithm?called UF-streaming+?for mining constrained FIs from uncertain data streams. When the uncertain data stream flows in, our proposed algorithm applies the UF-growth mining technique to the first batch of transactions in the stream. Recall that UF-growth captures the contents of (a static DB of) uncertain data in a tree structure called UF-tree, and tries to find itemsets having expected support ? user- specified threshold. Here, UF-streaming+ captures the con- tents of transactions in the first batch of dynamic streams of uncertain data in the UF-tree. A key difference between this  UF-tree and the usual FP-tree is that each tree node in the latter consists of two components?namely, an item and its (actual) support?whereas each node in the UF-tree contains an extra component?i.e., its occurrence count in addition to the item itself and its (expected) support. For precise data, the actual support of an item is the same as its occurrence count: If an item x appears in a transaction ti, x occurs once in ti with the actual support of x equals 1. For uncertain data, if x appears in ti with existential probability P (x, ti), x occurs once in ti with the expected support of x equals P (x, ti).

Once the UF-tree is constructed, our proposed UF-streaming+ algorithm mines ?frequent? itemsets from this tree using preMinsup (instead of the usual minsup).

An itemset X is ?frequent? if expSup(X) ? preMinsup.

Recall that data in the streams are not necessarily uniformly distributed, an itemset X that is infrequent in the current batch may be frequent in subsequent batches in the current sliding window (which may make X a frequent itemset in the current window). As data streams are continuous and unbounded, we can no longer go back to the current batch and reconsider X once we moved to subsequent batches.

Consequently, if expSup(X) is currently slightly lower than minsup, we better keep X. Otherwise, we may miss X (a possibly frequent itemset).

UF-streaming+ mines ?frequent? itemsets from the UF-tree in a depth-first divide-and-conquer manner. The algorithm first finds ?frequent? domain items x1, x2, x3, x4, ? ? ? . It then forms projected DBs for x1 and its extensions (e.g., {x1}-, {x1, x2}-, {x1, x2, x3}-projected DBs) to find ?frequent? itemsets con- taining x1. Afterwards, the algorithm forms projected DBs for x2 and its extensions to find ?frequent? itemsets containing x2.

This mining process is repeated for all other ?frequent? domain items (e.g., x3, x4, ? ? ? ).

As a result, UF-streaming+ found all the ?frequent? item- sets. It then stores and maintains them in another tree structure called UF-stream. In this structure, each path represents a ?frequent? itemset. Common items in itemsets share the tree path in a similar fashion as in the FP-tree or the UF-tree.

However, each node in this UF-stream structure contains (i) the item and (ii) a window table (containing a list of w expected support values, one for each batch of streaming transactions).

As users are often interested in recent data than older data, the UF-stream structure focuses on capturing only the w most recent batches of transactions in the stream. So, when a new batch of transactions flows in, the window slides and the expected support of each node in the UF-stream structure also shifts.

The above mining procedure is repeated for each subsequent batch of transactions in the uncertain data streams. In other words, for each batch, our proposed UF-streaming+ algo- rithm (i) finds ?frequent? itemsets and (ii) stores the mined ?frequent? itemsets in the UF-stream structure by sliding the window and shifting the w expected support of each node in the UF-stream structure so as to ensure that it always captures the contents of the w most recent batches of transactions in uncertain data streams. As a post-processing step, the     mined ?frequent? itemsets stored in the UF-stream structure are finally checked against the user-specified constraints. Only those satisfying the constraints are returned to users. To gain a better understanding of the UF-streaming+ algorithm, let us consider Example 1.

Example 1: Consider the following uncertain data stream: Batch Transactions Contents  t1 {a:0.9, b:0.7, c:0.7, e:0.6} first t2 {a:0.9, c:0.8, e:0.7}  t3 {b:0.9, d:0.9, e:0.1} t4 {b:1.0, c:0.3, d:1.0}  second t5 {a:0.9, c:1.0} t6 {b:0.4, c:0.5, d:1.0} t7 {a:0.8, c:0.9}  third t8 {b:1.0, c:0.1, d:1.0} t9 {a:0.9, c:0.9, d:0.2}  with auxiliary information: Items a b c d e  WBC (?103/?L) 11.5 11.0 10.5 9.5 9.0 Here, each transaction contains items and their corresponding existential probabilities (e.g., the existential probability of item a in transaction t1 is 0.9).

Let the user-specified support threshold minsup be set to 1.2, and let the user-specified constraint be C1 ? min(X.WBC) ? 10000/?L. Our proposed UF-streaming+ algorithm applies the UF-growth mining technique to the first batch of transactions in the uncertain data stream using a preMinsup lower than minsup (say, preMinsup = 0.9). The algorithm constructs a UF-tree by scanning and inserting each transaction into the UF-tree. It first inserts the contents of t1 into the tree, and results in a tree branch ?(a:0.9):1, (b:0.7):1, (c:0.7):1, (e:0.6):1?. It then inserts the contents of t2 into the UF-tree. Since the expected support of a in t2 is the same as the expected support of a in an existing branch (i.e., the branch for t1), the tree node (a:0.9) can be shared.

So, the occurrence count for the node (a:0.9) is incremented to 2, and the remainder of t2 is added as a child of the node (a:0.9):2. As a result, we get the tree branch ?(a:0.9):2, (c:0.8):1, (e:0.7):1?. Afterwards, the contents of t3 are inserted as a new branch ?(b:0.9):1, (d:0.9):1, (e:0.1):1? because the node (b:0.9):1 cannot be shared with the node (a:0.9):2.

Consequently, at the end of the tree construction process, we get the UF-tree, shown in Fig. 1(a), capturing the important contents of the first batch of uncertain data.

Once the UF-tree is constructed for the first batch, the algorithm first finds the ?frequent? domain items a, b, c, d and e (with their corresponding accumulated expected support of 1.8, 1.6, 1.5, 0.9 and 1.4 ? preMinsup). It then recursively mines ?frequent? itemsets from this tree with preMinsup as fol- lows. It starts with item e (with expSup({e})=1.4): It extracts from two tree paths?namely, (i) ?(a:0.9), (b:0.7), (c:0.7)? that occurs once with (e:0.6) and (ii) ?(a:0.9), (c:0.8)? that occurs once with (e:0.7)?and forms the {e}-projected database.

Then, expSup({a, e}) = (1 ? 0.9 ? 0.6) + (1 ? 0.9 ? 0.7) = 1.17 ? preMinsup, and expSup({c, e}) = (1 ? 0.7 ? 0.6) + (1 ? 0.8 ? 0.7) = 0.98 ? preMinsup. (Note that item- sets {d, e} & {a, c, e} are infrequent as expSup({d, e}) =  (e:0.1):1  (d:0.9):1  (b:0.9):1(a:0.9):2  (e:0.7):1  (c:0.8):1  (e:0.6):1  (c:0.7):1  (b:0.7):1  (a) The UF-tree for transactions in the first batch  d[0,2.0]c[1.35,0.9]  a[1.8,0.9]  e[1.17,0]  b[1.6,1.4] c[1.5,1.8] d[0.9,2.0] e[1.4,0]  e[0.98,0]  (b) UF-stream for ?frequent? itemsets found in the 1st & 2nd batches  d[2.0,1.0]c[0.9,1.53]  a[0.9,1.7]  e[0,0]  b[1.4,1.0] c[1.8,1.9] d[2.0,1.2] e[0,0]  e[0,0]  (c) UF-stream for ?frequent? itemsets found in the 2nd & 3rd batches  Fig. 1. The UF-tree and UF-stream structures for Example 1  1 ? 0.9 ? 0.1 = 0.09 < preMinsup and expSup({a, c, e}) = (1 ? 0.9 ? 0.7?0.6) + (1 ? 0.9 ? 0.8?0.7) ? 0.88 < preMinsup.)  Next, the algorithm extracts appropriate paths to form the {d}-projected database, and finds no ?frequent? itemsets.

Afterwards, the mining process ends with item c. As a result, UF-growth found ?frequent? itemsets {a}, {a, c}, {a, e}, {b}, {c}, {c, e}, {d} and {e} (with their corresponding expected support of 1.8, 1.35, 1.17, 1.6, 1.5, 0.98, 0.9 and 1.4 ? preMinsup). (Among them, only {a}, {a, c}, {b}, {c} and {e} are truly frequent as their expected support values ? minsup.)  Let the window size w=2 batches. Our proposed UF-streaming+ algorithm then stores the eight mined ?fre- quent? itemsets in a UF-stream structure, which consists of eight nodes representing the eight itemsets. Afterwards, the second batch of uncertain data stream flows in. Our proposed UF-streaming+ algorithm applies the same mining procedure to the batch. Specifically, it first constructs a new UF-tree, from which ?frequent? itemsets {a}, {a, c}, {b}, {b, d}, {c} and {d} (with their corresponding expected support values of 0.9, 0.9, 1.4, 1.4, 1.8 and 2.0) can be found. It then updates the existing UF-stream structure by storing these itemsets in it. The result- ing UF-stream structure, as shown in Fig. 1(b), consists of nine nodes (due to the addition of the node d[0,1.4] on the path ?b[1.6,1.4], d[0,1.4]? representing the new ?frequent? itemset {b, d} with expSup=0 in the first batch and expSup=1.4 in the second batch). The node b[1.6,1.4] represents the ?frequent? itemset {b} with expected support values of 1.6 and 1.4 in the first and second batches respectively.

When the third batch flows in, the algorithm finds ?frequent? itemsets {a}, {a, c}, {b}, {b, d}, {c} and {d} (with their cor- responding expected support values of 1.7, 1.53, 1.0, 1.0, 1.9 and 1.2). It then slides the window (of size w=2 batches) and shifts the expected support of each node in the UF-stream     structure to make room for the third batch. The resulting UF-stream structure, as shown in Fig. 1(c), captures the expected support values for ?frequent? itemsets found in the second and third batches. (Note that nodes with zero expected support, such as e[0,0], can be pruned; this results in six nodes in the structure.)  Finally, as a post-processing step, the algorithm checks the ?frequent? itemsets stored in the FP-stream structure against the aggregate constraint C1 and find only {a}:2.6, {a, c}:2.43, {b}:2.4 and {c}:3.7 satisfying C1.

Analytically, UF-streaming+ captures the contents of each batch of streaming transactions containing uncertain data?one batch at a time?in the UF-tree, from which O(p) ?frequent? itemsets are found (where p 2m for m domain items). Note that the size of the UF-tree is bounded above by the number of item:expSup in a batch. All O(p) ?frequent? (valid as well as invalid) itemsets from the w most recent batches are stored in the UF-stream structure (where w is the window size).



IV. UF-STREAMING? : CHECKING CONSTRAINTS EARLY WHEN MINING CONSTRAINED FREQUENT ITEMSETS FROM  UNCERTAIN DATA STREAMS  Although UF-streaming+ finds constrained FIs from uncer- tain data streams, it checks constraints in a post-processing step. As a result, it wastes lots of space as it stores both valid as well as invalid itemsets in the UF-stream structure. Here, we propose another algorithm?called UF-streaming??which performs constraint checking as an intermediate step (instead of a post-processing step). Specifically, the algorithm first uses the same UF-growth mining technique to find all ?frequent? itemsets, and it then checks the mined itemsets against user- specified constraints before storing the constrained itemsets in the UF-stream structure. By doing so, the UF-stream structure stores only valid itemsets. See Example 2.

Example 2: Revisit the uncertain data stream in Exam- ple 1. With our proposed UF-streaming? algorithm, only valid itemsets are stored in the UF-stream structures. See Fig. 2 (cf. Fig. 1(b) & (c)). Some memory space is saved as each UF-stream structure consists of only four (instead of eight or nine) nodes.

Analytically, UF-streaming? captures the contents of each batch of transactions in the UF-tree, from which O(p) ?fre- quent? itemsets are found. All these O(p) itemsets are  c[1.35,0.9]  a[1.8,0.9] b[1.6,1.4] c[1.5,1.8]  (a) UF-stream for ?frequent? itemsets found in the 1st & 2nd batches  c[0.9,1.53]  a[0.9,1.7] b[1.4,1.0] c[1.8,1.9]  (b) UF-stream for ?frequent? itemsets found in the 2nd & 3rd batches  Fig. 2. The UF-stream structures for Example 2  then checked against the user-specified constraints, and only O(?p) valid ?frequent? itemsets (where constraint selectivity ? ? [0,1]) from the w most recent batches are stored in the UF-stream structure.



V. CUF-STREAMING: EXPLORING CONSTRAINTS WHEN MINING CONSTRAINED FREQUENT ITEMSETS FROM  UNCERTAIN DATA STREAMS  Along this direction, we propose the third algorithm?called CUF-streaming?which pushes the user-specified constraints inside the mining process and explores the properties of these constraints. Specifically, when a batch of streaming transactions containing uncertain data flows in, the algorithm inserts items in each transaction into the UF-tree, in which items are arranged according to some order R depending on the type of constraints. Here, the new transaction is merged with a child (or descendant) node of the root of the UF-tree only if the same item:expSup exists in both the transaction and the child (or descendant) nodes. The occurrence count of a node is at least the sum of occurrence counts of all its children nodes.

Once the UF-tree is constructed, CUF-streaming recursively mines constrained ?frequent? itemsets from the tree in a depth- first divide-and-conquer manner: The algorithm first finds some selected ?frequent? domain items x1, x2, x3, x4, ? ? ? . It then forms projected DBs for x1 and its extensions to find constrained ?frequent? itemsets containing x1. Afterwards, the algorithm forms projected DBs for x2 and its ?extensions? to find constrained ?frequent? itemsets containing x2. This min- ing process is repeated for the remaining selected ?frequent? domain items. Unlike UF-streaming+ or UF-streaming?, our proposed CUF-streaming algorithm does not need to form projected databases for all itemsets (domain items or their extensions) as CUF-streaming explores the property of con- straints. Thus, only some itemsets are selected to check against the constraints, and only some itemsets are selected to form projected DBs during the mining process. The selection depends on the type of constraints.

TYPE I: ANTI-MONOTONE CONSTRAINT. Let attr denote an attribute of an itemset X and const denote a constant. Our proposed CUF-streaming algorithm arranges domain items in a monotonic increasing or decreasing order R of attr values such that invalid items come before/below valid items in the UF-tree. For instance, if C is of the form min(X.attr) ? const (e.g., C1), domain items are arranged in non-descending order R+ of attr values (i.e., ?i, xi.attr ? xi+1.attr) from leaves to the root; if C is of the form max(X.attr) ? const, items are arranged in non-ascending order R? of attr values (i.e., ?i, xi.attr ? xi+1.attr). By doing so, the algorithm checks C against each domain item one at a time and stops as soon as it finds the first valid item xv because all remaining items xv+j (?j ? 1) are guaranteed to be valid (due to R).

The algorithm only needs to form a projected DB for each xv+j (?j ? 0) because any itemsets that can be found in {xk}-projected DB (for 1 ? k < v) are invalid due to the anti- monotonicity of C (i.e., if xk violates C , then all supersets     of xk also violate C). For each xv+j , the algorithm forms projected DBs for itemsets xv+j ? Y (for Y ? {xi | i > v + j}). ?Frequent? itemsets found in these projected DBs are guaranteed to be valid (due to the anti-monotonicity of C).

Note that, instead of checking all O(p) ?frequent? item- sets against C (as in UF-streaming+ and UF-streaming?), CUF-streaming only needs to check O(m) domain items (where m p 2m) against C .

TYPE II: MONOTONE CONSTRAINT. CUF-streaming ar- ranges domain items in a monotonic decreasing or increas- ing order R? of attr values such that valid items come before/below invalid items in the UF-tree. For instance, if C of the form max(X.attr) ? const (e.g., C2), items are arranged in non-ascending order R? of attr values from leaves to the root; if C is of the form min(X.attr) ? const, domain items are arranged in non-descending order R+ of attr values. By doing so, the algorithm checks C against each domain item until it finds the first invalid one xh because all remaining items xh+j (?j ? 1) are guaranteed to be invalid (due to R?). For each xv (for 1 ? v < h), the algorithm only needs to form projected DBs for itemsets xv ? Y (for Y ? {xi | i > v}). ?Frequent? itemsets found in these projected DBs are guaranteed to be valid due to the monotonicity of C (i.e., if xv satisfies C , then all supersets of xv also satisfy C).

Again, CUF-streaming only needs to check O(m) domain items against C .

TYPE III: CONVERTIBLE ANTI-MONOTONE CON- STRAINT. Like Type II, CUF-streaming arranges domain items in R? such that valid items come before/below invalid items in the UF-tree. For instance, if C is of the form avg(X.attr) ? const (e.g., C3) or sum(X?.attr) ? const (where X? is an itemset with non-positive attr value), domain items are arranged in non-ascending order R? of attr values from leaves to the root; if C is of the form avg(X.attr) ? const or sum(X+ .attr) ? const (where X+ is an itemset with non-negative attr value), items are arranged in non-descending order R+ of attr values. By doing so, the algorithm checks C against each domain item until it finds the first invalid one xh as all remaining items xh+j (?j ? 1) are guaranteed to be invalid (due to R?). Again, projected DBs are formed only for the valid items xv (for 1 ? v < h).

However, ?frequent? itemsets found in these projected DBs need to be checked against C as not all of them are valid.

By exploring the convertible anti-monotonicity of C (if X violates C , then all ?extensions? of X also violate C), the algorithm only needs to check C against each item in the projected DB until it finds the first invalid one.

TYPE IV: CONVERTIBLE MONOTONE CONSTRAINT.

Again, CUF-streaming arranges domain items in R? such that valid items come before/below invalid items in the UF-tree.

For instance, if C is of the form sum(X+ .attr) ? const (e.g., C4), domain items are arranged in non-ascending order R? of attr values from leaves to the root; if C is of the form sum(X? .attr) ? const, items are arranged in non-descending order R+ of attr values. By doing so, the algorithm checks each domain item against C until it finds the  first invalid item xh. All remaining items xh+j (?j ? 1) are guaranteed to be invalid (due to R?). Unlike the procedures for other three types of constraints, the algorithm forms projected DBs for ?extensions? of valid items as well as invalid items.

While further constraint checking is unnecessary for ?exten- sions? of the valid items (due to convertible monotonicity of C: All ?extensions? of a valid X are guaranteed to be valid), further constraint checking is needed for ?extensions? of the invalid items (because some of these ?extensions? may be valid).

With (i) m domain items and (ii) C of Type III or IV (having selectivity ? ? [0,1]), CUF-streaming checks O(?p) items (where m ?p ? p 2m) against C .

The mined ?frequent? itemsets that satisfy one of the above four types of constraints are then stored in the UF-stream struc- ture. Afterwards, CUF-streaming handles subsequent batches of streaming transactions of uncertain data in a similar fashion.

Like UF-streaming?, CUF-streaming also stores only valid itemsets in the UF-stream structure. Unlike UF-streaming?, CUF-streaming mines constrained ?frequent? itemsets more effectively as it pushes the constraint inside the mining process and explores properties of the constraint.

Example 3: Revisit the uncertain data stream in Example 1.

C1 is a Type I constraint. So, when the first batch of trans- actions from uncertain data stream flows in, our proposed CUF-streaming algorithm arranges domain items in ascending order R+ of WBC counts (i.e., e, d, c, b, a) from leaves to the root. The algorithm then checks C1 against each domain item in the UF-tree (i.e., e, d) until it finds the first valid item c. It forms projected DBs for valid items (i.e., c:1.5, b:1.6, a:1.8) as well as their ?extensions?. No more constraint checking is needed as any ?frequent? itemsets found in the projected DBs of valid items and their ?extensions? are guaranteed to be valid (due to anti-monotonicity of C1). From the {c}- projected DB, {a, c}:1.35. CUF-streaming then stores all four constrained ?frequent? itemsets {a}, {a, c}, {b} & {c} in the UF-stream structure. Same approach is then applied to the second batch, and results in the same UF-stream structure as shown in Fig. 2(a). Afterwards, CUF-streaming applies the same approach to the third batch; this results in the same UF-stream structure as shown in Fig. 2(b).



VI. EXPERIMENTAL RESULTS  We used different datasets for experimental evaluation. For space limitation, we reported here the experimental results on a dataset generated by the program developed at IBM Almaden Research Center [3]. This dataset contains 10M records with an average transaction length of 10 items, and a domain of 1,000 items. We assigned an existential probability from the range (0,1] to each item in each transaction. We set the window size to be w=5 batches and each batch to contain 1M transactions. In addition to this dataset, we also conducted the following experiments using some other datasets, including UCI real-life datasets as well as FIMI datasets. The observa- tions or trends were consistent.

10 20 30 40 50 60 70 80 90  R un  tim e  (in s  ec on  ds )  Selectivity (i.e., percentage of items selected)  CUF-streaming (w=5 batches, each with 1M transactions): Exploration of constraints C1-C4  Type IV constraint C4 Type II constraint C2 Type III constraint C3 Type I constraint C1            10 20 30 40 50 60 70 80 90  R un  tim e  (in s  ec on  ds )  Selectivity (i.e., percentage of items selected)  CUF-streaming (w=50 batches, each with 1M transactions): Exploration of constraints C1-C4  Type IV constraint C4 Type II constraint C2 Type III constraint C3 Type I constraint C1           0 0.001 0.002 0.003 0.004 0.005  R un  tim e  (in s  ec on  ds )  preMinsup (in percentage)  Runtime vs. existential probability & preMinsup  Items take on many different existential probability values Items take on an average number of existential probability values  Items take on a few unique existential probability values  (a) Runtime vs. selectivity (w=5) (b) Runtime vs. selectivity (w=50) (c) Runtime vs. preMinsup  Fig. 3. Experimental results: runtimes  All experiments were run in a time-sharing environment in an 800 MHz machine. The reported figures are based on the average of multiple runs. Runtime includes CPU and I/Os; it includes the time for both tree construction and frequent itemset mining steps. We evaluated different aspects of the proposed algorithms, which were implemented in C.

First, we compared the performance of the three proposed algorithms using four different constraints (one from each type of the above constraints). Experimental results showed that the runtimes for both UF-streaming+ and UF-streaming?  were constant regardless of the constraint selectivity because these two algorithms did not explore property nor did they push the constraints inside the mining process. Specifically, UF-streaming+ performed constraint checking as a post- processing step, whereas UF-streaming? performed constraint checking as an intermediate step prior to storing the ?fre- quent? itemsets into the UF-stream structure. In contrast, CUF-streaming was more interesting as it runtimes depended on the type of constraints as well as the constraint selectivity.

Specifically, the algorithm explored the anti-monotonicity of C1, the monotonicity of C2, the convertible anti-monotonicity of C3, and the convertible monotonicity of C4. As it ex- plored properties of these four constraints and pushed the constraints inside the mining process, CUF-streaming required shorter runtimes than the other two algorithms. As shown in Fig. 3(a), the runtimes for handling all four types of constraints increased when the selectivity increased. Among them, C4 (a Type IV constraint) incurred the highest runtime because CUF-streaming ?extended? (i.e., formed projected DBs for) both valid and invalid items. C2 (a Type II constraint) and C3 (a Type III constraint) incurred the next two highest runtimes For C2, the algorithm ?extended? only valid items.

All ?extensions? of valid items were valid. Due to the item or- dering, the algorithm stopped checking constraints whenever it detected the first invalid items. However, for C3, the algorithm applied constraint checking on projected DBs for valid items as well as their ?extensions? because not all ?extensions? of valid items were valid. C1 (a Type I constraint) incurred the lowest runtime among the four types of constraints because CUF-streaming formed fewer ?extensions? (as they consisted of only valid items). Again, due to the item ordering, the algorithm stopped checking constraints whenever it detected the first valid items.

Next, we repeated the above experiment with a different the window size: w=50 fixed-sized batches with each batch containing 0.1M transactions (instead of using w=5 fixed-sized batches with each batch containing 1M transactions). With this setting, each batch was smaller (0.1M vs. 1M transactions).

Thus, each batch required lower runtime (e.g., for constructing and mining UF-trees). However, the number of batches was higher (50 vs. 5 batches) than the previous setting. This explains why the runtimes (see Fig. 3(b)) took on a broader range than the previous experimental results. For example, when the selectivity of C2 was low (say, 10%), only a few small UF-trees were constructed and mined (as the algorithm only ?extended? valid items) and a shorter runtime ? 110 sec.

(cf. 160 sec. in Fig. 3(a)) was required. As another example, for C4, many bigger UF-trees were constructed and mined (as the algorithm formed projected DBs for both valid as well as invalid domain items), which took ? 400 sec. (cf. 230 sec. in Fig. 3(a)).

As all three algorithms are approximate algorithms, we evaluated the effect of preMinsup on the mining results. For example, using w=5 batches, when preMinsup = 0.8 ? minsup, 90% of the mined constrained ?frequent? itemsets were truly frequent. When preMinsup = 0.9 ? minsup, 95% of the mined constrained ?frequent? itemsets were truly frequent.

However, lowering preMinsup or having more batches in the sliding window had the benefits of increasing the chance of not pruning relevant expected support information for truly frequent itemsets. Moreover, as shown in Fig. 3(c), when preMinsup increased, fewer itemsets had expected support ? preMinsup, and thus shorter runtimes were required. The figure also showed the effect of the distribution of item existential probability. When items took on a few unique existential probability values, the UF-tree became smaller. Thus, times for both UF-tree construction and mining became shorter.

In addition, we also measured the number of nodes in each UF-tree. The experimental results showed that the total number of nodes in a UF-tree was no more than the total number of items (with their existential probability) in all transactions in the current batch of uncertain data stream. Furthermore, we measured the number of nodes in the UF-stream structure as well. As UF-streaming+ performed constraint checking at a post-processing step, the size of UF-stream was observed to be independent of the constraint selectivity. In contrast, as     UF-streaming? and CUF-streaming both pushed the constraint early, the corresponding size of UF-stream was proportional to the selectivity of constraints.

Finally, we evaluated the functionality and applicability of our proposed algorithms. We again used four different constraints, and we also set the constraint selectivity be 100% (i.e., all items are selected). Then, we compared our three proposed algorithms with UF-streaming [6] (which was designed to mine unconstrained ?frequent? itemsets from uncertain data streams). In terms of efficiency, the experi- mental results showed that UF-streaming was slightly faster because it did not perform any constraint checking whereas our three proposed algorithms performed the extra constraint checking step. Among them, CUF-streaming only performed constraint checking on some ?frequent? itemsets, and the other two performed constraint checking on all ?frequent? itemsets.

However, in terms of the mining results, we observed that all four algorithms returned the same collection of ?frequent? itemsets. This illustrated that our proposed algorithms could be used for mining unconstrained frequent itemsets from uncer- tain data streams. Moreover, it is important to note that, while the UF-streaming is confined to finding ?frequent? itemsets satisfying constraints with 100% selectivity, our algorithms are capable of finding ?frequent? itemsets that satisfy constraints having lower selectivity.

Along this direction, we set w=1 batch containing the entire dataset. Then, we compared our algorithms with UF-growth [22] by assigning to each item in every transaction in a dataset an existential probability of 1 (i.e., all items are definitely present in the dataset) and preMinsup = minsup.

Again, we observed that all four algorithms returned the same collection of frequent itemsets. This illustrated that our pro- posed algorithms could also be used for mining unconstrained frequent itemsets from static uncertain datasets.



VII. CONCLUSIONS  Frequent itemsets generally serve as building blocks for various patterns in many real-life applications. Most of the existing algorithms find unconstrained frequent itemsets from traditional static transaction databases consisting of precise data. However, there are situations in which ones are uncertain about the contents of transactions. There are also situations in which users are only interested in some subsets of all the mined frequent itemsets. Furthermore, a flood of data can be easily produced in many situations. To deal with all these situations, we proposed three tree-based algorithms? namely, UF-streaming+, UF-streaming? and CUF-streaming? which integrate (i) mining of uncertain data, (ii) constrained mining, and (iii) mining of data streams. These algorithms effectively mine constrained frequent itemsets from uncertain data streams.

