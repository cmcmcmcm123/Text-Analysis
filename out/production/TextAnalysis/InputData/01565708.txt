A Border-Based Approach for Hiding Sensitive Frequent Itemsets

Abstract  Sharing data among organizations often leads to mutual benefit. Recent technology in data mining has enabled ef- ficient extraction of knowledge from large databases. This, however, increases risks of disclosing the sensitive knowl- edge when the database is released to other parties. To address this privacy issue, one may sanitize the original database so that the sensitive knowledge is hidden. The challenge is to minimize the side effect on the quality of the sanitized database so that non-sensitive knowledge can still be mined. In this paper, we study such a problem in the context of hiding sensitive frequent itemsets by judiciously modifying the transactions in the database. To preserve the non-sensitive frequent itemsets, we propose a border-based approach to efficiently evaluate the impact of any modifica- tion to the database during the hiding process. The quality of database can be well maintained by greedily selecting the modifications with minimal side effect. Experiments results are also reported to show the effectiveness of the proposed approach.

1 Introduction  Information sharing may require an organization to re- lease its data to public or to allow another party to access it. However, some sensitive information, which is secret to the organization, needs to be hidden before the data is released. Data mining technology enables people to effi- ciently extract unknown knowledge from a large amount of data. This, however, extends the sensitive information from sensitive raw data (e.g., individual identifier, income, and type of disease identifier) to the sensitive knowledge (e.g., sales pattern of particular product). In [12], the prob- lem of hiding sensitive knowledge has been considered as an important issue of privacy preserving data mining. To preserve data privacy in terms of knowledge, one can mod- ify the original database in some way so that the sensitive knowledge is excluded from the mining result.

In this paper, we study the problem of hiding frequent itemsets from a transaction database. The motivating exam- ples of this research problem have been discussed in [3, 6].

Here we give another real scenario. In Australia, the su- permarket COLES and K-MART share the same customer bonus card, by which the customer id can be identified dur- ing the transactions. To facilitate business cooperation (note that the products sold in these two supermarkets are not much overlapped), two supermarkets may integrate their transaction datasets and analyze the ?inter-associations? be- tween their products. However, before releasing the dataset to the other party, each supermarket wants to hide sensitive frequent itemsets/association rules of its own products.

Generally, to hide the sensitive frequent itemsets, the original database D needs to be modified into D?, called result database. Considering the goal of information shar- ing, releasing a garbage database is meaningless. Therefore, during the process of hiding sensitive frequent itemsets, the quality of the database needs to be preserved so that the impact on the non-sensitive frequent itemsets is minimized.

Previous work [10, 2, 4, 6, 8, 9, 7] on hiding sensitive fre- quent itemsets has applied different heuristics to preserve the quality of the database. However, during the hiding process, none of these studies really evaluates the impact of each modification on the database. In this paper, we use the border [5] of the non-sensitive frequent itemsets to track the impact of altering transactions. During the hiding process, instead of considering every non-sensitive frequent itemset, we focus on preserving the quality of the border, which can well reflect the quality of the result database. Ac- cording to this idea, a border-based approach is proposed to efficiently evaluate the impact of any modification to the database during the hiding process. The quality of database can be well maintained by greedily selecting the modifica- tions with minimal side effect.

The contribution of this paper is as follows. First, we adopt the well-known border concept to the problem of preserving non-sensitive frequent itemsets. Furthermore, during the hiding process, while previous work follows heuristics (rather than really evaluating the impact of each      Tid Items  1 abcde 2 acd 3 abdfg 4 bcde 5 abd 6 bcdfh 7 abcg 8 acde 9 acdh  (a) Database  Frequent itemset : Support abd : 3, acd : 4, bcd : 3, cde : 3  ab : 4, ac : 5, ad : 6, bc : 4, bd : 5, cd : 6, ce : 3, de : 3 a : 7, b : 6, c : 7, d : 8, e : 3 (b) All frequent itemsets  Expected Frequent itemsets on D? acd, cde  ab, ac, ad, bd, cd, ce, de a, b, c, d, e  (c) Non-sensitive frequent item- sets  Figure 1. An example  change), our approach ensures that any modification on the database is controlled according to the impact on the result database. An efficient algorithm is devised to identify the candidate change that has minimal impact on the border.

Specifically, for each sensitive frequent itemset X , the al- gorithm determines the optimal deletion candidate (T, x), i.e., the item x and the transaction T from which item x is deleted. For each item x ? X , we provide an efficient way to estimate the upper and lower bounds on the impact (to the border) of deleting x. With these bounds, the item which may potentially minimize the border impact can be straightforwardly selected. We then determine from which transactions to delete that item.

The rest of this paper is organized as follows. In Section 2, we give the background of hiding sensitive frequent item- sets and formulate our research problem. A border-based approach is proposed in Section 3. Section 4 gives algo- rithms to hide sensitive itemsets. The experiment results are shown in Section 5. Section 6 reviews the related work.

Finally, we conclude this paper in Section 7.

2 Hiding Sensitive Frequent Itemsets  The work in this paper is based on the concept of frequent itemset, which is defined as below. Let I = {i1, . . . , im} be a set of items. An itemset is a subset of I . A transaction T is a pair (tid, X), where tid is a unique identifier of a transaction and X is an itemset. A transac- tion (tid, X) is said to contain an itemset Y iff X ? Y. A database D is a set of transactions. Given a database D, the support of an itemset X , denoted as Supp(X), is the number of transactions in D that contain X . For a given threshold ?, X is said to be ?-frequent if Supp (X) ? ?.

Suppose that L be the complete set of ?-frequent item- sets in D and ?L ? L be the set of frequent itemsets that need to be hidden. In the process of transforming D to D?, we have the following considerations.

1. Any X ? ?L is not a ?-frequent itemset in D?. To ensure that, during the hiding process, for any sen- sitive frequent itemset X , we delete an item x ? X  from a transaction that contains X to the decrease of Supp(X).

2. Suppose that L? be the set of ?-frequent itemsets in D?  and Lr be the set of non-sensitive frequent itemsets de- termined by L and ?L. we try to minimize |Lr ? L?|, i.e., to avoid over-hiding non-sensitive frequent item- sets. Note that according to the Apriori property [1], Lr can be computed by removing each sensitive fre- quent itemset and its supper-itemset from L.

3. We try to maintain the relative frequency of non- sensitive frequent itemsets in Lr. Let us consider the following example. Suppose that we want to hide a set ?L of sensitive itemsets in a given database D un- der the threshold 200. Let abc and bcd be two non- sensitive frequent itemsets with Supp(abc) = 500 and Supp(bcd) = 400. Assume that there are two result databases D? and D?? corresponding to differ- ent hiding processes. In D?, Supp?(abc) = 330 and Supp?(bcd) = 390, while in D??, Supp??(abc) = 400 and Supp??(bcd) = 320. Apparently, the quality of result database D?? is better than D? as the relative fre- quency of itemsets is maintained much better.

To give more intuitive explanation of our hiding prob- lem, we give the following example. A transaction database D is given in Figure 1(a). Let the support threshold ? be 3.

Figure 1(b) shows all ?-frequent itemsets (with their sup- port) in D. Among those frequent itemsets, abd, bcd, and bc are sensitive itemsets (non-sensitive frequent itemsets are shown in Figure 1(c)). The question is how to transform D into the result database D? in a sensible way such that the sensitive frequent itemsets become infrequent in D? and the quality of D? in terms of consideration 2) and 3) is main- tained.

3 A Border-Based Approach  In this section, we propose a border-based approach to address the hiding problem. The key idea is that we use the      border of non-sensitive frequent itemsets (Lr) to track the impact on the result database during the hiding process, and maintain the quality of the result database by selecting the modification with minimal impact at each step.

Figure 2. Lattice and border 1  Figure 3. Lattice and border 2  The concept of border is initially introduced in [5] and it is well applied in the research of maintaining the frequent itemsets (e.g., in [11]). For the completeness of the pre- sentation, we review the concept of border here. Formally, given a set of itemsets U the upper border (respectively, lower border) of U, denoted as Bd+ (U) (respectively, Bd? (U) ), is a subset of U such that 1) Bd+ (U) (respec- tively, Bd? (U)) is an antichain collection of sets1 and 2) ?X ? U, there exists at least one itemset Y ? Bd+ (U) (respectively, Y ? Bd? (U)) holding X ? Y (respec- tively, X ? Y ). An itemset in the upper border or lower border is called a border element. In the running example, we have Bd+ (L) = {abd, acd, bcd, cde} and Bd? (?L) = {abd, bc} . The itemset bc is a border ele- ment of Bd? (?L) . Note that due to the Apriori property, we only need to hide the lower border Bd? (?L). Figure 2 gives a graphic representation of Bd+ (L) in the itemset lattice. Figure 3 shows Bd+ (Lr) in our example (note that the sensitive frequent itemsets are circled).

Now let us examine why the border of Lr is helpful to maintain the quality of the result database, i.e., taking care of the consideration 2) and 3). First, according to the Apriority property, concentrating on the border Bd+ (Lr) during the hiding process is effective in avoiding the over- hiding non-sensitive frequent itemset. Secondly, keeping the relative frequency among border elements is helpful to maintain the relative frequency in Lr. It is because that the  1A collection S of sets is an antichain if for any X, Y ? S, both X ? Y and Y ? X hold.

support of border elements is relatively low and the relative frequency among them is sensitive to the sanitization. In- tuitively, focusing on the most sensitive part of the frequent itemsets can effectively avoid the significant change on the relative frequency. In addition, because of the Apriori prop- erty, the support of the border could also reflect the support of the other frequent itemsets to some degree.

3.1 Hiding One Itemset with Minimal Impact on Border  For brevity, we use border Bd+ and Bd? to denote Bd+ (Lr) and Bd?(?L) in the rest of this paper. In this section, we analyze the problem of hiding a given itemset with a minimal effect on Bd+. Given a frequent itemset X , let ? (X) be the set of transactions that con- tain X . The set C of hiding candidates of itemset X is defined as {(T, x)|T ? ? (X) ? x ? X}. Note that once a hiding candidate (T0, x0) is deleted, i.e., x0 is deleted from transaction T0, the new set of C ? hiding candidate is C ? {(T, x) |T = T0}.

The basic idea of our border-based approach is as fol- lows. Each border element B in Bd+ is assigned a weight, showing its vulnerability of being affected by item dele- tion. The weight of B is dynamically computed based on its current support during the hiding process. When we try to reduce Supp(X) of a sensitive frequent itemset X , for each hiding candidate c, we can calculate its impact on the border as the sum of weights of the border elements that will be affected by deleting c. Each time the candidate item with a minimal impact on the border Bd+ is deleted until Supp(X) drops to ? ? 1.

We examine the weight of a border element first. To take care of the consideration 2) and 3) stated in Section 2, we give the following definition for the weight of a border el- ement. The larger the weight of a border element B is, the more vulnerable B is to further change, therefore, the lower priority of having B affected.

Definition 3.1. Given a database D and a border element B ? Bd+, let Supp (B) be the support of B in D. In ad- dition, let D? be the database during the process of transfor-  mation and ?Supp (B) be the support of B in D? (note that at  the beginning of transformation, D? = D and ?Supp (B) = Supp (B)). The weight of border element B is defined as:  w(B) =  ?? ?  Supp(B)? ?Supp(B)+1 Supp(B)?? ,  ?Supp (B) ? ? + 1 ? + ? ? ?Supp (B), 0 ? ?Supp (B) ? ?  From the above definition, we can see the following points: 1) For a border element B, when the current sup-  port of B, ?Supp (B), is greater than the threshold ?, w(B)  is no more than 1. When ?Supp (B) equals to ?, w(B) is      assigned a large integer ?, where ? > ? > |Bd+| . The intuition behind this is: if the border element B is about to be infrequent, a large value is assigned to w(B), indi- cating low priority of being affected. If B is already over-  hidden ( ?Supp (B) < ?), B should also be avoided for fur- ther change. In that case, w(B) is decided by ? and the amount of ?Supp (B) less than ?. 2) If ?Supp (B) > ? + 1,  with the decrease of ?Supp (B) , w(B) increases under the rate of 1Supp(B)?? . This reflects the consideration of check- ing the risk of destroying the border element and maintain- ing the relative frequency among itemsets on the border. For example, consider the case for two border elements B1 and B2, where Supp (B1) = 30, Supp (B2) = 15 and ? = 10.

When we need to hide a sensitive itemset by affecting the support of B1 and B2, we can see that B1 has higher prior- ity of being affected until its support is down to 26. Then B2 starts to decrease its support by 1, followed by another  4 changes on Supp (B1) if necessary. 3) After ?Supp (B) drops below the threshold, the weight is increased at rate 1  with the decrease of ?Supp (B) (note that this rate is no less than the rate of 1Supp(B)?? ). Considering two border ele- ments with their current support below the threshold, they will have the same increase rate on their weights (i.e., their support difference in original database D is ignored). The reason is that we want to keep the support of every disap- peared border element close to the threshold.

X ? Bd? T1 T2 T3 T4 T5 T6 T7 T8 T9 abd 1 1 1 bc 1 1 1 1  w (Bi) Bi ? Bd+ 1 ab 1 1 1 1  bd 1 1 1 1 1  1 acd 1 1 1 1  ? cde 1 1 1  Figure 4. Demo  In our running example, the weight of each border ele- ment in database D is shown in Figure 4 (with value on the left-most column).

After defining the weight of the border element, we next discuss the impact on the border caused by deleting a hiding candidate. Hiding a frequent itemset may not po- tentially affect all border elements. Apparently, only the border element that has intersection with the sensitive fre- quent itemset may be affected during the hiding process.

Given a sensitive frequent itemset X and a border Bd+, we define the potentially affected border of X , denoted as Bd+|X , as the set of border elements of Bd+, which may potentially be affected by hiding X . Formally, we have Bd+|X = {Bi|Bi ? Bd+ ? Bi ? X = ?}. Clearly, for evaluating the impact of hiding X on Bd+, only Bd+|X needs to be considered.

For a hiding candidate u of sensitive frequent itemset  X , we can determine the set Su of border elements that will be affected by deleting u (note that Su is a subset of Bd+|X ). The impact of deleting u on the border should be the sum of the weights of border elements in Su. For- mally, let Bd+|X be {B1, . . . , Bn} and a lexicographical order can be imposed among B1, . . . , and Bn. Given a hid- ing candidate u of sensitive frequent itemset X, we have a relevance bit vector b1b2 ? ? ? bn such that bi = 1 if u is a hiding candidate of Bi (i.e., deleting u will decrease Supp(Bi)), otherwise bi = 0. The relevance bit vector of u shows which border element Bi will be affected if delet- ing u. In our running example, for sensitive itemset abd, Bd+|abd = {ab, bd, acd, cde} . The relevance bit vector of hiding candidate (T1, a) and (T3, b) are 1010 and 1100 re- spectively.

Definition 3.2. Given a hiding candidate u of a sensitive itemset X , w (Bi) for each Bi ? Bd+|X , and the relevance bit vector b1b2 ? ? ? bn of u, the impact of u on Bd+, denoted as I (u) , is defined as: I (u) =  ? bi ? w (Bi).

The value of I (u) is the sum of the weights of border elements that will be affected by deleting u. In our running example, if we first delete (T1, a) to reduce Supp(abd), the impact I ((T1, a)) is computed as w (ab) + w (acd) = 1 + 1 = 2.

According to Definition 3.2, at each step, we can com- pute the impact for each hiding candidate and select the one with minimal impact to delete. Remember that we have ? > |Bd+|. So, when the border Bd+ is intact, for any change that will break the border (i.e., affect the border ele- ment with weight ?), the impact of it must be greater the impact of the change that will not break Bd+. In other words, our strategy guaranties to select the change that will not break the border as long as such possible change exists.

3.2 The Order of Hiding Itemsets in Bd?  In this part, we discuss the appropriate order of hiding frequent itemsets in Bd?. The reason why we should con- sider the order of itemsets in Bd? is as follows. If there ex- ists two itemsets X, Y ? Bd?, where Bd+|X ? Bd+|Y = ?. Hiding X may change the weight of border element B ? Bd+|X ? Bd+|Y , therefore, affect the process of hid- ing Y . In general, different orders may lead to different results. Let us consider the following example. Suppose that abcd and de are two sensitive itemsets and bcd ? Bd+.

bcd is directly related to hiding abcd and indirectly related to de (because abcd is a super-itemset of bcd but de is not).

If Supp(abcd) and Supp(bcd) are close, hiding abcd may have the risk of over-hiding bcd. Note that, hiding de may also affect the support of bcd. However, this may be re- garded as a side effect on maintaining bcd because they are less correlated. To keep the border element bcd frequent,      we want to maintain that every decrease of its support con- tributes to the hiding of abcd. In this case, we need to con- sider abcd first to avoid any side effect on the vulnerable border element bcd. In general, the longer border element is vulnerable to be over-hidden. In this case, for any two sensitive frequent itemsets, we consider the longer sensi- tive itemset first. For two sensitive itemsets with the same length, we hide the one with less support first for the same reason.

4 Algorithm  In this section, we give the border-based algorithms to efficiently hide sensitive frequent itemsets. Figure 5 shows the main algorithm of hiding sensitive frequent itemsets, which is a summary of the approach we described before.

The key step is to efficiently find a hiding candidate with minimal impact on the border. After selecting a candidate, we need to update the hiding candidate set and the weights of the border elements, respectively. Note that for each sen- sitive frequent itemset, we update the database once after selecting all hiding candidates (to be deleted).

Main Algorithm Input: A database D, the set L of ?-frequent itemset in D and the set of sensitive itemsets ?L Output: D? so that the quality is maintained Method: Compute Bd? and Bd+; Sort itemsets in Bd? in descending order of length and ascending order of support; for each X ? Bd? do  Compute Bd+|X and w (Bj) where Bj ? Bd+|X ; Initialize C (C is the set of hiding candidates of X); for(i = 0; i < Supp(X) ? ? + 1; i + +) do /* Candidate selection algorithm*/  Find ui = (Ti, xi) such that I (ui) = Min {I (u) |u ? C} ; Update C = C ? {(T, x) |T = Ti}; Update w (Bj) where Bj ? Bd+|X ;  Update database D; Output D? = D;  Figure 5. Main algorithm  4.1 Candidate Selection Algorithm  The candidate selection algorithm gives the core of the border-based approach, which is to efficiently find the hid- ing candidate with minimal impact on the border. Remem- ber that at each step, the search space of hiding candidates  is {  (T, x)|T ? ?? (X) ? x ? X }  , where ?? (X) is the set of  transactions in the current database D? that contains X . For a large database and a long sensitive itemset X , it is costly to evaluate the impact of every hiding candidate in C?. In this section, we propose some heuristics to speed up this search.

To efficiently select one hiding candidate with minimal impact on the border, our strategy is to quickly select the  item x first and then decide a transaction T ? ?? (X).

To find an item x that may bring the minimal impact,  we first estimate the possible impact of deleting a hiding candidate with item x. Based on the estimation, we select an item with possible minimal impact on the border.

In general, for a sensitive frequent itemset X, when deleting the hiding candidate with x ? X, we can find some border elements that must be affected and some border ele- ments that could be affected. According to this observation, for any item x ? X, we can use an interval to estimate the possible impact (to the border) of deleting the hiding candi- date with x.

Formally, given a sensitive itemset X , the affected bor- der Bd+|X can be partitioned into direct border and in- direct border, denoted as Bd+|1X and Bd+|2X respec- tively, such that ?Y ? Bd+|1X , Y ? X. For example, given a sensitive itemset abd with its potentially affected border {ab, bd, acd, cde} , {ab, bd} is its direct border and {acd, cde} is its indirect border. Let u = (T, x) be a hiding candidate of a sensitive itemset X . For any direct border element Y ? Bd+|1X and x ? Y , deleting u must decrease the support of border element Y . For an indirect border el- ement Z ? Bd+|2X and x ? Z, the support of Z decreases iff T ? X ? Z.

Given a sensitive frequent itemset X and an item x ? X , we can use an interval i (x) = [Il, Ir] , called impact inter- val, to represent possible range of the impact of changing a hiding candidate with item x. i (x) can be interpreted as: changing a hiding candidate with item x will cause at least Il impact on the border for sure and with the risk of Ir im- pact in the worst case.

At the first iteration, for any x ? X, i (x) .Il = ?  w (Yi) where Yi ? Bd+|1X? x ? Yi and i (x) .Ir =  ? w (Zi)  where Zi ? Bd+ ? x ? Zi. The left bound Il is the sum of the weights of all relevant direct border. The right bound Ir is the sum of the weights of all relevant border element of Bd+.

Having known the impact interval of each item, to show the priority of being changed, we define partial order ? on items based on the following principle: for any two items x1, x2 ? X , if i (x1) .Ir < ?? i (x2) .Ir ? ?, x1 ? x2; on the contrary, if i (x2) .Ir < ? ? i (x1) .Ir ? ?, x2 ? x1; otherwise, x1 ? x2 ? i (x1) .Il < i (x2) .Il or i (x1) .Il = i (x2) .Il ? i (x1) .Ir ? i (x2) .Ir  The intuition behind this ordering is: for any two items, if we know one has risk of damaging the border but the other does not, we will change the no risk one to guarantee that the border is intact. Otherwise we always select the item with less possible impact on the border. Let us consider our running example. The impact interval for item a, b, and d are initialized as [1, 2] ,  [ 2 ,   ] , and  [ 2 , ? +   ] , respec-      tively. We have the order a ? b ? d. In general, based on the order ?, we can select an item x. Now we show how to determine the transaction T.

After finding an item x, we calculate the impact of delet- ing x for each transaction and find the one with the mini- mal impact. To hide a sensitive frequent itemset X, we use a bit map representation to reduce the size of ? (X) . We maintain |? (X)| bit vectors, each of which corresponds to a transaction T ? ? (X). The length of each bit vector is |Bd+|X | . For a bit vector of the transaction T ? ? (X) , b?i = 1 iff T contains Bi, where Bi ? Bd+|X . In Figure 4, we can see bit vectors for T1, T3, T5 are 1111, 1100, 1100.

Given a hiding candidate u = (T, x) of sensitive frequent itemset X and the bit vector b?1, . . . b  ? |Bd+|X | of T, the rel-  evance bit vector b1, . . . b|Bd+|X | of u can be computed as bi = b?i ? (x ? Bi) for i = 1, . . . , and |Bd+|X | . In our ex- ample, Bd+|abd = {ab, bd, acd, cde} and the relevance bit vector of (T1, a) is computed as 1010. For each transaction T ? ? (X), the impact of deleting (T, x) can be computed by the formula given in Definition 3.2. After scanning the bit map once (in a worst case), we can find the hiding can- didate with minimal impact on the border.

Note that once a hiding candidate is selected and deleted, for any affected border element Y , its weight needs to be updated from w (Y ) to w? (Y ). Accordingly, the impact interval i (x) of each item x are also updated to i? (x) .

5 Experiment Results  In this section, we evaluate the effectiveness and the ef- ficiency of our border-based approach by comparing it with a heuristic-based approach, which is referred as Algorithm 2.b in [13]. The heuristic in Algorithm 2.b for selecting a hiding candidate is straightforward. Given a sensitive fre- quent itemset, for all the transactions containing this item- set, Algorithm 2.b first identifies the transaction with the shortest length. In such a transaction, the candidate item with the maximal support value is deleted to decrease the support of the sensitive itemset. This approach hides the fre- quent sensitive itemsets efficiently and meanwhile demon- strates good effectiveness on minimizing the side effect on the result database. In the rest part of this section, we denote it as the heuristic approach.

We evaluate our border-based approach on three syn- thetic datasets, which are created by IBM synthetic data generator [1]. The characteristics of datasets are given in Table 1.

Table 1. Characteristics of datasets Dataset |T | |I| |L| |D| N Size in Megabytes T10I6L1.5K 10 6 1.5K 100K 1K 5.8 T10I6L1K 10 6 1K 100K 1K 5.8 T20I8L2K 20 8 2K 100K 1K 10.44  As our goal is to maintain the quality of the result dataset D?, the quality of the result dataset D? could be measured  as: Q = |L ?|  |Lr| . Apparently, the percentage of over-hidden non-sensitive frequent itemsets is 1 ? Q.

For each given dataset, we evaluate the quality Q of the result dataset under different sets of sensitive frequent item- sets. Now we first define three characteristics of Bd? (the lower border of the set ?L of sensitive frequent itemsets) as below:  1. Number of sensitive itemsets in Bd?, denoted as |Bd?| .

2. Average support difference, formally defined as: Avg Difsup =  ? (Supp(Xi)??+1)  |Bd?| , where Xi ? Bd? and ? is the support threshold.

3. Average length of itemsets in Bd?, defined as: Avg len =  ? len(Xi) |Bd?| , where Xi ? Bd? and len(Xi)  returns the length of Xi.

For example, if Bd? is {a : 10, bc : 8, def : 6} and the support threshold ? is 5, we have |Bd?| = 3, Avg Difsup = 4, and Avg len = 2.

In our experiments, for each dataset, we evaluate the quality Q of result dataset in terms of the size of the lower border (|Bd?|) and the average support difference (Avg Difsup) of Bd?. Figure 6 gives the complete results of effectiveness evaluation by comparing with the heuristic approach. Figure 6(a)?6(c) show the impact of |Bd?| on the quality of the result dataset. Let us take Figure 6(a) as an example. The corresponding experiments are performed on dataset T10I6L1.5K with the percentage of support thresh- old (i.e., ?100K ) always set as 0.6%. We intentionally cre- ate multiple sets of sensitive frequent itemsets such that in each set, the average length is 3 and the average sup- port difference is controlled within the range from 10 to 12. In this case, we can evaluate how |Bd?| impacts the quality Q. This result shows that the quality of the result dataset is well maintained with over 98% of non-sensitive frequent itemsets preserved. In general, the quality of the result dataset decreases with the increase of |Bd?| . In Fig- ure 6(d)?6(f), the impact of Avg Difsup on Q is shown on the condition that |Bd?| and Avg len are constant in each dataset. It is also clear that the quality Q degrades with the rise of Avg Difsup (the reason is that the increase on Avg Difsup requires to delete more hiding candidates, thus, leads to more impact on Q).

Based on the experiment results, we can see that our approach outperforms the heuristic approach in terms of the quality of the result dataset (i.e., protecting more non- sensitive frequent itemsets from being over-hidden). In all these figures, the maximum improvement by the border- based approach is around 5%, i.e., up to an additional 5%      (a) T10I6L1.5K. ? 100K  : 0.6%, Avg len : 3, Avg Difsup : 10?12  (b) T10I6L1K. ? 100K  : 0.8%, Avg len : 3, Avg Difsup : 49?52  (c) T20I8L2K. ? 100K  : 0.8%, Avg len : 5, Avg Difsup : 20?24  (d) T10I6L1.5K. ? 100K  :  0.6%, Avg len : 3, ??Bd??? : 4  (e) T10I6L1K. ? 100K  :  0.8%, Avg len : 3, ??Bd??? : 5  (f) T20I8L2K. ? 100K  :  0.8%, Avg len : 4, ??Bd??? : 5  Figure 6. Effectiveness evaluation  of the non-sensitive frequent items could be oven-hidden by the heuristic approach. Note that |Lr| is often a large number (it is roughly ranged from 1000 to 2000 in our ex- periments). So, little difference in the percentage indicates more significant difference in the actual number. Also, the over-hidden non-sensitive frequent itemsets are close to the border, which often carry more significant information than the itemsets at the lower level of the lattice.

Figure 7 shows the efficiency of the border-based ap- proach on the basis of the dataset T20I8L2K. Particularly, Figure 7(a) and Figure 7(b) depict the performance of hid- ing frequent itemsets in terms of |Bd?| and Avg Difsup respectively. From both figures, we can see that while the heuristic approach takes less time than our border-based ap- proach, their performance curves are very close. This is because the most time-consuming step of hiding sensitive frequent itemsets lies in the dataset scan. Both approaches require the same number of dataset scan, i.e., |Bd?| . Al- though our border-based approach is more complex in the step of selecting hiding candidates, the heuristics introduced in Section 4.1 offers an innovative algorithm which effec- tively reduces the computational cost.

Further, we examine the scalability of our approach. Fig- ure 7(c) shows the response time of hiding one sensitive itemset against the number of transactions in the dataset.

We can see that our approach is linearly scalable.

6 Related Work  Privacy preserving data mining [12] has become a pop- ular research direction recently. The problem of hiding frequent itemsets (or association rules) was firstly studied in [2] by Atallah et al. In this work, finding the optimal sanitization solution to hide sensitive frequent itemsets was proved as a NP-hard problem. Also, a heuristic-based so- lution was proposed to exclude sensitive frequent itemsets by deleting items from the transactions in the database. The subsequent work [4] extended the sanitization of sensitive frequent itemsets to the sanitization of association rules.

The work prevented association rules from being discov- ered by either hiding corresponding frequent itemsets or re- ducing their confidence below the threshold. The work pro- vided some heuristics to select the items to be deleted, with the consideration of minimizing the side effect under the as- sumption that sensitive frequent itemsets were disjoint. The later work [10] further discussed the problem of hiding as- sociation rules by changing items to ?unknown? instead of deleting them.

Also, substantial work [6, 8, 9, 7] has been done in this area by Oliveira and Zaiane. Generally, their work focused on designing a variety of heuristics to minimize the side ef- fect of hiding sensitive frequent itemsets. Particularly, in [6], the Item Grouping Algorithm (IGA) grouped sensitive association rules in clusters of rules sharing the same item- sets. The shared items were removed to reduce the impact      (a) T20I8L2K. ? 100K  : 0.8%, Avg len : 5, Avg Difsup : 20?24  (b) T20I8L2K. ? 100K  :  0.8%, Avg len : 4, ??Bd??? : 5  (c) T20I8L2K (D400K).

?  |D| : 0.8%, ??Bd??? : 1  Figure 7. Efficiency evaluation  on the result database. In [8], a sliding window was ap- plied to scan a group of transactions at a time and sani- tized the sensitive rules presented in such transactions. In recent work [9], they considered the attacks against sensi- tive knowledge and proposed a Downright Sanitizing Al- gorithm (DSA) to hide sensitive rules while blocking infer- ence channels by selectively sanitizing their supersets and subsets at the same time.

In summary, the challenge of hiding sensitive itemsets (or association rules) is to minimize the side effect on the result database. In previous work, a variety of approaches have been proposed based on different heuristics. However, during the hiding process, none of them really evaluates the impact of each modification on the database.

7 Conclusions  In this paper, we have studied the problem of hiding sen- sitive frequent itemsets, with a focus on maintaining the quality of the result database. The originality and con- tributions of our work include the following aspects: 1) We considered the quality not only based on the number of non-sensitive frequent itemsets preserved in the result database, but also in terms of their relative frequency. 2) Most importantly, to minimize the side effect on the result database, we provided the first efforts to evaluate the impact of any modification to the database during the hiding pro- cess. Thus, the quality of database can be well maintained by controlling modifications according to the impact on the result database. 3) We applied the border concept in the context of frequent itemset hiding problem. A border-based approach was proposed to efficiently select the modification with minimal side effect. 4) We studied the performance of the proposed approach and the results were superior to the previous work in effectiveness, at the expense of a small degradation in efficiency.

