Temporal Pattern Mining (Invited Paper)

Abstract?Data analysis has become an integral part in many economic fields. In this paper, we present several real-world applications occurring in the fields of automobile development and manufacturing, finance, and online communities. The given examples share one aspect in common: time. It is not only the fact to find patterns inside data volumes but also to identify them based on their temporal behaviour.

We will give examples of dealing with different models of in- corporating the temporal aspects. Furthermore some new results in the area of Visual Data Analysis are presented. These methods offer intuitive methods of guiding the user through the process of data and model inspection and assist in drawing conclusions with the help of meaningful graphical representations.



I. INTRODUCTION  Automatic data analysis is a crucial part in strategic plan- ning for business organizations around the world. Global com- petition, shorter production cycles and increasing customer requirements put high demands on the responsiveness of every company.

One of the core concepts of visual analytics is to integrate automatic and visual analysis techniques to explore complex unknown data sets. The vital objective is to quickly identify meaningful patterns within the collected data volumes. Such patterns comprise changed customer interests that should be addressed in order to prevent churning. A car manufacturer may want to detect gradually occurring failures in delivered vehicles to prevent expensive callbacks. Financial service providers are highly engaged in detecting fraudulent behavior of customers and co-contractors.

But change is not necessarily related to problems. Change can also mean an opportunity (like an evolving group of customers) to a business. Therefore, (pattern) change detection is a vital task in order to survive or to win.

Many data collected are already time-stamped. Due to its temporal nature business data reflect external influences like management decisions, economic and market trends and thus captures the changes a business is interested in.

All these above-mentioned scenarios share a common prop- erty: the respective patterns are not likely to arise out of a sudden but will rather evolve slowly over time. Conversely, after a problem has been addressed, it would be naive to  expect the patterns disappear immediately. Again, an effect of a counteraction will need time to become visible. Thus, the temporal evolution of such patterns carries valuable?if not vital?information about the urgency of the underlying problem (or the effectiveness of the treatment).

However, a fully automatic approach has its limitations. In order to minimize response times to problems, data analysis results must be interpretable by technical staff that normally has no statistical background. In addition, the analysis should be as transparent as possible to comprehend all inferences and conclusions that were drawn.

To address this, we introduced an interactive visual analysis approach [3], [16], [13] for so-called association rules [1]? one of the most prominent data mining techniques.

The next section will briefly review the announced visu- alization technique. Section III will discuss results from two cooperations with the automotive industry whereas we sketch a project carried out in the financial setting in section IV.

A last success story from the analysis of user behavior in an 3D online environment is told in section V before we conclude the paper in section VI.



II. RULE VISUALIZATION  In order to visualize a single (association) rule, our method of choice is an icon-based representation [3], [18]. This is justified in the fact that usually we have a finite and rather low number of antecedent attributes together with a low number of derived measures. This icon shall contain visual information about the support of the rule (that is the number of covered database cases), the confidence of the rule (indicating the validity or degree of applicability), the consequent attribute value and the antecedent attributes values. Such an icon is depicted in Figure 1: Every rule is represented as a circle the size of which represents the support of the rule.

The interior is solidly filled with a color denoting the consequent attribute value. The saturation of this color is used to indicate the confidence of the rule: full saturation represents 100% confidence (that is a deterministic rule) while white would technically correspond to 0% validity. In some applications, customers asked for another representation: we  Copyright ? 2010 by Institute of Electronics, Silesian University of Technology    Fig. 1. The visualization of a single association rule as it is used in this paper. The outer ring encodes the values of the antecedent attributes whereas the interior represents the class value and the confidence of the rule.

also used pie charts to denote the confidence as we will see later.

The antecedent of the rule is shown in the border of the circle. For every possible attribute (i. e. for every attribute except the class attribute) a unique fragment is reserved.

The fragments are equally sized and are ordered clockwise, starting at the right. Every fragment is filled according the following policy: If the respective attribute is referenced in the antecedent, the corresponding fragment is filled with a color that uniquely represents the value of the attribute?s domain.

This way of representation, of course, is only feasible if the number of attributes and the size of their domains is small.

Otherwise, we simply omit the other antecedent ring.

Now that we have an icon for a single rule, we need to arrange them in a chart. We propose to assign as coordinates the value of association rule evaluation measures [19]. More specific, we recommend the lift value of a rule as its y- coordinate and the recall as its x-coordinate (the confidence as a third interesting measure is represented by the (de-)saturated inner color and the support is represented by the circle area).

Doing so, we are able to encode four numeric dimensions into a two-dimensional image without redundancy. Of course, every other selection of four measures is possible, however, we found the above measures intuitive and will use them in every figure of this work.



III. AUTOMOBILE MANUFACTURING AND DEVELOPMENT  A. Fault Detection  A major cooperation with an automobile manufacturer led to the investigation of a vehicle database containing ap- proximately 300 000 cases that exhibited 180 attributes. The manufacturer kept record of the configuration of every car that left the production plant. Whenever a failure occurred, the database was updated. Every tuple in the dataset represents a unique car that left the production plant of the manufacturer.

Since for every car the time of a failure was logged as well, we were able to partition the full set of cars into data sets of (in this case) equal length. We used a preprocessing technique [13] based on Bayesian networks [12] to induce a set of attributes that should serve as antecedent attributes of the rules to be visualized. It was possible to identify a small set of meaningful attributes (out of the 180) that were used  to generate rules whose temporal trajectories were visualized.

Figure 2 shows a set of 760 rules at three different times.

There are numerous rules that were interesting to experts.

We selected four rules, two showing an evolving problem and two representing a vanishing group of failed cars. To simplify the assessment, we superposed the rule locations of the second and third chart with the first and indicated the motion with arrows. Four rules that showed an interesting behavior and could be assigned a meaning by experts are numbered in the figure: rule 1 and 2 represent shrinking sets of cars whose confidence is also dropping. More interesting, however, is the rapidly lessening lift which gave rise to the conjecture that the cause for the failure had been successfully addressed. Contrary, rules 3 and 4 represent sets of cars with increasing failure rate (confidence is increasing indicated by the darkening of the interior of the icons).

B. Safety Systems  Every millisecond todays cars must decide based on dozens of signals whether (and if so which) components of the restraint systems shall be deployed or not. The airbag e. g.

shall only deploy in severe crash situations, so-called fire crashes. Unfortunately in the first moments of a crash, the signals during a no-fire crash are very similar to those obtained by a fire crash. Matters are complicated further by the crash test costs that usually come up to a car?s price. In addition, dissimilar vehicles behave different in a standardized crash test. The decision logic developed by automobile suppliers must therefore be accurately aligned to every car platform.

Decision rules, e. g. based on fuzzy logic, are formulated by domain experts doing tedious detail work to meet most requirements of a car manufacturer and technical inspectors.

The rule base to be generated must be both highly interpretable and very accurate regarding crash discrimination. to learn such decision logic is still open. Many attempts to automatically induce fuzzy rules from observations (see e. g. [11], [5], [17]) fail since they generate either way too many rules composed of many clauses or too simple rules leading to a high error- proneness.

We try to find an algorithm which is capable to induce a decision logic that is accurate and interpretable. The problem to maintain high accuracy has been already solved using statistical learning methods, e. g. SVMs [6], [9], [10]. Efforts so far to induce interpretable and accurate fuzzy rules from SVMs failed [7].

Note that each algorithm must deal with the preference structure of this binary classification problem. That is, at each point in time which is labeled negatively (i. e. no-fire) must be predicted correctly. Every false positive may lead to severe injuries. On the contrary it is enough to classify one positive instance before a certain time has past. Recognizing a fire crash after this point in time could be fatal. Each signal can be seen as criterion, i. e. an attribute with preference-order domain. Clearly, the higher one of the signals, the more likely it is to fire at that point in time. So, both decision classes (i. e.

fire and no-fire) are preference-ordered as well. We conclude     Fig. 2. Real-world application of a set of vehicles with a binary class variable: failure and no failure. Only rules indicating a failure are depicted.

Two attributes were used to form the rules, hence there are two filled regions in the outer ring of every rule. The three charts shows the rules at the beginning, the middle and the end of the production period. To assess the motion of the rules, we superposed the final locations of the rules with the first image and indicated the corresponding rule with an arrow. Bold arrows indicate the four rules that were found interesting by experts.

Fig. 3. Fuzzy partitioning of attributes: First, the crisp rule thresholds ?i,j are clustered with k-means and k = 6. Then, the cluster centroids are used as intersecting points of neighboring fuzzy sets. The arbitrary widths wi,j of the overlapping fuzzy sets are found by an EA.

that the task to solve is an ordinal classification problem with monotonicity constraints. Due to that we apply the Variable Consistency Dominance-based Rough Set Approach (VC-DRSA) [4]. It enables us to directly incorporate the above mentioned preference structures. Its key idea is to approximate the given data table by dominance relations. These relations induce decision rules of the preferential information in the data. Here, all rules predicting the fire class shall be certain, i. e. they must not cover any negative instance. Certain rules can be written as  Ri : if x1 ? ?i,1 and . . . and xp ? ?i,p then class = 1.

We use the algorithm VC-DomLEM [2] to induce crisp rules of that kind. Note that the obtained rule base only covers positive examples. The negative class is predicted when no positive rule fires.

Every rule is fuzzified by a fuzzy partitioning of all attributes in order to further compress the rule base. The partitioning of every attribute xj with 1 ? j ? p is performed by several heuristic steps. First, the clause thresholds ?ij of all rules Ri with 1 ? i ? r are collected. Second, an arbitrary number of splits kj is determined, e. g. by the user. Note that xj is partitioned into kj + 1 fuzzy sets. Here, suppose that kj = k = 6 for all j ? {1, . . . , p}. Then, we group all elements of {?ij | 1 ? i ? r}, e. g. using k-means clustering. Finally, the cluster centroids c1,j , . . . , ck,j represent the intersecting points of neighboring trapezoidal fuzzy sets ?i,j , ?i+1,j such that  ?i,j(ci,j) = ?i+1,j(ci,j) = 0.5.

The complete process of fuzzification is shown exemplarily for x1 in Figure 3.

The only parameters left for tuning are the intersecting widths wi,j of the overlapping trapezoidal fuzzy sets. We simply apply a steady-state evolutionary algorithm to find good assignments of every width. Each candidate solution is represented by a vector of reals, i. e. all wi,j?s. The following procedure is called for evaluating the fitness of a candidate solution: First, we partition the attributes according to the fixed centroids and the variable widths. Second, we fuzzify every rule?s clause using the fuzzy partition. Finally, the fitness is     given by the specificity of the fuzzy rule-based classifier. That is, the ratio of true negatives divided by the sum of true and false negatives.

We finally obtained a minimal set of certain rules that in average only use 25% of all possible attributes as fuzzy clauses. Whereas the specificity of the crisp classifier is nearly perfect, the one of the fuzzy classifier is significantly lower as expected. The discretization of all attributes, however, increases the readability of the rules. Further improvements can be made using, e. g. multiobjective optimization. We plan to incorporate temporal dependencies of the data points into the VC-DRSA. We expect temporal rules of a more general form to further exploit the problem description. The integration of time series data mining techniques, e. g. motif discovery [8], might additionally boost the performance.



IV. FINANCIAL APPLICATIONS  The ongoing financial crisis has revealed how sensitive and almost unpredictable a system the financial market is. Subtle changes in customer behavior or credit loss rates may be indicators that lead to premonitions upon which one may react in order to narrow down monetary failures or losses.

An international capital company raised a project in which we were to reassess credit contracts from a past trading year. For every credit contract that was still to be repayed the company used an in-house business intelligence system (in connection with human experts) to infer the likelihood of the contract to default. While this predictive system is highly confidential, we were asked to assess the temporal development of consecutive creditworthiness assessments.

Technically speaking, we employed a decision tree induc- tion algorithm to arrive at an intuitive starting position. Since every path of a decision tree can be considered an association rule, we ended up with a rule set for visualization. Since the target variable of the decision tree was chosen to be the (boolean) failure indicator variable, we arrived at association rules with consequents referring to exactly that variable of interest.

Figure 4 shows one of the results. For the sake of brevity we only show the first and last time step which are one fiscal year apart. The confidence was encoded as a pie chart instead of a shaded interior as this was considered more intuitive by the customer. The two arrows in Figure 4 point out three rules that showed a rather rapid increase in both lift and confidence and the respective antecedent attributes were found worth a further investigation (e. g. to use them as predictive features in a future assessment).



V. ONLINE COMMUNITY ANALYSIS  This application stems from a cooperation with a company creating content for the 3D online community SecondLife.1  In order to evaluate whether online content such as buildings mimicking online stores or museums function as intended, a set of virtual sensors is deployed in that environment.

1http://www.secondlife.com/  Fig. 4. Rules regarding credit failure rate at the beginning and the end of one fiscal year. The two arrows mark three rules that showed a strong lift and confidence increase. The respective contracts underwent a deeper investigation.

Whenever a user passes by such a sensor (within a prespecified vicinity) a visit event is logged. We analyzed such a log file for 100 sensors that had been logged for six months. The dataset was then discretized month-wise. A common representation of such a logfile is a so-called cooccurrence graph. The nodes comprise the sensors whereas the edges represent that the sensors connected by that edge have been visited by some set of common users (within a certain timespan). Edge weights are used to denote the cardinality of such sets. The cooccurrence graph of the dataset discussed so far is depicted in Figure 5.

Even though we have proposed methods to deal with tem- poral changes within series of cooccurrence graphs [14], [15], we can extract more information when we induce rules from these graphs and apply our visualization. This is advantageous because of the following reasons: A rule of the form ?If sensor X was visited, sensor Y was also visited in p% of all cases? is more meaningful than just an edge with an associated weight (which corresponds to the absolute support). The above rule depicted with our visualization immediately tells how     Fig. 5. Cooccurrence graph representing six months of visiting events of 100 sensors deployed on certain islands in SecondLife.

many visitors compared to other rules are covered and how that number compares to the general visiting activity of a sensor. Further, a certain rule set will have a fixed visual representation whereas the corresponding graph needs to be layouted which can create different impressions based on the layout algorithms. However, coordinates from any graph representation cannot carry the information of the rule icons.

Figure 6 shows the rules induced from the visiting events of the above-mentioned log file at three different time steps.

Rules have been induced that covered at least 1% of all visits and had a minimum confidence of 1%. Note that this low confidence value is not unusual as we can expect a meaningful lift value since this is the ratio between confidence and consequent probability. Note also, that since a rule represents an edge, the antecedent contains exactly one attribute. Hence, we have omitted the border of the rule icons.

We focus our findings on two subsets of rules. The subset marked by the ellipse in Figure 6 corresponds to a triangle in the original graph.2 All rules show a lift increase to the middle of the time period after which it decreased again. Semantically, this corresponds to an uprise and loss of predictability of the sensors in that subset: a high lift tells that given a visit at one sensor, we can conclude a more likely visit at the sensor described by the rule consequent. Of course, not necessarily by the same user. An inquiry at the company confirmed that a certain modification had been undertaken at the 3D content in the vicinity of these sensors during the middle of the  2As the graph is undirected, it is possible that two rules are induced for a single edge if they match the specified criteria. Hence, a triangle might be represented by up to six different rules.

Fig. 6. Rules induced from the cooccurrence graph in Figure 5 at three different time steps. The two rule sets that are marked by the ellipse and rectangle were identified to correspond to modified artifacts inside the 3D world: The rule set marked by the ellipse showed a collective lift increase towards the middle of the logging period and diminished after. The rules marked by the rectangle exhibit a lift decrease and support increase.

log period. The rules marked by the rectangle in Figure 6 correspond to a single edge that was heavily visited as can be seen from the rule icon size. The lift is decreasing, which is not surprising in the underlying 3D setting. The two sensors were placed along a frequented pathway that more and more users were accepting to use. If the general usage increases (and thus the consequent probability) the lift will decrease, as we observe here.



VI. CONCLUSION  Data collection is a process in time. Hence, it is almost always possible to attach to a database entry a time stamp.

We have shown that real-world applications can greatly benefit when the time dimension is taken into account.

This article has demonstrated how complex data analysis tasks can be greatly simplified by appropriate visualization and temporal change mining methods. We therefore encourage to focus research onto the augmentation of other soft computing techniques in this respect.

