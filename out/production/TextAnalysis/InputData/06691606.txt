Top-K aggregation over a Large Graph Using

Abstract?Analyzing large graphs is crucial to a variety of application domains, like personalized recommendations in social networks, search engines, communication networks, computa- tional biology, etc. In these domains, there is a need to process aggregation queries over large graphs. Existing approaches for aggregation are not suitable for large graphs, as they involve multi-way relational joins over gigantic tables or repeated mul- tiplications of large matrices. In this paper, we consider top-K aggregation queries that involve identifying top-K nodes with highest aggregate values over their h-hop neighbors. We propose algorithms for processing such queries over large graphs in a shared nothing environment. Using the notion of graph parti- tioning, we propose an update-based algorithm that minimizes network overhead by propagating updates in the neighborhood information.The algorithm partitions a graph across a number of processing nodes, and uses an iterative join algorithm within each node. We present a hybrid scheme to further reduce the network overhead during a few initial iterations. We develop a baseline algorithm based on distributed joins. Our experimental results validate the effectiveness of the proposed algorithms in reducing  the aggregation time and in scaling the aggregation computation over a number of distributed hosts.



I. INTRODUCTION  Analyzing large graph data has seen renewed importance due to increased interests in a number of applications such as chemical data, biological data, XML data, social network data, communication networks, etc. In each of these applications, the underlying graphs are very big in size. There is current trends in advanced analysis of social network graphs aiming at evaluating the network values of customers [5], link predic- tion [15], etc. In large graph analysis tasks all the vertices and edges of the entire graph are accessed multiple times in a random fashion. Examples include Page-rank [2], social network influence analysis [13], recommender systems [1], etc.

In social and biological network graphs, each node is often associated with an attribute set. The value of each attribute for a node represents some features of the entities represented by the node. For example, a node representing a Facebook user may have an attribute indicating his/her interest in a particular online game. Within the applications domains, there is a growing need to process standard queries efficiently over large graphs. For example, for each node find the aggregate value of an attribute for all its neighbors lying within h-hops in the graph. Such a query could identify the popularity of a game within one?s social circle. Also, such queries could help  ?The research work was partly done while the author was working as a post-doctoral researcher at IBM T.J. Watson Research Center, NY, USA.

finding the influential nodes (based on some attributes) in the social network and help in placing online advertisements.

The issue of processing aggregate queries over graphs has been addressed in [24]. However, this approach assumes a pre-computed data structure storing the h-hop neighbors of each graph. The algorithms only save computations in calculating the aggregate score of each node by pruning nodes selectively. We observe that computing h-hop neighbors is a crucial operation in the graph aggregation, and the aggregate scores can be computed or materialized during neighborhood discovery; therefore, a separate phase for computing aggregate scores from the h-hop neighborhood information for each node is redundant. Moreover, the proposed algorithms work only in a single node, and does not consider the issue of distributing aggregate processing loads over a shared nothing system.

Parallelizing or distributing processing loads of the transitive closure algorithm (i.e., Floyd-Warshall Algorithm [6]) over a shared nothing system is infeasible due to a large num- ber of synchronization barriers. Reference [22] improves the Floyd-Warshall algorithm by proposing a blocking algorithm to reduce random memory accesses and to increase cache utilization. Reference [12] extends the Floyd-Warshall algo- rithm in a GPU-based system using a block-based algorithm; the algorithm attempts to increase the instruction execution throughput using the GPU. However, the approaches do not show significant performance gains even in the multi-threaded, shared-memory system.

In this paper, we consider the issue of parallelizing the ag- gregation queries over a shared-nothing system. Neighborhood aggregation queries are computed in three phases (section II): h-hop neighbor computation, aggregate score evaluation and top-k collection. Computing h-hop neighbors for all nodes in the graph is a computationally expensive operation. If the average degree of each node in the graph is m, the total number of edges to be accessed while computing h-hop neighborhood for a node would be around mh|V |. In applications with a large graph (with millions of nodes) such a computational cost is prohibitive. Though h-hop neighborhood within a graph can be found using Floyd-Warshall algorithm [6], such an algorithm is difficult to parallelize even within a single node due to a large number of synchronization barriers. Crecelius and Schenkel [4] propose algorithms to incrementally maintain an index structure?that stores the nearest neighbors for all nodes, i.e., all-pairs shortest path?with changes in the graph.

However, the paper does not present efficient, parallel tech- niques for the initial computation of the index structure.

In this paper, we propose distributed algorithms to process      h-hop neighborhood information of a graph within a shared- nothing system. We divide the graph into a number of parti- tions and assign a partition to each of the host machines. We propose a join-based algorithm that computes h-hop neighbor- hood by applying distributed joins in an iterative fashion. Such an algorithm needs to shuffle a large volume of data across the host machines and also requires a large number iterations for a large h-value. Considering the limitations, we propose an update-based algorithm, that allows each host machine com- pute h-hop neighborhood locally. The algorithm propagates updates across the machines in an iterative fashion until the system converges (i.e., no host has new updates to propagate to other hosts). At an inter-partition level, the algorithm uses a technique similar to that with route discovery in a large network. The algorithm proceeds in phases, and at the end of a phase, each host propagates updates (observed within its partition) to the relevant hosts. The update-based algorithm incurs high communication volume during a few initial phases.

Considering the limitation, we propose a hybrid algorithm that, instead of sending h-hop neighborhood information of each node across the hosts, transfers the graph partitions during initial/start-up phases; the receiving hosts regenerates the h- hop neighbors for each graph node from the received partition.



II. PRELIMINARIES  In this section, we present a formal definition of aggre- gation queries over graphs, and discuss graph partitioning concepts and various data structures used in the paper.

A. Aggregation over Graphs  The aggregation queries over graphs are relevant to a number of emerging applications in online social communities such as book recommendation on Amazon, target marketing on Facebook, etc. A top-k aggregation query needs to solve three issues [24]:  1) Compute the h-hop neighbors of the all the nodes in the graph.

2) Evaluate the individual relevance function f(u) and the aggregate score or collective strength of a node F (u). f(u) could be binary value 0/1 (e.g., if a user plays a game or not). F (u) is an aggregate function (e.g., SUM, AVG, etc.) over the h-hop neighbors of a node u.

3) Find and collect the top-k nodes with the highest scores.

In a graph G(V,E), SUM aggregation of h-hop neighbors of a node v (i.e., aggregate score) can be given as,  F (v) = ?  u?Sh(v)  f(u)  Here, Sh(v) is the h-hop neighbors of node v.

B. Partitioning and Data Structures  Given a graph G = (V,E), a graph partitioning algo- rithm divides the node-set V into disjoint subsets of nodes {V1, V2, . . . , Vn} such that ?iVi = V , and Vi ? Vj = ? for i ?= j. A partition Pi of the graph denotes the subgraph  J  PP  P  P3    A  B E  G  F  C  D  L N  O  M  H  I K  Fig. 1. Entry nodes in a partition are marked with green color  Gi(Vi, Ei) induced by nodes in Vi; Ei is the set of intra- partition edges (u, v) ? E, such that u ? Vi and v ? Vi.

The (edge) cut-set Ec is the set of edges whose vertices lie in different partitions. Inter-partition cut-set Eci,j is the set of cut edges incident from partition Pi to Pj . E  c i is  the set of all cut edges emerging from partition Pi, i.e., Eci = ?(1?j?n)?(j ?=i)E  c i,j . We use the term out-table to denote  the inter-partition cut-set Eci , and refer to E c i,j as a fragment  of the out-table.

An adjacency representation, AL, of a graph consists of a number of lists, one for each of the nodes in the graph. We refer to such nodes of the lists as source nodes. We denote the set of source nodes in an Adjacency structure AL as V (AL).

An adjacency list for a source node u, denoted as AL(u) is the set of nodes that are descendants of node u is the graph. For a partition or graph Gi(Vi, Ei) with Ni nodes, the adjacency structure contains Ni lists. For each descendant nodes v, the list keeps the distance (hop-count) to the node v from the source node u. So, each entry in AL(u) is a pair (v, d), that represents a shortest path from node u to node v with a distance d. The adjacency list provides a few functions. It provides a method DISTANCE(v) to denote the distance of the node v from u as stored in AL(u). We assume that the operation gives ? when v is not reachable from u. Also, the adjacency list AL(u) provides an operation INSERT(v, d?) that replaces the entry (v, d) in AL(u) with (v, d?) if d? is less than d; the INSERT operation adds the entry (v, d?) in the list AL(u) when there is no such entry in the list. The list AL(u) provides another operation EXIST(v) that checks for the membership of v within the list, and returns TRUE if node v exists in the list AL(u), and FALSE otherwise.

For each partition Pi of the graph, we identify the Entry nodes in a partition Pj(j ?= i)?denoted as ENi,j?which are the end vertices of the inter-partition cut-set (edges in a segment of an out-table) Eci,j incident to the partition Pj .

Figure 1 shows a graph with three partitions; nodes marked with green color are the entry nodes within a partition. For the partition P1, the edges marked as blue are the (cut-)edges in the out-table Ec1; the two segments (of the out-table E1) Ec1,2 and E  c 1,3 have two edges and one edge, respectively.

P3  P0  P1  P2  H2H0  H1  H  Fig. 2. Distributed join processing among partitioned adjacency lists

III. JOIN-BASED ALGORITHM  The top-k aggregation query we consider in this paper is processed in two phases: neighborhood discovery and top-k computation. In the first phase, every node u in the graph discovers its h-hop neighbors and computes the relevance function F (u) of the node. Discovering the h-hop neighbor- hood for all nodes in the graph is a (h ? 1)-way self-join operation. We present an algorithm based on iterative joins and use it as a baseline algorithm. The join operations should be carried out over the partitioned graph in a distributed fashion.

The relevance function of the nodes can be computed either incrementally during the neighbor discovery process or off-line after discovering all the h-hop neighbors of every nodes in the graph. In the second phase, the top-k computation process identifies the top-k nodes among all the nodes distributed within the networked host machines. In the subsequent part of this section, we describe the two phases of the baseline algorithm in details. As the relevance function, we consider only a count operation; however, supporting other relevance functions, like sum, average, etc., is straightforward.

We present the baseline algorithm to process top-k aggre- gation query within a distributed system with multiple host machines connected through a network. Such a distributed join framework is similar to cyclo-join [7]. In the subsequent part of this section, we discuss various components of the algorithm to process top-k aggregation based on multi-way joins. We start with a simple horizontal graph decomposition scheme, and then present the distributed join processing technique that is the core operation of the top-k aggregation. The top-k collection scheme (i.e., the second phase of the aggregation operation) is similar to the one presented in section VI.

A. Horizontal Partitioning  We distribute the graph among the set of host machines in the system using a simple decomposition of the graph that partitions the vertices among the host machines such that each host owns all the outgoing edges of a node assigned to it. Such a decomposition can be regarded as a row-wise (horizontal) decomposition of the adjacency matrix representing the graph, and we refer to such a decomposition as a horizontal decom- position of the graph. While distributing the nodes across the  Algorithm 1 DISTRIBUTEDJOIN(ALi, AL R i , h)  Require: An adjacency structure ALi in host Hi, a parameter h specifying the depth/levels of joins  Ensure: ALRi contains the result of h-way joins among ALi and all n partition  1. ALRi ? ALi 2. for k ? 1 to h? 1 do 3. Bsnd ? ALi 4. for iter ? 1 to n? 1 do 5. NOBLOCKSEND((i + 1)%n, Bsnd) 6. ALRi ? ADJJOIN(AL  R i , Bsnd)  7. Brcv ? BLOCKRECEIVE((i ? 1 + n)%n) 8. Bsnd ? Brcv 9. if iter = n? 1 then  10. ALRi ? ADJJOIN(AL R i , Brcv)  11. Barrier()  hosts, we balance the number of edges assigned to the hosts by considering the out-degree of the nodes. Such a distribution eliminates skews in workloads among the hosts, as the loads of a join operation depends on the size of the table (i.e., total edges). Uniformly distributing the nodes results in a skew in total edges within the hosts, as nodes with higher out-degrees might be assigned to a host.

Algorithm 2 ADJJOIN(ALa, ALb, h)  Ensure: An adjacency structure ALR with the results of the join between ALa and ALb  Definitions: V (ALi): set of source nodes for adjacency structure ALi  1. ALR ? ALa 2. for ur ? V (ALR) do 3. for ub ? V (ALb) do 4. if ALR(ur).EXIST(ub) then 5. for (v, db) ? ALb(ub) do 6. dr ? ALR(ur).DISTANCE(ub) 7. if dr + db ? h and v ?= ur then 8. ALR(ub).INSERT(v, dr + db) 9. return ALR  B. Distributed Joins  Using the horizontal partitioning scheme, we distribute the nodes of the graph into multiple partitions, assign a partition to each of the hosts, and maintain an adjacency representation ALi for the partition at a host Hi. To process joins over the distributed adjacency lists, we arrange the hosts in a circular ring as shown in Figure 2. Each of the hosts communicates only with two neighboring hosts, and data is sent over the links only in one direction. A host Hi receives partition data from its left neighbor (i? 1+ n)%n and sends the data to its right neighbor (i+1)%n, where n is the total hosts (or graph parti- tions) in the system. Each host Hi maintains a result adjacency structure ALRi , initialized to ALi, to store the join results.

After receiving a partition from its left neighbor, the host machine joins the received adjacency structure with its local result ALRi using an adjacency-join algorithm (Algorithm 2)      2H1  1 2 3 pmap  :  1 2 3  32 1  1 2 3  32 1 1 2 3  32 1  H1  H2 P3 P1H3  H2 H3  pmap :  pmap : pmap  :  pmap  pmap 1:  2: 3 1 1 2 3 1 2  P  (a)  B  1 H2 H3  N  C F  1,2 A 3,1  3,2 N  JH  JH  EN EN EN   2 3  2,3 EN  EN  3,2  3,1 F  BEN  EN2,1  2,3 N  N  EN  EN1,3EN  1,3EN  EN EN  A  C  H  (b)  Fig. 3. pmap and EN lists before and after all-to-all communication  described later in this subsection. After the join operation, the host forwards the received partition to its right neighbor. Once a partition in Hi is propagated all way through the ring to its left neighbor host H(i?1+n)%n, the respective partition get joined with all the n partitions stored in n hosts, and this completes a cycle of the propagation process. After such a cycle, each host stores the output data ALRi , which is the result of joining its local partition with all the partitions of the graph.

To process h-way joins, we need to carry out the h?1 cycles of the join operations. Algorithm 1 shows the process in details.

The algorithm uses two buffers Bsnd and Brcv to pipeline the join processing with data communication by using non- blocking send and receive primitives. The algorithm has h? 1 cycles (loop in Line 2), and in each cycle, a host Hi receives n ? 1 partitions from its left neighbor and forwards n ? 1 partitions to its right neighbor. Line 8 copies the received data to Bsnd to overlap data transfer (receive) with computation in the subsequent iteration. Line 10 joins the received data in the last iteration to complete the join cycle.

Algorithm 2 presents the details of the ADJJOIN method.

Line 4 checks if a node ub is reachable from ur in the adjacency structure ALR. If reachable, the method updates the list ALR(ur) by inserting into it all the nodes in list ALb(ub) that are within a distance h from the node ur(line 5?8). Line 7 checks the distance limit (first part in the expression) and ensures that addition of the node ub does not form any loop in ALR (the second part in the expression).



IV. UPDATE-BASED ALGORITHM  The iterative, join-based approach to process h-hop ag- gregation over a distributed system requires (h ? 1) cycles; in each of the cycles, the algorithm computes joins over the whole graph. Such an approach is costly both in terms of computation and communication. To efficiently process h-hop neighborhood, we develop an incremental approach based on update propagation, that converges to equilibrium after only  Algorithm 3 INCRDISCOVERY(ALRi , ENi, h)  Require: An adjacency structure ALRi storing the h-hop neighborhood information in host Hi, entry-node infor- mation ENi for the partition in the host Hi, a parameter h specifying the depth/levels of joins  Ensure: ALRi contains, for all the local nodes within the partition in the host Hi, the global h-hop neighbors across all the partitions.

Definitions: Eci,j : out-tables listing the intra-partition cut-edges from host Hi to host Hj ULi, UL  new i , ULsnd, ULrcv: Adjacency structure for  updates rcvH, sndH : host number to receive updates from and send updates to, resp.

1. cycle? 0 2. while flag do 3. for u ? ?j ?=iENi,j do 4. ULi(u)? ALRi (u) 5. rcvH ? (i? 1 + n)%n 6. sndH ? (i+ 1)%n 7. while sndH ?= i do 8. for u ? ENi,sndH do 9. ULsnd ? ULi(u)  10. EXCHANGEUPDATES(sndH, rcvH,ULsnd, ULrcv)  11. PROCESSUPDATES(cycle, rcvH , ULrcv, h, AL R i ,  ULnew) 12. rcvH ? (rcvH ? 1 + n)%n 13. sndH ? (sndH + 1)%n 14. CLEANUP(ULi) 15. ULi ? ULnew 16. CLEANUP(ULnew) 17. cycle? cycle+ 1 18. flag? CHECKCONVERGENCE()  a few cycles. This approach partitions the graph across the processing nodes. The algorithm proceeds in two phases: local- compute phase and incremental discovery phase. During the local-compute phase, each node applies an algorithm to com- pute h-hop neighbors locally over its own partition. During an incremental discover phase, each partition (or node), sends the updates to relevant partitions. A separate preprocessing phase partitions the graph and stores the intra-partition out-tables. We describe the preprocessing and neighborhood processing phase (local-compute and incremental phase) in the subsequent part of this section.

A. Graph Preprocessing  We partition the graph into n partitions using a graph parti- tioning algorithm that minimizes the number of cut-edges. Our proposed algorithms are oblivious to graph partitioning tech- niques, and hence can use any graph partitioning algorithms.

(However, as we illustrate later in the paper, the performance of our algorithms depends on the graph partitioner.) We store two files for each partition: an intra-partition file and an inter- partition file. An intra-partition file stores the edges within a partition; an inter-partition file for partition Pi maintains     all cut-edges emanating from partition Pi, grouping the cut- edges based on their destination partitions. Each host stores the mapping from host ID to partition number. During the initialization phase, each host performs an all-to-all broadcast to distribute its local partition number to all other nodes. After the all-to-all shuffling, each host gets the same global mapping table pmap, where pmap[j] gives the host ID that stores the partition Pj .

A host Hi reads the intra-partition file for the partition assigned to it and creates an adjacency structure ALi. The host then reads the inter-partition file, and using the mapping array pmap, groups the cut edges in the out-table Eci according to host ID and maintains one fragment (in the out-table) for each of the hosts. An out-table fragment Eci,j contains the set of inter-partition cut-edges emanating from the partition in host Hi and incident to the partition in Host Hj .

Now, we need to determine the entry nodes for each partition in a host Hj . From Figure 1, we observe that the entry nodes for the partition in host Hj can be obtained from out-table Eci,j , which is located in the host Hi. So, each host Hi should compute the list of entry nodes ENi,j for all other hosts Hj (j ?= i). Now, nodes in the list ENi,j are the entry- nodes within host Hj for the cut-edges incident from partition in host Hi. Hence, host Hi should send the information to host Hj . Therefore, after computing the entry-nodes, each host participates in an all-to-all communication phase to shuffle the entry-node information across the hosts. Considering the graph and its partitions shown in Figure 1, Figure 3(a) and Figure 3(b) show, respectively, the lists pmapi and ENi within each host before and after the all-to-all shuffling of the information about partition mappings and entry nodes within each host.

B. Local-Compute Phase  In a local-compute phase, each host computes the h-hop neighbors for the nodes within the partition assigned to the host. We use multi-way joins to compute the h-hop neighbors.

At a first glance, the Floyd-Warshall Algorithm to calculate transitive closures appears to be the better candidate from the perspective of computational complexity. However, we find that given a particular graph and h-values within a limit ( e.g., h ? 35), an iterative multi-way join performs better that the Floyd-Warshall algorithm while computing h-hop neighbors.

So, each host applies the ADJJOIN algorithm iteratively to compute (h ? 1)-way self-joins over the local adjacency list ALi, and stores the results in an adjacency structure AL  R i .

C. Incremental Discovery Phase  The Incremental discovery phase consists of multiple cycles. In each cycle, the hosts participate in an all-to-all propagation operation to shuffle, among the hosts, the local updates in the adjacency lists. Updates within each host Hi are stored in an adjacency structure called Update List ULi, and this ULi stores the updates in neighborhood information: before the onset of the incremental discovery phase, each host Hi initializes ULi with with adjacency lists in AL  R i , and each  host creates a new update list ULnewi during each cycle. The new update list ULnewi records all the updates (v  ?, d?), incurred within a cycle, in the adjacency list ALRi (u) for each node  Algorithm 4 PROCESSUPDATES(cycle, rcvH , ULrcv, h, ALRi , AL  inv i , ULnew)  Require: An adjacency structure ALRi storing the h-hop neighborhood information in host Hi, AL  inv i storing the  inverted adjacency lists, ULrcv containing the updates from host with ID rcvH , a parameter h specifying hop- count for aggregation  Ensure: Append to ULnew any changes in h-hop neighbors within ALRi ? if it is the first cycle  1. if cycle = 0 then 2. for ?uo, vo, do? ? Eci,rcvH do 3. ALstage(uo).INSERT(vo, do)  ? Stage the updates from the remote host 4. for ur ? V (ULrcv) do 5. for ?uo, vo, do? ? Eci,rcvH do 6. if ur = vo then 7. for ?vr, dr? ? ULrcv(ur) do 8. if do + dr ? h and uo ?= vr then 9. ALstage(uo).INSERT(vr, do + dr)  ? Propagate the updates 10. for us ? V (ALstage) do 11. for ?vs, ds? ? ALstage(us) do 12. for ?v?, d?? ? ALinvi (us) do 13. if d? + ds ? h and v? ?= vs then 14. ALstage(v  ?).INSERT(vs, d ? + ds)  ? Incorporate updates within result structures 15. for us ? V (ALstage) do 16. for ?vs, ds? ? ALstage(us) do 17. ALRi (us).INSERT(vs, ds) 18. if ds < h and us ? ?jENi,j then 19. ULnew(us).INSERT(vs, ds)  u ? V (ULi); (v?, d?) is regarded as an update to the adjacency list ALRi (u) if (a) there is no entry in the list for node v  ?, or (b) (v?, d?) replaces an old entry (v?, d) and d? < d. Note that V (ULi) is the set of all entry nodes in ENi (i.e., ?j ?=iENi,j), and is also the set of source nodes of the adjacency structure ULi.

Algorithm 3 shows the pseudo-code for incremental dis- covery operations. Each iteration of the outer loop (line 2? 18) represents a cycle. Within a cycle, each host propagates updates in its adjacency lists to all other hosts through an all- to-all propagate phase (line 7?13). For the all-to-all propagate phase, we use a ring-based algorithm as proposed in [3].

We arrange the hosts in a ring; in each iteration within a cycle (line 7?13), each host receives updates from a host HrcvH and sends its updates to host HsndH . Each host Hi collects, from ULi, the updates for the entry nodes ENi,sndH and creates the adjacency structure ULsnd (line 8?9). The Procedure EXCHANGEUPDATES sends the updates in ULsnd to the host HsndH , and stores in ULrcv updates received from the host HrcvH (line 10). The procedure marshals the adjacency structures over the network link at the sending end, and reconstructs the structure at the receiving end. To facilitate the overlapping of computation with communication,     the procedure employs a non-blocking send and a blocking receive. Also, while transferring the updates across the hosts, the procedure uses a bounded buffer to prohibit buffer overflow in case the aggregate updates across all the partitions become very large. Algorithm 5 shows the details of the procedure.

Algorithm 5 EXCHANGEUPDATES(sndH , rcvH , ULsnd, ULrcv)  Require: ULrcv containing the updates to be sent to sndH , rcvH to receive updates from  Ensure: ULrcv contains the update entries/tuples received from host with ID rcvH  Definitions: Sbuf : (send or receive) buffer size Bs, Br: send and receive buffer, resp.

1. scount? 0 2. for us ? V (ULsnd) do 3. Bs[count]? ?us, NAD? 4. for ?vs, ds? ? ULsnd(us) do 5. Bs ? ?vs, ds? 6. scount? count+ 1 7. if scount = Sbuf ? 2 then 8. send scount to host HsndH 9. send (non-blocking) Bs to host HsndH  10. if rF lag = True then 11. receive rcount from host HrcvH 12. if rcount > 0 then 13. receive update tuples from HsndH and store  in Br 14. INSERT(ULrcv, Br, rcount) 15. else 16. rF lag ? False 17. wait for non-blocking send 18. scount? 0  19. send scount to host HsndH 20. send (non-blocking) Bs to host HsndH 21. if scount > 0 then 22. send (non-blocking) 0 to host HsndH  23. if rF lag = True then 24. repeat 25. receive rcount from host HrcvH 26. if rcount > 0 then 27. receive contents from HsndH and store in Br 28. INSERT(ULrcv, Br, rcount) 29. until rcount ?= 0  30. wait for non-blocking send  After receiving updates from a remote host, the receiving host Hi invokes a procedure PROCESSUPDATES (described later in this subsection) that incorporates within its adjacency structure ALRi the newly published neighbor information ULrcv from the remote host HrcvH . The procedure also appends to ULnew any updates to the adjacency lists of entry nodes within the partition. After the all-to-all shuffle, each host Hi cleans up the current update list Ui and initializes it with the recent update list ULnew (line 15), and use the ULi to propagate the updates in the subsequent cycle. Line 16  garbage collects the list ULnew destroying all the updates in the adjacency structure. Line 18 checks for the convergence or termination of the incremental phase. The incremental phase converges when no host in the system has new updates to send.

During the incremental phase, once the system converges, all the nodes of the graph have discovered all of its neighbors lying within the proper distance (h). Is should be noted that the maximum number of cycles in the incremental discovery phase is bounded by a parameter pmax. Suppose p indicates the number of partitions spanned by a shortest path (with the distance ? h) between two nodes in the graph. The parameter pmax is the maximum p over all pairs of nodes within the graph. Although there are pmax total iterations before the convergence, total volume of updates shuffled across the nodes will decrease drastically after the first few iterations. Devising a graph partitioner to minimize the the total iterations in the aggregation algorithms outside the scope of the paper, and is a topic of future research.

Algorithm 4 presents the details of the procedure PROCES- SUPDATES used in the incremental discovery phase. Instead of directly updating the result structure ALRi , the procedure first store and manipulate the tentative updates within an intermediate storage ALstage, and then propagates the updates to the adjacent nodes (within the partition) in a separate phase; the procedure then incorporates the finalized updates within the result structure ALRi . During the fist cycle of the incremental discovery phase, the procedure scans the edges of the out- table, and adds the destination nodes, with the proper distance do(= 1), as the updates to the source nodes (Lines 1?3).

Lines 4?9 join the adjacency structure ULrcv, that contains updates received from the remote host, with the respective segment Ei,rcvH in the out-table Ei, and stores the result in an staged adjacency list ALstage. This join operation propagates the updates published by the entry nodes (of the remote host) to the source nodes of the out-table segment. The loop in line 7 scans the updates published by an entry node ur. Line 8 checks whether the destination node of an update lies within a distance h from the source node uo in the out-table segment; the second part of the expression eliminates loop in the adjacency list of the source node uo. Line 9 inserts the update tuple into the adjacency list of the source node uo. Note that, the update entry ?vr, do + dr? is added to the adjacency list of uo if and only if there is no entry with the destination node vr or the new distance metric in the update entry is smaller than the distance metric of an existing entry for the destination node vr.

After the join operation (in Lines 4?9), the intermediate (staged) adjacency list ALstage contains a list of update entries for the respective source nodes. Lines 10?14 propagate the updates to the other adjacent nodes within the partition.

The propagation process uses an inverted adjacency structure ALinvi , where each list AL  inv i (u) for a source node u stores  every node within the partition that reaches the node u through a shortest path of distance less than h. Once the updates are propagated to the nodes within the partition, the new updates for each node in the partition are added to the result adjacency structure ALRi (Line 15?19).

Algorithm 6 INNETWORKTOPK(FListi, k)  Require: An list FListi storing the ?node,F-value? pairs for all the nodes within the partition in Host Hi  Ensure: compute local top-K and propagate in the network; the host H0 stores the global top-k  Definitions: Loc: local top-k ?node, F ? value? pairs Left: top-k pairs received from left sub-tree Right: top-k pairs received from right sub-tree i: host ID n: total hosts in the network  1. Loctopk ? COMPUTETOPK(FListi, k) 2. if (2 ? i+ 1 < n) then 3. receive topk pairs from H2?i+1 and store in Left 4. Loc? COMPUTETOPK(Loc ? Left, k) 5. if (2 ? i+ 2 < n) then 6. receive topk pairs from H2?i+2 and store in Right 7. Loc? COMPUTETOPK(Loc ?Right, k) 8. if i ?= 0 then 9. send Loc to host Hi/2

V. A HYBRID APPROACH  In the update-based algorithm, during a few initial cycles, the total volume of updates generated by each host could be very large. Shuffling the raw updates across the hosts increase the communication loads. During the first few cycles, the total update volume within each host might be a few order of magnitude larger than its partition size. In such a situation, sending the graph partition instead of the update lists might reduce the communication loads on the network.

With such a partition shipment, the receiving host iteratively applies, as in the local compute phase (of the update-based algorithm) in Subsection IV-B, the ADJJOIN algorithm to compute the (h ? 1)-hop neighbors of each node within the partition received from the host HrcvH . The receiving host Hi uses the entry-node list of the host HrcvH to regenerate the update list ULrcv, and processes the incoming updates using the PROCESSUPDATES algorithm as described in Algorithm 4.

The hybrid algorithm works in two modes: update- shipment mode and partition-shipment mode. Initially the algo- rithm is set in partition-shipment mode, and the INCRDISCOV- ERY algorithm (Algorithm 3) shuffles partitions (in line 10), instead of update logs, across the hosts. The receiving host regenerates the (h?1)-hop neighbors of each node and creates the update lists as described above. At the end of each cycle, the hosts participate in all-to-all communication to exchange the size of the update entries ULnew generated within the hosts. The hosts switch to update-shipment mode if the size of the generated updates within each of the hosts falls below a predefined threshold. Once in update-shipment mode, the incremental discovery phase proceeds on the way as given in the update-based algorithm (Algorithm 3).



VI. TOP-K COLLECTION  Once the neighborhood discovery phase is finished, each node computes top-k results locally. To compute the global top-K over all the nodes, we use a tree-based collection  TABLE I. SYNTHETIC GRAPHS  Name |V | |E| Cut Description  A 22.5K 44.7K 972 2D Grid (n=200,m=200)  B 10K 19.8K 620 2D Grid (n=100,m=100)  C 20K 160K 38114 Small World Model  (n=20K,k=8,p=0.2)  D 20K 80K 44000 Erdos Renyi Model  (n=20K,m=80K)  E 20K 160K 6295 Small World Model  (n=20K,k=8,p=0.01)  F 40K 40.3K 4231 Power-Law graph  (n=40K,p=3)  TABLE II. REAL-WORLD GRAPHS  Name |V | |E| Cut Description  S 18772 198050 61248 Collaboration Graph  T 23133 93499 20554 Collaboration Graph  U 34546 420877 82583 Citation graph  V 27770 352285 95215 Citation graph  W 26518 65369 27170 Peer-to-peer network  mechanism, where all the nodes are organized in a binary tree structure. Each node receives top-K values from its two children and computes top-K nodes locally combining its own local top-K with the data received from the children. The node then sends the final local top-K to its parent. The global top-K over the whole graph can be found at the root node. The code for in-network processing of top-K is given in Algorithm 6.



VII. EXPERIMENTAL RESULTS  In this section, we evaluate our implementation of the Top- k aggregation system. All experiments are carried out in a 20-node compute cluster. Each compute node has a quad-core AMD Opteron system with 4GB of RAM, runs 64-bit Red Hat Enterprise Linux, and is connected via 10 Gigabit Ethernet.

We implement the three algorithms in C++. We contrast the performance of various approaches by collecting processing (CPU) time, communication time and total time to process aggregation queries over a wide range of synthetic and real- world graphs, as listed in Table I and Table II, respectively. We generate the synthetic graphs using the SNAP graph generation software [20], and take the real-world graphs from SNAP datasets [20], [14]. All synthetic graphs are undirected graphs; among the real-world graphs, S and T are undirected, and U , V , and W are directed graphs. Unless stated otherwise, the default number of partitions or hosts is taken as 12, and the default value for h is set to 10. We set the value of k to 200 nodes. Table I and Table II show the 12-way cut for the graphs considered in the experiments. We partition the graph using an off-the-shelf partitioning algorithm, METIS [11].

In Figure 4a, Figure 4b and Figure 4c, we plot the average CPU time, average communication time (across the host ma- chines) and total time for different graphs. Each of the graphs shows the comparative data for three algorithms?update- based, join-based and hybrid. The average communication time across the host machines (Figure 4b) for the hybrid algorithm is minimum among the three algorithms, however this hybrid algorithm incurs higher CPU time (Figure 4a) than the update- based algorithm. Among the three algorithms, the join-based algorithm usually requires the highest CPU time for the given hop-length of 12. As shown in Figure 4c, except for graphD, the total time to complete the aggregation query is the lowest        1e+06  1e+07  1e+08 T  im e  (m se  c)  Graph  graphA graphB  graphC graphD  graphE graphF  update-based join-based  hybrid      1e+06  1e+07  1e+08  T im  e (m  se c)  Graph  graphA graphB  graphC graphD  graphE graphF  update-based join-based  hybrid     1e+06  1e+07  1e+08  T im  e (m  se c)  Graph  graphA graphB  graphC graphD  graphE graphF  update-based join-based  hybrid  (a) Average CPU time (b) Average Communication time (c) Total time  Fig. 4. Synthetic graphs (a) Average CPU time within the participating hosts (b) Average communication time across the host machines, and (c) total time to finish the aggregation query for the different graphs (x-axis is plotted on a log-scale)    1e+06  1e+07  1e+08  T im  e (m  se c)  Graph  graphS graphT graphU graphV graphW  update-based join-based  hybrid    1e+06  1e+07  1e+08 T  im e  (m se  c)  Graph  graphS graphT graphU graphV graphW  update-based join-based  hybrid    1e+06  1e+07  1e+08  T im  e (m  se c)  Graph  graphS graphT graphU graphV graphW  update-based join-based  hybrid  (a) Average CPU time (b) Average Communication time (c) Total time  Fig. 5. Real-world graphs (a) Average CPU time within the participating hosts (b) Average communication time across the host machines, and (c) total time to finish the aggregation query for the different graphs (log-scale along the x-axis)  in case of the update-based algorithm. For the graphD, update- based algorithm does not perform well due to a large fraction of (12-way) cut edges in the partitions of the graph. For such a graph, the communication volume dominates the total time to finish the query, and hence the hybrid algorithm fares well for such a graph.

Figure 5a, Figure 5b and Figure 5c show the CPU time, the communication time and the total time for different real-world graphs. As shown in Figure 5c, in all the real-worlds graphs, the total time to process the aggregation query is minimal for the update-based algorithm. For some graphs, the completion time for the update-based algorithm is two order of magnitude lower than that with the join-based algorithm (please note the logarithmic scale along the x-axis).

A. Varying h-values  Figure 6 shows the total time to compute top-k nodes (in graph E) with varying h-values. As the hop-count increases, the time to compute the h-hop neighbors also increases; hence, the total time to compute top-k nodes in the graph also increases. As observed in the Figure 6, the total time for the update-based algorithm is up to two order of magnitude lower than that in the algorithm based on distributed joins (the join- based algorithm); please note that the plot uses a logarithmic scale long the x-axis.

B. Varying host machines  Figure 7 and Figure 8 present the scalability results for the top-k computation (in Graph A) within a shared-nothing    1e+06  1e+07  1e+08  4  6  8  10  12  14  16  18  20  T ot  al ti  m e  (m se  c)   h-value  Update-based Join-based  Hybrid  Fig. 6. Total time to compute top-k nodes with varying h-values: total time increases linearly with h-value (note the log-scale along the x-axis)  system. As shown in Figure 7, the total time to compute top- k nodes decreases linearly with the increase in the number of machines (note the log-scale along the x-axis). Within the range of host population, the delay for the update-based algorithm is an order of magnitude smaller than the join- based algorithm (the figure uses a log-scale along the x- axis). Figure 8 shows the total iterations with varying host population. As we increase the number of hosts (partitions), total number of iterations also increases. As the number of partitions (or hosts) increases, a shortest path of length h might be split across a larger number of partitions, which leads to a higher pmax value (Section IV-B) for the system.

1e+06  1e+07  8  10  12  14  16  18  20  T ot  al ti  m e  (m se  c)   Total hosts  Update-based Join-based  Hybrid  Fig. 7. Total time to compute top-K nodes with varying hosts: the proposed algorithms scales with the number of hosts and achieves an order of magnitude speed-up, while compared to the base-line algorithm (log-scale on the x-axis)           8  10  12  14  16  18  20  T ot  al it  er at  io ns  Total hosts  Update-based Join-based  Fig. 8. Total cycles/iterations with varying hosts: with the base-line algorithm, the total number of iterations is fixed at h; with the update-based algorithm, it varies with the number of hosts (or partitions)

VIII. RELATED WORK  There is an increasing trend in processing graphs within large-scale, distributed systems, resulting in a number of commercial and open-source systems being developed, for ex- ample, Pegasus [9], Pregel [17], GraphLab [16] etc. A number of techniques to manage partitions across distributed systems have been proposed [18], [25], that aim at minimizing inter- machine communications while supporting simple queries that access or manipulate a set of nodes. These techniques dynam- ically maintain the partitions of graphs with changes in query workloads. The key idea is to replicate a set of nodes across the machines based on cross-partition query loads, query hot- spots, and read/write patterns of the queries. Contrary to their partitioning scenario, the type of queries we consider are both compute and communication intensive in nature, and they need to access the whole graph. PowerGraph [8] considers graphs with a power-law degree distribution and proposes a graph partitioning or distribution abstraction exploiting the structure of the power-law graphs. The authors observe that a balanced p-way edge-cut technique to distribute nodes across the machines does not perform well with a power-law graph.

Our work is orthogonal to graph partitioning or distribution framework, and our proposed algorithms can be incorpo- rated within any graph partitioning or distribution abstraction.

Supporting compute- and communication-intensive algorithms (e.g., aggregation over h-hop neighborhood) with a coarse-  grained model of computation is the main theme of our work.

Crecelius and Schenkel [4] presents an algorithm to dy- namically maintain nearest neighbor lists for all the nodes in a graph under updates to the graph (i.e., node insertions and decrease in edge weights). Their algorithm is relevant to a centralized or single-machine system, and it does not consider the initial computation of the index structure. Though such a computation of the index structure is an one time and off-line operation, for a large graph such a computation requires enormous processing time; thus, an efficient approach to create the index structure storing the all-pairs shortest paths is necessary even within a single-machine system. We consider the problem of parallelizing, across a set of distributed machines, the process of computing all-pairs shortest paths and top-k aggregation over h-hop neighborhood within a large graph.

There is a body of work on graph aggregation and summa- rization. Tian et al. [21] develop a graph summarization oper- ation, SNAP, that groups graph nodes based on user-specified node attributes and relationships. The users can drill-down and roll-up through the summaries using the node attributes and relationships, that captures the hierarchy information of the summaries. Navlakha et al. [19] apply a Minimum Description Length (MDL) principle to generate coarse-level representation of a input graph. The compressed graph representation consists of two parts: a graph summary and a set of corrections. Wu et al. [23] present graph aggregation or clustering techniques based on multi-level geodesic approximation. These papers address the aggregation issue that is related to cluster formation or hierarchical summarization over graphs, which is different from the neighborhood aggregation problem addressed in this paper.

Yan et al. [24] present techniques to prune nodes of a graph while computing the top-K nodes with the highest aggregate scores. The paper does not consider the issue of parallelizing the neighborhood aggregation process across mul- tiple machines. Also, the paper assumes that the neighborhood information is already computed, and takes the neighborhood information of the nodes as input. We maintain that computing the neighborhood information, a compute-intensive operation, is the crux of the aggregation problem, and present algorithms to parallelize the neighborhood computation and top-K aggre- gation across a distributed system.



IX. CONCLUSION  In this paper, we present algorithms to process aggrega- tion in graphs exploiting computational resources within a number of networked machines. The update-based algorithm incrementally propagates the updates across the host machines.

Such an incremental propagation reduces network loads and total iterations while compared to a baseline algorithm based on distributed joins. The algorithm reduces synchronization barriers by allowing each node to proceed with the local computation independent of the others. We propose a hybrid algorithm that minimizes communication volumes (during a few initial iterations) for a graph with a large number of cut edges. Our experimental results show the update-based algorithm achieves an oder of magnitude speed-up while computing top-k aggregation over a wide variety of graphs.

There exists a number of open issues that we plan to investigate further. First, developing a partitioning algorithm that minimizes the parameter pmax (maximum number of partitions spanned by the shortest paths in the graph) is an open research issue. Such an algorithm might decrease total iterations before convergence, reducing the synchronization overheads in a system with a large number of processing nodes.

Second, in a distributed environment with a large number of nodes, allowing synchronization barriers at the onset of each iteration results in high overheads due to unbalanced computation within nodes or noises within computing nodes or networks. We plan to devise a lazy protocol for propa- gating updates eliminating barrier synchronization. Third, we are working towards developing incremental algorithms to dynamically maintain the top-k nodes or h-hop neighborhood information with changes in the graph. We have plan to incorporate the distributed graph processing techniques within the framework of a workflow provenance tool, Karma [10].



X. ACKNOWLEDGMENTS  The author thanks the reviewers for their helpful comments.

The author is grateful to Dilma Da Silva for her comments and feedback on the paper. This work has been funded in part by National Science Foundation under grant ACI 1148359, and the Pervasive Technology Institute, Indiana University.

