eCAD: Cloud Anomalies Detection

Abstract?Recent years have witness the booming of the cloud computing, which provides customers with guaranteed services.

Since any violation would inevitably lead to a degraded quality of service (QoS), anomalies detection has become a demanding task in cloud environments. To solve the above problems, the unsupervised clustering approaches were put forward for identifying those anomalies. However, they all failed to work out the anomalies detection from an evolutionary view. In this paper, we present a cloud anomalies detection framework called eCAD.

Motivated by the evolutionary clustering, our eCAD employs an evolutionary algorithm with DBSCAN to detect cloud anomalies as time steps. Besides, we also propose an M-Nearest Neighbors (MNN) algorithm to conduct the inference for those induced anomalies. Our eCAD is evaluated on the top of VICCI platform, which is a federated cloud test-bed in IaaS level. As demonstrated in our experiment, our framework shows an advantage over its counterparts.

Index Terms?Anomalies, Cloud Computing, QoS, SLO, VICCI Platform, Evolutionary Clustering.



I.  INTRODUCTION In recent years, cloud computing is regarded as an  emerging network computing model, aiming to provide users with services of computing resources. While providing basic resources and services, cloud computing has been devoted to guarantee the quality of service (QoS) [1]. With the benefit of cloud computing model, customers would not need to be concerned on their service-level, which in turn poses a challenge for those cloud providers. Although many researches, such as performance prediction, load balancing, and anomaly detection, have been set off in the field of the cloud QoS guarantee, it is still difficult for providers to address the default of the promised SLO (Service Level Object) [2]. The SLO is a contract negotiated between the cloud provider and its customers for guaranteeing the QoS in cloud. Any violation related to SLO would inevitably bring about a huge loss for both parties involved. Therefore, we need to preserve those underlying anomalies so as to take action in time.

As NIST defines, cloud computing comprises three levels, namely IaaS, PaaS and SaaS [3]. However, different levels may expose various demands their QoS. To some extent, the service quality of IaaS cloud always determines that of the upper levels.

Consequently, we mainly focus on the anomalies detection in  IaaS cloud, where the virtualization technology plays an important role.

In terms of the virtualized infrastructure, the performance of the cloud environment might be challenged by many aspects, such the vulnerability in VMM (Virtual Machine Monitor), rootkit, and DOS (Deny of Services) [4]-[6]. Since the lower infrastructure in cloud is shared through virtualization technology, anomalies as well as bottlenecks of the virtualized resources might happen frequently [7]. As one of the most demanding work, anomaly detection focuses on preserving those virtualized resources, which might lead to a violation on the SLO. Similar to our work, previous researches undertook the clustering approach for identifying outliers in cloud, such as the DBSCAN algorithm [8]. However, they neglect taking the transitions of those anomalies and its surrounding neighbors into consideration.

In this paper, we present a framework named eCAD (Cloud Anomalies Detection) from an evolutionary view for detecting the anomalies in the level of the cloud infrastructure. Our system is deployed on the VICCI cloud platform, which is a global wide cloud test-bed established by Princeton University.

Many institutes as well as enterprises utilize it to achieve an IaaS cloud similar to Amazon EC2 or OpenStack. Compared with the traditional clustering methods, our eCAD can not only identify evolutionary exceptions, but also help to make anomalies inference.

Our system?s main contribution includes: ? Our eCAD has the ability to find the outliers in cloud  environment based on the evolutionary clustering algorithm with DBSCAN.

? Besides, we also propose an MNN (M-Nearest Neighbors) method to infer the reasons resulting in the anomalies.

? Our system is deployed on the global cloud test-bed, and focuses on the anomalies caused in the HADOOP programs.

The rest of our paper is organized as follows. Section II discusses some related works on cloud anomalies detection.

The motivation and description of our eCAD framework are described in Section III. Section IV examines the results of our eCAD on the basis of the VICCI cloud platform. Section V presents the conclusion and introduces some future work.

DOI 10.1109/CLOUDCOM-ASIA.2013.57    DOI 10.1109/CLOUDCOM-ASIA.2013.57

II. RELATED WORKS As mentioned by Tan et al., the approaches in the field of  detecting anomalies can be divided into following two aspects, namely the pre-active and re-active ones. The former one predicts the anomalies and then makes preparation before it happens, while the latter one perceives the outliers at running time. The representatives of the above two approaches would be described as follows.

A. Pre-active Methods  Researchers in MSRA put forward a multivariate linear regression model named ARMR [9], which could track the virtualized resources changing over the time. However, this approach tends to model a single resource and ignore the latent relation with each other.

Tan et al. [10][11] described the transitions of the states for virtualized resources using a FSM (finite state machine). After that, they take advantages of the decision tree model to separate the virtualized resources apart from different states. Since their work defined the states as normal, anomaly and alert ones, it was arduous for them to label those samples. At the same time, they also utilize the Triple-States-Markov-Chain to predict the potential anomalies in advance.

Besides, Qiang et al. [12][13] proposed an ensemble of Bayesian and decision tree classification models to detect the anomalies in the virtualized infrastructure. Their researches illustrated the metrics that need to be supervised for virtualized resources in cloud.

Generally speaking, pre-active methods build the detection models based on the probability distribution of the previous state information. It is often time-consuming for them to label between the normal and anomaly ones. Thus, they tend to be in a lack of the generalization for the new comers. Therefore, our paper would mainly focus on those re-active algorithms.

B. Re-active Methods  For the re-active methods, the most commonly used one is to monitor those resources based on the pre-defined thresholds or even the SLO. Once a violation was detected, the alert information would be pushed to the scheduler centers. A majority of the current monitoring tools are taking advantages  of it, including Ganglia, Nagios, Nimsolf and so on. Although such method is relatively straightforward, it might lay a hinder for specifying those thresholds, which must be improved.

Dependable Computing Systems Laboratory (DCS Lab) in the University of North Texas has carried out a lot of work on the re-active methods [14][15]. In the beginning of the learning process, there are only normal nodes existing in the cloud, while the anomalies would come later. To avoid the lack of sufficient labels, Song et al. took a cascading model of one- SVM and SVM to solve this problem through iterations.

However, it usually took too much time to build such a cascading model and the boundaries between them are often indistinct. Meanwhile, SVM might come across the over-fit problems in front of the high dimension datasets.

Since the clustering method has the ability to identify the noise in the dataset, Qiang et al. [8] came up with a model on the basis of the density-based clustering algorithm, which is similar to our work. Prior to detection, they exploited the PCA (Principal Component Analysis) technology to extract the significant features of the monitored attributes. Compared with our methods, it failed to take the evolution of the anomalies into consideration.

Daniel et al. [16] presented the self-organizing map (SOM) model to work out the outlier detection problems. The virtualized resources would be defined as neurons of the SOM and then the outliers could be caught according to the relations between them. However, it showed a poor perform with the accumulation of the monitored metrics.

In addition, methods derived from the field of information theory also played an important role. The EBAT frameworks proposed by Wang et al. [17] employed the ?local entropy? as well as the ?global entropy? to quantify the changing of resources. Although it could be used in different levels, this method had a lower precision rate on the whole.

In sum, re-active methods can build models without the aid of previous labeled data. It contributes to the anomalies detection in the real time, especially for the cold start period.

However, most of current approaches employ some static strategies, neglecting the evolution for those anomalies.

Moreover, few of them pay attention to the anomalies inference.

In this paper, we will discuss the above two problems.

??? ??????????? ?  Fig.1.  Motivation for cloud anomalies detection between: (a) static clustering, and (b) evolutionary clustering.



III.THE FRAMEWORK OVERVIEW OF ECAD  In this section, we will describe the motivation of our eCAD and overview the framework of our system.

A. Motivation In practice, we all know that virtualized resources of the  same kind tend to have similar values on their QoS metrics. As an unsupervised method, clustering can automatically divide the samples into different categories based on the distance, density and similarity between them. It has been widely used in the field of anomalies detection [8]. Enlightened by this, we would take clustering as our basic approach. Compared with the traditional classification approaches, the clustering algorithm would not only identify the outliers with less prior knowledge, but also resist to the change of the cluster numbers.

Here, we use a simple example to illustrate our motivation.

Fig.1(a) presents a solution for anomalies detection problem with DBSCAN (Density-Based Spatial Clustering of Applications with Noise), which is one of the density-based clustering algorithms. For it is only capable of recognizing the outliers separately at each monitoring period, we define it as a static clustering baseline algorithm in our paper.

However, as a result of the accumulated monitored data, the lower clustered resources would evolve over time. To this end, we need to develop an evolutionary method to trace those transitions of virtualized resources, as shown in Fig.1(b). We name it as the evolutionary clustering algorithm.

B. Framework As mentioned by Ferretti et al [18], our eCAD functioning  as a middleware should comprise three main components: monitoring composite, detection composite, and inference composite. Our eCAD is deployed in the middle of the cloud infrastructure and remote users, which can be seen in the Fig.2.

To emphasize, we only focus on those anomalies that occur in virtual machines of the service level.

? Physical level and service level are two abstract plains in the IaaS cloud. The basic physical devices are deployed in the lower layer of cloud, while the VMs (virtual machines) provided to users outside the cloud lies in the service layer.

? Monitoring composite collects the basic information, such as the CPU and Memory usage, from the virtual nodes in the cloud. Some of them might be running in the kernel level, and some of them in the user level.

? Detection composite is in charge of finding the anomaly nodes based on the information collected by the monitoring composite. The key algorithm used in this module is derived from the DBSCAN clustering, which would be discussed in the following sections.

? After the above two stages, inference composite sets off to explore the reasons for those anomaly nodes detected earlier.

In general, the real-time performance information generated in the physical and service level is intercepted by the monitoring composite. Then the detection composite use the evolutionary clustering algorithm proposed to perceive the anomalies among the VMs in the service layer, in order to  avoid the SLO negotiated between the counterparts. At last, the inference composite utilize the algorithm based on the M- Neighbors to present the reasons that might lead to violation.



IV. EVOLUTIONARY CLUSTERING FOR ANOMALIES DETECTION  In this section, we will first overview the framework of the current evolutionary clustering. After that, we propose an evolutionary approach with DBSCAN for cloud anomalies detection. Our method is considered to have a strong resistance to the noise and capable of finding anomalies in the cluster with arbitrary shapes. Besides, the algorithm for inferring the anomalies will be presented in the end.

A. Overview The evolutionary framework of our research is primarily  based on the following temporal smoothness penalty theory, which was put forward by Chakrabarti [19]. Its main idea aims to add a constraint cost function to the traditional static clustering algorithms.

Here, we assume that ??  and 	?  are respectively the evolutionary and static clustering results of virtualized nodes in cloud at time t. Given a time step t, the object of evolutionary clustering is to find the optimal ??  that minimizes the cost function as follows.

?????? ? ????? ?? ? ????????	?? ? ?? ? ?? ? ??????? ????? ?????????  where the snapshot function snap() denotes the current cost between evolutionary and statistic clustering results. Thus, the temporal function temp() denotes the historical cost between time step t and t-1. Theoretically, we could use any continuous k steps before time t to depict the temporal cost in temp(), instead of only two steps. Moreover, as mentioned before, we could explain the above cost function either into the PQM framework or into the PCM framework [20]-[21].

Fig.2.  eCAD framework under the cloud environment     As a weight between the snapshot and temporal cost, the parameter alpha (!) in Equation (1) can be set according to the status of the cloud environment. In particular, the evolutionary algorithms are regarded as an incremental clustering, when !

equals to 0. Meanwhile, it turns out to be a static clustering, when ! equals to 1. In addition to the linear combination above, we might also use another non-linear weight to demonstrate the balance of cost between snap() and temp().

In our context under the density-based evolutionary clustering with DBSCAN, we should redefine the snapshot and temporal function to illustrate the anomalies detection from an evolutionary view.

B. Evolutionary DBSCAN DBSCAN, which is short for the density-based spatial  clustering of applications with noise, has become one of the classical algorithms for anomalies detection. It can not only identify the abnormal virtualized nodes, but also automatically distinguish those nodes into different clusters.

Core nodes, boundary nodes and anomaly nodes are three basic components in the DBSCAN. For a given virtualized node, it would become a core node, only if the number of its neighbors exceeds an upper threshold. On the contrary, those virtualized nodes within a limited distance are defined as the boundary nodes, which are mainly determined by the core nodes. Besides, the remaining nodes would become anomaly nodes, which usually take a minority of the dataset. Core nodes and their boundary nodes that could be directed within a limited distance are eventually divided into the same class.

Anomaly nodes would not belong to any class and thus should be regarded as outliers in the virtualized layer. As mentioned before, several researches have taken advantage of this method to accomplish the anomalies detection in cloud. However, they failed to take the transitions of those anomalies and its surrounding neighbors into consideration.

From the analysis above, we could infer that the cores nodes are determinant factors in the DBSCAN clustering.

According to it, we should trace the variation of the core nodes to precisely character the evolution for different clusters.

Nevertheless, we all know that the core nodes are determined by two main parameters, EPS and MINPTS, which are regularly settled on the basis of data density in DBSCAN clustering. To fully trace the variation, we could use the neighbor vector to represent the core nodes, which is defined as follows.

(Definition): The element of the neighbor vector represents the number of neighbors within the EPS for each virtualized node, that would be a core node only if its value is large than MINPTS.

We formally define the vector 	?"""""#? as the number of neighbors for each virtualized node at time t. Meanwhile, the vector ??"""# should be the number of neighbors for evolutionary clustering. It can be described as follows.

?"""""# ? $??????%? & ???'(??  ??"""# ? $ ??? ?%? & ? ?'(??                                         (2) where ??)  and ?)  represent the statistic number and evolutionary number of neighbors for virtualized node i at time  t. Here, we assume that the node number would not vary with time in our cloud environment.

Given a neighbor vector, we can quantify the temporal smoothness cost function into the following one,   ??*??"""#+ ? ?????""""# ,? ? -?* ?) ? ??)+  % ? ?? ? ?? ? -?* ?) ? ???) + %.? / ?????0?  In such occasions, our goal is to find the optimal solution of above function for the evolutionary DBSCAN. Meanwhile, all the evolutionary approaches discussed in our context are derived under the PCM framework as mentioned before.

Based on the above observation, we could easily know that the value of ??"""#  is optimal only when the first-order derivative of the object function equals to zero. It can be seen as follows in the Equation (4),  ??"""#1234?????567 ? ? ? 	?"""""# ? ?? ? ?? ? ?????""""""""#?/?????????????????????????????8?  In light of this, we can use the above neighbor vector to find the outliers in cloud at each time step. The details of our evolutionary clustering are described as follows.

C. Further Discussion The evolutionary clustering with DBSCAN above is  presented under an ideal assumption that the number of virtualized nodes would not vary with the time.  Since the virtualized nodes are not always stable in the cloud, those anomaly nodes should be migrated or killed at a certain period.

There, we need to discuss the movement of such nodes.

Moreover, with the variation of cluster nodes, the configuration  Algorithm 1: Evolutionary DBSCAN  Input: set of virtulized noedes VNSet, ????, 	?, ? EPS, and MINPTS Output: anomalies at time t  1 foreach 9) : ;<=?  do 2  calculate evolutionary neighbor vector ??"""# at time t,  ??"""# ? ? ? 	?"""""# ? ?? ? ?? ? ?????""""""""#?/?? 3 end 4 foreach  ?) : ?""# ? $??? ?>? & ? ??(   do 5 if  ?)> MINPTS  then 6  label the nodes 9) as core node 7 end 8 end 9 foreach  ?) : ?""# ? $??? ?>? & ? ??(   do  10 if  ?)< MINPTS  and  distance(vi)<EPS then 11  label the nodes 9) as boundary node 12 else 13 label the nodes 9) as anomaly node 14 end 15 end       of parameters, such as the EPS and MINPTS, is also needed to be tuned, which is beyond the scope of our paper.

In terms of the removal and entrance of the cluster nodes, our evolutionary framework can easily deal with these conditions. To begin with, we could neglect the neighbor vectors of those removed anomaly nodes in the subsequent intervals, which can be done by deleting a row in the vector??"""#.

At the same time, we could initial the neighbor vectors of these new comer nodes as zeros, which can be done by adding a row to the vector ???""""# . Since the derived neighbor vectors for the evolutionary clustering is a linear combination of the snapshot cost and temporal cost, the removal and entrance of the virtualized nodes at any time step would naturally not affect the evolution process.

D. Anomaly Inference In order to make adjustments in time, our eCAD also needs  to undertake the anomaly inference afterwards. Since the anomalies in our framework are mainly determined by the MINPTS nodes within the EPS, we suggest using the distance or similarity of the nearest MINPTS nodes to infer between different attributes, which is defined as the M-Nearest Neighbors (MNN) algorithm for inference in our paper.

Besides, we only focus on the categories of those anomalies, rather than their specific attributes. For example, the attributes contained in the memory-oriented category might include memory usage, virtual memory usage and so on. The pseudo- code is described as follows.



V. IMPLEMENTATION AND EXPERIMENT In this section, we first introduce the basic environment and  metrics in our context. Then we present the experiment evaluation results of our eCAD on the VICCI based test-bed under three different scenarios. Compared with the traditional static clustering for anomalies detection, our eCAD can performs much better.

A. Environment The prototype of our eCAD is established on the top the  VICCI, which is promoted by researchers from the PlanetLab  programs. As a kind of federated cloud, the VICCI platform consists of distributed cluster servers in the Princeton University, Stanford University, Washington University and other seven institutions. Similar to the other IaaS clouds, it can provide the VMs for users outside the cloud.

In our experiment, we use the HADOOP WORDCOUNT AND SORTED programs as our benchmark. All of the scenarios are hence running under the HADOOP framework.

In order to obtain the monitored information, we also install the VMSTAT tool in each VM. The detailed representative symbol of each metric could be found in the Table I.

TABLE I.  MONITORED METRICS  Categories Symbols  CPU-oriented us / sy / id / wa  Memory-oriented swpd / free/ buff / cache  IO-oriented bi / bo / si / so  Process-oriented b / r / in / cs  B. Simulated scenario In our experiment, we simulate three different scenarios  with injected CPU errors to evaluate our eCAD. It can be seen as follows.

(Scenario 1) NoHead: The eCAD was deployed on the 180 virtual nodes, where the HADOOP WORDCOUNT benchmark ran. Under such a situation, the virtual nodes were in single HADOOP mode and independent with each other.

Besides, all the clusters as well as the anomalies are labeled based on the injected CPU errors.

(Scenario 2) Head: The eCAD was deployed on the 50 virtual nodes with distributed HADOOP mode. Different from the above NoHead Scenario, the SORTED benchmark was adopted in this situation.

(Scenario 3) HeadSLO: This scenario took on the same configuration as Head Scenario. Nevertheless, we labeled the anomalies on account of the OUTPUT/INPUT rate in the service level, so as to investigate the effects of anomalies on the SLO.

C. Index To evaluate our approaches, we employ three indexes,  including Precision, Recall and F-Score, which are the popular evaluation approaches in field of machine learning. The details of each index are introduced as follows. In the following equations, ??@AB/ CDEDFEDG? and ??HDI/ CDEDFEDG?  respectively indicate the number of normal nodes and anomalies detected. ?       J? ?KK ? ??L??/??M?? ?N???L??/ M?? ?N? ? ??L??/??O?N?? ?N? ?P?  LQ? ????? ? ??L??/ M?? ?N???L??/ M?? ?N? ? ??<?R/ M?? ?N? ?S?  Algorithm 2: Anomalies inference  Input: set of attributes AttriSet, set of anomalies AnoSet, EPS, and MINPTS Output: Categories of anomalies 1 foreach 9 : T??=?  do 2  foreach  ?) : TQ?=?  do 3   calculate distance N) ? - 19 ??9)?1 between 9 and other MINPTS nodes 9) within EPS on ?) 4  end 5  calculate weight U) ? N) -N)V  on attribute ?), 6  output the category of the ?) with ??W??U) 7 end          D. Results Before conducting our evaluation, we need to select the  parameters in our evolutionary algorithm. According to distance trend of the nearest K-neighbors, we theoretically set (EPS=10, MINPTS=3) in scenario 1 and (EPS=10, MINPTS=4) in the other two scenarios.

Since each alpha indicates a different strategy, Fig.3 presents the impact of alpha on our eCAD. As mentioned before, our evolutionary approach would be a static clustering when the parameter alpha is equal to 1, while it would become an incremental algorithm when the alpha is 0. As seen from the Fig.3, the static clustering showed the poorest performance in cloud anomalies detection. When the alpha equals to 0.1, the eCAD can obtain the highest precision in our experiment scenarios.

In terms of the anomalies detection with evolutionary time steps, we can find that our eCAD framework exceeds its counterparts in Fig.4. At the beginning of our simulated experiments, the eCAD with evolutionary approaches takes an  advantage. When the CPU errors are injected, it starts to fall down and returns to a stable status later on. Under that condition where the alpha equals to 0.25, the eCAD achieves its highest performance.

To emphasize it, the static strategy always stayed in an unstable drift, fluctuating up and down with time stepping.

Besides, the over-all results shown in the scenario 2 and scenario 3 resembles with each other. To some extent, we justify that the upper SLO values can reflect statuses of the lower virtual resources, and could also help to label those anomalies.

After detection, we utilize the proposed MNN algorithm to infer the reason that leads to anomalies. In our experiment, we regarded the CPU errors as our incentives. The inference results can be found in Table II.

TABLE II.  ANOMALIES INFERENCE  Scenarios  # Anomalies # Inferred Precision  NoHead 47 33 0.721  Head 57 39 0.6842  HeadSLO 30 7 0.333   From the table above, we can notice that our eCAD shows  a poor inference under the HeadSLO scenario. In such an  X = ?Q? ? > Y LQ? ????? Y J? ?KKLQ? ????? ? J? ?KK ???????????? ?Z?  ? ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????F?? ?  Fig.4.  The performance of eCAD under the: (a) NoHead, (b) Head and (c) HeadSLO scenarios.

? ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????F?? ?  Fig.3.  The impact of alpha values on eCAD under the: (a) NoHead, (b) Head and (c) HeadSLO scenarios.

occasion, the labels were determined based on the upper SLO.

Thus, other attributes might interfere with its precision. We would improve it in the future works.



VI. CONCLUSION In this paper, we proposed a framework named eCAD  from an evolutionary view to identify anomalies in the IaaS cloud. To illustrate our framework, we established a VICCI cloud equipped with HADOOP benchmarks. Our experiment reveals that the evolutionary clustering method can identify cloud anomalies more precisely than its counterparts, especially for the traditional static clustering approach.

Moreover, our framework can also infer the reasons for the above anomalies.

As immediate future work, we plan to simulate more scenarios with other injected errors. With the accumulation of the anomalies, they might come into being a whole anomaly cluster, which would also be solved within our following researches.

