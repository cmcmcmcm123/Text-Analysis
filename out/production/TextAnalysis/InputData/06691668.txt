Digging into Human Rights Violations: Data modelling and collective memory

Abstract?Archives of human rights violations reports, by virtue of their poor metadata, basis in natural language, and scale, obscure fine grain analyses of violation event patterns.

Cross-document coreference of victim or perpetrator occur- rences from across a corpus is challenging, particularly when those mentions relate to different events. These challenges are emblematic of the transition from small scale to big data analysis in the humanities. This paper discusses these issues and proposes a framework to address these challenges so as to explore narrative construction and the formation of collective memory. Though our framework is based on processing human rights violation reports, it can be readily extended to support other big data problems in the humanities.

Keywords-Digital Humanities; Big Data

I. INTRODUCTION  Records pertaining to human rights violations are prin-  cipally consulted so as to better understand the history of  an event or potential responses to ongoing events. These  records have heterogenous origins, capturing material pro-  duced by civilians, governments, and NGOs, and by victims,  observers, and perpetrators. They are produced both during  an event, frequently as governments bureaucracies document  their own behavior, and after an event, as witnesses emerge  to speak out against violators, or to participate in truth  and reconciliation proceedings. They contain heterogeneous  information types ranging from aggregate lists of atomized  acts to geospatial information regarding sites significant  to the prosecution of violations such as mass graves to  interview or interrogation transcripts to observation reports  by professional observers. The desired analytic outcomes  are equally broad, encompassing, 1) attempts to quantify  the scope or frequency of violations so as to make deter-  minations of the character of a violation pattern, 2) deter-  mine emerging patterns of violations and assess possible  interventions, 3) attempts to study the generalizability of a  given records collection in relation to a violation context,  4) attempts to gather correlated evidence for truth and  reconciliation or prosecutorial efforts, or 5) attempts to tell  the history of an event, for the assuaging of public memory,  for the scholarly record, or for the prosecution of suspected  violators. This heterogeneity of form, a heterogeneity not  uncommon for big data problems in the humanities, makes  analysis challenging. As one example, consider that most  corpus processing methods only function for narrowly de  fined data models: e.g. a key words in context script in R  relies on the predictable extraction of well-transcribed text.

Although human rights corpora are smaller than the  datasets addressed as big data in the sciences or social  scientific projects examining the social web, various features  of human rights data and its analysis make this a big data  problem. These features include the high dimensionality of  this information, the heterogeneity and number of reports  in a given corpus, the population level coverage of the  corpora, the requirement for real-time analysis, and the  analysis requirement for veridicality. Methods that address  these requirements are drawn from the core elements of big  data analysis: statistics, machine learning, data visualization,  data architectures, corpus linguistics, high performance com  puting, and HCI.

Survivors and witnesses to human rights violations are  placed in the complicated and problematic place of describ  ing and relating information about a traumatic event. In each  persons individual recall, incidents and details regarding the  event may not always line up with one another perfectly.

This is problematic as it creates multiple accounts of a  singular event; however, it is also interesting in that it allows  for other considerations to be made to a narrative history that  make up a collective memory and shape cultural identity  regarding the traumatic event. Problems, however, abound  when considering the scope of human rights violations as  many narratives become part of the larger narrative and not  always in the same form. How does one create meaning from  these records collections when their scale and fragmentary  nature resist access and interpretation? How does one reveal  meaning in ways that preserves the specific language and  character of the witness observations? How can NLP assist  humanistic researchers in the performance of these tasks?

From photos to memos to transcripts to field reports, portions  of traumatic narratives live in a variety of documents that  are not only multiplied by the number of atrocities, but also  by the ways in which they are recounted. To this end, big  data analysis can serve as a tool for constructing a narrative     out of many.

Our framework is designed to process large numbers of  narratives, parse elements describing time, location, person,  and semantic context, and allow for larger narratives to  emerge. This cross-document narrative can then be used  to understand how collective memories are formed and  how history is made. This same process can also identify  perpetrators and victims on the basis of their linguistic and  narratological structures, and to contrast how groups self  identify, on linguistic levels. This transversal reading of large  text archives of human rights abuses can also facilitate the  discovery of the stories of victims and perpetrators hidden  by the scale of these corpora.



II. RELATED WORK  A. Human Rights Data Analysis  Narratives pertaining to the violation of human rights  have occupied the humanities since the development of  linear historiography [1] with the Homeric retelling of the  annihilation of Troy, and documentation at scale has existed  since the Federal Work Progress Administration sent writers  throughout the American South to interview 2,300 former  slaves between the years of 1936 and 1938. What differen-  tiates the contemporary context is twofold. First, the scale of  the records collections that document atrocities has grown  astronomically, and second, the records being analyzed are  in some instances produced contemporaneously with the  violation. Three examples indicate different aspects of the  scale of contemporary data analysis in human rights viola-  tions research: 1) the Guatemalan National Police Historic  Archive is estimated to contain approximately 80,000,000  text records, 2) two organizations, the Shoah Foundation  and the Fortunoff Archive for Holocaust Video Testimony  have combined to produce over 100,000 hours of Holocaust  survivor testimony, and 3) in trying to estimate the num-  ber of casualties in the current Syrian crisis, the Human  Rights Data Analysis Group (HRDAG) had to work across  the ledgers of eight separate organizations, each of which  was conducting idiosyncratic casualty counts over varying  periods of the conflict. Near real-time reporting of violations  both enables human rights workers to respond to events in  progress, such as in the work of Best et. al. on the use of  mobile devices in election monitoring [2], and the work of  Norheim-Hagtun and Meier on real-time crisis mapping in  the aftermath of the 2009 Haiti earthquake [3].

In order for scholars from the humanities to engage with  emerging big data analytical tools important changes are  required to the weakly structured nature of traditionally  collected data. These changes both enable and proscribe  the analyses that scholars can perform upon their data sets.

Migrating data, for humanities scholars, relied traditionally  on methods of sampling, such as qualitative coding, string  matching, or transposition. These classifications and opera-  tions limit data to normative values. Data storage technolo-  gies such as structured databases also limited humanistic  data, and required careful processing of data sources and  schema design to ensure informational integrity. One way  to redress the necessity of sampling operations could be  derived from expanding data storage techniques associated  with big data such as non relational database systems. These  possibilities present tempting options for humanistic inquiry  as they can reduce the necessity to preprocess data via  sampling methods.

This project is predicated on the premise that given a  large collections of records describing a population, figures  will recur in multiple documents. The idea that records  over-represent victims in explicit and implicit ways emerges  from research by Silva and Klingner et. al. in [4], and  Patrick Balls 2003 report on the number of killed or missing  Peruvians from 1980-2000 [5]. That research examines  collections of human rights documentation, such as the  49,000 documents recovered from the Chadian Documen-  tation and Security Directorate, and the 24,000 reports of  missing Peruvians submitted to the Comisin de la Verdad  y Reconciliacin (CVR). By applying statistical methods for  population counts such as Multiple-Systems Estimation, [5],  [6] have screened collections of reports about victims of  abuse for recurring individuals, and provided estimates of  how many people enumerations missed. That relatively sim-  ple statistical models of population analysis can determine  quantitatively that victims of violence show up in multiple  documents within a collection implies that other methods  can be used to determine who those victims are. Their  methods, by necessity, focused not on identifying those  individuals, but on using statistical modelling of archives  to determine the number of victims. This over-reporting of  individual victims within human rights corpora is endemic,  and presents an opportunity for a system that can read across  a corpus. Our work builds on their quantitative approach to  human rights records by offering a qualitative method for  assembling the fragmented stories of those victims. In our  method, data provides the framework within which human  beings communicate meaningful testimony; that testimony  is embedded in, distributed through, and obfuscated by the  archives of human rights violation reports. The difficult  work this project takes on is identifying the recurrence of  those individuals and extracting the text that embodies that  recurrence.

Our first argument, that fragmentary descriptions of per-  petrators, victims, and abuses recur throughout a collection  is important in human rights and truth and reconciliation  contexts. Revealing these implicit stories adds evidence to  truth and reconciliation efforts, and supplements historians  access to the stories of an event. This requires identifying  related fragments of text within documents from across  an archival collection, and stitching them together in ways  that preserve context to present a coherent, accurate figure.

This mode of reading resembles traditional documentary     biography, and is already challenging with limited corpora,  such as a victims diary or a collection of reports from one  police precinct. When facing archives exceeding millions  of words, reading is impracticable, and marginal figures  effectively invisible.

B. Big Data Models  The choice of data model is complicated as the big  data turn implicates data collection methods, both enables  and constrains analyses, and influences the constitution of  knowledge [7]. Replicating data models more comfortable  with high-dimension data such as free text may expand the  analytical possibilities available to humanities scholars. For  example, big data analytical tools such as those developed by  the Culturomics project [8]or cultural analytics project by L  Manvoich [9] require that researchers first render texts into  accessible forms. Both projects narrow the analytic scope  to features susceptible to computational methods such as  saturation or simple strings. In order to leverage the power  of big data tools, researchers need to develop models that  balance scalability, flexibility, and mobility.

C. Information Modeling and Extraction  Most commonly, approaches to information extraction in  free text blend low context attempts at statistical analysis of  material restricting scope to an internal frame of reference,  and high context approaches that seek to link extracted  information to external data stores such as Wikipedia [10].

A low context approach for a corpus study of human rights  violations reports might see the problem as one of document  clustering and apply a method such as Term Frequency  - Inverse Document Frequency so as to build identifying  keyword sets for each document. Those keyword sets, when  compared, imply document families. A more complex, but  ideologically similar approach, might use a technique from  latent semantic analysis to infer topics for each document,  and produce clusters based not on explicit keywords but on  implicit topics. These techniques look to model a corpus  based on typicallity at the level of the document, and thereby  facilitate a human researcher?s engagement with a collection  through categorization at a particular level of granularity. A  high context approach would check extracted information  against an external reference so as to attempt more complex  tasks such as disambiguation, or geocoding of place names  so as to map the various locations in a report. Low context  approaches seem best at the type of general engagement with  a corpus described as distant reading [11], while high context  approaches attempt to synthesize information beyond the  limit of a given record. Other information extraction and  emplotment projects such as ChartEx [12] and Trading  Consequences [13] demonstrate two approaches to the prob-  lem of disambiguation of entities and place names. The  first relies on a diagrammatic approach to locations and  rigid genealogies of entities. The second correlates extracted  location names to mapped places, but then has to do so at a  very general, country level at which disambiguations do not  occur.

Research into the problem of Information Extraction (IE)  from unstructured and fragmentary documents has yielded  many techniques, but few are usable by the non-computer  science researcher, and few work for the particularities of  large collections of violations reports. Often, the report  formats are idiosyncratic, metadata is sparse, English is  frequently a second language, and narrative is the dom-  inant conceptual mode. These reports begin as free text  that quickly gets sublimated in a reporting framework so  that the violation information can be separated from the  stories. Soderland et al. [14] describe many computational  techniques in the area of IE that have been applied to various  domains such as game scores from the National Football  League, and the Intelligence Community. The difficulty of  using these techniques and their inherent limits in the face of  unstructured text has left witness reports, medical narratives,  and many other domains unaddressed.

Humanistic work to elicit narrative connections across  sets of documents pertaining to traumatic events has been  limited to visualizations of the metadata contextualizing the  free text, and statistical understanding of populations and  casualties. These elisions were dictated by the purpose of the  projects and the difficulty presented by unstructured text. For  example, visual explorations of the Afghanistan War Logs  by McCormick, et al. [15] for The Guardian UK focused  on geographic, abstract understanding of the Logs metadata:  stories behind the data were necessarily elided.

Work in the broader field of Narrative Intelligence of  pattern detection and representation of a series of linked  acts was pursued by an interdisciplinary reading group  at MITs Media Lab in the early 1990s [16]. Frameworks  to facilitate the development of textual data analytics tools  include UIMA, GATE, Seeker and SemTag, Cerno, and  Armadillo. Like the more familiar SemTag, Cerno [17] and  Armadillo are focused on Semantic Web applications and  so are most useful for automated tagging and retagging of  documents in a corpus. Each produces XML annotations  of the source text, and definitions for the annotations. Of  the three, Armadillo is the most flexible in regards to  document regularity, and the most automated. UIMA and  GATE are extensive in terms of the text processing modules  and are open source, allowing for further customization and  extension. Each allows for analysis of document corpora  through the tokenization and parsing of the text as symbolic  and syntactic objects. Additional functionality is available in  both frameworks for pipelining Java programs that introduce  AI text-mining methods such as LSA, or for structured reg-  ular expression searches based on user definitions. Methods  that rely on user-defined patterns are both time consuming  for the end user and fundamentally miss the central problem  presented by implicit figures in large text archives the     patterns are not visible to unaided readers because the  scale of the archive resists conventional modes of reading.

Armadillo relieves some of the burden on the end user  but retains a focus on highly local identifications, i.e., to  what category does a particular phrase belong. Despite the  wealth of computational techniques for text analytics, a gap  exists between a humanistic researcher?s conceptualization  of the tasks required to make sense of records and the  computational models currently used by IE tools.

D. Anaphora Resolution  Anaphora resolution has been a topic of interest in NLP  since the work of Sidner[18] on pronominal anaphora reso-  lution using domain and linguistic approaches, and Hawkins  [19] on associative anaphora. The period from 1990 un-  til today has focused on shallow, low context, syntactic  approaches [20][21]. Anaphora, most often recognized in  pronouns, refer to any substitution of an ambiguous phrase  for a more specific phrase. Similarly, exophora occur when  the specific subject is not present in the current document.

The current state of the art for computational resolution of  anaphora is best described in the work of R. Mitkov, S.

Lappin, B. Webber[22], M. Denber, and M. Dimitrov[23].

Anaphora resolution systems have been developed and ex-  amined in the work of Mitkov, Webber, Lappin and others,  and are classified as knowledge-poor or knowledge-rich.

An example of a knowledge-poor system, i.e. one that  does not take into account complex linguistic, semantic,  or grammatical rules, is Dimitrov et als GATE-based im-  plementation. Dimitrovs system drew on earlier work by  Lappin and Leass [24], [25] pronominal resolution that used  indicators such as definiteness, heading, collocation, referen-  tial distance, and term preference. Mitkovs system achieved  pronominal resolution rates of 89.7% within a corpus of  technical manuals[23]. Dimitrovs system read a corpus of  approximately 180,000 words drawn from broadcast and  print news sources. These genre specifications and corpora  limits are typical of anaphora resolution systems. Our work  began with the core of Dimitrovs knowledge-poor system.



III. METHOD  Our framework describes a low context approach to the  problem of transversal reading. We propose a two tiered  framework with Data and Presentation layers. The data layer  is responsible for data extraction, parsing and running NLP  modules to get the entities and events. The presentation layer  handles the visualization of the data and the feedback loop  which modifies the data based on the user feedbacks.

A. Data Layer  This layer deals with how the data is extracted, processed,  and stored in the backend, supporting the presentation layer.

This layer consists of the extraction module and the NLP  module. The extraction module is responsible for digitizing  and parsing the documents. A large portion of human rights  violation documents that we obtained were stored in hard  copy format and, in many cases, handwritten. We used  commercial OCR tools like OmniPage to digitize some of  the documents. In cases where the materials are barely  readable due to poor archiving, manual transcription is  required. After digitizing, the document is parsed and the  output is routed to the NLP module.

The goal of the NLP module is to facilitate cross-  document coreference of entities in collections of witness  statements and interviews within the domain of rights. The  module?s approach to resolving the task [26] described as  ?whether or not two mentions of entities refer to the same  person,? begins by considering exophora and relies on plac-  ing pronominal entities within a high-order Event Storygram  of location, time, name, and semantic context. Because tem-  poral information is so often referential and ambiguous, and  therefore difficult to extract and correlate [27] our approach  uses a phrase-based establishment of semantic context to  support identifying the temporal context and to reinforce  the automatic matching of elements within the Storygram.

Our model for processing linguistic uncertainty extends the  work on veridicality in [28], [29], [30], incorporates semi-  supervised machine learning methods with a taxonomy.

This model describes the validity of automatically generated  correspondences amongst the relations of person to time to  location to semantic context.

This noun- and verb-phrase extraction, collocation de-  tection, and semi-automated matching, feeds a 2D-planar  visualization similar to network graph models. Uncertainties  in the document and information retrieval processes are  visualized to allow researchers to confirm whether entity oc-  currences should be conflated. Because much human rights  documentation contains sensitive information that cannot be  made public, this project is prototyping with both publicly  available and restricted data. Publicly available data sets used  in this work include interviews with first responders to the  World Trade Center attacks and documentation exproduced  by or related to the Extraordinary Chambers in the Courts  of Cambodia (ECCC) as well as redacted version of reports  describing contemporary violations committed by the Lord?s  Resistance Army in central Africa.

1) Event summarization based on matching phrases:  Our main stratagem is to situate entities in the series of  events that define their appearances. Phrases useful for this  process accord to a ?journalist template,? of Who, What,  When, Where, and Why, and are situated in the events  reporting schema developed by Patrick Ball for human rights  violations reporting [5], [6]. The goal is a system that  can automatically extract these important entities as phrases  and based on these extracts, allow for the recognition of  duplicate entities across documents. A perceptual diagram  of the system is shown in Figure 1.

After the noun and verb phrases are extracted and passed     Figure 1. Model used in the NLP module for cross-referencing the entitites.

on from the extraction module, a phrase classifier is used to  determine which phrases fall into important entity categories  such as Person Names/Geographic Locations/Date/Time or  depiction of an event. After classifying these phrases into  categories, a module called Collocation Detector detects  which of the entities are described in the same context  within a passage in the corpus. This collocation of phrases  is different from the collocation of words usually used in  NLP in that it captures instances of the collocations, instead  of a global probability. A collocation is only true when  multiple elements correlate. After a set of collocated phrases  have been detected, they are placed into the event template  and fed into a visualization engine. Human observers then  decide which cross-document entities are identical. The  engine computes automatic scores to make suggestions to  the observers on which events and entities should be merged.

2) Phrase extraction and classification: The first step  of phrase extraction is done by running a full parser on  each document and then extracting all the retrieved noun  phrases and verb phrases from the parse tree. We decided  against using a shallow parser (chunker) because it has lower  recall (may not capture all the desired phrases) than a full  parser. The parser we are using is the Stanford parser [31].

From the extracted phrases, we formulate a classification  task for labeling important phrases for event extraction. The  important phrase classification in our research is different  from the traditional named entity recognition (NER) problem  in NLP in that we are seeking to connect names to un-  named entities. We have 8 categories for important phrases:  Organization, Person, Title, Location, Date, Time, Event,  Miscellaneous, and the background category of Unimpor-  tant. Not all of these categories are visualized. Of these  categories, some are traditional NER or TimeML categories.

Event and Miscellaneous labels are new and determine some  important phrases that might not be readily interpreted as  named entities. Phrases such as ?the pedestrian bridge,? ?the  ferry,? or ?the second tower? which are not identifiable as  a particular named entity but might be crucial in depicting  the event are classified as Miscellaneous.

To maximally utilize human knowledge in the phrase  labeling phase, an unsupervised selection mechanism selects  the phrases to be labeled. In this mechanism, phrases are  ranked by a score that is similar to a frequency or N-gram  model, but it discounts the probability of a phrase if it is  very common in a background corpus  Sc(phrase) = logP (phrase)  ?max(logPbg(phrase)  ?logP (phrase), 0)  (1)  where logP (phrase) is computed by an N-gram language model trained on the current corpus, and logPbg(phrase) is based on a N-gram language model trained on a background  corpus that is supposed to contain documents of all kinds.

Under this model, the probability of a phrase is only  discounted if Pbg(phrase) > P (phrase). This application of Term Frequency Inverse Document Frequency[32] helps  us to find frequent phrases in the corpus which are not  popular in the background corpus. The phrases with top  scores are manually labeled. Through this approach, we can  obtain the labels for the most frequent and unique phrases  in the corpus, which are likely to be more important in  isolating an event. Our N-gram training uses the modified  Kneser-Ney smoothing [33] from the MitLMpackage [34].

The background language model is obtained from Microsoft  Web N-gram Services. Given a set of human-labeled phrases,  we then train two levels of classifiers on these phrases. At  the first level, a binary Important versus Unimportant phrase  classifier is trained. At the second level, a one-against-  all multi-class classifier is trained for each of the phrase  categories described above, except Miscellaneous, which  serves as the background category for important phrases. The  features used for the classifiers are common NER features  [35], [36], plus standard bag-of-words features. For the Date  and Time phrases, we make use of the SUTime library [37]  which matches date and time expressions using an extensive  set of rules defined by regular expressions. The classification  of these phrases do not depend on the labels.

For the collocation we use a simple metric: a Gaussian  kernel on the distance between mentions of different phrases.

Formally, the collocation probability of one occurrence of a  phrase, given a set of other phrases is defined as     P (p1|p2, ..., pk) = exp(?? ?  i  (S(p1)? S(p2)) 2) (2)  where S(pi) is the sentence number where pi occurred.

Given the defined conditionals, one can compute the joint  probability P (p1, p2, p3, ..., pk) and use a threshold to de- termine which phrase set goes to an event template.



IV. PRESENTATION LAYER  The presentation layer consists of the visualization module  and the feedback module. The visualization module consists  of Storygraphs [38] and Storygrams. Storygraph, our earlier  work, is a 2D visualization technique for presenting time  and location on the same chart. It consists of two parallel  vertical axes, which are used for latitude and longitude, and  an orthogonal horizontal axis, which is used for time. An  event, E(lat, lng, time) is mapped in into the Storygraph by first drawing a line segment connecting the corresponding  latitude and the longitude in two vertical axes. A marker is  then placed on the line above the corresponding time of the  event. Hence a pre-requisite for using Storygraph is that the  data needs to be structured and precise, i.e it needs to have  a precise geo-coordinates and timestamp. Any additional  attributes like the type of event can be shown by changing  the size, shape, and color of the marker.

One of the visualization layer goals is to present certain  as well as uncertain data. Depending upon the context,  uncertainty refers to semantic uncertainty, ranged values  or missing data [39]. In our framework, uncertainty is  introduced starting from the extraction phase as described  above. These uncertainties include the temporal (?By this  time, it had to be 11:00 o?clock at night? and ?at that time  I noticed?), locative, (?I guess that would be North End  Avenue?), and entity (?At this point I had my five guys?)  [40]. We use Storygrams to present uncertainty.

A Storygram is a 2D-planar diagram consisting of events  as its building blocks. An event in this case is a 3-tuple  consisting of time, location, and entity. Storygrams are  represented visually as triangles using the entities as vertices  connected with weighted edges. The weights represent the  confidence value in the relationship obtained from the NLP  module. These confidence values show the certainty of the  connection between two elements. Figure 2 shows a trigram  with confidence values and elements.

To reduce visual clutter due to the excess of edges and  vertices, we employ details on demand [41] and filtering  capabilities on the dataset. The users can also drag and  drop the events over other events and manually enter the  confidence values to increase the association between two or  more events. Since the users load one corpus at a time, this  feature enables users to visually associate events in different  documents.

The uncertainty in data also affects the output of semantic  context and the verdicality modules. To address this issue,  Figure 2. A model of Storygram showing events and uncertainty within the entities of the event.

we intend to make Storygrams interactive in that users can  merge two vertices if they think they are the same. Merging  of two vertices also marks the underlying data to be changed.

The change is witheld until another expert verifies it. When  the vertices are merged, the edges are also merged in many  cases. In these cases the confidence values are also merged.

This module is still under construction, and we leave it as  a future work.



V. DISCUSSION  Applying our framework to various datasets describing  genocide in Cambodia from 1974-1979, a contemporary  militant movement in central Africa, the attacks on the  World Trade Center (WTC) in 2001, and U.S. military  logs from Afghanistan has yielded visualization showing the  movements of individuals and groups across the time and  space of event contexts. Not per se indicative of violations,  two of our corpora are included in this research because  they are publicly available and share syntactic and semantic  features to primary rights violations data. Those corpora  are the WTC Task Force Interviews conducted with first  responders to the attacks of September 11, 2001, and the  material published by The Guardian UK as the ?Afghanistan  War Logs.? At 511 interviews and 1.6m words, the first  dataset does not qualify as ?big,? but the type of analysis  our system affords is not replicable by non-computational  methods as it relies on the cross-referencing of hundreds  of thousands of granular phrases and the calculation of  correlating uncertainties. Our analysis of the WTC material  revealed that one individual, Father Mychal Judge, appeared  86 times in 33 interviews. These appearances describe his  time on scene from when he arrived at the site to his death  during the collapse of the north tower as bodies fell on  the roof of 6 World Trade Center to his laying in state at  St. Peter?s. Extractions of these observations by survivors  allow for the stitching together of a narrative describing the  last day of Father Judge?s life. We are still working on this  corpus to extract narratives of other victims.

In the public reports from central Africa describing viola-  tions perpetrated by the Lord?s Resistance Army [42], there  are noticeable differences in reports regarding important  details such as number and descriptions of victims and  perpetrators, locations of incidences, and variations in time     and date. Similarly, detailed information may be given in  ECCC interviews; however, key information such as what  methods of interrogation and torture were permitted may  vary depending on the individual and his or her position in  the case. For example, discrepancies appear in the interview  material in approved torture and interrogation techniques  between Kaing Guek-Eav and Prak Khan[43], [44]. An  important feature of our model is the conflation of multiple  accounts describing a singular event. These multiple ac-  counts provide a sense of depth and intricacy to the emerging  broader narrative. Mining these details allows scholars to  examine the multiple perspectives embedded in collective  memory and cultural identity. Problems, however, abound  when considering the scope of human rights violations as  many narratives become part of the larger narrative and  not always in the same form. From photos to memos to  transcripts to field reports, portions of traumatic narratives  live in a variety of documents that are not only multiplied  by the number of atrocities, but also by the ways in which  they are recounted. One such example can be found in  the ECCC documentation and the euphemism used by the  Khmer Rouge. In one of his interviews, Kaing Guek-Eav  details the meanings of the key terms ?smash? (execution),  ?resolve? (execution), ?purge?(arrest), and ?sweep cleanly  away? [45]. Furthermore, his descriptions regarding which  administrators used which terms expand the ability to rec-  oncile other narratives to others. Through recognizing these  terms and their variety of applications in regards to entities  and times, one can better reconcile these voices with other  narratives in history and help to shape identities within a  given perspective.

As discussed in [38], our Storygraph visualization as  seen in Figure 3 shows each individual report as a dot in  a coordinate plane of latitude, longitude, and time. What  our visualization revealed are both patterns in the corpus  indicative of documentation practices and patterns in the  event the reports describe. One such pattern, seen in the void  spaces numbered 1, 2, and 3, corresponds to lulls in violence.

Vertical banding as seen at A, B, and C, indicate events  happening simultaneously across the geography described  by the corpus. In this case, they indicated elections. In  Figure 4, also discussed at length in [38], individual units  within the corpus can be seen traversing the geography and  the temporality of the corpus. Each colored line indicates  one unit, and their path corresponds to the mentions within  documents. An implication of these visualizations is that  they flatten the corpus so as to enable a holistic perspective  on the data, and may thereby lead researchers to focus  on contextual analyses, rather than granular analysis of  individual victims and perpetrators. However, a holistic view  of human rights data sets such as we are undertaking with the  confidential LRA material, when modelled on the approach  seen in the Storygraph figures, facilitates drilling into the  data. Seeing the pathway taken by an individual through  a corpus when connected to the document fragments that  were mined to produce that pathway deeply connects the  documentary evidence to the final analytic visualization.



VI. CONCLUSION  This paper explored the challenges faced by researchers  working on human rights violations, and proposed a frame-  work for addressing some of those challenges. That frame-  work included elements for processing the textual data,  visualizing that data, and feeding judgments made by users  of the framework back into the processing layer. Principally,  our methods for information extraction, data visualization,  and user feedback work to generate narratives that traverse  document boundaries. These techniques for transversal read-  ing of large historical corpora allow for the investigation of  the relationship between big data and collective memories of  traumatic events. Memory, as defined by Maurice Halbwachs  in 1948 in the seminal work on collective memory [46], is  by definition collective and only exists in the conversation  undertaken by groups; individuals have no capacity for  memory, only fantasy. This founding theory on collective  memory corresponds to our projects argument related to  cross-document coreference; information is only valid when  it can be validated.

In the future, we plan to refine two areas of the framework  and explore the potential applicability of this framework  to corpora describing other categories of events. Areas of  the framework that we plan to improve are our methods  for determining the semantic context of Storygrams and our  modeling of linguistic uncertainty and concomitant veridi-  cality. Other categories of events that may be susceptible  to this approach are environmental catastrophes, popular  uprisings, and political movements.

