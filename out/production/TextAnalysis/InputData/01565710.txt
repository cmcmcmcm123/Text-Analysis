A Random Walk through Human Associations  Raz Tamir

Abstract Letting one's thoughts wander is not simply an arbitrary  or rambling process. It can better be described as "associative thinking", where a complex chain of associative thoughts and ideas are linked. It is our contention that this seemingly chaotic process can be modeled by a random walk in a weighted directed graph.

Furthermore, is it possible to predict mathematically the "steady state" of such a process, to determine where such wandering is leading.

The random walk process uses rules of association, defined by the Local Confidence Gain (LCG) interestingness measure. Extracted concepts are used as nodes of a directed graph. The associative "forces" between any two concepts (measured by LCG) are used to weigh the edges connecting the nodes that create a graph of associations.

It is common, yet not trivial, for people to look for data about a subject without knowing its exact nomenclature (for example, finding the name of a disease just by knowing its symptoms). Random walk in association graphs can discover highly informative phrases that can be used for query expansion in a way that better expresses the user's initial search goals. A different usage is to create a user profile representing his current interests.

We used a modified version of the Turing Test to show that the random walk process discovers association rules that conform to a human associations generating process.

By constructing the user associations we were able to build a profile representing the user's "line of thoughts". The suggested algorithm can be used in any database and can implement the ranking measures of other association rules.

1. Introduction This chapter presents the keystones of the random walk  based algorithm. Section 1.1 discusses the role of associations in the cognitive processes that deal with information retrieval from memory. Section 1.2 and 1.3 discuss the random walk basics and its convergence criteria. In section 1.4 we present the Confidence Gain (CG), an association interestingness measure and in section 1.5 we present the Local Confidence Gain (LCG) measure, which is a low complexity version of the Confidence Gain.

Both CG and LCG represent the edges of the directed graph used for the random walk process. The nodes of such a graph are concepts (phrases) extracted from Internet pages.

Section 1.6 presents the contribution of this paper.

Since this work links association rules interestingness to human cognitive association processes, some of the terms differ from common terminology. We shall sometimes use terms such as "associative forces" or "generating associations" to describe the process of discovering and evaluating the interestingness of association rules between pairs of concepts. When describing an asymmetric association we shall use the term "stimulus" as the input and "response" as the output of the process. Furthermore, the phrase ?user profile? represents the most significant concepts (phrases) that can describe user interests. A user profile is extracted by tracking user behavior while surfing the Internet, and extracting the most significant phrases from the sites he chooses to visit. Finally, a few terms are used to specify the words that hold the information and are extracted during the calculations ? ?terms?, ?concepts?, "phrases" and ?associations?.

1.1 Intelligence and Associations Associations are cognitive links between data units in  human memory. The discussion on imitating human associations goes back to Alan Turing, in his original article about an imitation game test of intelligence [3].

Turing made two basic claims. The first is that if a machine could pass the Turing Test, it would necessarily be intelligent. The second is that in the not too distant future it could in fact be possible to actually build such a machine.

We shall not discuss here the first claim but rather ask what it takes to build a Turing-Test-qualified machine.

Researchers such as French [24] claimed that only things that have experienced the world as we have experienced it could pass the Turing Test. On the other hand, others [20] claimed that subcognitive questions, which are designed to probe a network of cultural and perceptual associations, could be answered using statistics over large corpuses. In this article we agree with the latter opinion. We claim that to some extent, the human associative world, representing the culture and understanding of the environment, can be reconstructed just by looking at the occurrences of certain terms with respect to each other. We claim that if a machine produces outputs that make sense and furthermore help get better search results, its purpose is fulfilled whether the machine is aware of what it is doing or not [4],[11],[15]     1.2 Random Walks in Weighted Directed Graphs Let G=(V,E) be an undirected connected graph with n  nodes and m edges. A random walk in G starts at node 0v .

At each step, the next node in the walk is selected randomly from the neighbors of the last node in the walk. The probability to move from node tv  to any of its neighbors is  ( )tvd 1  where ( )tvd  is the degree of node tv . The  sequence of visited nodes is a Markov Chain, where the matrix of transition probabilities is  [1] ( ) ?  = otherwise  Ejiifid pij 0  ),(1  We can similarly formulate the transition probability matrix for a weighed directed graph as  [2] ?  = ??  otherwise  Ejiifww p k  kiji ij   ),(  Here, jiw ?  is the weight of the edge from i to j and  ? k  kiw  is the sum of all weights for outgoing edges of  node i. Eq. [1] is, obviously, a private case of eq. [2].

1.3 Perron-Frobenious Theorem  An nn?  matrix M with real entries ijm , is called a stochastic matrix provided that (i) for all ijm 10 ?? ijm ; (ii)  1= i  ijm  for all j. A stochastic matrix M is called regular  (or primitive) provided there is an integer 00 >q  so that 0qM has all positive entries. Let )0(jx be an element at  location j of a vector x at time 0.

l  ljl xm )0(  is the new  value of the element at location j at time 1. In matrix notation, )()1( qq Mxx =+  represents the probability distribution of being at any node at time q+1. A regular stochastic matrix M has some interesting properties: (i) 1 is always an eigenvalue; (ii) the eigenvector v1 can be chosen so that all entries are positive and .11 =  i iv (iii) for all other  eigenvalues i? , 1<i?  ; and (iv) for any probability  distribution p 1vpM q ?  as q goes to infinity. These characteristics enable us to calculate the final probability distribution of all the nodes. Thus, if the graph representation of the associations could be formulated into a regular stochastic matrix, the "steady state" associations' distribution could then be calculated. A comprehensive discussion on Perron-Frobenious theorem can be found in [5].

1.4 Confidence Gain Interestingness Measure  CG [23] represents the gain of the current association over average confidence of similar associations. Formally, let I={Ij, j=1,2?m} be a set of items. Let X?I, Y?I be two different items. Let D be a set of transactions, where each transaction t is a set of items. An association rule of the  form X Y is defined as follows. Support measure Supp(X) is the fraction of transcriptions that contains item X in database D. The degree of support and confidence (Conf) for the rule (X Y) is defined by:  [3] )|(  )(Supp )(SuppY)X(Conf  ),( |||| Y&X  Y)Supp(XY)Supp(X  XYP X  YX  YXP D  ==  ===      Here ||D|| is the total number of the transactions, ||X|| is the number of transactions containing X, and  ||X&Y|| is the number of transactions containing both X and Y. For the current discussion, X represents a stimulus term, and Y represents a possible response. A transaction is a web page containing a set of items. For a fixed stimulus, Conf is a function of all possible responses Y. The average confidence of response Y )(Conf Y  is calculated by eq [2].

[4] =  = n  1i i  i )Supp(I Y)Supp(I1(Y)Conf   n  where n is the number of valid instances. A valid instance is a rule (Ii Y) having support and confidence above certain thresholds.

CG, is a function of both X and Y: The averaged confidence is the essence of the new  measure. It enables computing the average (expected) confidence of a term, while preserving a sense of locality, by performing the averaging process over other terms from a finite yet representative list of terms statistically connected to the stimulus term X. The process is based on parsing a set of the first 250 pages retrieved by Google search engine for a stimulus query phrase. For each phrase, the number of pages in which the phrase appears (1 to 250) is counted. The CG algorithm includes the following stages:  a. Perform a search over stimulus term and retrieve 250 sites using Google-api [7].

b. Scan sites text and select 100 words having the highest frequency of appearance.

c. Form a list of all possible couples (stimulus-response)  of two words (10,000 couples).

d. By using Google-API get the number of appearances  over the Internet for each couple and its stimulus (first term).

e. Calculate confidence of each couple by formula [3].

f. Calculate averaged confidence for each of the 100  terms by formula [4].

g. Calculate CG of each of the couples by formula [5].

[5]  =  ?  ==  n  1i i  i  )Supp(I Y)Supp(I  Supp(X) Y)Supp(X  (Y)Conf Y)Conf(XY)CG(X      n     h. Return top ranked words sorted by descending CG value.

Note that the numbers 250, 100 used in the algorithm were chosen empirically since they are large enough to represent the query results space, while still enabling a reasonable execution time of the algorithm. The algorithm was found to be not sensitive to the exact numbers.

1.5 Local Confidence Gain The main idea behind the Local Confidence Gain  ("LCG") is to make better use of the retrieved documents.

While in the basic CG measure, the retrieved sites are used merely for building the most frequent terms list, the local CG uses the term distribution for calculating the average confidence and CG. In order to compensate for the "locality" of the data (using a small number of web pages as against using the entire Internet for the calculation), the frequency of each response in the local dataset is compared to the average frequency over the Internet. The compensation factor LC ("lingual confidence") is defined as:  [6] N n  Rn RNRLC ?= )( )()(  where N is the number of pages in the local dataset (typically 250), n is the total number of web pages available through a given search engine, N(R) is the number of sites in the local dataset that contain the term R and n(R) is the number of web pages in the Internet that contain the term R. LCG is formulated as:  [7] )()()( YLCYXCGYXLCG ?=  Note that if the local dataset is the entire Internet, LC=1 and LCG=CG. The modification can be justified in many ways such as looking at LC as a measure of the frequency of a specific term compared to its frequency in the English language. The advantage of LCG is its low complexity.

Since stage e in the LCG algorithm is performed over the local dataset containing the retrieved sites, it is significantly faster. The LC calculation demands only 100 queries of Google-api, instead of 10,000 needed for calculating CG. A typical execution time for calculating LCG (for 250 pages and 100 most frequent terms) is smaller than one minute.

1.6 Current Contribution This article extends the concept of random walk in  directed graphs to the associative world. The LCG measure enables the creation of a directed graph representation of the links between concepts. The links are both directed and weighted. Using the random walk process on graphs enables us to simulate the associative flow between concepts (the nodes of the graph) and reach a steady state, revealing the probable outcome of a process. It is possible to simulate the importance of any link in the graph to the final outcome and possibly suggest a change in specific links so that the wanted outcome will be reached. The algorithm is called "Associations Rank" (AR). AR turns the PageRank algorithm to being query dependant. This enables the result to be the most relevant, whereas the  simple PageRank algorithm only retrieves the most important documents.

A large scale Turing Test was performed to show that concepts chosen by the AR algorithm are similar to associations generated by human beings for the same stimuli (inputs). Then, the random walk process was used to extract concepts representing the user contemporary profile. By analyzing proxy logs of the surfer's history the essence of the user interest was extracted. Finally, AR was used to "answer" user questions asked in natural language.

2. Random Walk in Association Graphs 2.1 Associations Rank Algorithm Formulation  Two of the most important characteristics of human associations are their asymmetry (e.g. the association of "Shampoo"  "Hair" is much stronger than "Hair" "Shampoo") and the extensive usage on contextual (or background) knowledge. As mentioned before, terms such as "Shampoo" and "Hair" are the nodes of the graph while the edges exist where there is a high CG connection between any two nodes (terms).

As in the concept of the PageRank algorithm, let us begin with terms represented as unconnected nodes randomly located on a plane. Suppose we begin with a random node (concept) and then perform a random walk to another node. The edges that connect the nodes are the associations. The following measure can then be formulated (named ?Association Rank? or ?AR?):  [8] ( )?? ??  ?  ? ??=  Eijj Eljl  k jk  i ljLCG ijLCGARAR  : :  )1( )( )(  Here )(kiAR  is the Association Rank of site i in the k iteration. AR represents the probability that concept i will be accessed after concept j is perceived.

( ) ??  ? ?  Eljl ljLCG  ijLCG  :  )(  is the normalizing factor. The  basic concept behind the AR formula is that the probability that a concept will be reached is the sum of probabilities of reaching it from any related concept. For each concept j the probability to move to concept i is normalized by dividing the strength value (LCG) of the association between them (in direction from j to i) divided by the total association strength for all concepts that are linked to concept j. If we define matrix LCG as ( )  ( ) ??? ?  = ??  otherwise  EjikiLCG jiLCG  LCG Ekiij  :  we get the matrix formulation: [9] )1()( ??= kk ARLCGAR  where Tkn kkk ARARARAR ]...[ )()(2  )(  )( = . The AR process can be solved in two ways. The first is by iterating formula [9] and the second is by finding the eigenvector that matches the largest eigenvalue, as discussed in the following section.

2.2 "Associations Rank" Algorithm Convergence Criterions  In order for the process to converge, let us recall the Frobenious theorem. For a regular stochastic matrix M, if p is any probability distribution then 1vpM q ?  as q goes to infinity. AR can easily become a probability distribution if every entry is divided by the sum of all the entries  =  = n  l l  i  AR  AR AR   . This will not alter the ranking. In order  for the LCG matrix to become regular, each entry must be made positive by replacing each null entry (i.e. any entry which represents a case where there is no associative link) with a small positive value? . Replacing null LCG values by ?  is equivalent to saying that there is a very small probability to move from one concept to any other concept.

This intuitively does not contradict any of the pervious assumptions.

Finally, the process of setting each entry ij to  =  = n  l il  ij ij  LCG  LCG LCG   makes LCG  matrix stochastic. This  normalization can be easily justified, since  ??+?? =  nLCG n  l il 11   thus, LCGLCG ? . The process will  assume the form: [10] )1()( ??= kk ARLCGAR  Since LCG  is regular and stochastic the process must converge to the eigenvector that corresponds to eigenvalue one. The following figure gives an example of a directed graph containing four nodes (concepts),  Figure 1.   Example of a Directed Graph Containing Four Concepts  where Li j is the measure LCG(i j). Li i represents the small probability for staying on the same node, and is inserted to ensure algorithmic convergence. Thus, the probability of moving from node 1 to node 3 at a stage n is given by the expression:  ( )  ( ) ( ) ( ) ( ) ( )  ( )31)1(  31)1(  )1(31  )1(  )1(  31)1()(  ??=  ?+?+?+? ??  = +++  ?=?  ?  ? ????  ??  LCGAR  LCGLCGLCGLCG LCGAR  LLLL L  ARP  n  n  nn  2.3 Using AR Algorithm to Create the User?s Contemporary Profile  The actual pages the user visits can be used as the dataset, instead of extracting text from Internet pages retrieved by a search engine. Since the visited sites are temporarily stored on the user?s local machine, the process is usually much faster than retrieving sites from the Internet. The algorithm can be performed in background at a preset frequency (typically every five minutes).

a. Retrieve new pages visited since last execution.

b. Create a list of all phrases of consecutive words up to  n terms (n is a parameter set in advance ? typically 2).

c. Select the top 100 phrases that occur in as many of  these documents as possible.

d. Form the LCG matrix. Set the diagonal values and  every null entry to 1e-8. Form LCG  matrix by normalization.

e. Perform 100 iterations (see eq. [10]). Compute the AR  measure for the vector of phrases.

f.  Store the 100 phrases and their AR  ranks as a user profile.

The diagonal values are set to a small value (step d) since they represent an unwanted static association. All other nulls are set to small values for the matrix to be positive. In practice it was found that it is much faster to solve eq. [10] iteratively rather than directly computing the eigenvector that corresponds to the eigenvalue one.

This process enables the user to get useful data as he surfs the Web, as we shall demonstrate later.

2.4 Using AR Algorithm to Extract Highly Informative Phrases  One of the weaknesses of the Google's PageRank algorithm is that it retrieves the most important pages rather than the most relevant ones. This phenomenon occurs since the basic ranks are determined prior to the query (even though some adjustments are performed after the query phrase is presented). Our approach goes a step farther.

After retrieving an initial set of pages, the AR algorithm is applied to find the most informative phrases within these pages. These phrases are then used to narrow the initial query. The "narrowed" phrase gives more focused results.

The narrowing process can be unsupervised or supervised where the user selects which of the phrases best reflects his intentions. Phrase extraction is also needed where the user wishes to retrieve phrases that are more accurate, such as an exact disease name or a technical expression (e.g. "sunk cost", "solar eclipse"). The justification for such a process emerges form the fact that in many cases people can identify and categorize phrases as relevant even if they     didn?t think of them in the first place. For example, if one is looking for information on the moon as it looks at the beginning of the month, then a good query phrase would be "lunar phases". To most people the term "lunar phases" would not "jump into thought" as appropriate, but when presented with it they will agree that it is a good addition to the query terms.

3. Experimental Results 3.1 Free Associations Turing Test  The Turing Test is based on his claim that for a machine to succeed it must "fool" a human being. In the main experiment that tests this premise, the machine and a human are located in separate rooms. They must respond to identical questions posed by a human interrogator. If the interrogator cannot differentiate between the machine and a human, the machine is passes the Turing Test. In the current research 900, people were given questionnaires containing two sets of stimuli and responses. These people played the part of the interrogators in the Turing Test.

Several researches explored association rules measures (see [19], [2]). In order to compare the AR algorithm with other association rules measures and human association, we selected seven asymmetrical association measures from [19], as we did in a previous research [22]. To this list we added Lift measure (as a representative of the symmetrical association rules measures) LCG and AR measures. We focused on asymmetrical measures since, as indicated before, human associations are strongly asymmetrical.

[11]  )()|(),(  )()|( )(1  )()|(int  ),( )()(  )|( )()(])|( )|()[(])|( )|()[(  )( )|(log),(  )( )|(log),(     YPXYPYXPKlosgen  YPXYPValueAdded YP  YPXYPFactoryCerta  YXP YPXPConvistion  XYPConfidence YPYPXYP XYPXPXYP XYPXPIndexGini  YP XYPYXP  YP XYPYXPMeasureJ  ??=  ?= ?  ?=  =  = ??  ++  +=  +=?  We used two large psychological databases as sources of human associations: 1. "The MRC Psycholinguistic Database: Machine Readable Dictionary" [13] (denoted EAT database); 2. "The University of South Florida Word Association, Rhyme, and Word Fragment Norms" [18] (denoted FAN database).

The following method was employed in order to select valid stimuli set: a. FAN and EAT databases were compared and the top 100 stimuli terms having the largest overlap (identical responses) were selected (the stimuli set  denoted as "S"); b. For each stimulus, the first 250 web pages (retrieved by Google search engine) were scanned and the 100 terms that appeared with the highest frequency in the scanned pages were stored (denoted as "top 100" set).

The S set was used as a base dataset for evaluating the performance of the association measures. The asymmetric measures presented in formula [28] were used to rank each "top 100" set. For each measure, the response with the highest associative strength to the stimulus was given the value 1, and the response having the weakest associative strength to the stimulus was given the rank 100. The criterion for comparison was the claim that a better measure corresponds with a smaller sum. The following table shows the sum of overlapping terms ranks for each measure.

Table 1.  Sum of Overlapping Response Ranks for the "Top 100" Response List  Sum of Overlapping Response Ranks Measure  40866 Added Value 18165 AR 41258 Certainty Factor 44400 Confidence 41258 Conviction 39412 Gini Index 37875 J Measure 23615 Klosgen 16897 LCG 28883 Lift  We selected the most promising five association rules, having the smallest sum of overlapping terms ranks, for further evaluation (LCG, AR, Lift, Klosgen and J- Measure).  We then randomly selected 50 of the 100 top overlapping terms and built 70 questionnaires, with each containing two lists of stimuli (input terms) and responses (output terms). One of the sets was extracted from the EAT database while the other was a ranked set of responses of one of the examined association measures. Ten questionnaires containing only human associations were used as a validity check for the test. The following table contains the total number of correct and incorrect answers for each measure. A correct answer was where the interrogator succeeded to identify the human associations.

Table 2.  Distribution of Correct and Incorrect Answers.

The score of each measure was calculated by  [% incorrect answers]/50*100  Measure Correct Answers  (%)  Incorrect Answers  (%)  Score  AR 55 45 90 LCG 62 38 76 Lift 65 35 70 Klosgan 69 31 62 J-Measure 76 24 48  The best measure was the one who scored the highest ratio of number of "wrong" answers, since it was capable of "fooling" more people. Logically, the upper boundary of     the wrong-to-total ratio was 50%, since the best an associations measure can do is to perform as well as a human being. In this case, the interrogator was not able to decide who is the human and the answers would distribute evenly giving about 50% of the votes to each candidate.

Thus, by defining the score of each measure as %Incorrect Answers x 100/50, the AR was ranked highest by 90 (out of 100).

A test group was then given a questionnaire containing human associations in both candidates. While being asked to decide which of the two subjects is human the results were 48% of the votes for candidate 1 and 52% of the votes for candidate 2. This result conformed to the expected values (50% for both candidates).

The current test had one significant difference from the original Turing Test. While in the original Turing Test, there is an unlimited set of questions from which the interrogator selects those to ask, here a given set of questions and answers are given to the interrogator.  This might have biased the decision of interrogators who might not have understood some of the terms. In order to overcome this problem, each interrogator was asked to grade his/her confidence level (0 ? not confident  5 ? very confident). By selecting only those who rated a high confidence level (4 or 5), we were able to eliminate cases where the interrogator did not understand the stimuli or responses. The measures performances are shown in the following table.

Table 3.  Distribution of Correct and Incorrect Answers for  Confidence Level > 3  Measure Correct Answers  (%)  Incorrect Answers  (%)  Score  AR 56 44 88 LCG 66 34 68 Lift 68 32 64 Klosgan 70 30 60 J-Measure 80 20 40 Although the AR score is somewhat smaller in this case  ? it is still better than other measures.

The following two sections demonstrate the ability of  the AR measure to extract valuable phrases from text.

While section 3.2 focuses on the ability to track the user actions and predict user interests while surfing, section 3.3 starts with a seed query phrase and uses the AR algorithm over the query results to extract highly informative terms.

These terms can be used to expand the query and extract more focused results.

3.2 Analyzing User Surfing History  The IRCache Internet pages caching project [9] founded in 1995, has two goals: provide operational hierarchical caching services and provide large amounts of trace log file data to researchers and other organizations. The cache contains logs of proxy request collected from ten servers in the USA. We analyzed data taken on 12 September 2004 (containing more than five million entries). The data in the log files, sanitized to hide users' identity and query terms, contained the sorted list of pages visited by the many users.

We selected four logs and focused on the html pages. The following table shows the raw list of pages (for convenience only the first four pages are presented), the results of AR algorithm (the ten highest ranked terms) and the speculated interest topic of each session.

Table 4.  Using the AR Algorithm to Analyze User Surfing  Logs and Extract the Key Interest Topics  Pages Visited (first four pages)  AR Highset Ranked Terms  Speculated Topic  1.http://afiwcweb.lackland.

af.mil/battlelab/index.htm 2.http://fmso.leavenworth.

army.mil/fmsopubs/fmsop ubs.htm 3.http://images.kazaa.com/ generic_report/272.html 4.http://images.kazaa.com/ generic_report/res_1024.ht ml  warfare; infrastructure;  defense; attack; joint; command;  force; potential  Military Warfare  1.http://mailing.worldsexci nema.com/galleries/animal fuckers/galleryaf09_y1.ht ml 2.http://publish.aps.org/ST ATUS/wvman1.html 3.http://www.aip.org/pubs ervs/style.html 4.http://www.aip.org/pubs ervs/style/4thed/toc.html  ivan; hash hurricane; split; cnn;  tropical; force; florida  Florida Hurricane  Forcast  1.http://mail.gnome.org/arc hives/gtk-list/2004- September/msg00049.html 2.http://mail.gnome.org/arc hives/gtk-list/2004- September/msg00044.html 3.http://mail.gnome.org/arc hives/gtk-list/2004- September/msg00005.html 4.http://207.242.75.40/derb tech/windtunl.htm  akron; derby; racer;  soap; rally; championship;  gtk; gnome  Car Racing  1.http://www.jhuapl.edu/st ates/fl_0.html 2.http://www.chubby- pussy.com/gal010/thehun.

html 3.http://www.cnn.com/200 4/SHOWBIZ/09/09/showb uzz/index.html 4.http://www.cnn.com/200 4/SHOWBIZ/Movies/09/0 9/neve.campbell.reut/index .html  cnntogo; endorse;  headline; cnn; escape; korean;  external; japanese  News  As can be seen, the terms extracted by the AR algorithm are good descriptors of user interest. These terms can function as the user contemporary profile and be used for further extraction of relevant web pages. Note that the fourth column (Speculated Topic) is only presented for the     reader's convenience. In actual cases the top ranked terms are selected as topic descriptors.

3.3 Extracting Highly Informative Terms  In our third experiment we began with just a simple question or a phrase. This phrase was fed into the Google search engine. The query results were analyzed by the AR algorithm, and the highest ranked terms were collected for further use. We then used the highest ranked terms to expand the original query and refine the search. The following table holds the query phrases, the AR algorithm highest ranked terms, the revised search phrase and it's first retrieved web page.

Table 5.  Using the AR Algorithm to Extract Highly Informative Terms (The extracted terms are used for query  expansion)  Original Query Phrase  AR Highest Ranked Phrases  Revised Query Phrase  Topic of the First Retrieved Page  Sleep and appet-ite  depressed; mood; anxiety; disorder  Sleep and appetite Depressed mood anxiety disorder  Informatio nal Reports on Depression and Stress  What is the origin of the human race  racial; evolution; ancient; scientific  What is the origin of the human race racial evolution ancient scientific  The Races of Humanity  What is the furthest star in the solar system  orbiting; asteroid; gravitational ; pluto  What is the furthest star in the solar system orbiting asteroid gravitational Pluto  HubbleSite - All FAQs for Solar System  Hitchc-ock movies  marnie; lodger; vertigo; psycho  Hitchcock movies marnie lodger vertigo psycho  Auteur theory of Alfred Hitchcock  Once again the AR algorithm extracted terms that served as descriptors and could be used for farther query expansion. We chose to expand the query phrase by simply adding the first four terms extracted by the AR algorithm.

A different approach would have been to give the user the list of extracted terms. The user would then be able to select from the list the most suitable terms to be added to the original query phrase.

Note that the question "what is the furthest star in the solar system?" was not stated properly, since the users meant to find the furthest Planet and not Star. The AR algorithm was able to find the term, "Pluto" (the fourth of the ranked terms) despite this "mistake". Furthermore, very common terms such as "who", "what", "is" are ignored by  the Google search engine. Thus, for example, the query "what is the furthest star in the solar system", translated to only four terms "furthest, star, solar, and system".

3.4 Discussion  This chapter presented three different approaches for validation of the AR algorithm. First - the "Free Associations Turing Test" confronts the question whether the AR algorithm can "imitate" human associations. The second - analyzing proxy logs deals with the question whether the AR algorithm can extract human-interest topics from the user's surfing history. And finally, the third part deals with the question whether the AR algorithm can help the user to better express his thoughts and reach valuable information more efficiently.

While the answer to the first question is based on a solid statistical base (we compared ten association rules measures, 100 seed terms, 70 different questionnaires and 900 people who answered the different questionnaires), the latter two were based on much smaller samples. Thus the conclusions were qualitative rather than quantitative. Still, we believe that the potential for creating a user profile or using the AR algorithm to extract highly informative phrases has been shown.

4. Concluding Remarks  The AR algorithm that extracts highly informative phrases was shown capable of both "imitating" humans by creating human-like associations and giving highly informative descriptors of Internet pages. The extracted descriptors were used to reveal the user current interest topics and to supply query expansion terms. The algorithm complexity is low, and can be performed in near real-time.

The success of the algorithm raises the question whether actual human thinking processes are performed similarly.

The algorithm is rather robust and can be employed for various usages, or to even use other asymmetrical association measures as links between concepts. One of the most promising usages we forecast for this algorithm is user conceptual modeling and prediction. By using the AR algorithm to extract concepts and associations from the pages the user visits, an analysis of "flow" in a conceptual graph can show trends and determine the probability for the user to reach certain concepts within his current surfing session. In future work we intend to identify common properties for similar users, and define a profiling technique based on the random walk algorithm.

Many other interesting challenges are still waiting to be addressed in the web search algorithms area. For a good review of the most promising is the partitioning of the web by eigenvectors see [14],[10],[8].

5. References [1]  A. Farahat, T. LoFaro, and J.C. Miller,  "Modification of kleinberg's hits algorithm using matrix exponentiation and web log records", Research and Development in Information Retrieval (SIGIR 2001), New Orleans, USA, September 2001.

[2]  A.A. Freitas "On rule interestingness measures".

Knowledge-Based Systems 12, 2000, pp. 309-315.

[3]  A.M. Turing, "Computing machinery and intelligence", Mind, Vol. 59, No. 236, 1950 pp. 433- 460.

[4]  A.P. Saygin , I. Cicekli , and V. Akman, "Turing Test: 50 Years Later", Minds and Machines. 10(4), 2000, pp. 463-518.

[5]  C.R. MacCluer, "The many proofs and applications of Perrons?s theorem" SIAM Rev. 42(3), 2000, pp.

487?498.

[6]  Gantmacher F.R. "The Theory of Matrices", Volume I, II, Chelsea Publ. Co., New York, 1959.

[7]  Google API. http://www.google.com/apis/ [8]  Haveliwala T.H., and S.D. Kamvar, "The second  eigenvalue of the Google matrix". Stanford University Technical Report, 2003.

[9]  IRCache Internet caching project.

See http://www.ircache.net/ (current 6.2005)  [10]  J. Shi and J. Malik, "Normalized Cuts and Image Segmentation", Proc. Computer Vision and Pattern Recognition, 1997.

[11]  J.R. Searl , "Minds, Brains and Programs", Behavioral and Brain Science 3(262), 1980, pp. 26- 31.

[12]  L. Kleinberg J. "Authoritative sources in a hyperlinked environment", Proceedings of the ACM- SIAM Symposium on Discrete Algorithms, 1998.

[13]  M.D. Wilson, "The MRC Psycholinguistic Database: Machine Readable Dictionary", Behavioral Research Methods, Instruments and Computers, 20(1), 1988, pp. 6-11.

[14]  M.R. Henzinger "Algorithmic Challenges in Web Search Engines", Internet Mathematics, Vol. 1, No.

1, 2003, pp. 115-126.

[15]  Michie D., "Turing's Test and Conscious Thought", in P. Millican and A. Clark, eds. Machines and Thought: The Legacy of Alan Turing, Oxford, UK: Oxford University Press, 1996, pp. 27-51.

[16]  N. Deo, and P. Gupta,  "Sampling the Web With Random Walks", Congressus Numerantium, 149 on Combinatorics, Graph Theory and Computing, Feb. 26 - Mar. 2, 2001, Baton Rouge, LA. pp. 143- 154.

[17]  N. Deo, and P. Gupta, "Graph-Theoretic Web Algorithms: An Overview", Lecture Notes in Computer Science, (eds. T. B?hme and H. Unger), 2026 (2001), Innovative Internet Computing Systems, June 21-22, 2001, IImenau Technical University, Germany, pp. 91-102.

[18]  Nelson D.L., C.L. McEvoy, and Schreiber T.A.

"The University of South Florida Word Association, Rhyme, and Word Fragment normsNorms", 2002.

See http://w3.usf.edu/FreeAssociation/Intro.html (Current 6.2005).

[19]  P. Tan, V. Kumar, and J. Srivastava, "Selecting the right interestingness measure for association patterns", Technical Report 2002-112, Army High Performance Computing Research Center, 2002.

[20]  P.D. Turney, "Answering Subcognitive Turing Test Questions: A reply to French", Submitted to the Journal of Experimental and Theoretical Artificial Intelligence, 2001.

[21]  R. Lempel and S. Morran, "SALSA: The stochastic approach for link-structure analysis", ACM Transactions on Information Systems, 19(2), 2001  [22]  R. Tamir, ?On Confidence Gain Measure for Association Rules Generation and Scoring?, VLDB Journal, forthcoming, 2005.

[23]  R. Tamir, and R. Rapp, "Mining the Web to Discover the Meanings of an Ambiguous Word", Proceedings of the third IEEE international conference on data mining, 19-22 November 2003, Melbourne, Florida, pp. 645.

[24]  R.M. French, "Subcognition and the Limit of the Turing Test", Mind, Vol. 99, No. 393, 1990, pp. 53- 65.

[25]  R.M. Shiffrin, "Modeling Memory and Perception", Cognitive Science 27, 2003, pp. 341-378.

[26]  S. Brin, and L. Page, "The anatomy of a large-scale hypertextual Web search engine" Proceedings of the 7th International World Wide Web Conference, Brisbane, Australia, 1998.

[27]  Strang G. Linear Algebra and Its Applications, Third Edition, Harcourt Brace Jovanovich Publishers, San Diego, 1988.

[28]  T.J. Palmeri, "An exemplar-based random walk model of perceptual categorization". M. Ramscar, U.

Hahn, E. Cambouropolos, & H. Pain (Eds.), Proceedings of the Interdisciplinary Workshop on Similarity and Categorisation, Edinburgh, Scotland: University of Edinburgh (1997), pp. 181-187.

