Tape Cloud: Scalable and Cost Efficient Big Data Infrastructure for Cloud Computing

Abstract?Magnetic tapes have been a primary medium of backup storage for a long time in many organizations. In this paper, the possibility of establishing an inter-network accessible, centralized, tape based data backup facility is evaluated. Our motive is to develop a cloud storage service that organizations can use for long term storage of big data which is typically Write-Once-Read-Many. This Infrastructure-as-a-Service (IaaS) cloud can provide the much needed cost effectiveness in storing huge amounts of data exempting client organizations from high infrastructure investments. We make an attempt to understand some of the limitations induced by the usage of tapes by studying the latency of tape libraries in scenarios most likely faced in the backing up process in comparison to its hard disk counterpart.

The result of this study is an outline of methods to overcome these limitations by adopting novel tape storage architectures, filesystem, schedulers to manage data transaction requests from various clients and develop faster ways to retrieve requested data to extend the applications beyond backup. We use commercially available tapes and a tape library to perform latency tests and understand the basic operations of tape. With the optimistic backing of statistics that suggests the extensive usage of tapes to this day and in future, we propose an architecture to provide data backup to a large and diverse client base.



I. INTRODUCTION The last decade has seen an explosion of data generated  by individuals and organizations. For instance, the amount of video data captured by a single HD surveillance camera at 30fs in 14 days requires 1TB storage space [1]. The number of CCTV cameras in UK alone is estimated to be 1.85 million [2]. There are more than one factors that organizations con- sider before investing in a certain type of storage infrastructure [3] [4]:(1)Longevity of the data or the intended period that the data needs to be stored or backed up; (2)Durability of the storage media which should have low susceptibility to physical damage and be tolerant of a wide range of environmental conditions without data loss; (3)Obsolescence of the storage technology or inversely, the technology?s ability to be easily updated based on the availability of newer solutions; (4)Cost or the over all expense of ownership including cost of pur- chasing and maintaining the necessary hardware, software and the media require; and (5)Capacity or the overall amount of data that need to be stored or backed up for the organization?s future use, (6)Data Criticalness or the importance of data that needs to be stored[5].

Based on these factors, organizations requiring data storage solutions can either have in house data back up facility or rely on a service provider to carry out the task efficiently and economically [6]. However, there are many intermediate  considerations that both players need to consider. Choice of storage media can be meagerly categorized on the basis of the capacity of data that need to be stored because although it is a nearly irrefutable factor, it has a close relationship with the overall costs. Storage service providers and organization will be forced to decide, in most cases, on a tradeoff between a smaller, high speed, expensive storage media and larger, low speed, inexpensive storage media [7]. A high initial investment for the operating hardware and software may force smaller organizations to discard the option although the cost of the media itself is very economical. Similarly, high media costs can create scalability issues for organizations that needs stor- age expansion on a regular basis even in a nearly automated environment.

Magnetic tapes, which started off as a primary storage media decades ago, have been preferred for backup storage of data generated by organizations for a long time now. There has been a continuous development of quality, form factor, capacity and robustness of the storage cartridges[8]. It also continues to be a very economic type of storage media. The data stored on tapes has a property that justifies this choice; in that, this data may or may not be accessed in the near future. For instance, studies of enterprise file servers show that in a three month period, more than 90 percent of the data on the servers was never accessed [9]. Tapes serve its purpose well in situations where information was lost due to natural calamity, human error or system failures. For this reason, trained personnel are hired in order to maintain and service tape drives and tape libraries [10].

Despite the advantages of tapes, there has not been an increase in its usage due to high initial investment of the operating hardware and software which force smaller or- ganizations to discard the option although the cost of the media itself is very economical [6]. This defeats the very purpose of affordable backup and limits the storage bandwidth significantly. Another important reason for the flat rate of increase in tape usage is its inability to promise high data rate transactions. The fact that tapes are linear data access media causes many processes waiting for data input/output from tapes to stay idle for longer periods accounts against tapes to be used for storing data which needs a more volatile storage environment as compared to backup or archival data.

The main contributions of our work are as follows, ? We propose and evaluate for the first time in literature,  an exciting new alternative in tape based big data stor-   DOI 10.1109/CLOUD.2013.129     ?????? ???????	? ??????????  ????????? ????????  ????????? ???????????? ????????? ????????  ????????? ??????????? ??  ???  ?????????   ???? ???!??????"?????  Fig. 1. Tape Cloud is a cloud storage service that uses magnetic tapes as the main storage media to store unstructured and big data unlike most of the commercial cloud storage solution available today.

age model that allows organizations to benefit from a large storage bandwidth without having to invest in the infrastructure itself. The crux of this approach lies in the optimization of the usage of the most affordable media.

This allows storage service providers to focus on the technicalities such as using affordable media, required infrastructure and man power to handle them and permit client organizations to concentrate monetary expenditures away from investing on the fundamentals of data backup and storage.

? The design of a cloud based storage framework provided as a service which implements the necessary middleware for seamlessly integration of large scale multi-user tape libraries with the cloud model is presented. The tape cloud is designed to work autonomously or in conjunction with the current cloud infrastructure.

? An attempt to optimize performance of the tape cloud with a multi-level hybrid storage model where hard disks are employed for providing cache spaces for data to be stored or retrieved from tape libraries(one LTO 5 tape library can provide the storage capacity from 72TB to more than 1PB) is made and the observations are reported.

? Solutions are devised to support efficient multi-user ac- cesses to tape cloud using optimized hardware config- urations and combinations, and high level scheduling approaches which are evaluated for their performance.

? The proposed tape cloud framework points to a new direction for creating service oriented, cost effective, massive scale infrastructure to meet the growing storage challenge in the coming era of big data enabled industries and research.

The paper is organized as follows. A study of economic impacts of tapes is done in section II. The system design for the tape cloud along with operational details are provided in section III. We propose models for testing hypotheses on I/O performance on tapes, details of which is discussed in section IV and results of which have been posted in section V. A note on works related to our research is given in section VI and the conclusion and our future plan is given in section VII.



II. BACKGROUND  Magnetic tapes have been in active usage over the last 60 years, a time period that is comparatively high for storage technology in the growing IT age. This has been possible due to the continuous improvement in the quality, capacity,  durability and areas of application of tapes. The dominant motivation to use tapes, however, arises from the fact that tapes are highly economical and ideal to store large amount of data which may or may not be useful to the organization in the near future. The Linear Tape-Open is a set of standards that directs development and manages licensing and certification of media and mechanism manufacturers.The standard form-factor of LTO technology goes by the name Ultrium, the original version of which was released in 2000 and could hold 100 GB of data in a cartridge.

LTO version 6 released in 2012 can hold 2.5 TB in a cartridge of the same size as its predecessors. Another very important storage media that has been around for a long time is the Hard Disk Drive. The similarity between hard disk and tapes is only the principle of using magnetic material to store data, hard disks rose to the scene as a better option to store data that needed faster retrieval. The cost of operating hard disks has been an attractive tradeoff to make to the cost of the media itself. This also has proved to be the ?tape killer? as the hardware required to operate tapes and personnel required to maintain them costs unreasonably high. A comparative study performed let us see that the monetary expenses in the form of initial investment involved create an obstruction for many small organizations to have backup solutions on tapes despite its competitive cost advantages, results of which have been shown in table I.

TABLE I COMPARISON OF COSTS OF STORING UNIT DATA ON DIFFERENT  STORAGE MEDIA  Solid State Hard Disk Tapes $/TB/yr % total $/TB/yr % total $/TB/yr % total  Media 19456 96.0 220 25.1 23 5.4 Capital 197 0.009 163 18.6 155 36.4 Maintenance 152 0.007 176 20.1 88 20.7 Facilities 221 0.010 118 13.5 56 13.1 Personnel 234 0.01 197 22.5 103 22.4 Total 20260 874 425  ?  ??  ??  ??  ???  ? ??? ??? ???  ?  ? ? ? ?? ?  ?? ? ?? ??  ? ?  ?? ???	??????  ????????????????? !?"? ??	????# $%&?$??? ?"?!'()?? *?+?????&,(??-.!( ?	??	????-????!

?????????,??????/ !?-? ?	??	??????.?? 0  Fig. 2. Operating Temperatures of different media  --  ?"-?  ??-?  .?? ???  1??  ?."?  ?  .?? ?-?  ??? ?? ?-?  "??  ?  ????  ????  ????  ??	 ?????? ?????????? !?"?  $%& $??? ?"?!'()??  *?+???? &,(??-.!(  ?	?? ???-????!

?????? ??,??????/ !?-?  ?	?? ?????.?? 0  (? ? ?2 ?3 ? ??  ?4 ?? ??  ?????	?&	??  $?5 ????62  Fig. 3. Unused State Energy Con- sumption of different media  Another contributor to costs is the overall energy consump- tion of storage media and the associated infrastructure. Power inefficiencies in storage systems can arise at two stages, when the system uses a lot of energy even in the idle state (Figure 3) and when the operating temperatures (Figure 2) of the system results in the need for external conditioning systems. A study performed with a tape drive and six hard disk drives of     #$%&?'&(??  #%?%)*??  #$?(*+?+++??  #$$+?$(+??  #+  #,++?+++  #%++?+++  #*++?+++  #-++?+++  #$?+++?+++  #$?,++?+++  #$?%++?+++  #$?*++?+++  ?????.?/????????? ???? ?????)?0??"?????????? ?????.?/????????? ???? ?????)?0????"??????????  Fig. 4. Tape and Disk: Acquisition and Energy Cost from INSIC Study [11].

The five year period energy cost of disks is comparable with the acquisition cost of the tapes with equal storage capacities.

different specification and manufacturer reveals that tape is much more power efficient than hard disk when used on a large scale such as in data centers. The fact that hard disks need to be electrically powered for operation is a mammoth disadvantage over tapes which is a non powered static storage and has very low power consumption per unit data [6]. Figure 4 shows the relative expenditure of acquiring the two different kinds of media as compared to the operational costs in terms of power required for their usage. From the perspective of the storage infrastructure service provider, scalability becomes very expensive [12] [13] [14].

The infrastructure for using tapes which, in most cases, is the library, introduces a new kind of expenses in the form of delays which need to be incurred due to the use of robotics inside the device. The delay caused can create a slowdown of I/O processes also affecting business processes of the organization [8]. We study some of these device specific delays. We conducted experiments over commercial LTO-5 tape libraries (e.g., Tandberg tape library[15]).

Figure 5 shows the diagrammatic representation of the simple tape library (the number of tape slots is a parameter that varies among different tape library products). The numbered slots are tape cartridge holders. The robotic cart runs on the rail in front of the tape driver and helps load the tapes into the driver. To complete our analysis, we have made a multi trial recording of delay of the various operations performed within the library. We can understand the basic principles of operation and create a time profile for these operations which helps us in creating faster and more efficient hardware.

Table II in section V shows the delay incurred in moving tape cartridges in the numbered slots to the drive and back to the slots after performing the operation. The average time taken for the transport of cartridges in both cases is more than a minute. Once the tape is in place, it takes nearly 30 seconds for it to load and be ready to read or write. The tape transportation cart has an upward path time of 2.6 seconds and a total end to end path time of 7.4 seconds. The robot usually performs both together during the load or unload operation from a slot at the very end of the library so time can be saved. Based on the numbers, we can get a clear time profile about the tape library operations. One of our design objectives is to reduce  ?  ?  ?  ?  ?  ??  ?    ??     ?  ??  ????????? ??????????????  ??? ???????????  ???????????  ???????????? ?!?"? ?? ??"#?$?????!%!?&'???()??  ?"'??"????????  ?????'??*????+??,  Fig. 5. Representation of the Tandberg T24 Tape Library. At the bottom is the Tape library showing parts with numbers mapped to the representation above it. Bottom right is a single LTO5 tape cartridge.

time spent on the tasks such as movement of tapes from the slots to the drive.



III. SYSTEM DESIGN  Figure 6 shows a bird?s eye view of the tape cloud architec- ture. Our effort in creating a cloud storage with a media that is known to be at the bottom of the ladder of performance, calls for alteration of hardware assembly and design. In order to get best results, we create a design for the hardware that cases the tapes along with special instructions for multiple reader rewinder sets aimed to keep the setup cost effective. Although the unit operations such as writing data onto tapes remains similar, we consider some augmentative enhancements that are specific to our case. The software of the tape cloud can be effectively termed as a middleware which operates between faster yet smaller hard disk buffers and comparatively slower yet larger tape backend storage. This middleware needs to function as an agent arbitrating various components in order to reduce the overhead caused by using a slower backend media [16] as shown in Figure 9. It also performs various other tasks such as data set segmentation, scheduling, encryption, load balancing and management of database containing the meta data and block IDs of data stored in tapes. Figure 8 provides an understanding of the middle ware that operates between the distributed file system and tape storage. This serves as an agent aiding the distributed file system in overcoming the latencies of using a slower media and also serves as an abstraction in clouds using hybrid storage infrastructure.

A. An Abstraction for Tape Library Hardware  In order to make the workflow seamless and application independent, special considerations about the hardware setup and configuration need to be performed. The massive scale of our focus encompasses multiple drives, high speed robotics that seek, grab and load tapes to drivers at high speeds and hazard resistant tape storage space. To make data retrieval and deposition more efficient, we consider a conventional driver to be split into two parts. One part rewinds the subsequent scheduled tapes to the correct position and the other part is involved in reading and writing into tapes (the read and write head also has a rewinding functionality to perform small and quick seeks so it could be the addition of a rewinder in the     ?"?1???????????	?????  2?1???????????	?????  ???????????   3?????????4 3?5????? ????????!?????????????  ??? ?!????????? ?????  ?????!?????????????  ????????????  Fig. 6. Implementation Architecture of Tape Cloud. The arrows represent the direction of flow of data.

system). This way we can pipeline the rewinding process and isolate the rewind latency from the overall read operation.

B. Multi Tier File System  FUSE [17] is a framework to help develop customized file system. FUSE module has been officially merged into the Linux kernel tree since kernel version 2.6.14. FUSE provides 35 interfaces to fully comply with POSIX file operations.

We design a file system using FUSE used at different tiers in the architecture. The file system is monolithic but logi- cally distributed and staged based on functionality as shown Figure 7. Figure 8 provides a block representation of the filesystem which is an important part of the middleware.

The collection servers (the PUT-Collection servers and GET- Collection servers) implement modules which acquire client data to be written to or retrieved from tapes. Based on client specific policies, the data to be stored on tapes is encrypted and segmented. Each of the segments are identified and accounted in local databases. Similarly, to retrieve data from tapes, the filesystem queries the local databases and requests particular blocks from tapes and converts it to the pristine data. The Tape Interface Machines (TIM) are networked to the collection servers and blocks of data are sent and received via high speed connections. The load balancing server manages a workload based scheduling system in order to efficiently distribute data to various TIMs to avoid IO bottle necks. The filesystem is highly customizable in the sense, data from clients can be blocked and stored based on preference chosen by the clients.

For example, video surveillance data, should the client be able to obtain data by the hour, must be handled differently as compared to be able to obtain data by days. So the collection servers are responsible to block these data in a manner easy for retrieval.

C. Load Balancing  The duties of the Load balancing servers are by far the most critical in the system. It is involved in many services to all other entities and ensures that there is no data clogging at a certain point in the network. First of all, it acts as a contact point to all the clients who wish to store or retrieve data from the tapes. After the client is authorized, the load balancing  ??????????  ?????? ?????? ??????  !?????????????  ??? ?????  ?6?	??? ?????? ???????  ????????!????? ????????  ????? ??????  ????? ??????  !?????????? ? "?????????? ???????  6????????????? ?????????? 7??8???  6????????????? ?????????????  3? ??  ?????,  ?????$  Fig. 7. Stages and functions of each stage of the filesystem for Tape Cloud.

Although distributed by functionality, the filesystem is monolithic across the storage system.

??????? ?????? ???	??????	?????? ?????????	?????	???  ???????	 ??? ??????  ??!???????"?	 ???	??##??  $?????""??	 ???	?????"  ????	?????"? %????  ???	&???' ?????????	?????	???  ???	????	?????? ????(??  ???"  ??? ?????"  ???	??????"  ????	$!????? )?"?	*??????  ???	????	?????? ??????  +???	?????"? %????  Fig. 9. The block representation of the middleware on the basis of roles and responsibilities within the system.

server requests a memory allocation on the collection servers where the user data is going to be collected. It also creates and manages database entries for the blocks that are created and manages redundancy to avoid data loss and increase availi- bility. The load balancing servers connect and talk directly with the metaserver and the central database which contain information about the load on the Tape Interface Machines and currently serviced clients.

D. Data Handling  a) Data Acquisition: The system?s data collection from the user can be performed in two main ways. Data sets of smaller sizes which can be uploaded directly through the internet are stored in the Collection servers. The client side has an application that requests a data upload session from the Load balancing server which then allocates the required amount of space in the collection server and initiates the client to directly upload its data to the specified location. Another data acquisition protocol is designed for very large data sets where data is received in the form of storage media units itself.

The collection servers, in this case, provide a docking interface functionality where the media can be mounted directly on the servers and the extracted data be treated in the same way as uploaded data.

3????? ? ???  92	"?:?????? 3? ??? 	???????  3??????  3???????? ???"5??  ? ??!

;?????5??????? ??? ????  ?????9  ???????  9????????????? ???  9????????????? ???  9????????????? ???  9????????????? ???  ???? ;?? ??  ???? ;?? ??  ???? ;?? ??  ???? ;?? ??  ??????????!???  ?? ??43????????? ???"5??  ? ??!

?9 ????????  ?9	? 3????????  ????????????  3?  ??8????!????  Fig. 8. Logical diagram of a tape cloud node to show the relationship between the various functional units within the FUSE enabled tape cloud infrastructure.

$ ) &  , * $+  ( ' $$  % - $,  $ ) &  , * $+  ( ' $$  % - $,  ;".? $<	"?$  $+?!?6?:  ;".? '<	"?,  $+?!?6?:  ;".? &<	"?$  $+?!?6?:  =;??" (<	"?$  $+?!?6?:  ;".? )<	"?,  $+?!?6?:  $ ) &  , * $+  ( ' $$  % - $,  $ ) &  , * $+  ( ' $$  % - $,  ;".? $<	"?$  $+?!?6?:  ;".? '<	"?,  $+?!?6?:  ;".? &<	"?$  $+?!?6?:  =;??" (<	"?$  $+?!?6?:  ;".? )<	"?,  $+?!?6?:  "?<?$ 	"?<?,  "?<?,	"?<?$  !?????	??? ?????  .????	??? ?????  ;?? ?4?=?????>??   ;?? ?4?=?????>??   Fig. 10. The Closest Process First Schedule: before scheduling(top) and after scheduling(bottom)  b) Data distribution to Tape Interface Machines (TIM): Once the data has been segmented at collection servers, it is distributed to the TIM computers. Data is distributed to a TIM based on its current IO queue depth. Overloading a single TIM computer with more than required write requests would cause a long latency in the read process as they tend to pile up at a single TIM computer. A record or a map of these segments are made and stored in the meta server.

c) Distributed Tape Write/Read: Like any other cloud service, the architecture is complex with a large number of tape drives. In order to increase the IO bandwidth, parallel reading and writing of data into multiple tapes can be envisioned. This consideration, however, needs to be analysed from more than one perspective. The larger the number of subdivisions, the larger the latency induced by seeking for the data. Yet, a huge single read or write to tape can cause an increase in the waiting time of other requests. Many task scheduling algorithms have been designed[18] to improve performance of IO in tapes and better utilize drive resources, but little has been done with the latest LTO5 tapes drives. Based on the latency test results of the tape device, we see that the seek time of the robotics takes a considerable longer time than reading or writing of data to the disk. In order to improve time efficiency, we schedule the I/O requests such that each request spends the least amount of time performing seek operations and longer periods of read or write operations.

E. Closest Process First Scheduling  From the latency analysis, we can see that the the largest time consumption is by the seek, grab and load into driver operations. In order to make the system time efficient, we need to let the system spend most of the time reading from or writing data onto the tapes. For this reason, we have to schedule read and write operations in a way that the next operation to be performed is on a tape that is either the same tape or one that is closest to the current tape. The scheduling algorithm is applied at the Tape Interface Machines after it obtains the data from the collection servers or after it receives a read read request from the load balancer. Figure 10 has a representation of how the scheduling is done at the TIMs. We evaluate our design and latency model for a scheduler called Closest Process First that does exactly this. The algorithm is shown below.

Algorithm 1 Closest Process First Scheduler Input: m tapes: ?t = {tape1, tape2, ? ? ?, tapem}  n requests: ?r = {request1, request2, ? ? ?, requestn} Output: Drive/Rewinder Seek Path  1: New Request reqx 2: Location value of reqx = Lx = Infinity 3: Record current location of the drive/rewinder Lc 4: while true do 5: wait for a request 6: if new request reqx arrives then 7: Get location Lx of process to be performed 8: for i ? n do 9: if Lx is closer to Lc than Li then  10: place reqx before i in the schedule queue 11: else 12: move to next request Li + 1 13: end if 14: end for 15: end if 16: end while

IV. SYSTEM MODELS  In this section, we describe several models to analyze system response time. The assumptions and notations for the models are as follows.

? Tseek(i) is the time to move the tape reader to the correct tape for the ith request, and load the tape to the reader.

? The data users request is loaded by blocks. Once a block is loaded, users can start processing the blocks, for example, applying the video analysis on the loaded block of videos, while the next block can be loaded simultane- ously. The size of each block is chose to minimize the average response time. The size of a block is denoted as BLK.

? Twind(i,j) is the time to wind the tape fast forward to the first byte of the jth block to read for ith request.

? T?wind(i) is the average time to wind the tape fast forward to the first byte read for ith request.

? Ttransfer(i,j) is the time to either read the jth block frome the tape for the ith request.

? T?transfer(i) is the average time to either read a block from the tape for the ith request.

? Rtransfer is the data transfer rate of the tape; Ttransfer =  Sizetransfer Rtransfer  ; Rtransfer varies from 30MB/s to 160MB/s [19].

? Rconsume(i) is the data consumption rate of the ith  request, which describes how fast users can finish pro- cessing the data that they request.

A. Response Time Model  The response time of a user request is defined as the length of the period starting from when the request is released and ending at the point all the data are fully consumed by the user.

For example, if one user requests 200 GB data given Tseek = 10s, Twind = 4s, Ttransfer = 1, 400s, Tconsume = 20, 000s, then the response time is 21,414s. If multiple requests are queued and processed in the order of their arrival only after the only after the completion of the previous request, the average response time would be very long. As discussed before, the data consumption rate is usually much smaller than the data transfer rate, therefore, it is beneficial to read data partially in order to reduce the average response time for each request.

B. System I/O Model  The I/O time based on the experiments conducted on the tape library has been analyzed to be the sum of three major components: TI/O = Tseek + Twind + Ttransfer.

Tseek is proportional to the distance between the tape drive  and the location of the tape in the library that the robot has to cover. Seeking process is very time consuming. Thus, we have to minimize the seek time by reducing both the number of the seeks and the time for moving the tape reader. For the first purpose, the system chooses a large block size. For the second purpose, the system has to use an optimal schedule for processing the requests.

Block Size: The data stored on the tape are logically organized as blocks. A block is considered the minimum unit when reading from or writing to the tapes. The block size can not be either too large or too small. If the block size is too large, it will block the following requests or make them to wait much longer. On the other hand, if it is too small, frequent seeking to perform the next IO will degrade performance. Generally, the block size is set based on the  fact that the transfer time of one block should be larger (or much larger) than the average seek time. Here is an example to demonstrate how the block size is determined. Assume that there are two requests in the queue, each of them asks to load D GB data. Since the system uses a block size of BLK, after the jth block is loaded from the tape to the hard drive, it will take BLK/Rconsume units of time to process. During this period, the reader can switch to process the other request to load the kth block, which will take Tseek(2)+Twind(2, k)+Ttransfer(2, k) to load the kth block for that request. Then the reader switches back, and loads the (j + 1)th block for the first request, which will take Tseek(1) + Twind(1, j + 1) + Ttransfer(1, j + 1). Ideally, if BLK/Rconsume ? Tseek(2)+Twind(2, k)+Ttransfer(2, k)+ Tseek(1)+Twind(1, j+1)+Ttransfer(1, j+1), the first request will not even notice that its data loading process has been interrupted. Generally, if there are w requests waiting in the queue, as long as  BLK Rconsume(i)  ? ?wp=1(Tseek(p) + T?wind(p) + T?transfer(p)), the ith request will be processed smoothly without noticing that the data loading process has been interrupted.

C. Multiple Reader Model  If there are more than one readers available, the scheduling problem is known to be NP-Hard. Since it is impossible to find the optimal solution, we propose a partitioned solution for multiple reader scheduling as shown in Alg.2. In this design, each reader is assigned to be responsible for a specific set of tapes. Each reader only stops at the tapes assigned to it and skips the others. For example, if there are two readers and 100 tapes, the first reader may take care of the first 50 tapes while the second reader are in charge of the rest. The tapes in one set should be physically close to each other.

However, this design may result in ?hot spot?, that is, one reader may be busy all the time while others are idle. To solve this, the system allows the idle readers help to process the requests, but have to be back to their own duties if requests to their assigned tapes arrive.

Algorithm 2 Partitioned Task Scheduling Algorithm for Mul- tiple Tape Readers Input: m tapes: ?t = {tape1, tape2, ?, tapem}  n tape readers: ?tr = {reader1, reader2, ? ? ? , readern} Output: Online Schedule for each coming requests  1: Assign the m tapes to n readers. Each reader will take care at most ?m/n? tapes which are close to each other.

2: Store the assignments to the global schedule manager.

3: while TRUE do 4: Wait for the next request, req.

5: Get the meta-information of the req from the database and  find the tapeid for req.

6: Forward req to the reader, which is in charge of the requests  for tapeid.

7: Schedule req at the reader locally using elevator schedule  algorithm.

8: end while     TABLE II TANDGERG T24 LOAD AND UNLOAD DELAYS  Type From To Motion(sec) Load(sec) Type From To Motion(sec) Load(sec) LOAD 1 Drive 62.4 33.3 UNLOAD Drive 1 61.6 30.1 LOAD 2 Drive 62.9 31.9 UNLOAD Drive 2 62.3 30.6 LOAD 3 Drive 64.06 32.6 UNLOAD Drive 3 62.26 30.3 LOAD 4 Drive 65.2 34.6 UNLOAD Drive 4 64.0 30.3 LOAD 5 Drive 62.42 34.0 UNLOAD Drive 5 61.3 30.9 LOAD 6 Drive 63.3 33.6 UNLOAD Drive 6 61.76 31.01 LOAD 7 Drive 64.2 31.3 UNLOAD Drive 7 62.22 30.1 LOAD 8 Drive 65.45 33.9 UNLOAD Drive 8 63.8 29.62 LOAD 9 Drive 61.8 34.0 UNLOAD Drive 9 60.7 30.3 LOAD 10 Drive 62.3 31.6 UNLOAD Drive 10 61.4 30.34 LOAD 11 Drive 63.7 32.23 UNLOAD Drive 11 61.97 33.9 LOAD 12 Drive 64.02 33.8 UNLOAD Drive 12 63.6 32.59 Average ? ? 63.64 33.1 Average ? ? 62.37 31.21  ?  ??  ?.?  ???  ?-?  ???  ?? -? ??? ??? ?1? ?"? .?? ???  !# ? ?  ? ) ??  ?? ? ??  ?  ?  ?  ??  %5??7???8	??/%?  Fig. 11. Average Response Time under Different Block Sizes

V. EVALUATION  We simulate a tape cloud node having 100 tapes and up to 10 tape readers used simultaneously to evaluate our design.

The parameters used in the simulation are measured from an actual Tandberg LTO-5 tape library [15], [19]. The physical capacity per cartridge is 1.5 TB; the data transfer rate is 140 MB/s; the rewind speed is 10 meters/sec; the tape length is 846 meters; the cartridge memory is 8 KB; the load and unload delays are summarized in Table II.

In the first set of experiments, the average response time under different block sizes are studied. Each user request requires 1TB tape data in total. In this experiment, the number of tape reader is one. User requests are randomly generated.

Data is read and processed by blocks. The consumption rate is 20MB/s. As discussed in section IV-B, once one block of data is loaded, the tape reader switch to serve the next request.

As shown in Figure 11, when the block size is between 50GB and 100GB, the average response time is minimized.

?  ??  ???  ???  ???  ???  ???  ???  ? ?? .? "? -? ???  !# ? ? ? ) ??  ?? ? ??  ?  ? ?  ?? ?  9??:??	;?	?????<?????  *?*? ?,*  Fig. 12. Average Response Time under Different Scheduling Algorithms  ?  ?  ??  ??  ??  ??  ? ?? ?? ?. .? ?" "? ?-  !# ? ?  ? ) ??  ?? ? ??  ?  ?  ?  ??  9??:??	;?	????<?????  0???????/5?65?,??????? 0????/5?65?,??????  Fig. 13. Average Response Time Using Globally Partitioned Workspace  ?  ???  ???  ???  ? ? . " - ??  !# ? ?  ? ) ??  ?? ? ??  ?  ? ?   ?? ?  9??:???	????#	??  ,???????	? ???	??5??? 3?????????	? ???	??5???  Fig. 14. Average Response Time Using Partitioned and Unpartitioned Scheduling Algorithms  In addition, if the block size is too large, the average response time will increase significantly. The average response time with the block size of 500GB is almost 1.5 times of that with the block size of 50GB.

Second, the average response time results under two differ- ent scheduling algorithms, i.e., First Come First Serve (FCFS) and Closet Process First (CPF), are shown in Figure 12.

Up to 80,000 requests are generated and their response times are measured. According to the results, the CPF scheduling algorithm saves around 80% of the response time.

When handling multiple tape drives, we either have the option of creating an any-tape to any-drive environment where a drive has access to any tape in a limited system or we can partition the system such that a drive services only a specific set of tapes, referred to as Global partitioning. Global partitioning also avoids uncontrolled tape seek time Tseek which, from our study, can be seen to be most expensive.

Employing global partitioning decreasing the average response time for requests as shown in Figure 13.

We also found that the scheduling algorithms get affected when we use global partitioning. The average response time results when using multiple readers are evaluated in Figure 14.

One or more (up to 10) tape reader are enabled to work simul- taneously. Two different scheduling algorithms are compared, the partitioned algorithm shown in Alg.2 and the unpartitioned algorithm, which allows each reader to serve any tapes in the library. The results are presented in Figure 14. As the number of working tape readers increase, the average response time is reduced accordingly. However, when the number of tape readers goes beyond 5, both scheduling approaches show     little changes in response time. This is because under the current workload, 5 readers are able to serve the requests efficiently. Even if more readers are added, some of them will be idle for most of the time. Figure 14 indicates that the partitioned scheduling algorithm performs much better than the unpartitioned one.



VI. RELATED WORK  In spite of the potential to be the most scalable and cost efficient solution for meeting the growing storage demand, cloud based services leveraging tapes have received little or almost no attention from the academic research community.

To our best knowledge, our paper is the first and the only one that provides a complete tape based service model and integrates large scale tape infrastructure with the cloud. In industry, there has been a long history of improving the existing and designing new tape libraries for enterprise cus- tomers (e.g., Hewlett-Packard StorageWorks ESL/EML, IBM TS3400/TS3500, Quantum Scalar, Oracle StorageTek). These tape libraries are designed as end-user products for the enter- prise users to own their own on-premise tape infrastructures.

Our efforts represent an opposite off-premise cloud based direction with the objective to provide low cost tape based storage to the users without the requirement to own a private tape infrastructure.



VII. CONCLUSION AND FUTURE WORK  We present and evaluate a design to provide a cloud storage system with tape media as backend which is a first in its kind which implements centralized data storage facility using tape media. We recognize some of the barriers of using tape tech- nology both economically and operationally. Some solutions and ideas are evaluated in order to reduce the consequences faced by these barriers and provide a smooth performance in data storage and retrieval process. From the results obtained in design evaluation, we can see that there is an increase in I/O throughput using the scheduling and parallel I/O models that are discussed in the paper, thus paving way for the possibility of having large scale operations using these techniques.

The speed of data retrieval is critical for a cloud service.

Tape, by nature, has been used for backing up data and creating archives which means that request for data stored on tapes arrives with a probability that varies well below 1 as compared to other day-to-day cloud storage. In this case, the tape cloud data centers provide sufficient security and safety against theft, fire and natural calamity as compared to on site backing up.

One of the most exciting aspects of our research is the opportunities it presents for future work. Understanding the economics of revisiting a legacy system to solve the data explosion problems of today requires an overhaul of nearly every piece of technology associated with the storage system.

An interesting study to perform is the relation between the number of media units and the number of drivers used in large scale deployments. Also we would like to implement an easy and direct interfacing system which allows the tape cloud to be connected directly to major cloud storage providers such  as Dropbox or Google Cloud and see the effects of using tape media in conjunction with the storage media used by other storage providers. Other plans of extension include the evaluation of tape cloud in applications which require much higher I/O throughput.



VIII. ACKNOWLEDGEMENT  We would like to thank the reviewers for their comments which significantly improved the paper. This research is par- tially supported by the National Science Foundation under Award Number CNS 1205708. The conclusions contained in this document are those of the authors and should not be interpreted as representing the opinions or policies of NSF.

