

Discovering Relational Patterns across Multiple Databases* Xingquan Zhu"3 and Xindong Wu2  'Dept. of Computer Science & Eng., Florida Atlantic University, Boca Raton, FL 33431, USA 2Dept. of Computer Science, University of Vermont, Burlington, VT 05405, USA  3Graduate University, Chinese Academy of Sciences, Beijing 100080, China xgzhu@cse.fau.edu; xwu@cs.uvm.edu  Relational patterns across multiple databases can reveal special The problem of finding global patterns is surely important in pattern relationships hidden inside data collections. Existing reality, as it reveals knowledge which is unavailable from each research in data mining has made significant efforts in single database point of view. There is, however, another discovering different types of patterns from single or multiple problem involved in pattern mining from multiple databases - databases, but how to find patterns that have a higher support in discovering relational patterns and their relationships across database A than in database B with a given support threshold UX is databases. Taking a retail store with two branches A and B as an still an open problem. We propose in this paper DRAMA, a example, if a store manager were organizing data from these two systematic framework for Discovering Relational patterns Across branches for intelligent analysis, he/she may easily raise concerns Multiple dAtabases. More specifically, given a series of data like (1) what are the frequent patterns in both A and B? i.e., (A > collections, we try to discover patterns from different databases ux) & (B . ux), where Ux is the threshold in finding frequent with patterns' relationships satisfying the user specified patterns, and A > ux means that a pattern's support value in constraints. Our method seeks to build a Hybrid Frequent Pattern database A should be no less than the value u; (2) what are the tree (HFP-tree) from multiple databases, and mine patterns from frequent patterns which appear more often in A than in B, i.e. A > the HFP-tree by integrating users' constraints into the pattern B . u; and (3) what are the patterns whose support differences in mining process. these two stores are no less than the value a, i.e., IA-BI 2 a. There  are possibly many other concerns in this regard, but unfortunately, 1. Introduction no systematic solution has been proposed to address this issue in  an effective way, such that the discovered relational patterns canMany real-world applications involve the collection and management .multiple databases.Examplesincludemarke be used to support efficient and effective data and knowledgemanagement of multiple databases. Examples include market magent  basket transaction data from different branches of a whole sale management.

store, data collections of a particular branch in different time In reality, when users are exposed to the data collected from periods, census data of different states in a particular year, and multiple sources, it is a natural sense to refer to a contrast study data of a certain state in different years. For years, knowledge for knowledge and pattern discovery. Examples include national discovery and data mining (also referred to as KDD) [1-2] has census data analysis, network intrusion detection, and molecular been proven to be an effective tool to search for novel and genetic data analysis. We list here two motivating examples.

actionable patterns and relationships that exist in the data. When Example 1: Considering a data expert who is interested in patterns take the form of association rules, existing research in studying residents of north eastern states of America (i.e., the so the area has made significant efforts in discovering patterns called "New England" area including the states of Connecticut (frequent itemsets, closed patterns or sequential patterns) from (CT), Maine (ME), Massachusettes (MA), New Hampshire (NH), different types of data environments, with solutions roughly fall Rhode Island (RI), and Vermont (VT)), this expert may be also into the following three categories: (1) finding patterns from a interested in finding the similarity/differences between residents single (large volume) database; (2) finding patterns from multiple in this area and the residents on the West Coast, say California databases; and (3) finding patterns from continuous data streams. (CA). For these purposes, the following queries are likely to be The essential goal is to enhance mining algorithms such that they raised by the expert.

can scale up well to large volumes of (centralized, distributed or continuous) data. Query 1. Finding patterns that are frequent with a support level ofc ) in all of the New England states, but significantly infrequent To find patterns from multiple databases, a common concern is to with support level of/ in California, i.e, {(CT. ux) & (ME. ux) & discover knowledge which does not exist unless one unifies all (MA> x) & (NH. x) & (RI> x) & (VT.o)} & {CA < P}.

data collections into a single view. For this purpose, existing Query 2. Finding patters that are frequent with a support level of research has been mainly targeted to discovering global patterns, in the New England area w. r. t. all states i.e.

with assistance of a local data mining process. Collective data t(CTinMeMNew England a w l s mining [3] is one of the most representative efforts in the area {(CT+ME+MA+NH+RI+VT) . u} with the objective of unambiguous local analysis that can be used Query 3. Finding patterns that are frequent with a support level of as a building block for generating the correct global results. A a in all New England States, but with their supports declining common practice is to conduct data mining on each single from northern to southern states, i.e., {ME > (NH VT) > MA > database, and then forward promising meta patterns to a central (CT RI) . ux} place for analysis [4]. Example 2: Recent development in microbiology and  ___________________________________bioinformatics has made it possible to extract gene expression * This ~~~~~~~~~~~~~datafor molecular genetic analysis. One of the most importThsresearch has been supported by the US National Science apiain st s uhgn xrsindt o eei  Foundation (NSF) under Grant No. CCF-0514819 and the National diesprfln,orxa l,thmlcurcnerlsiiain Science Foundation of China (NSFC) under Grant No.60674109. dsaepoilg o xml,temlclrcne lsllao     [5]. In order to detect signature patterns for Leukemia, say Acute one has to list all the candidates, if he/she does intend to do so.

Myeloid Leukemia (AML) and Acute Lymphoblastic Leukemia Therefore, PPM will concurrently mine patterns from all other (ALL), a microbiologist can split the underling data into four parts (CT, ME, .., and VT), and then pass on the patterns to CA to datasets, with D1 containing gene expression data of normal verify whether they satisfy CA < P.

tissues, D2 containing data ofAML tissues, D3 containing all ALL tissues, and D4 containing all other cancer tissues. Queries of the Both SPV and PPM rely on the results discovered from a single following types can then be used to capture the signature patterns database for pattern verification, where the mining process for cancer classification. (candidate generation and pruning) at each single site does not  consider the existence of other databases at all (unless the Query 1: Finding the patterns thatare frequen wt a sut patters were forwarded to other databases for verification). As level ofi t in either of the cancer datasets: D2, D3, or D4, but are we will discuss later, this single database based framework will significantly infrequent in D1. i.e., {(D2 |D3 |D4) . } & {(D1 < forbid both SPV and PPM from answering some complex queries.

f3)} However, this disadvantage can be overcome by CPM, which Query 2: Finding the patterns that are frequent with a support unifies all databases in the query into one view for candidate level of x in all cancer datasets, but with support in Leukemia generation and verification. The theme of CPM is to generate tissues higher than other cancers tissues. i.e., {(D2 D3) . D4 . ( length-/ candidates from each single database, with all candidatesforwarded to a central place for candidate justification, such that There are many other applications, aside from the above two only candidates satisfying certain conditions are redispatched to examples, that users will have to deal with data from different each database for the next round of pattern growing (length-/+i).

sources. In addition, it is often the case that users know some This procedure repeats until no more candidates can be further basic features of these data collections, such as the date and time generated.

each database was collected, or the region or entity each database All the three methods above can somewhat fulfill the goal of may represent. What remains unclear is the relationship of the finding relational patterns across multiple databases, although notpatterns hidden across multiple data collections. As a result, the necessarily for all types of queries. For example, SPV and PPM needs of comparing patterns from different datasets and cannot possibly answer Query 2 in Example 1. Because a patter understanding their relationships are emerging. For example, the satisfying {(CTHMEMAHANH-RI-VT) . u} can take any support store managers may want to find gradually increasing shopping value in each single database. For example, if a patten's support patterns of their customers in a certain period of time, or a  ,microbiologist may want to find patterns of the diseases along an wa 0 in CT ME MA If n I u nV,i ol tl evolving order. For these purposes, discovering relational satisfy the query. To find such patterns, SPV and PPM have toevoling rderForthes puroses disoverng rlatinal list all possible candidates (by setting each database's thresholdpatterns across multiple databases can be a very important part of  v alltossich isatesn(by ginfeachbdatabase's theshold the KDD process. Although well motivated, the solution to this value to 0), which is technically infeasible. In fact, the most end, however, requires an efficient mechanism for complex serious disadvantage of all these three methods lies in the fact quering and mining on multiple databases. that they are all Apriori-based, where pattern generation and  database rescanning for verification will significantly reduce their 2. Simple Solutions and Challenges speed in finding relational patterns. It is commonly recognized  that database rescanning for pattern verification could be very In a naive sense, the problem of discovering relational patterns time consuming, especially when the underlying data volumes across multiple databases can be solved by three simple solutons: are large. Therefore, we need a fundamentally different design (1) Sequential Pattern Verification (SPV); (2) Parallel Pattern which should take the following concerns into consideration in Mining (PPM); and (3) Collaborative Pattern Mining (CPM). discovering relational patterns.

SPV starts pattern mining from a seed database (which can be a (1) Being able to unify all databases in the query to fulfill the subset of a database in the query) and then passes on the pattern discovery process. In other words, conducting discovered patterns to the second database for verification. Such pattern mining from a single database without considering a sequential process repeats until patterns have been verified by all other databases is not an option for us.

all the databases involved in the query. For example, to answer Query 1 in Example 1, SPV may start from the CT database to (2) Being able to meet all queries listed in the above two find frequent patterns, then pass on patterns to database ME to examples. In Section 4, we will formally define our find patterns frequent in both CT and ME. Any patterns which do problem and queries, which should also be addressed by not satisfy the query will be pruned out immediately. This our solutions.

process repeats until all the databases in the query have verified (3) Being able to scale well to large data volumes and can be the pattern. easily extended to discover other types of relational Instead of verifying patterns in a sequential way, PPM patterns other than frequent itemsets.

concurrently discovers patterns from each single database, and In this paper, we take the above concerns into consideration and then forwards all frequent patterns (from each single database point of view) to a central place to find the ones which satisfy the proposeea brid atte HFP)tree base sution Ourmethod seeks to build a single HFP-tree for each query, wherequery constraints. For example, to answer Query 1 in Example 1, ptengerioadvrfctonufyheneligdtbss PPM concurrently discovers patterns from each single database t pe ptepuigpoes xeietlcmaiosfo (CT, ME, .. and VT), and then checks whether a pattern satisfies bt ytei n elwrddtbsswl eosrt htti the query or not. One should be aware that it is technically faeokcnsgiiatyehnetesedi idn infeasible to find patterns which satisfy CA <fD by using database relational patterns, where the improvement canl be as much as CA only, because no deterministic pruning rules will hold and over 100 times better than simple solutions.

3. Related Work of such a query must explicitly specify one single database and its corresponding threshold value. Their method, however, cannot  The problem of handling data from multiple databases is a answer a complex query like Queries 2 and 3 in Example 1, and nontrivial task in reality, and it often raises concerns like how to therefore its applicability is limited in reality.

compare or unify different parts of data to achieve a common goal. Domains of applications include classification [6], frequent Table 1 Two toy datasets D1 and D2 itemset mining [7-8], clustering [9], and OLAP [27]. For example, Database D1 Database D2 Yin et. al [6] have previously proposed a CrossMiner for Trans ID IItems Trans ID Tttems classification from multiple databases. The problem of 1 a, b, d 1 {c, f, g} association rule mining from distributed databases has also been 2 la, b, d} 2 {a, b, d,g} well studied [10-14], where count distribution, data distribution, 3 a, b, c, d 3 a, b, c and candidate distribution are three basic mechanisms for 4 la, c, d, g 4 la, b, d;  4_ la, c,ed,g____4 la, __b__d effective mining from multiple databases [14]. However, among 5 lb, d, f} 5 la, c; all these research activities, the focus has typically been on 6 la, b, d, g} 6 le, c, d mining a single database (whether it is distributed or centralized), 7 le, f, d; 7 {a, c, d, f,gi with the objective of unifying patterns discovered from each 8 la, b, c, e, g _ single database into new knowledge and patterns. In comparison, 4 Problem Definition our research focuses on finding patterns and their relationships across multiple databases. A pattern, P, discussed in this paper takes the form as an itemset, When the underlying data involve multiple (distributed i.e. a set of items which satisfy the user specified constraint(s).

/centralized) sources, one of the most important tasks is to assess The support of the pattern P in database D, denoted by Sup P the similarity between the datasets, such that the structural represents the ratio between the number of appearances ofP in D information among the databases can be provided for analysis and the total transaction number in D. Unless specified otherwise, such as clustering. [15] and [16] have previously addressed the we always use this ratio to denote a pattern's support.

problem of database similarity assessment by comparing association rules from each component database, e.g. how many The users' constraints specify the patterns they intend to discover of those rules are identical, and what are the numbers of instances from the database. For example, a user can specify {D . a} to covered by those identical rules? In comparison, we are interested indicate that he/she is intending to find patterns from database D, in finding patterns across multiple databases. with all qualified patterns' support larger than the given threshold  Tr. A user can specify multiple databases in their constraints, forThe importance of finding differences between databases has example {A . B . a}, which indicates a pattern with its support been noticed by many researchers in the area [17-20], with the values in A and B both larger than u, and in addition, a pattern's main focus on exploring differences between two databases at a support in A should be larger than its support in B. In this paper, time. Webb et al. [18] proposed a rule based method to explore a wepdefine thollogtw typesofpreltinshIp tors anfr contrast~ ~ ~~sebetwee tw daaae.J ta [0 aepooe we define the following two types of relationship factors and fourcontrast set between two databases. Ji et al. [20] have proposed operators to describe a user's constraints.

methods to explore minimal distinguishing subsequence patterns between two datasets, where the patterns take the form of Relationshipfactors: "frequent in database A, but significantly less frequent in * X2 oa(X> a) indicates thatXis no less than a (Xis larger database B", i.e. {(A . u)&(B < f)}. All those methods are than a) interested in finding differences (in terms of data items or * X< a ( X< a) indicates that Xis no larger than a (Xis less patterns) between two datasets, but cannot support the complex than a) queries we mentioned in Example 1.

in Operators: The research in database queries has made significant efforts in * X + Y indicates the operation of summing up the support supporting data mining operations [21-23], with extensions of the values in both Xand Y database query languages to support mining tasks, but most of * X - Y indicates the operation of subtracting the support in Y these efforts focus on a single database with relatively simple from the support in X query conditions. Among them, the most relevant work related to * X& Y(X Y) indicates the operation ofXand Y( Xor Y) this research is the complex mining optimization system * Il indicates the absolute support value in X.

proposed by Jin and Agrawal [22]. They presented an SQL-based mechanism from querying frequent patterns across multiple Notice that "+" directly sums up support values from participant databases, with the objective of optimizing the users' queries to databases. The results from this operator do not reveal patterns' find qualified patterns. There are, however, essential differences support values from the union of the participant databases. This between their work and what we will propose here. (1) The operator is helpful when a data manager intends to find the efforts in [22] only focus on the problem of enumerating different average support of the patterns from multiple databases.

query plans and choosing the one with the least cost. The pattern A user's query is simply the user's constraints, taking the form of mining methods they adopted are actually the simple solutions as  a combination of the above relationship factors and operators, inwe discussed in Section 2. Instead of optizing queres, our * findng relaIonal patterns across multiple databases. Moreresearch Will propose a data mining framework in supporting usrsqure to fin reaioa patens (2 Beas .oh specifically, a query should involve at least one database and one  limitations of their pattern mining framework (relying on each relationship factor, say {A . ut}. A query may also involve single database), the solution in [22] can only answer simple multiple relationship factors and multiple operators, which is queries like {(A . u1l) & (B . ?(2) & (C . ,8}, i.e., each element often the case in reality, such as the query {ME > (NH VT) >     MA > (CT RI) . ux} in Example 1. A pattem which satisfies the = {a 1:0}. It is obvious that z)currently has no child, so we build user's query is called a relational pattern. Due to the limitation of another node z1= {b 1: 0}, and set this node as the child of H. It the pattern mining process, a user's query cannot take an arbitary means that itemset {ab} has appeared once in D1 but 0 time in D2.

form as he/she wishes, instead, we confine that a query must Finally, we move to the third item d in T. We find d is not a involve at least one relationship factor . (or >) with a numerical 1 threshold value immediately following this factor. A query which child of the recently built node z), so we build another new node complies with this confinement is called a valid query. For 2= {d 1:0} and set it as the child of zl. Again, it means that example {A . B . C} is not a valid query; and however, {A . B itemset {abd} has appeared once in D1 but still 0 time in D2. For . C . ux} is. The reason we define a valid query is because any other transactions in D1 or D2, we will repeat the same without a specific threshold u, it is technically infeasible to find procedure. Take the third transaction in D2, T72 ={a, b, c}, as an all patterns satisfying {A 2 B 2 Cf.3 example. We first check whether Root has any child node named The procedure of discovering relational patterns across multiple a, and since we have previously constructed such a node, we databases is an interactive process, where a user provides a query know for sure that it does exist. Denoting this node by x, we will and the system finds all patterns satisfying the query, in an increase x's frequency count for database D2 by step 1, then we effective way. In this paper, we only deal with the problem of check whether x has any child node named b, i.e., the second item pattern discovery. We assume that users' queries and the of T . We increase the frequent count for D2 by step 1, if such a underlying databases are immediately available. The problems of 3 effective/efficient user interactions and data privacy/security are node indeed exists; otherwise, we simply add a new member {b not of our concern at this stage. 0:1 } as the child node of x. We recursively repeat the above  procedure until we finish the last item in T 2. The constructed 5. Hybrid Frequent Pattern Tree Construction HFP-tree for D1 and D2 is shown in Figure 1.

The frequent pattern tree (FP-tree) [7] is a well-known data To speed up the tree transversal, a header table is built for all structure in mining frequent itemsets. The merits of the FP-tree items ever listed in the HFP-tree. As shown in Figure 1 (for a lie in the fact that it stores the set of frequent items of each clearer presentation, we only list the header table for items a, c, e, transaction in a compact structure, which can avoid repeatedly and g). For each item, say g, its list records all the locations scanning the original database during the mining process. In this where g has ever appeared in the HFP-tree. The purpose of the section, we proposes a solution to have multiple databases joined header table is to facilitate the access of the item sets ending with together to build a single Hybrid Frequent Pattern tree (HFP-tree), the same item letter. For example, in Figure 1, if we want to find which will be used to discover relational patterns at a later stage. the set ending with pattern letter c, we may simply go through all  Different from the traditional FP-tree which works on a single records of c's header list, and at each location, tracking upwards database, the purpose of an HFP-tree is to find the set of frequent to the Root will produce an item set associated with item c. In itemsets from transactions in all databases. For this purpose Figure 2, we have listed detailed information of building an HFP- changes and extensions have been made accordingly. As one of tree from multiple databases. But before we go any further, we'd the major changes, each node of the HFP-tree takes the following like to solve a particular issue raised by multiple databases.

form {x Y Y2: ... :Yn}, where x is the name of the item stored at  . . . * ,, Header Table ~~~~~~~~~~~~~~HEP-Treethe current node (denoted by item name), and yl, Y2,.,Yn are the header Tabl numbers of times that a particular itemset has appeared in databases D1, D2, .., D, respectively. Take D1 and D2 in Table 1a 0 ----------------------------------1-------3 as our example databases. Assuming they are joining together to b-a ----6:---- C -- construct an HFP-tree, each node in the tree will take the form of b4---c 1:f-1 {x Y1:Y2} withy1 and y2 denoting the numbers of times that the d d 1:0 Q f 0 'c 0:1 itemset, with items starting from the Root and ending at the e ------ c 2 d2d1 f:0 10 1d1d current node x, has appeared in databases D1 and D2 respectively. f. ---\-\--- If D1 and D2 are joining together to build a tree, they must agree g e 1 0 d 10: with, in advance, the order of the items listed in the tree. Here, we assume D1 and D2 agreed to list their items according to the 1 0 alphabetic order (we will discuss the generation of this list in Section 5.1). We also discard any threshold value at this stage, Figure 1 Example of constructed HFP-tree forD1 and D2 in and therefore all items will be added into the HFP-tree. Given a Table 1.

transaction in D1, say Trans 1 T1l ={a, b, d} where items have 5.1 Joint Ranking List been sorted according to the alphabetic order, the HFP-tree In the above example, we assume that all parts participating in construction will start from the first item, a, and check whether the HFP-tree construction use the same predefined item list (the any child node of the Root has the same item_name. Since we alphabetic order of the items). In reality, the order of the list playsknow the HFP-tree is empty at this stage, a is not a child of Root. an important role to build a compact HFP-tree. Take a dataset As a result, we construct a new child node z) {a 11:0} for Root, containing four transactions {d}, {c, d}, {b, c, d}, and {a, b, c, d} which specifies that a iS a child of Root with a appearing once in as an example. A frequent pattern tree built by using the items' D1 and zero time in D2. After that, we move to the second item b alphabetic order, i.e, a, b, c, and d, will have 10 interior nodes  in T1 , and check whether b is a child of the recently built node K) (excluding the Root). On the other hand, if items were previously ranked in the descending order of its frequency, i.e., d, c, b, and a,     the corresponding FP-tree will have 4 interior nodes only, which j 1 M S-S R is about 60% of tree size reduction. Reducing the tree size will R Z R (2) eventually lead to dramatic time saving in building the pattern Mm S tree. To solve the problem, the original FP-tree algorithm [7] 5.2 HFP-tree Construction scans the database beforehand to produce the ranking list, and then use this list to build the FP-tree. Figure 2 lists the algorithm details in building an HFP-tree from  multiple databases D1,-.DM. We assume here that each database When several databases join together to build an HFP-tree, a Di comes with a minimal support threshold in determining its simple solution is to use a predefined item list to build the HFP- frequent items. In the next section, we will explain the details on tree. This, however, will significantly deteriorate the system how to parse a user's query to generate such threshold values.

performances, because any list without taking item frequency into consideration will lead to an inferior solution and eventually 6. Discovering Relational Patterns Using HFP- raise the cost in tree construction. For this purpose, we propose a rank-join based ranking mechanism. Tree  Given M databases D1, D2, .., DM for HFP-tree construction, 6.1 User Query Decomposition assume I, I2, .., and IN are the union of the items in the databases. As we have discussed in Section 4, a user's query may involve For any database Di, we scan it and rank all items in Di in a multiple relationship factors and operators. When submitting descending order of their frequency. Denoting Rj the ranking such a complex query to the data mining model, it is often the order of item Ij in database Di (with the first item in the list case that not all parts of the query comply with the down closure denoted by 1), then Eq. (1) will represent the average ranking property, i.e., the subset of a frequent itemset may also be order of each item Ij. The final ranking list for all items is frequent. For example, the "<" and "<" relationship factors  constrg R j ,2,..N, in an ascending order, normally do not comply with the down closure property. It isconstructed by ranking R ', j=1,2,. .N, in an ascending order, obvious that even if a pattern iB, say {abc}, does not satisfy B < where items with the least average ranking are listed at the top. ,, but its superset, say {abcd}, may still comply with B . 3  i = I M Therefore, the mining process must preprocess a user's query and M Yi=1 1 explicitly decompose it into a set of subqueries which do comply  The above mechanism joins the ranks of each item in all with the down closure property, such that the mining model can databases together to produce the final ranking. By doing so, we use these subqueries to facilitate the candidate pruning process.

assume that databases are equally weighted, and the rank in all For this purpose, we list five properties here, and will use these databases plays an equal role in deciding an item's final ranking. properties to decompose each query before it is submitted to the In reality, the size (number of transactions) of the databases data mining model. All decomposed subquieries (which comply involved in the query may vary significantly, where a database with the down closure property) are placed into a Down Closure containing more transactions should carry more weight in (DC) subset, and meanwhile the original query is still kept to deciding the final ranking of a particular item. For this purpose, check a pattern's validity at the final stage.

we revise Eq. (1) by taking the size of each database into Property 6.1.1 If a subquery has a single database and a consideration. Assume Si is the number of transactions in Di, then threshold value x listed on the left and right side of the S=Sl?S2?..?Sm denotes the total number of transactions. The relationship factor "2" or ">" respectively, then this subqueryweighted average ranking order is then represented in Eq. (2). complies with the down closure property.

Input: Databases D1, , DM, and their minimal support thresholds a,,.., am. This property is based directly on the Apriori rule in frequent Output: Hybrid Frequent Pattern tree, HFP-tree itemset mining. If a pattern P's support in a database is less than  1. Initialize an empty HFP-tree with node Root only a given threshold cc, then any supersets ofP (the patterns growing 2. Scan each database Dl,...,DM once, and calculate the ranking order of each from P) will also have their support less than ux. Therefore, if a  iteml,b.,IN in each single database (items with their support less than the query involves multiple databases, factors"2" or">",anda corresponding~~ ~ ~ ~ ~ ~ ~ qer involvesdmutil datbaes facors">"ort>" ndcorresponding threshold are eliminated). single threshold value U, we may decompose this query into a set3. Use Eq. (2) to produce -a joint ranking list L of subqueries with each single database and the threshold value uX4. For each transaction 7k in Di, sort items in Tk according to the listL. Denote listed on the left and right sides of the factor. For example, the  sorted Tk by tk with items in Tk denoted by I,..h query {A . B . C . a} can be decomposed into three subqueries 5. e- Root; K -- (A . cc ), (B >.u ), and (C >.u ), and placed into the DC set. It is 6. For all children x of z obvious that if a pattern P violates any one of these three  a. If x.item_name = I_.item_name, increase the corresponding subqueries, there is no way for P, as well as P's any supersets, to frequency count by step 1 (the one corresponding toDi) be a qualified pattern. It is worth noting that subqueries in the DC i. L <- x, K - K+1. Repeat step 5 until K=K. set are merely for pattern pruning purposes, and one should not  b. If no child of zhas item name I' .item name, create a new node y use them to replace the original query. The original query will . . K . still be used to verify the patterns at the final stage (as we willwithy.item name= IT.item name. Initializey's frequency to zero discuss in the next subsection).

except for Dd, which is set to l1.

i. Inserty as z)'s child Property 6.1.2 If a subquery has the sum ("-+") of multiple  ii. z)*y, K *- K?1 . Repeat step 5 until K=K. databases and a threshold value a listed on the left and right side 7. Repeat step 4 for all databases D1,............,DM,then return the constructed HFP-tree of factor "2" or ">" respectively, then this subquery complies  with the down closure property.

Figure 2 Hybrid Frequent Pattern tree construction     For example, a subquery like {(A+B+C) . a} complies with the 6.2 Relational Pattern Discovery Using HFP-tree down closure property, and can be directly put into the DC set.

The proof of this property is trivial. Given a pattern P and any of The construction of the HFP-tree ensures that the set of frequent its subpatterns Q, assuming P's and Q' s supports in A, B and C itemsets for transactions in all databases can be enclosed into a are subpattemP3sn Q,assumingQ'sresp y itisupobs thAt Q .C compact tree structure, but this does not automatically produce are PI, P2P3Dan DJf(D2?D?3Dec 2iv then it is obvious that the relational patterns to meet our needs. In this subsection, wePI, '2, '3 '3 V]'2P3) t introduce the HFP-tree based mining process in discovering(Q1+Q2+Q3) . (P1+P2+P3) . a. Therefore, the property is true. relational patterns. Figure 4 gives the pseudo code of the mining This property states that if a subquery sums up multiple databases togP gprocess, which mainly consists of two procedures: HFP-mining and is followed by factors "2" or ">" and a threshold value a, and HFP-growth. In the main procedure, HFP-mining, an input then it should be placed into the DC set for pattern pruning. query Q is first decomposed into a set of subqueries (DC). Then Property 6.1.3 If a subquery has the support difference of two the system recursively calls HFP-growth to discover relational databases, say (A-B), and a threshold value ux listed on the left patterns from the HFP-tree, where the DC set is used to prune out and right side of factors "2" or ">" respectively, then this unnecessary candidates on the fly, and the query Q is used at the subquery can be further transformed into a subquery like A . ( final stage to assert the validity of the patterns.

which still complies with the down closure property. Given an HFP-tree built from the multiple databases, the HFP-  It is obvious that if (A-B) . a, then A . (B+a). Since a pattern's growth first checks each node ai in the header table of the tree.

support in a daabascanotbnegtivesoehaeA xBecause the header table has recorded the locations where ai hassupport in a database cannot be negative, so we have A . a.

ever appeared in the tree, we can start from each of ai's locations Property 6.1.4 If a subquery has the absolute support difference lij and track upwards towards the Root, which will produce a of two databases, say IA-BI, and a threshold value cc listed on the hybrid prefix path HPPij for ai (w.r.t. to the current location 1-).

left and the right side of factors "2" or ">" respectively, then this Figure 3 pictorially demonstrates the concept of a hybrid prefix query can be transformed into a subquery like {(A . a) (B a)}, path for item g of the HFP-tree in Figure 1 (for simplicity, we which still complies with the down closure property. only show branches involving g). In Figure 3, g's header table  has recorded six (denoted by 6 digital numbers from 1 to 6). For It is obvious that if IA-BI 2 a, then we have (A - B) . a or (A - B) each location, say location 1, tracking from g upwards towards < -u, which lead to the inequations A . (B+-x) or B . (A+a), i.e. the Root will produce a set {ecba}. We replace the support of {(A . a) (B. a)}. For any pattern P, if its supports in A and B each item in the set by the current support of g, and it will are both less than u, there is no way for P's superset to have a produce a path {e 1:0, c 1:0, b 1:0, a 1:0}, which is called a higher support than ux. Therefore, it still complies with the down hybrid prefix path (HPP) for g. It is understandable that an HPP closure property. records items (and their frequencies w.r.t. to each database)  which co-occur with g and have a higher rank than g in the list L.

Property 6.1.5 A subquery involves relationship factors "<" or Parsing all the HPPs of g should be able to produce frequent "<" will most likely not comply with the down closure property, itemsets associated with g (The HFP-growth will start from the and therefore cannot be placed into the DC set. item with the lowest rank for pattern growth). For this purpose, With the above five properties, we can decompose most complex for any item in the hybrid prefix paths of g, we sum up its queries into a set of subquries which comply with the down frequencies (w.r.t. to each database) from all locations, which closure property, and use the DC set to support efficient pattern will directly indicate whether this item is frequently associated pruning. For example, Query 3 in Example 1, {ME > (NH VT) with g or not. For example, the other five hybrid prefix paths in > MA > (CT RI) . ux} can be decomposed into a set of Figure 3 are {d 1:1, b 1:1, a 1:1}, {f 0:1, dI 0:1, c 0:1, a subqueries like ME . u, (NH VT) . u, MA > u, and (CT RI) . 0:1}, {d 1:0, c 1:0, a 1:0}, {fI 1:0, d 1:0, a 1:0}, and {fI 0:1, u, which will be used to check all candidates during the pattern c 0:1 }. The total frequencies of items in g's hybrid prefix path growing process. are Freqg={a 4:2, b 2:1, c 2:2, d 3:2, e 1:0, f 1:2}.

Dividing all the frequency values by the total number ofHEP-tree MetaHFP-treehfgg transactions in each database (Dp=8 and D2=7) will produce the Root Root support values of each item Supg=={a 0.5:0.29, b 0.25 :0.14, c  0.25:0.29, d 0.38:0.29, e 0.13:0,f 0.13:0.29}. Given a query Ahybridprefix a 6c 0:1Q{D, . D2 . 0.25}, the query decomposition process willpath ofg  21 d 21 <X>produce a DC set like DC={(D1 . 0.25) AND (D2 . 0.25)}.

b 4:3) c 1:2 d 1:0 f 0:1Comparing all items' support values in Supg with the DC set will d/\\:1 \ \ CHexplicitly indicate that any of the following items, {b 0.25:0.14}  1d2 2: td i Figure3(b)ThemetaHFP-treeforT{g} {e 0.13:0}, and {f 0.13:0.29}, cannot form an itemset with g to 6 satisfy the query Q. Therefore, we can prune out those  1:L f 0:1 1:0 jj< MetaHFP-tree hfggd unqualified items directly, with filtered HPPs ofg denoted by {c7t'> > (m>< 1:0, a 1:0}, {d I1:1, a 1:1}, {d 0:1, c 0:1, a 0:1}, {d 1:0,c ''/ 2-l ' 5 +R 1:0, a 1:0}, {d 1:0, a 1:0}, and {c 0:1} . After that, we take  V32#'0:1 each filtered HPP as a meta-transaction, and build a Meta HFP- 1 a 3:2 ~~~~~~~~~~~~~~treefor g, as shown in Figure 3 (b).

Figure 3 (a) The hybrid prefix paths of g Figure 3 (c) The meta HFP-tree forT={gd}AtaysgeifamaHFtrehsmoehnoept,  we will have to recursively call the HFP-growth procedure to Figure 3 A running example of hybrid prefix paths and a meta check each node in the header table of hfpii, and build a meta  HFP-tree for item g HFP-tree for the node. The mining process recursively calls the     HFP-growth procedure, until the meta HFP-tree eventually 7. Experimental Evaluation contains one path only. In Figure 3(b), because the meta HFP- tree of g, hfpg, contains more than one path, we will recursively In this section, we report experimental evaluations and a call the HFP-growth to build a meta HFP-tree for each of the comparative study with two simple solution based relational- nodes in hfpg (i.e., item d). For this purpose, HFP-growth will pattern discovery mechanisms. Our test datasets are collected push the current item g into a base set T={g4:3} (which records from two sources: (1) synthetic databases generated by using the frequent items so far), and conduct recursive pattern growth. IBM Quest data generator [8][24]; and (2) the IPUMS (Integrated  Public Use Microdata Series) 2000 USA census micro-data with The recursive HFP-growth process will eventually lead to a meta 1% sampling rate [25]. All experiments are performed on a 2.0 HFP-tree containing one or zero path. At this stage, there is no GHz Pentium PC machine with 512MB main memory. All the need to grow patterns any further; instead, we can directly programs are written in C++, with the integration of an STL-like produce patterns by enumerating all the combinations of the C++ tree class [26] to fulfill the tree construction and access.

nodes in the tree and appending any of the combinations to the Although it is possible for DRAMA to reuse a previously underlying base set T to generate a pattern P, as indicated on line constructed HFP-tree to answer multiple queries, for fairness in "e" of the HFP-growth procedure (Figure 4). Meanwhile, P's comparison, DRAMA will initiate HFP-tree construction and final supports are the minimal support of all involved items (w.r.t. HFP-mining for each query. In the following tables and figures, to each database). Ie., p {min {pp[} min UP2[ where unless specified otherwise, the runtime always means the total  k=lk1.,K k=l,..K { execution time, i.e., the tree construction plus the mining time.

SUPP[k] means the support value of the jh item in P (w.r.t. to For a comparative study, we implement two simple solutions, SPV and CPM, as we have discussed in Section 2. While SPVdatabase Di) and K iS the number of items in P. For example, seunilymnsadvrfespten'rmec aaae  Figure 3 (c) shows a one path (actually one node) meta HFP-tree CPMewill gener andie s from each database, buit fr bse et=t :3,d 1:21 Apeningthi ony nde  CPM will generate candidates from each component database,built for base set T={g 4:3, du32 ppending th o node and refer to the collaborative mining process for candidate to the current base set Twill produce a pattern P=T u {a 33:2}. pruning. For SPV, we use FP-tree instead of the Apriori The final supports ofP are the minimal support value of the item algorithm to mine patterns from the first database. Because CPM in P, which is {3:2}, i.e. Psup=O{.38:0.28}. As we have analyzed needs candidates generated at each single database for in Section 6.1, the DC set is not equivalent to the original query, collaboativemng,nerappl athetaditionaledaprioia ori but rather for pattern pruning purposes only. Therefore, a pattern on each database. The runtime of CPMis the pattern mining time P which is generated by using the down closure rule in the DC of the databases with the largest time expense plus the time for set, does not necessarily comply with the original query Q. A collaborative mining and pattern verification.

validity check must be conducted to assert whether P indeed complies with the query Q or not. This can be easily achieved by Because real-world databases can vary significantly in size, we comparing pattern P's support Psup with the original query Q. It is generate four synthetic databases with different sizes, as shown in obvious that the supports of P={g 4:3, d 13:2, a 13:2} are Table 2. The explanations of the database description can be Psup=O{.38:0.28}, which satisfy Q-{D1 . D2 . 0.25}, then found in [8]. In short, T10.l6.D300k.N1000.L1000 means a pattern P is eventually appended to the relational-pattern set RP. database with 300,000 transactions and 1000 items, where each  transaction contains 10 items, and each pattern contains 6 items Input: an HFP-tree hfp built fromM databases, ranking list L, and the original query Q ctais  Output: Rational-pattem set, RP on average. It, S understandable that the runtime of the systems  Poutput: Rat-ional-patter ret,Q' will crucially rely on the underlying queries. For an objectiveProcedure HFP-Mining (HFP-tree, Q) assessment, we define five queries, as shown in Table 3, and will 1. Down Closure Set (DC) *- Query-Decomposition(Q) demonstrate the average system runtime performances in 2. RP-0,T*-0 answering these queries.

3. HFP-growth (HFP-tree, T, RP, DC, Q, L)  Procedure HFP-growth (hfp, T, RP, DC, Q, L) Table 2 Synthetic database characteristics For each node ni in the header table of hfp (in inverse order of the ranking list L)  a. Si *- 0; T*- Tu ni. The supports of T are the minimal support values of all the Database Database description nodes in T(w.r.t. each database) D1 Tl0.16.D300k.NO000.L1000  b. For each of ni's location au in the header table ofhfp D2 T I0.16.D200k.Nl000.L1000 i. Build a hybrid prefix path, HPPJ, for au,  ii. Si+- Si u HPPU D3 Tl0.I6.DlOOk.NlOOO.L1000 c. Prune items in Si based on the down closure rule in the DC set D4 Tl0.16.D5Ok.NI000.L1000 d. Build a meta HFP-tree, hfpi, based on the remaining items in Sii and ranking list L e. If hfpi contains a single path PS  i. For each combination (denoted by i) of the nodes in the path PS Table 3 Query plan descrpton 1. Generate pattern P *- T uff, the supports ofP are the minimal QueryQueryconstraints  support values of the node in i7 (w.r.t. each database)  T5p=Jlmin$z/I3[k]} min][f]},,where SH9t] means the support {D/ 1D22 D3 a} k=l, QKk=l,..,K 2{(D1+D2) a}{(D3+D4) }  value of the h item inP (w.r.t. to database Dl) and Kis the number of items in P 2. Check whether P complies with the query Q; if it does, RP + RPu P {(Dl -D2). (D3-D4) .a}  f. Else Q24 {D1.2(D2 |D3)2Oa}& {D4.</J} i. HFP-growth (hfpi, T, RP, DC, Q, L) 5 {lD1-D2.2(D3+D4.c2 }  Figure 4 Relational-pattern mining using HFP-tree     7.1 HFP-tree Construction Results consequently grow faster in finding frequent patterns, as shown in Figure 6(c).

In Section 5.1 we have proposed a joint ranking list which ranks items from different databases for HFP-tree construction. We Since the joint ranking list unifies the ranking order of each item report in this section the performance of this ranking mechanism from different databases, one may argue that why we don't just in facilitating tree construction and pattern growth processes. We treat all items as they were from one single database, e.g., apply Q, in Table 3 on the synthetic databases, and use both the D=DJ+D2+D3, and then rank the items according to their total joint ranking list and the fixed ranking list to build HFP-trees. frequencies (with infrequent items in each database removed We report the results in Figure 6, where Figure 6(a) denotes the beforehand), just like the traditional FP-tree method does.

comparison of the HFP-tree construction time, Figure 6(b) However, such a global ranking list reviews items as they come represents the comparison of the total number of HFP-tree from a single database without considering their frequencies in interior nodes, and Figure 6(c) reports the comparison of the each single database, which may produce a list inferior to the one HFP-growth time. In all figures, the x-axis denotes the support from the joint ranking list. For example, if the frequencies of threshold a in Ql, and the y-axis denotes the results of different items {a, b, c} in D1 and D2 are {3000, 1000, 900} and {100, measures. The meaning of each curve in Figure 6 is explained in 2000, 1000} respectively. The global ranking list will sum up Figure 5. each item's frequency and produce the list L=abc; on the other  hand, the joint ranking list will produce the list L=bac.

As shown in Figure 6(a), the proposed joint ranking list can Considering that the most possible frequent itemsets in D1 and D2 dramatically reduce the time in building an HFP-tree from are {bc} instead {ac} or {ab}, the joint ranking list may lead to multiple databases, where the lower the support threshold a, the better results in reality. Figure 6(a) also reports the HFP-tree more significant the improvement can be observed. When wa=2%, construction time of the global ranking list, which further it will cost the fixed ranking list and joint ranking list about 98 supports our analysis. The HFP-growth on the tree built from the seconds and 60 seconds respectively to build the HFP-tree; on global ranking list also needs more time than the one built from the other hand, when ux becomes significantly low, say 0.01%, the joint ranking list, and we therefore omit the results from this the cost of the joint ranking list increases to about 98.5 seconds, mechanism in Figures 6(b) and 6(c).

which is about 3.5 times less than the time of the fixed ranking list (364.8 seconds). A low ux value will have most items in the 7.2 Query Runtime Comparison database become frequent, and therefore be added into the HFP- Figure 7 reports a detailed runtime performance comparison tree. This can be very time consuming, if the inserting process between DRAMA and two simple solutions (SPV and CPM) on does not take item frequency information into consideration, Q, in Table 3, where the x-axis denotes the support threshold because each item needs to check with the existing HFP-tree to value ux and the y-axis represents the system runtime in seconds.

find whether the current path already contains this item or not. For a detailed comparison, we also list the actual value of each The more the frequent items, the fatter the HFP-tree, and the method in the figure. When the threshold value is relatively small, more time is going to be spent on this process. On the other hand, say 0.05% or 0.01%, the runtimes of SPV and CPMare extremely a ranking order which unifies the item frequency information large, which makes no sense for comparison (the empty cells).

from all databases can significantly reduce the time in inserting each transaction into the HFP-tree, because each item a will have Overall, DRAMA linearly responds to the threshold value uX and less search space in verifying whether the current node (of the does an excellent job in answering the query Ql. When the value HFP-tree) already contains a or not. In addition, since the joint of ox is larger than 1.5%, we notice that DRAMA is inferior to ranking list has items sorted by their frequencies before they were both SPV and CPM. A further study shows that for large ux values, inserted into the HFP-tree, it will have a better chance, compared the time for HFP-tree construction becomes significant, to the fixed ranking list, to force items in a frequent itemset to follow a single path, and consequently reduce the size of the constructed HFP-tree. As shown in Figure 6(b), the interior node Joint ranking list Fixed ranking list Global ranking list number of the HFP-tree built from the joint ranking list is about _ 1% to 10% less than the tree built from the fixed ranking list.

Because of the HFP-tree quality improvement (more compact Figure 5 The meanings of curves in Figure 6 and less interior nodes), the HFP-growth process will  2913270-  ........................... 12663270 51I 3300- _=U1 ~~~~~~Z ~~~~~~~~~2413270- 1  co 260- o 6220 121632706  18 &L 1913270- 60 U--LL0 A714---__________ 1663270  100 1413270 1  0.01 0.05 0.1 0.5 1 1.5 2 0.01 0.05 0.1 0.5 1 1.5 2 0.01 0.05 0.1 0.5 1 1.52  Support Threshold (%) Support Threshold (%) Support Threshold (%)  (a) HFP-tree construction time (b) # of HFP-tree interior nodes (c) HFP-growth time Figure 6 HFP-tree construction comparisons on Query 1 in Table 2. (a) the HFP-tree construction time; (b) the total number of HFP-tree  interior nodes; and (c) the HFP-growth time     compared to the time for HFP-growth. For example, when ofDRAMA consistently and significantly better than CPM for all lo=1.5%, DRAMA spends about 68 seconds on building the the queries.

HFP-tree; however, it only costs about 9 seconds for the HFP- growth to mine the patterns. At this support value level, SPV SPy CPM applies an FP-tree based algorithm on D1, which outputs only 96 100000 patterns forD2 to verify. So the performance ofSPVat wx=1.5% is really just the runtime of the FP-tree mining on D. On the other 10000 hand, when the threshold value decreases, the patterns generated from D1 can significantly increase, which leads to a huge runtime expense for D2 to verify these patterns (notice that database 100 scanning for pattern verification can be very expensive, especially for large databases). For example, when a=0. 10, DI 10 00 . . .

will generate about eighty thousand patterns which need to be a 0.01 verified by D2, among which about ten thousands patterns will DRAMA 131.6 122.6 116.4 106.9 96.1 77 67.3 further need to be verified by D3. As shown in Figure 7, the sPv 15787 10233 759 169 72 24 sequential verification mechanism of SPV needs more than ten CPM 29192 1064 245 89 29 thousand seconds to check all those patterns. For DRAMA, SupportThreshold (%) although the tree construction at this level (a=0.1%) costs about 96 seconds, the integrated pattern pruning mechanism will Figure 7 Query runtime comparison on Q, in Table 3 significantly reduce the HFP-growth time to about 20 seconds only. So in total, DRAMA can answer Q, in about 106 seconds, Table 4 Query runtime comparison on Q2, Q3, Q4, and Q5 in which is a huge improvement compared to SPV. Table 3 (a-0.5%, ,&F0.01%) Although our analysis in Section 2 suggests that Collaborative Algorithm Q2 Q3 Q4 Qs Parallel Mining (CPM) may possibly outperform SPV, because of DRAMA 157.1 135.6 129.5 147.7 the underlying collaborative mining process in candidate pruning. CPM 3172 1125 1094 2957 The results in Figure 7 indicate that this is not the case. Because CPM needs multiple databases to forward their candidates to a central place for collaborative mining (by pruning unqualified 7.3 Case Study on a Real-world Dataset candidates), we can only apply Apriori on each single database. To further assess the system erformance of our roposed effort So the system performance of CPMis crucially bounded by the y pe p pon real-world datasets, we download the US 2000 census micro-poor performance of Apriori based algorithms. When the support data from the IPUMS [26], which provides census information value a is large, say 2%, the performance of Apriori and FP-tree about the US residents (individuals and households). We use 1.

is almost identical (since not many items can be frequent). sample of the year 2000 census data with forty seven attributes.

However, for small x values, the situation can be totally different. Those attributes cover age, household/personal income, education, For example, when a=0.1%, about 680 items in D1 are frequent, race, citizenship, poverty, and family relationship etc. Because which produces more than 230 thousand length-2 patterns from many attributes contain multiple attribute values, and some D1 (although collaborative pattern pruning can somewhat remove attributes are numerical, we further discretize each continuous some candidates, it still leaves a large number of candidates for attribute and extend the total attribute to 587 distinct items. We D1 to evaluate). This huge burden significantly slows down the intentionally collect the data from four states (California, New performance of CPM, and makes it almost unbearable in York, Florida, and Vermont), corresponding to datasets CA, NY, answering many queries. However, being worse than SPV does FL, and VT. Depending on the number of populations in each not necessarily mean CPM is useless. As we have analyzed in state, the size of the dataset varies from 6000 (Vermont) to over Section 2, some queries like Q2 in Table 3 cannot be answered by 330,000 records (California).

SPV, because no mining from a single database can produce answers for Q2. For such situations, CPMbecomes useful. Table 5 reports a runtime performance comparison among  To answer a qeylk 2weneamDRAMA, CPM, and SPV, with dataset settings Dp=CA, D2=NY,To answer a query lke Q2, we need a minng process which iS D3=VT, and D4=FL, and two sets of support threshold values.

able to unify multiple databases into one view. Both DRAMA Because SPV is not able to answer Q2 and Q5, its results in the and CPM can possibly attain this by using their collaborative corresponding cells are set to N/A. Because census data are not mining and pattern pruning process, where only patterns with randomly generated (like the synthetic data), item frequencies are their support satisfying (D1 + D2) . ut or {(D3hD4) . u are kept not random with many items' frequencies significantly higher for further actions. For DRAMA, instead of prefiltering any than others. So the support threshold values (a and 8) we choose single infrequent items before the HFP-tree construction, we will are relative high. But even so, we can see that DRAMA4 build an HFP-tree by using all items in the transactions, and then consistently outperforms both SPV and CPM with a significant let HFP-growth prune out the candidates on the fly. This runtime improvement. The results in Table 5 indicate that mechanism turns out to be very efficient in reality, as the HFP- different from the synthetic data, CPM actually has a much better tree construction in this case spends only 105 seconds (which is performance than SPV in answering some queries. In fact, when about 7 seconds more than ou=0.010 ). As shown in Table 4 a40%~ and ,fr50 , although it will cost FP-tree mining about 5 (where the value of Ut is fixed to 0.50%), the run time performance seconds to mine patterns from D1, there are over ten thousand of DRAMA is much better than CPM in answering Q2. Table 4 patterns generated from D1 with the longest pattern containing 13 has further listed a runtime comparison between DRAMA and items. All these patterns need to be verified by D2, which CPM in answering other queries in Table 3, with the performance increases the runtime significantly. On the other hand, at the     same threshold level, there are about eighty frequent items in D1, References which produces about thirty two hundred length-2 candidates for CPM. Meanwhile, the collaborative pattern pruning from [1]. U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, & R. Uthurusamy, multiple databases will have those unnecessary candidates pruned Advances in Knowledge Discovery and Data Mining, AAAI/MIT out on the fly. So CPM can receive better performances than SPV Press, 1996.[2]. J. Han & M. Kamber, Data mining: Concepts and techniques, on the census data. Morgan Kaufmann, 2001.

Our experimental study on the census data also brings some [3]. H. Kargupta, B. H. Park, D. Hershberger, and E. Johnson, interesting findings. For example, one rule we discovered states "Collective data mining. A new perspective toward distributed datainteresting finngF emPLe, one re tae mining," in Advances in Distributed and Parallel Knowledge  that the relational pattern P={"Resident is not one of the Discovery. Cambridge, MA: MIT/AAAI Press, 1999.

following races: African American, Asian, or Pacific Islander" & [4]. X. Wu & S. Zhang, Synthesizing High-Frequency Rules from relationship VT (75.0%) > NY (63.2%) > CA (61.2%) > 60%, Engineering, 15(2003), 2: 353-367.

where the value in the brackets indicates the actual support value [5]. T. Golub, et al. Molecular classification of cancer: class discovery of the pattern in the given dataset. This pattern may possibly and class prediction by gene expression monitoring, Science, imply that from California to New York to Vermont, general US vol.286, Oct. 1999.

residents (American Indian N"ite) tend to live separately with [6]. X. Yin, J. Han, & P. Yu, Crossminer: Efficient classification acrossresidents (American Indian or White) tend to live separately with multiple database relations, Proc. ofICDE, 2004.

their children (and grand children), which is more likely a [7]. J. Han, J. Pei, & Y, Yin, Mining frequent patterns without traditional American family style. So from CA to NY to VT, this candidates generation, Proc. ofACMSIGMOD, 2000.

culture is likely more preserved. [8]. R. Agrawal & R. Srikant, Fast algorithms for mining association  rules, Proc. of VLDB, 1994.

Table 5 Query runtime comparison on IPUMS census data [9]. T. Zhang, R. Ramakrishnan, & M. Linvy, BIRCH: An efficient data  clustering method for very large databases, Proc. ofACMSIGMOD, Threshold Algorithm Q, Q2 Q3 Q4 Q5 1996.

cr--25% DRAMA 25.3 37.4 24.3 21.3 39.6 [10]. R. Agrawal and J. C. Shafer, Parallel Mining ofAssociation Rules,  SPV 3123 N/A 3201 3194 N/A No. 6, December 1996, pp. 962-969.

cr-40% DRAMA 18.1 30.9 17.8 17.6 33.2 [11]. A. Manjhi, V. Shkapenyuk, K. Dhamdhere, & C. Olston, Finding CPM 347 492 362 273 419 (recently) frequent items in distributed data streams, Proc. ofICDE, SPV 1192 N/A 1049 901 N/A 2005.

[12]. M. Otey, A. Veloso, C. Wang, S. Parthasarathy, & W Meira, 8. Conclusions Mining frequent itemsets in distributed and dynamic databases,  Proc. ofICDMConference, 2003.

A contrast study is a common means of data analysis in [13]. E. Han, G. Karypis, & V. Kumar, Scalable parallel data mining for discovering useful patterns from the data. A real-world association rules, Proc. ofACMSIGMOD, 1997.

application often involves data collected from different sources, [14]. D. Cheung, V. Ng, A. Fu, & Y. Fu, Efficient mining of association where data managers or consumers often know simple dataset rules in distributed databases, IEEE Trans. on Knowledge and Data relationships, e.g., when and where the data in each dataset were Engineering, vol. 8, 1996.

collected orwhich entity each dataset corresponds to. However, [15]. S. Parthasarathy and M. Ogihara. Exploiting Dataset Similarity forcollected or whlch entty each dataset corresponds to. H ow e Distributed Mining. Proc. of 3rd Workshop on High Performance they are usually not aware of the relational patterns across Data Mining, 2000.

multiple databases, which are often the driving force and the key [16]. T. Li, M. Ogihara, & S. Zhu, Association-based similarity testing in understanding the detailed data correlations. Although the and its applications, Intelligent Data Analysis, 7(3):209-232, 2003.

users can rely on some simple solutions to find such relational [17]. S. Bay & M. Pazzani, Detecting group differences: Mining Contrast patterns, the runtime performances of these methods turn out to sets. Data Mining andKnowledge Discovery, 5(3):213-246, 2001.

be unbearable in reality, due to the possibly large data volumes [18]. G. Webb, S. Butler, & D. Newlands, On detecting differences and the complex relationships among users' constraints. In this between groups, Proc. ofthe 9th ACMSIGKDD Conference, 2003.

paper, w a p e d A[19]. G. Dong & J. Li, Efficient mining of emerging patterns:paper, we have presented DRAMA (Discovery Relational patterns dicvrn trnd an difrecs Po.fththACSGKD Across Multiple dAtabases) to solve the problem in an effective 1999.

way. The theme of DRAMA is to unify multiple databases to [20]. X. Ji, J. Bailey, & G. Dong, Mining minimal distinguishing build a Hybrid Frequent Pattern tree, and further decompose the subsequence patterns with gap constraints, Proc. of International user's query into a rule set with the down closure property. After Conference on Data Mining, 2005.

that, DRAMA pushes such rule sets into the mining process to [21]. C. Bucila, J. Gehrke, D. Kifer, & W. Whote, Dualminer: a dual- speed up the pattern growing. DRAMA has demonstrated a pruning algorithm for itemsets with constraint, Proc. of ACM significant runtime improvement in finding relational patterns SIGKDD Conference, 2002.

from both synthetic and real-world databases, where the [22]. R. Jin & G. Agrawal, A systematic approach for optimizing  performances of emrha 0iebtcomplex mining tasks on multiple databases, Proc. ofICDE, 2006.performances of DRAMA can be more than 100 times better than [23]. D. Tsur, J. D. Ullman, S. Abitboul, C. Clifton, R. Mot- wani, & S.

simple methods. Nestorov. Query flocks: A generalization of association-rule mining.

Proc. ofACM-SIGMOD Conference, 1998.

Discovering contrasts and complex relationships among Prc fC-IMD ofrne 98[24]. IBM Quest Data Mining Project. Quest synthetic data generationcollections of data is a challenging and important task in data code, http://www.cs.umbc.edu/-cgiannel/assoc gen.html mining. To solve the problem, DRAMA has focused on frequent [25]. Integrated Public Use Microdata Series, itemset mining. But the framework proposed here is able to be http://www.ipums.umn.edu/usa/index.html extended to handle other relational patterns as well, such as [26]. tree.hh: an STL-like C++ tree class, constrained frequent itemsets, closed frequent patterns, and htp/wwaimgd/pea/re sequential patterns. [27]. B. Chen, L. Chen, Y. Lin, R. Ramakrislnan, Prediction Cubles,  Proc. Ofthe 31st VLDB Conference, 2005.

