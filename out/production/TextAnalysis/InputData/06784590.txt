Kernel SODA: A Feature Reduction Technique Using Kernel Based Analysis

Abstract?A feature extraction technique called Successively Orthogonal Discriminant Analysis (SODA) has been recently proposed to overcome the limitation of Linear Discriminant Analysis (LDA), whose objective is to find a projection vector such that the projected values of data from both classes have maximum class separability. However, in LDA, only one such vector can be found due to the rank deficiency for binary classification problems. On the other hand, as a feature extraction technique, the proposed algorithm SODA attempts to obtain a transformation matrix instead of a vector. In this paper, the kernel version of SODA is presented in both intrinsic space and empirical space. To obtain the solution without sacrificing numerical efficiency, we propose a relaxed formulation and data selection for large scale computations.

Simulations are conducted on 5 data sets from UCI database to verify and evaluated the new approach.

Keywords-Feature extraction, SODA, Kernel, Discriminant Analysis, big data

I. INTRODUCTION  As a solution to the curse of dimensionality, feature reduction [1] for classification has been a popular topic for decades. There are many reasons why we should care about the dimensionality. 1) Overfitting is unevadable for high dimensional feature space, which might ruin the gen- eralization ability of the classifier. 2) When the number of variables is too large, high storage capacity is required, and computational complexity is yet another issue. 3) In many cases, high dimensionality causes computational instability and singularity [2] 4) Class separability is very likely to be enhanced by eliminating redundant information.

There are two types feature reduction techniques: feature selection and feature extraction. Feature selection [3] is to select a subset of the variables with respect to some criteria. On the other hand, feature extraction attempts to find a function f(x) : Rm ? Rk, which transforms data from the original space Rm to a low dimensional feature space Rk, where m > k. In this paper, we focus on the development of a feature extraction technique for classification. The proposed approach is the kernel extension of a previously presented technique called Successively Orthogonal Discriminant Analysis (SODA) [4]. The original technique is closely related to Linear Discriminant Analysis (LDA) [5] and Principal Component Analysis (PCA) [6].

The paper is organized as follows. Section II reviews the relation between LDA, PCA and SODA. Section III presents the formulations of Kernel SODA in both intrinsic space and empirical space. For simple numerical implementations, a relaxed KSODA algorithm is developed in Section IV. The experimental results are shown in the last section to verify the proposed technique.



II. RELATED WORK  A. Linear Discriminant Analysis (LDA)  Linear Discriminant Analysis (LDA) has a very long history. The underlying idea is based on Fisher criteria for maximizing the class separability.

Given class label c ? {+,?} and training data Xc = {xc1, ? ? ? ,xcNc}, the class separability is measured using the ?between-class scatter matrix? SB and the ?within-class scatter matrix? SW , which are respectively defined as:  SW =  N+  N+? i=1  (x+i ? ?+)(x+i ? ?+)T  +  N?  N?? j=1  (x?j ? ??)(x?j ? ??)T  SB = (? + ? ??)(?+ ? ??)T (1)  where ?+ and ?? are the mean vector estimated from the corresponding class + and ?. In LDA, we would like to find a vector w that maximizes the following Fisher score:  J = wTSBw  wTSWw , (2)  and the solution vector is the generalized eigenvector cor- responding to the largest generalized eigenvalue of the problem SBw = SWw?. However, problems occur when the within matrix SW is singular. There are many ways of tackling this problem [7], [8] and one way is to compute the eigenvector of the Fisher matrix F = SW+SB. We will adopt this method in this paper.

DOI 10.1109/ICMLA.2013.20    DOI 10.1109/ICMLA.2013.20    DOI 10.1109/ICMLA.2013.20    DOI 10.1109/ICMLA.2013.20     B. Principal Component Analysis (PCA)  One of the most famous dimensionality reduction tech- niques is the Principal Component Analysis (PCA), which finds the subspace with the largest variation. PCA can be formulated in an iterative fashion. Namely, we are trying to find a matrix W =  [ w1, ? ? ? ,wk  ] whose columns satisfy:  maximize wi  wTi Swi  subject to wi ? w1,...,i?1 wTi wi = 1  wi ? Span(S)  (3)  where S = XXT is the covariance matrix of the training data and Span(S) denotes the range space of matrix S. When the dimensionality of the data vector is high, PCA can be implemented iteratively without computing the covariance matrix S [9]. The solution of PCA provides a transformation matrix that projects the data to a low dimensional subspace, where the data representation is enhanced. The data vector in the principal component space is thus represented as:  x?WTx. (4) C. Successively Orthogonal Discriminant Analysis  Successively Orthogonal Discriminant Analysis (SODA) integrates the ideas of LDA and PCA. Defined as a succes- sive approach, SODA adopts the Fisher discriminant score as its objective function for each iteration. The output of SODA is a transformation matrix that projects the data onto a new feature space with enhanced class separability. Similarities and differences of LDA, PCA and SODA can be found in Table I. The formulation of SODA is illustrated as follows.

SODA formulation. The matrix W = [ w1 ? ? ?wk  ] defines  a map Rm ? Rk, whose columns satisfy:  maximize wi  wTi SBwi wTi SWwi  subject to wi ? w1,...,i?1 wTi wi = 1  wi ? Span(SW )  (5)  where Span(SW ) denotes the range space of matrix SW .

Similarly, the resulting data vector is mapped to a reduced feature space:  x?WTx. (6) SODA and PCA share the same concept of optimal  subspace projection. PCA?s optimality enjoys the advantage of having a global criterion, however it does not take into account of teacher?s information. On the other hand, SODA takes full account of teacher?s information, but its optimality is formulated and executed step by step. Both PCA and LDA  have their kernelized variance called KPCA and KDA. This motivates us to extend SODA to its kernel model.

The existing feature reduction techniques based on dis- criminant analysis [11] mostly depend on the number of classes C. For binary classification problem, ?.e. C = 2, due to rank deficiency, a transformation matrix can not be found. On the other hand, SODA overcomes this limitation by modifying the searching space. More details can be found in later sections.



III. THEORY OF KERNEL SODA  In kernel based analysis, we define a function ?(x) : x? ?, which maps the data vector from the original space to intrinsic space ? to obtain a better separation.

A. KSODA in Intrinsic Space  KSODA can be formulated in the intrinsic space with an explicit expression of ?(x). The transformation matrix is denoted by U =  [ u1, ? ? ? ,uk  ] in the intrinsic space.

Let c ? {+,?} denote the class label and Nc the total training size of class c, we can define the between-class scatter matrix S?B and within-class scatter matrix S  ? W in the  intrinsic space as follows:  S?B = (m ? + ?m??)(m?+ ?m??)T (7)  S?W = ?  c?{+,?}   Nc  Nc? j=1  (?cj ?m?c )(?cj ?m?c )T, (8)  where  m?c =  Nc  Nc? j=1  ?(xcj). (9)  KSODA (intrinsic space). The matrix U = [ u1 ? ? ?uk  ] defines a map RJ ? Rk, whose columns satisfy:  maximize ui  uTi S ? Bui  uTi S ? Wui  subject to ui ? u1,...,i?1 uTi ui = 1  ui ? Span(S?W )  (10)  where Span(S?W ) denotes the range space of matrix S ? W .

In this case, the data vector is transformed in the following way:  x? ?(x)? UT?(x). (11)  Note that after the transformation x ? ?(x), the SODA algorithm can be applied directly.

LDA PCA SODA Type Classifier Feature Reduction Feature Reduction  Output wm?1 Wm?k Um?k Purpose Enhance Class Separability Enhance Data Description Enhance Class Separability  Objective Function Direct Direct Successive Implementation Direct Direct / Iterative Iterative  Table I THE RELATION BETWEEN LDA, PCA AND SODA.

B. KSODA in Empirical Space  When the Gaussian RBF kernel is adopted, the dimension of the intrinsic space is infinity and hence computations can not be carried out directly. In this case, it is necessary to resort to a kernel learning model proposed below.

From the theory of Reproducing Kernel Hilbert Space (RKHS) [12], [13], [14], each vector ui ? H can be written as a linear combination of all the training data ?1, ? ? ? , ?N drawn from H, ?.e., we have  ui = N? j  ?jai,j = ?ai (12)  where ? = [ ?1, ? ? ? , ?N  ] is the training data matrix, the  scalar ai,j is the jth element of vector ai. Similar to the SODA formulation, vector ui is the ith column of the transformation matrix U that can be written as  U = ?A (13)  where A = [ a1, ? ? ? ,aN  ] As a result of plugging Equation (12) into Equation (10),  define matrices M and N [10]:  M = (M+ ?M?)(M+ ?M?)T . (14) and  N = ?  c?{+,?} Kc(I? 1  Nc E)KTc , (15)  where the row vectors of Mc are written as (Mc)j = Nc  ?Nc t=1 k(xj ,x  c t). Matrix Kc denotes the kernel matrix  Kc = ? T?c and E is a matrix with all ones as its entries.

This leads to an equivalent Kernel SODA learning model in the empirical space.

Denote k(x) = ?T?(x), we formulate KSODA in the empirical space as:  KSODA (empirical space). Find optimal vectors a1 ? ? ?ak, such that:  maximize ai  aTi Mai aiTNai  subject to aTi Kaj = 0, ?i ?= j aTi Kai = 1  (16)  The optimal transformation matrix U can be written as:  U = ? [ a1 ? ? ?ak.

]  Therefore, in the empirical space, the transformation follows:  x? k(x)? AT k(x). (17) Equivalence: The empirical formulation is a direct result  of kernelization of the intrinsic formulation [15], whose equivalence is straightforward ?.e. UT?(x) = AT k(x).



IV. IMPLEMENTATION AND APPROXIMATION  For user?s choice, we describe two variance of kernelized SODA learning models.

A. KSODA implementation  There are two major steps involved in the algorithm: ? Initialization:  Q(0) = N+, D(0) = K+, ? =M+ ?M? (18) ? Step 1: Computing ai and normalization  ai = Q(i)??  ?TQ(i)TKQ(i)? (19)  ? Step 2: Let D(i+1) = D(i) ? aiaTi , update Q(i+1) according to  Q(i+1) = D(i+1)ND(i+1) (20)  ? Go to Step 1 until i = k.

B. Approximation  For numerical efficiency, we introduce two approximation strategies:  - Relaxation on orthogonality condition.

- Data selection for big data scenario and the purpose of  numerical invertibility.

They are elaborated below.

1) Relaxation on orthogonality:: Due to the complexity of the weighted orthogonality constraint ATKA = I, we define a relaxed formulation called KSODA (II).

KSODA (II). Find optimal vectors a1 ? ? ?ak, such that:  maximize ai  aTi Mai aiTNai  subject to aTi aj = 0, ?i ?= j aTi ai = 1  ai ? Span(N)  (21)  where Span(N) denotes the range space of matrix N and the transformed data vector is represented as:  x? = [ a1 ? ? ?ak  ]T k(x). (22)  The optimization problem stated in (21) can be solved by Algorithm KSODA(II). Similar proof can be found in [4].

Algorithm KSODA(II)  - Construct the matrix X = [ x1, ? ? ? ,xN  ] . Define output  dimension k.

- Compute M and N from Eq. (14) and (15) respectively,  - Let N(0) = N  F(1) = (N(0))+M  - For i = 1 : k  - Solve for F(i)ai = ?iai  where ?i is the largest and only eigenvalue of F(i).

- Let D(i) = Im?m ? aiaTi be the deflation matrix N(i) = D(i)N(i?1)D(i)  F(i+1) = (N(i))+M  - Form matrix: A = [ a1 ? ? ?ak  ]  - Transformation of the features: x? = ATK(X,x)  C. Data selection and numerical invertibility  From the RKHS theories, the solution vectors ui can be written as a linear combination of all the training data, namely ui = ?ai. That means the size of the kernel matrix we have to compute is in the order of the training size N .

Furthermore, the solution of Formulation KSODA is based on the pseudo-inverse of a N ?N matrix N, which results in a N3 computational complexity.

To tackle this problem, we choose a subset G ? ? to approximate the span of the whole training space. We call  such matrix G the basis matrix. Note that without ambiguity, we use capital letter for the set of some training patterns (e.g.

? = {?1, ? ? ??N}) and boldface letter for the corresponding matrix (e.g. ? =  [ ?1, ? ? ? , ?N  ] ).

Figure 1. An intuitive illustration of the basis selection. If two vectors ?i and ?j are very ?similar? to each other, we assume that the spaces they span are collinear and there is no point to include both of them into the basis matrix G. The similarity measure is naturally defined by normalizing the kernel function k(xi,xj)?  k(xi,xi) ?  k(xj ,xj) . By excluding similar vectors,  we could reduce the number of vectors in the basis matrix.

The idea is described as follows: In the intrinsic space, given normalized vectors ?t, t = 1, ? ? ? , N , we would like to find a basis matrix G =  [ ?1, ? ? ??n  ] , such that for some  small number ?, GTG = ?, where ? is a full rank matrix and ?i,j?l,l < ?, ?l, i, j with i ?= j.

An illustrative example can be found in Figure IV-C.

As we know that similarities between vectors always cause singularity and hence numerical instability. Therefore, by finding such basis matrix G, the robust invertibility of the matrix N is enhanced.

Practically, the idea can be implemented as shown in Algorithm Basis G.



V. EXPERIMENTAL RESULTS  Parameter setting: In this section, we conduct simula- tions on 5 UCI [16] data sets to compare the classification results using original space, PCA, Kernel PCA, SODA, KSODA and KSODA (II). The classifier we used is Support Vector Machine (SVM) [17] with rbf kernel (? = 1).

We also compared the classification results with LDA and Kernel LDA. The parameter ? for the kernel based methods are set to be 0.5 for consistent and fair comparison. The reduced dimensionality for the feature reduction techniques under comparison is k = 4. This is chosen based on cross validation on the data set arcene and then applied to the rest of the data sets.

Data set Dimension Original PCA KPCA LDA KLDA SODA KSODA KSODAII  arcene 104(4)? 200 50% 21.67 % 18.75 % 37.48% 13.81% 31.99% 15.62 % 13.63 %  sonar 60(4)? 208 27.52% 33.89 % 35.83 % 27.83% 19.88% 25.42% 20.38 % 17.43 %  wdbc 30(4)? 259 9.17% 7.35 % 5.38 % 3.84% 7.18% 3.44% 2.44 % 2.36%  vehicle  van 18(4)? 199 3.27% 12.23 % 17.86 % 2.01% 8.42% 2.39% 1.83 % 1.51% saab 18(4)? 219 17.38% 26.20 % 28.19% 13.55% 18.19% 13.97% 12.16 % 10.55% bus 18(4)? 218 2.23% 11.78 % 10.40% 3.21% 2.39% 3.10% 1.92 % 1.21% opel 18(4)? 212 17.28% 27.53 % 27.48 % 13.04% 14.43% 12.99% 11.67 % 10.34%  segment  brickface 19(4)? 330 0.87% 6.91% 5.96% 0.67% 1.05% 0.62% 0.59 % 0.53% sky 19(4)? 330 0.25% 0.47% 0.26% 0.21% 1.78% 0% 0 % 0.01%  foliage 19(4)? 330 2.94% 7.69% 9.68% 4.03% 4.91% 3.28% 2.08 % 2.05 % cement 19(4)? 330 1.74% 8.09% 7.46% 3.65% 4.89% 1.83% 1.45 % 1.45% window 19(4)? 330 3.83% 10.20% 9.83% 4.34% 6.77% 3.66% 3.12 % 2.46%  path 19(4)? 330 0.42% 3.31% 5.73% 0.60% 1.01% 0.56% 0.33 % 0.32% grass 19(4)? 330 0.58% 0.37% 0.42% 0.42% 0.21% 0.39% 0.21 % 0.19%  Table II CLASSIFICATION ERROR COMPARISON BETWEEN DIFFERENT FEATURE REDUCTION TECHNIQUES. NOTE THAT THE KSODA IS ALREADY  SUBSTANTIAL BETTER THAN SODA. HOWEVER, KSODA (II) OFFERS NOTICEABLE FURTHER IMPROVEMENT OVER KSODA.

There are another two parameters to choose for Kernel SODA algorithms, which are the size of the basis matrix G and the tolerant ratio ?. These selections depend on the capacity of the computational device, the kernel parameters and the data properties. Here, we choose NG = 100 and ? ? [0.5, 1) is selected for each data set by cross-validation.

Data description: The data sets we used are arcene, sonar, wdbc, vehicle and segment. The basic properties of the data sets are summarized in Table II.

Vehicle and segment have more than two classes. Since we focus on the study of binary classification in this paper, the results shown are based on the averaged error rate of one-versus-one scheme for all classes from the data sets.

Testing method: We divide each data set randomly into training set (80%) and testing set (20%). This procedure is repeated for 10 times. Furthermore, at the first time of the tests, 20% of the training set is left out for cross validation.

The classification results in terms of error probabilities shown in Table II are based on the average error of the  two classes:  Perr =   ? c  # of misclassified testing data in class c # of testing data in class c ? {+,?}  (25) Testing results: On 13 out of 14 data sets, KSODA(II)  has achieved the best classification results in terms of the averaged classification error defined in Equation (25). First, the original space of data set arcene has 10000 variables. It is obvious that it suffers from overfitting using SVM with rbf kernel, which results in a 50% error probability. In such scenarios, PCA/KPCA with extremely low dimensionality will do an even better job than the original space. However, when the number of original variables is reasonably small compared to the number of samples, PCA/KPCA do not achieve a better performance in general. LDA outperforms SVM on original space for roughly 50% cases. Since the parameter selection of kernel SVM is a key for high perfor- mance, LDA enjoys the advantage of simplicity.

On average, SODA/Kernel SODA algorithms give the     Algorithm Basis G  - Let X = [ x1, ? ? ? ,xN  ] , construct an empty m?NG  matrix XG. Choose a small number ? as the threshold.

Choose a kernel function K and the size of the basis  NG.

Set counter k = 1.

- for i = 1 : NG Let XG(:, i) := X(:, k) (?) - for j = k + 1 : N  For given kernel function K, compute:  K = K(XG,xk) (23)  Normalization:  K?ij = Kij  ??(xi)?2??(xj)?2 , ?i, j (24)  where ??(xt)?2 = ? K(xt,xt) is the norm of  vector ?t (t < i) on intrinsic space.

- if: K  ? ij  min(Ktt) < ?, ?i ?= j, t < i,  let k = j and return to (?).

else: let k = k + 1.

- end - end - Replace the matrix X in Algorithm KSODA(II) by XG.

best classification accuracy. The reason is that they do not only take into consideration of the class separability on the best one dimensional subspace, but also extend the development on k dimensions, which allow high flexibility that LDA does not provide. Kernel SODA, on the other hand, uses the kernel trick to further explore the intrinsic non- linear structure in the data and therefore results in an even lower classification error rate. Examples of visualization for these feature reduction techniques on data sets sonar, wdbc, vehicle and segment can be found in Figure 1 and 2.



VI. ACKNOWLEDGMENT  This material is based on research sponsored by DARPA under agreement number FA8750-12-2-0126. The U.S. Gov- ernment is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.

This work is also sponsored by the Swedish Research Council (VR) which is gratefully acknowledged.

