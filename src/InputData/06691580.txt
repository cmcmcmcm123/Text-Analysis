A Selective Checkpointing Mechanism for Query Plans in a Parallel Database System

Abstract?Most existing parallel database systems achieve fault tolerance by aborting unfinished queries upon a failure and restart the entire from the beginning. This is inefficient for long running queries of OLAP workloads. To solve this problem, this paper presents a selective checkpointing mechanism which materializes the outputs of some necessary operators, enabling to resume queries from middle of the execution upon failures.

Each query is represented by a DAG of relational operators in which data are typically pipelined between operators. The goal of the mechanism is to find a set of operators whose outputs are worth being checkpointed to minimize the expected runtime of the whole query. It firstly provides a cost model to estimate the expected runtime of a whole query plan under a given failure probability for each operator. Then a divide-and- conquer algorithm is proposed to find a close-to-optimal solution to the problem. The algorithm divides the query plan into sub- plans with smaller search spaces. For a given query plan with n operators, the algorithm runs in O(n) time. The mechanism is implemented in a shared-nothing parallel database system called ParaLite which provides a coordination layer to glue many SQLite instances together, and parallelizes SQL queries across them. The experimental results indicate that different fault-tolerant strategies affect the overall runtimes of queries.

Our selective checkpointing mechanism can choose reasonable operators to be checkpointed and outperforms other fault- tolerant strategies. In addition, the divide-and-conquer algorithm taken by our mechanism has a smaller overhead than brute-force approach while keeping a similar effectiveness.



I. INTRODUCTION  Parallel database systems [1] are high performance com- puting platforms to provide a high-level programming envi- ronment and widely used in analytical data warehouse appli- cations (OLAP). As data that need to be analyzed continue to grow, the size of computing resources grows accordingly.

Thus, the probability of a failure during query processing is increasing rapidly. Most existing database systems achieve fault tolerance by aborting unfinished queries upon a failure and restart the entire query processing. This approach is reasonable for queries with OLTP workload as almost all transactions must be completed within a small amount of time. However, for long running queries of OLAP workload, it is costly to restart the query from the beginning as lots of work are lost. Therefore, we should distinguish the fault tolerance of read-only queries in analytical workloads from that of traditional transactional workloads.

While the basic mechanim for fault tolerance in parallel database systems is a transaction, MapReduce [2] and its open-  source implementation Hadoop [3] support fine-grained fault tolerance mechanisms. In Hadoop, when one node fails (or is slow), its jobs are rescheduled on another node, allowing the entire task to complete without restarting. The goal of Hadoop is to make sure that only the jobs running on the very failed node need to be re-executed. To accomplish this goal, Hadoop stores the output of every map task to the durable storage before the data are available for the reduce tasks. On the reducer side, each reducer fetches partitions from every map task and writes them to local disks before the operation is performed. While this strategy is safe-first, it is not always efficient for small to medium-sized jobs as writing all intermediate data to durable storage before making progress may not be better than re-executing the tasks.

Similar to workflows [4], a query is described in a database system through a directed graph structure, where nodes corre- spond to the activities (relational operator in database such as join, aggregate and sort) and edges correspond to the data dependencies between nodes. To achieve fault tolerance, workflow systems often use the combination of two tech- niques: checkpointing and rollback recovery [5]. There are two common checkpointing schemes in workflow systems based on the granularity level: task-level checkpointing which saves intermediate states of running tasks, so that in case of failure a task can be re-started from a previously saved intermediate state, and workflow-level checkpointing which captures the state of the workflow as a whole. The main difference between the execution of a workflow and a database query is that most of workflow systems store and transfer all intermediate data in/via files while database systems generally pipeline intermediate data to obtain better performance. Therefore, fault tolerance issue in workflow systems is more about the adaptation of the checkpoint frequency for reducing overall execution performance.

To make an efficient intra-query fault tolerance for long- running queries, we propose a selective checkpointing mech- anism which for a given query plan aims to automatically find necessary operators whose outputs are worth being check- pointed to minimize the expected runtime of the whole query in a failure-prone environment. The mechanism provides a cost model to simulate the execution of the query plan and takes a divide-and-conquer approach to solve the problem. Finally, the mechanism is implemented in ParaLite [6], a shared-nothing parallel database system which provides a coordination layer      to connect single-node database systems (SQLite [7]) and parallelize SQL queries across them.

In summary, the major contributions of our work are as follows: ? We provide a cost model to estimate the expected runtime  of a whole query plan under a given failure probability of each operator. The model takes fine-grained parallelism within each operator into consideration. It splits each operator into many small logical tasks which is loosely coupled with processors.

? We propose a divide-and-conquer algorithm to solve the problem. The algorithm divides the query plan into sub- plans with smaller search spaces. For a given query plan with n operators, the algorithm runs in O(n) time.

? We develop the mechanism in a shared-nothing parallel database system, ParaLite, and verify it with several queries from TPC-H benchmark[8]. The experimental results firstly show that different fault-tolerant strategies affect the overall runtimes of queries. Then our selective checkpointing can choose reasonable operators to be checkpointed and outperforms other fault-tolerant strate- gies. In addition, the divide-and-conquer algorithm taken by our mechanism has a smaller overhead than brute- force approach while keeping a similar effectiveness.

The rest of the paper is organized as follows: Section II presents related work. Section III gives a brief introduction of ParaLite. In Section IV, we introduce the problem setting and the detail of cost model for a query plan, and give the heuristic algorithm for the problem. Then Section V evaluates the mechanism and presents the experimental results. Finally, we draw conclusions in Section VI.



II. RELATED WORK  Most commercial parallel database systems [9] [10] [11] provide fault-tolerance through replication [12] [13] [14]. As they cannot handle the intra-query fault tolerance, if a failure occurs during a long running query, the entire query must be restarted from the beginning. To efficiently recover a long- running query from the middle of the execution, systems achieving intra-query fault tolerance have been proposed.

FTOpt [15] provides an intra-query fault tolerant framework which enables the mixing and matching of different fault- tolerant techniques in a single pipelined query plan. However, FTOpt focuses on non-blocking query plans, where data are pipelined from one operator to the next, producing results incrementally. In this case, they assume that aggregation operators, if any, appear only at the top of a plan. Besides, FTOpt uses a brute-force algorithm to enumerate through the search space to get an optimal combination of fault- tolerant strategies. For a given query plan with n operators, the algorithm runs in ?(3n) time. Another work, Osprey [16], provides the ability to detect and recover from failures (or slow nodes) in long-running queries. It divides queries into sub- queries to be executed by PostgreSQL [17], and re-schedules a sub-query which failed or progresses too slowly to a different node based on data replication. Similar to MapReduce, Osprey  adopts the MapReduce-style load balancing strategy of greedy assignment of work to solve the skew problem. However, Osprey is designed for data-warehouse applications in which tables are arranged in a star schema and it cannot support many other queries such as non-star joins and nested queries. On the other hand, ParaLite is designed for more general queries.

In contrast to parallel database systems, MapReduce model [2] [3] [18] [19] [20] provides fine-grained fault tolerance by storing all intermediate results to a durable storage before making a further progress. As a result, map jobs and reduce jobs running on a failed worker are rescheduled on another worker, allowing that task to complete without restarting.

However, this naive strategy is not always efficient especially for short to middle running jobs. To improve the performance, recent work [21] introduced the ability to partly pipeline data in Hadoop.



III. OVERVIEW OF PARALITE  ParaLite [6] is a workflow-oriented shared-nothing parallel database system based on a popular single-node database SQLite [7]. The basic idea of ParaLite is to provide a co- ordination layer to glue many SQLite instances together, and parallelize an SQL query across them. The architecture of our system is shown in Figure 1. It uses a classic master/worker pattern to organize resources. ParaLite is designed to be a serverless and zero-configuration system, so no process is running before a query is executed. ParaLite has multiple clients which present an SQL interface to users and allows a group of queries to be submitted at the same time.

The basic flow to process a query in ParaLite is as follows: A single (or a group of) query is issued through multiple clients. One authorized client starts the master process which then parses the query, translates the query into the logical plan (DAG) which is the key structure to connect each logical component. Each node in the logical plan is either a relational operator such as join and aggregate or a sub-query which in- tegrates one or more operators. Edges represent data transfers among operators. Based on the logical plan, the master spawns corresponding operators on worker nodes. Then the master schedules each operator to worker nodes by the runtime. Each worker process is responsible for receiving data from children operators, processing them and distributing the output to the next operator. The top (root) operator returns the final results to clients. All data are sent directly between worker nodes and thus the master is not a bottleneck for any data transfers.

As the intended applications for ParaLite are workflows typically built out of various independently developed executa- bles, ParaLite extends SQL to support seamless integration of arbitrary executable called User-Defined Executable (UDX) which can be written in any language. Long-running exe- cutables in workflow makes intra-query fault tolerance more important. UDX is very flexible because a user does not need to develop a program respecting to rigid formatting rules such as ?key, value? input/output format or write code with respect to strict specifications of databases.

Fig. 1. Architecture of ParaLite  The syntax of UDX is similar to that of the traditional User- Defined Functions, with the main difference being that the function can be given as a command line.

select col1, F(col2) as new_col2 from T where <predicate1, predicate2, ...> with F = "command_line"  A UDX can work on and produce arbitrary columns. To avoid registration to system before the query is executed, ParaLite allows users to define the UDX within the query using WITH clause. It starts from a command line followed by data format options, such as, input, input row delimiter, output and out row delimiter.



IV. SELECTIVE CHECKPOINTING MECHANISM  A. Query Model  A query is expressed by a DAG (query plan) of relational operators each of which forwards data tuples to the next. For example, the query plan of Query 3 of TPC-H benchmark [8] is shown in Figure 2. As ParaLite stores data in SQLite database, it uses SQL query to access data. To take advantage of database technologies of SQLite, e.g. indexing and query optimization, we push as much operations as possible into the query to the underlying SQLite. So the leaf nodes of the plan are sub-query operators which read relations using cor- responding predicates and produce a row-and-column subset of the relational table. Operators J3 and J4 join two relations.

The output from J3 is then aggregated and sorted. Specially, some operators can be integrated into a single query if no repartitioning operations are necessary for them. For example, if relations Customer and Orders are both hash-partitioned across data nodes by the join attribute, J4, S6 and S7 can be integrated into a single query.

Each operator is either a pipeline operator, which can process each tuple independently without the knowledge of all tuples, or a blocking operator which must receive all tuples before emitting the result, e.g, an aggregation operator and a sorting operator. For hash joins, we need to have all tuples from at least one table to build the hash table for it. Then we  select L.orderkey, sum(L.extendedprice * (1 - L.discount)) as revenue, O.orderdate, O.shippriority from Customer C, Orders O, LineItem L where C.mktsegment = ?AUTOMOBILE? and C.custkey = O.custkey and L.orderkey = O.orderkey and O.orderdate < ?1995-10-11? and L.shipdate > ?1995-10-11? group by L.orderkey, O.orderdate, O.shippriority order by revenue DECS, O.orderdate  Fig. 2. A Logical Plan for TPC-H Query 3  could in principle emit outputs as tuples from the other table are coming. To simplify the implementation, we consider a join operator as a blocking operator in our work.

Each operator is split into multiple logical tasks and as- signed to a set of processors. The number of tasks is de- termined by the number of partitions for the input tuples of the operator and is usually much larger than the number of assigned processors. If an operator?s successor is a pipeline operator, it forwards the output tuples of each task to the its successor as soon as the task is finished. The target processor is chosen based on its processing capacity in terms of estimated runtime. For an operator whose successor is a blocking operator, it holds all output data on memory until it reaches a threshold, at which point it writes them into an intermediate file. Tasks of a blocking operator is scheduled to processors using a greedy algorithm to balance the load across all processors. Once a processor becomes idle, a task is allocated to the processor.

Next, let?s consider the checkpointing strategy used in query plans. If all data are pipelined from one operator to the next, once a job fails, all operators must be re-executed because all intermediate data would be lost. To avoid re-execution from the beginning, a common strategy is checkpointing and rollback recovery approach which periodically records the state of operators to a durable storage. To ensure the right recovery from a failure, an operator is required to save sufficient information to replay its state such as join hash tables and partial aggregation results. Once failures occur, the operator restarts from the last state (checkpoint).

In our work, we take a simpler checkpointing method which does not save internal states of an operator but only save the output of an operator in a durable storage. This checkpointing method works because the execution of each operator can be divided into many independent tasks and only recovering failed tasks can reduce the work to be lost. The output tuples for each operator are split or hash partitioned into n parts according to its successor operator. Accordingly, the processing of the successor is split into n logical tasks each of which works on the corresponding part of data. The tasks are then scheduled to a specific number of processors (which is usually much smaller than the number of tasks) performing the calculation such as join and aggregation. For long-running operator i, a checkpointing should be made for i, that is, the output tuples of each task are materialized to the disk. Once     a failure occurs, saying a process for i fails, only the failed task (which is running exactly when the failure occurs) need to be re-executed. As a result, the recovery overhead of the failed operator is very limited as each task usually runs in a small time.

With checkpointing strategy, the places to insert check- pointing heavily affect the overall execution performance. On one hand, not making a necessary checkpoint may lead to a loss of important computation, degrading the overall execution performance. A checkpointed operator speeds up the recovery for its successor operator as reproducing the output tuples is simply re-reading materialized data. For example, for a query plan A?B?C?D with only A checkpointed, if D fails, operator B, C and D have to be re-executed while A does not.

So only checkpointing A leads to too much lost work when a failure occurs. On the other hand, making an unnecessary checkpoint leads to an increase in the checkpointing overhead.

We also take the above query plan as an example. If the operator C is light on CPU but emits lots of tuples, it is unlikely to be worth being checkpointed. An extreme case is that the checkpointing time is wasted if no failure occurs.

B. Problem Setting  Assume that the logical plan of a query is G =< V,E >, where V is a set of operators V = v1, v2, ..., vn and E is the set of edges (or dependencies) between operators E = (vi, vj)|vi, vj ? V . When the query is executed without any failure, the expected runtime of the query Ttotal is calculated as a function of the operator and edge sets:  Ttotal = F (V,E) (1)  In a failure-prone environment, we assume that each operator is executed with a probability of failure P = p1, p2, ..., pn. If an operator vi is failed, all of its children are required to be re- executed to replay the input for vi if they are not checkpointed.

Otherwise, the checkpointed operators can simply replay its output by reading them from a durable storage. The problem is that what operators should be checkpointed. Not making a necessary checkpoint may lead to a loss of important com- putation, affecting the overall execution performance while making an unnecessary checkpoint leads to an increase in the checkpointing overhead as well. Let?s denote another variable CK = ck1, ck2, ..., ckn) to mark if a checkpoint is necessary or not for each operator. In the case of failure, the expected runtime of the query is calculated by the following equation:  Ttotal = F (V,E,CK,P ) (2)  Therefore, the problem is equivalent to get the a set of checkpoints to minimize the expected runtime:  CKopt = argmin CK  F(V, E, CK, P) (3)  C. Cost Model  In this section, we explain how we model the cost of an execution plan of a query and propose the problem for the selective checkpointing.

Processing Cost: For operator i, the processing cost TPi is the delay introduced by the operator. TPi is the sum of execution time for its input tuples and the communication time for its output tuples as shown in Equation 4.

TPi = TEXEi + TSENDi (4)  TEXEi : the cost for executing input tuples of operator i; TSENDi : the cost of sending output data to the successor for operator i.

To estimate TPi , we assume the following two functions are known for each operator i: fi(Nin tuplei): this function provides the number of output tuples produced for a given number of input tuples Nin tuplei of operator i:  Nout tuplei = fi(Nin tuplei) (5)  gi(Nin tuplei): this function provides the time to produce all output tuples for a given number of input tuples Nin tuplei of operator i:  TEXEi = gi(Nin tuplei) (6)  In practice, such infomation could come from profiling and statistics, or could be supplied by user. Specifically, the function fi is simply given with respect to the selectivity for each operator while gi is more complicated estimated. The function gi models the processing for each tuple, e.g. for an operator only with a sorting algorithm, the gi(n) is of the form n? log(n). We ignore other fixed, auxiliary cost for operators such as initiating and terminating the operation. If the output tuples of the operator i is divided into Ntask tasks with hash function h(key) = i 1 ? i ? Ntask, the partitioning cost is added to TEXEi .

The cost for sending Nout tuplei tuples depends on the number of the successor operator. For the blocking successor operator, such as group operator, the output tuples are required to be re-partitioned on the group key and transferred to all successor processors. For other operators, the output tuples are transferred to only one or a few processors.

Checkpointing Cost: We set a checkpoint value CKi for operator i:  CKi =  { 0 if a checkpoint for i is not necessary 1 otherwise  For operator i, the checkpointing cost TCi represents the cost to write the output tuples of i to disk:  TCi = CKi ? TI/O ?Nout tuplei (7)  TI/O: the time taken to write/read a tuple from/to disk.

Recovery Cost: For operator i, the recovery cost TRi  depends on the checkpointing values of both i and its pre- decessors. The recovery time of an operator i contains two parts as equation 8 shows: T ?Ri which is the cost for getting the input tuples again and T ??Ri which is the cost for the re- execution of the operator i itself.

For the former, assuming that the operator i fails, its input tuples are required to be reproduced, that is, all of its     predecessor operators need to reproduce their output tuples.

Thus, the recovery time for the operator i is determined by the predecessor who takes longer time to reproduce its output tuples as equation 9 shows. For each of its predecessors j, the cost for reproducing the output tuples of j is reading data from disk if j is checkpointed (CKj = 1). Otherwise, if j is not checkpointed (CKj = 0), j must be re-executed.

TRi = T ? Ri + T  ?? Ri (8)  T ?Ri = maxj?PREDi ((TPj + T  ? Rj )? (1? CKj) + TCj ) (9)  For the second part of the recovery cost, T ??Ri , if the successor of operator i is a blocking operator, i keeps the output tuples of received tasks until all tasks are finished.

As a result, all tasks are re-executed if this operator is not checkpointed. If the operator is checkpointed, only the failed task (which is running at the time the operator fails) needs to be re-executed. If the successor of operator i is a pipeline operator, the output of a task is sent to its successor quickly.

So still only the failed task needs to be re-executed.

Objective Function: Assuming that infinite resources are used, the expected runtime of an operator i is the sum of the processing cost of the operator TPi (Eq 4), the expected runtime of its children, the recovery cost TRi (Eq 8) and the checkpoint cost TCi (Eq 7).

Ti = TPi + max j?PREDi  (Tj) + pi ? TRi + TCi (10)  PREDi: the set of predecessor operators of i pi: the probability of a failure for operator i.

Obviously, the expected runtime of a query plan Ttotal is  estimated by the following equation according to equation 2 (where operator 0 represents the root of the query plan):  Ttotal = F (V,E,CK,P ) = T0  = TP0 + max j?PRED0  (Tj) + p0 ? TR0 + TC0 (11)  The objective is to find the optimal checkpointing value for each operator to minimize the expected runtime of a given plan.

CKopt = argmin CK  Ttotal (12)  D. Heuristic Algorithm  The straightforward approach to the problem is to generate all solutions in the full search space and get an optimal one.

However, if the size of the query plan (saying that the number of operator is n) is large, this approach is not practical as the search space SG is 2n. So we propose a heuristic algorithm (using divide-and-conquer approach) to reduce the search space. Although the algorithm cannot produce the global optimal solution, its efficiency is verified by the experiments.

x y  CKx = ?

y  x1  x2  CKx2 = ?

CKx1= ?

(a) simple plan (b) complex plan  Fig. 3. Example of Simple and Complex plan  1) Reduction of Checkpointing Candidates: First of all, we use a simple inequality to filter the operators which should not be checkpointed. Let?s firstly consider a simple sub-plan in which each operator has at most one upstream operator as Figure 3 shows.

Assume that a failure occurs at the operator y, the runtime of y depends on whether operator x is checkpointed (CKx = 1) or not (CKx = 0). When x is not checkpointed, the recovery starts from the beginning and the runtime of y should be:  Ttotaly{CKx=0} = 2? Ttotalx + T ? Ry (13)  With the checkpoint, the recovery of y starts from read tuples from disk and the runtime of y with consideration of a checkpoint is:  Ttotaly{CKx=1} = Ttotalx + 2? tx + T ? Ry (14)  tx is the I/O cost for reading/writing the output of x from/to the disk. For n tuples, the I/O cost is estimated as tx = TI/O?n.

To get benefit from making a checkpoint with a failure, the runtime of y with a checkpoint should be smaller than that without a checkpoint:  Ttotaly{CKx=1} < Ttotaly{CKx=0} ? 2? tx < Ttotalx (15)  Inequality 15 intuitively shows that the process of a query plan can gain from a checkpoint of operator i only if the checkpointing cost (writing and reading the output) of i is smaller than its execution time. This observation generates the opportunities to reduce whole search space by setting CKi = 0 if i does not satisfy the inequality.

Next, let?s consider a more complex sub-plan as the right side of Figure 3 shows. Operator y has two predecessors x1 and x2 which are executed in parallel. We take the following algorithm to decide the checkpoint values for x1 and x2. Sibling Checkpointing Algorithm firstly decides the checkpointing value for the operator with larger execution time (denoted by i) according to the single Checkpointing function.

The checkpointing value of the operator with smaller execution time (denoted by j) is decided by i?s checkpointing value. If i is not worth being checkpointed, the algorithm thinks it is better for not checkpointing j because the recovery time for their parent is determined by the execution time of i. When i is checkpointed, j is also checkpointed if the overall cost of the processing and checkpointing for j is smaller than that of     i and otherwise the checkpointing value of j is set through the single Checkpointing function.

Algorithm 1 Sibling Checkpointing Require: two sibling operators: x1 and x2 procedure SINGLE CHECKPOINTING(i)  if inequality15(i) = True then return 1 else return 0 end if  end procedure procedure SIBLING CHECKPOINTING(x1, x2)  small? min execution time(x1, x2) big ? max execution time(x1, x2) CKsmall ? 0 CKbig ? single Checkpointing(big) if CKbig = 1 then  if TPbig + TCbig > TPsmall + TCsmall then CKsmall ? 1  else CKsmall ? single Checkpointing(small)  end if end if  end procedure  2) Divide-and-Conquer Approach: Divide-and-conquer ap- proach is a common strategy that comes next to the brute-force to reduce the search space. It works as follows: query plan G is divided into sub-plans, denoted by G(i), each with a smaller search space SiG such that the globally close-to-optimal choice in SG can be found by composing the optimal choices found for each SiG. For each sub-plan, we use a brute-force search to find an optimal decision. We assume that the checkpointing decisions for operators within a sub-plan affect each other but not affect the decisions for operators in other sub-plans. In other words, the goal is to break the large plan space SG into independent subspaces SiG such that SG =  ? S (i) G . Within  each G(i), we take a brute-force approach to get an optimal solution among all possibilities.

While the plan can be arbitrarily divided into sub-plans, we generate the sub-plans based on a key insight: how check- pointing decisions affect each other. In theory, a decision for a specific operator can influence the choice of any other operator.

But from our experience, the checkpointing decision for an operator mainly affects the decision for its successors. For example, a typical case is that when an operator A produces large data and its successor B produces small data with light computations, then it is better to checkpoint B rather than A.

Based on this observation, we make each sub-plan a small component of the original plan, as described below.

Partitioning into Sub-plans: Each sub-plan consists of one or several sibling operators and their unique successor. Sibling operators with the same successor are executed concurrently and the checkpointing decisions for them are influenced by each other as shown in algorithm Sibling Checkpointing.

We take a query plan for TPC-H Query 3 as an example (Figure 4). As the algorithm traverses the plan in topological  Fig. 4. An example for the sub-plan generation  order, the first sub-plan G(1) consists of operator J4 and its two predecessors S6 and S7. The next sub-plan G(2) comes with J4, its sibling S5 and its successor J3. When an operator has no siblings, the sub-plan only contains two operators such as G(3).

Search in a Sub-plan: For each sub-plan G(i), the algo-  rithm is to make the optimal checkpointing decisions for all operators in the sub-plan to minimizes the expected runtime of the successor of the sub-plan using a brute-force approach.

The number of operators within any individual sub-plan is typically small. With a query plan presented as a binary tree with n operators, the number of operators in each sub-plan is at most 3 and the number of sub-plans is between n+12 ? 1 (for full binary tree in which every operator other than the leaves has two children) and n?1 (for degenerate binary tree in which every operator only has one child) where n > 1. So the search space is varying from (n+12 ?1)?2  3 to (n?1)?23  which is much smaller than the original 2n.

Algorithm: Overall, the selective checkpointing algorithm  is described as follows: (1) For a given query plan G, the algorithm firstly finds the candidate operators to be check- pointed based on the Sibling Checkpointing Algorithm de- scribed above; (2) Then it generates the first sub-plan; (3) It enumerates all solutions within the sub-plan to find the optimal solution for the sub-plan; (4) It generates the next sub-plan and repeats the process until the entire plan is visited.



V. EVALUATION  We conducted all experiments with a variety of queries in a 16-node cluster. Each node uses 2.40 GHz Intel Xeon processor with 8 cores running 64-bit Debian 6.0 with 24GB RAM. The experiments are performed to verify: (1) Different fault-tolerant strategies heavily affect the overall runtimes of queries; (2) Our selective checkpointing mechanism can choose reasonable operators to be checkpointed; (3) The mechanism outperforms other fault-tolerant strategies; (4) The divide-and-conquer algorithm has a smaller overhead than brute-force approach while keeping a similar effectiveness; (5) Our mechanism can achieve similar slowdown with Hive (Hadoop) upon a failure; (6) The system (ParaLite) scales well with the mechanism.

15270600 600  84 0.002  150 150 150  600 270 270 6 1        110150 22 66 0.0006      S1 S1  S1  S6  S5  (a) Query 1  (b) Query 2  (c) Query 3  (d) Query 4  (e) Query 5  Fig. 5. Query plans of several TPC-H queries: all numbers are in millions  A. Effects of Fault-tolerant Strategy  We firstly show that different fault-tolerant strategies affect the overall performance of a query plan. We perform typical SQL tasks (selection (S), join (J) and aggregation (G)) through queries 1 and 2 from Figure 5 and a special task with User- defined Executable (U) by query 3. The number of tuples of each relations are shown in the query plans and each tuple is about 0.2KB.

Recall that our cost model requires two functions for operator i: fi provides the number of output tuples for a given number of input tuples and gi gives the processing time for a given number of input tuples (shown in equation 5 and 6).

Function fi is determined by the selectivity of the operator. We define gi based on our measurements. For example, for a sub- query, the execution time (seconds) is estimated as 3? 10?6  times the number of input tuples. We estimate the failure probability of each operator by pi =  Ti Ttotal  ? Z where Ti is the estimated runtime of operator i, Ttotal is the runtime of the query and Z is the expected number of failures for the query which can be specified by administrators according to their observations.

Figure 6 through 8 show the actual and predicted runtime for Queries 1 through 3. X-axises are the fault-tolerant strate- gies for operators. For example, in Figure 7, NN means No checkpoint for S1 and G0 while NC indicates No checkpoint for S1 and a checkpoint for G0. Note that the order of the operators is from left to right in the query plans. For Query 1, to show the results clearly, we assume that no checkpoint for S2. We inject a failure at the middle of the execution.

? Different fault-tolerant strategies heavily affect the overall runtimes of queries. The differences between the overall execution time with the best and worst strategy are high: For normal execution without a failure, the differences are 30% for Query 1, 15% for Query 2, and 61% for Query 3. For the execution recovered from a failure,  the differences are 14% for Query 1, 30% for Query 2, and 56% for Query 3. Each of queries 1 through 3 achieves the best performance with a different fault- tolerant strategy in both failure-free (normal) and failure- prone environments. For all queries, checkpointing noth- ing (NN) is the best option if no failure occurs. However, once a failure occurs, only Query 3 achieves the best performance with this strategy while Query 1 requires the strategy NC and Query 2 requires CC.

? Our mechanism can identify the best strategy for all queries. The predicted execution time can not exactly match but is close to the actual time. Most of the differ- ence come from the simple model for the data transfer and the assumption that data are balanced among processes, which are not satisfied in real cases. The model was still able to estimate the relative order between execution times of different strategies. Our mechanism simulates the execution in a failure-prone environment and predicts the runtime of a query plan with the assumption that each operator has a fixed probability of failure, so it considers the strategy with the smallest predicted execution time with failures as the best one. Therefore, for Query 1, the option of only checkpointing the output of the join operator (NC) is chosen although the execution time with option NC in normal situation is slightly larger than that with the option of checkpointing nothing (NN). For Query 2, the strategy of checkpointing everything (CC) is considered as the best strategy by our mechanism. For Query 3, it decides to checkpointing nothing.

? Finally, restarting a query, the strategy used in most existing database systems, produces the largest slowdown upon a failure among all strategies. Strategies other than RESTART reduce recovery times with minimal impact on the execution time without failures. For Query 1 through 3, RESTART is 14%, 30% and 39% worse than the best strategy respectively.

NN NC(S) CN CC RESTART  E la  ps e  T im  e( s)  Query 1  Real(Normal) Predict(Normal) Real(w/ failure) Predict(w/ failure)  Fig. 6. Query 1 (JOIN)  B. Effectiveness of Selective Checkpointing  In this section, we apply several fault-tolerant strategies to two TPC-H queries, Query 4 and 5 from Figure 5.

We compare the overhead for choosing checkpointing op- erators and the places of checkpointing with both Divide-          NN CN NC CC(S) RESTART  E la  ps e  T im  e( s)  Query 2  Real(Normal) Predict(Normal) Real(w/ failure) Predict(w/ failure)  Fig. 7. Query 2 (GROUP)          NN(S) CN NC CC RESTART  E la  ps e  T im  e( s)  Query 3  Real(normal) Predict(normal) Real(w/ failure) Predict(w/ failure)  Fig. 8. Query 3 (UDX)  and-Conquer (DaC) and Brute-Force (BF) algorithms. From Table I, we can see that although the overhead with both algorithms is very small but DaC is several times faster than BF. The differences exponentially increase with the number of operators. For the tested queries, the places for checkpointing are the same for both algorithms.

We inject a failure at 80% point of the execution for Query 4 and at 50% point of the execution for Query 5. Figure 9 and Figure 10 show the actual and predicted execution time with/without a failure. In the figures, CKNONE means that no operators are checkpointed; CKALL indicates all intermediate data are materialized; SELECTIVE shows that operators chosen by our selective checkpointing mechanism are checkpointed.

Firstly, SELECTIVE outperforms other strategies for both queries. It adds the minimal time upon a failure to the execution time of the NONE strategy in failure-free situations.

For Query 4, SELECTIVE adds 3% overhead to the execution time of NONE (the best strategy) when no failures occur, while it is 58% faster than NONE when a failure is injected. For Query 5, SELECTIVE is 6% slower than NONE in failure-free execution but is 17% better than NONE with a failure. While  TABLE I DIVIDE-AND-CONQUER COMPARED WITH BRUTE-FORCE  Overhead(second) Checkpoints Query4 Query5 Query4 Query5  Divide-and-Conquer 0.011 0.0095 S1, J2 G0,S2,J3,S4,S5 Brute-force 0.15 0.092 S1, J2 G0,S2,J3,S4,S5         CKNONE SELECTIVE CKALL RESTART  E la  ps e  T im  e( s)  Query 4  Real(Normal) Predict(Normal) Real(w/ failure) Predict(w/ failure)  Fig. 9. Query 4 (SJJJ)       CKNONE SELECTIVE CKALL RESTART  E la  ps e  T im  e( s)  Query 5  Real(Normal) Predict(Normal) Real(w/ failure) Predict(w/ failure)  Fig. 10. Query 5 (SJJG)  SELECTIVE and CKALL both produce several times smaller recovery time than NONE and RESTART, SELECTIVE out- performs CKALL in the overall execution time from 6% to 44%. Specially, although RESTART is at most 5% better than SELECTIVE when no failures occur, it is 25% and 60% slower than SELECTIVE upon a failure for both queries respectively.

Secondly, SELECTIVE chooses reasonable operators to be checkpointed. For Query 4, as the previous operator J4 produces a large number of output tuples, it is better to checkpoint its predecessor J2 if it takes a small time and produces a small number of output tuples. As we expected, SELECTIVE checkpoints the operator J2. Meanwhile, based on the Sibling Checkpointing Algorithm described in section 4.3, operator S1 should be checkpointed too. For Query 5, all operators are checkpointed except J1. This is also reasonable because re-producing the output of J1 is not time-consuming since its two input source are checkpointed.

C. Comparison of Slowdown  We conducted a set of experiments to compare the slow- down of Query 5 with ParaLite and Hive[18]. Hive is a data warehouse system built on top of Hadoop[3]. It translates the HiveQL (SQL-like language) into MapReduce jobs which are executed by Hadoop. Hadoop provides fine-grained fault tolerance because it stores all intermediate data into a durable storage (HDFS). Once a node fails, all failed tasks on that node rather than the whole job are re-scheduled to another node to be executed. ParaLite uses our selective checkpointing mechanism to materialize the output of a set operators and restart all related tasks once a failure occurs.

ParaLite Hive ParaLite Hive ParaLite Hive  E la  ps e  T im  e( se  co nd  )  20% 50% 80%  Recovery Normal  Fig. 11. The slowdown of Query 5 with ParaLite and Hive  The results are shown in Figure 11. The X-axis is the percentage of the normal completion time of Query 5 when a failure is injected. Firstly, it is not surprised that ParaLite is about twice faster than Hive. One reason for the superiority is that not all the operators are checkpointed in ParaLite.

Moreover, Hive uses sort-join while ParaLite uses hash-join.

In addition, the start-up overhead for Hadoop cannot be ignored. Hive uses 3 MapReduce jobs to express the query and each takes 10 15 seconds before all map tasks are started. Secondly, the slowdown for both system are similar.

However, the slowdonw of Hadoop is limited to the interval of heartbeat message send to the JobTracker(master) from each TaskTracker(worker). After a fix time (we set it 30 seconds) from receiving the last heartbeat, if the master does not receive a new heartbeat message, it sets the status of the worker failed and re-schedules the failed tasks. So for a light-weight job whose execution time is not much larger than the interval, the slowdown becomes larger because it needs to wait for the task re-execution. On the other hand, ParaLite detects the failure immediately after a process fails and then starts the recovery.

D. Scalability  To evaluate the scalability of ParaLite, we perform TPC- H Query 3 on 10, 20 and 30 nodes with the scale factor 100, 200 and 300 respectively. As Figure 12 shows, ParaLite scales well with the increase of nodes. The selective checkpointing mechanism adapts to clusters with different number of nodes.

10 20 30  E xe  cu tio  n T  im e(  s)  # of nodes  Fig. 12. The scalability of TPC-H Query 2

VI. CONCLUSION  To efficiently support intra-query fault tolerance for long- running queries, we propose a selective checkpointing mecha- nism which automatically chooses necessary operators whose output data are worth being checkpointed to minimize the expected runtime of the query in a failure-prone environ- ment. The mechanism provides a cost model to simulate the execution of the query plan and takes a divide-and-conquer approach to solve the problem. The mechanism is implemented in a share-nothing parallel database ParaLite and evaluated with several TPC-H queries. The experimental results firstly show that different fault-tolerant strategies affect the overall runtimes of queries. Our selective checkpointing can choose reasonable operators to be checkpointed and outperforms other fault-tolerant strategies. In addition, the divide-and-conquer algorithm taken by our mechanism has a smaller overhead than brute-force approach while keeping a similar effectiveness.

