Mining Frequent Closed Itemsets Using Antecedent-Consequent Constraint and  Length-Decreasing Support Constraint

Abstract?At present, many frequent itemsets mining algorithms adopt a constant support threshold value strategy, which is not convenient to potential valuable long itemsets discovery. Length-decreasing support constraints can address this problem probably. Existing algorithms make improvements on classical algorithm, which result to low efficiency. Tailored to the medical data, this paper proposes a frequent closed itemsets mining algorithm called ACLCMiner, which uses the antecedent-consequent constraint and the length-decreasing support constraint. With the antecedent- consequent constraint, the algorithm greatly reduces the number of generated frequent itemsets. The experimental results show that ACLCMiner is efficient and can find more long itemsets with potential values.

Keywords- frequent closed itemsets; antecedent-consequent constraint; length-decreasing support constraint; FP-tree

I. INTRODUCTION  Association rule mining is an important research subject in data mining. The study of association rules mainly contains two research directions: frequent itemsets discovering and rules correlation analysis. The contributing factor of the low algorithm efficiency is frequent itemsets discovering, so many frequent itemsets discovering algorithms have been raised to address the problem, including constraint-based frequent pattern mining[1-2], frequent closed itemsets mining algorithm[3], maximal frequent itemsets discovering algorithm[4] and so on. These algorithms make support threshold a constant, so they can not find long itemsets which have a low support with potential values. In practical applications, the users are often actually interested in the rules generated from long itemsets.

Thus, the length-decreasing support constraint[5] is presented, which can help to find the long itemsets. Then many algorithms based on the length-decreasing support constraint are proposed, such as, LPMiner[5], SLPMiner[6], WLPMiner[7-8], BAMBOO[9] and SCCMETree [10].

Association rules can be used in the medical field to improve the accuracy of diagnosis and treatment. Each rule represents a simple prediction model, and the decision attribute (diagnosis type) is determined by non-decision attribute (dialectical factors). In another word, the position which the attributes appear in the rule is limited. Therefore, according to the characteristics of medical data, this paper presents a frequent closed itemsets mining algorithm based on the antecedent- consequent constraint and the  length-decreasing support constraint(ACLCMiner). The antecedent-consequent constraint[11] is used to restrict the number of generated frequent itemsets, and the length-decreasing support constraint[5] is used to get more valuable long itemsets. ACLCMiner adopts the strategy of CLOSET+[3] to get frequent closed patterns and uses some pruning strategies to reduce the search space. The compound frequent pattern tree and Rtree are used to store transaction data and generated frequent closed itemsets, respectively.

Experimental results show that the efficiency of ACLCMiner is significantly higher than BAMBOO.



II. PROBLEM STATEMENT AND RELATED WORK  A. Problem Statement  Here we give the definition of association rules. Assume I={i1, i2,?, in}is an item set, which parameters i1, i2,?, in are called items. Transaction T is a set of item, T I. The set of transactions is denoted by TDB, as in Table ?. An itemset X is a non-empty subset of I and it is called k-itemset if it contains k items (i.e., |X|=k). A transaction T is said to contain itemset X if X?T. Association rules are those implications depicted as X Y, where X I, Y I, and X Y=?.

TABLE I. A transaction database TDB  tid Set of items Ordered item list  001 a, c, f, m, p f, c, a, m, p  002 a, c, f, i, m, p f, c, a, m, p, i  003 a, b, c, f, m f, c, a, b, m  004 b, f f, b  005 b,c,p c, b, p  Definition 1 The support of the rule X Y is the ratio between the number of transactions containing X and Y in TDB and all the transaction number (i.e., P(X?Y)), which can be written as support(X Y). The confidence of the rule X Y is the ratio between the transaction number including X and Y and those including X (i.e., P(Y|X)), which can be written as confidence (X Y).

Definition 2 Find out all X?s non-empty subset t to each frequent itemset X. Association rule t?X-t can be obtained when sup(X)/sup(t)>=minconf and minconf is defined as the minimum confidence of the rule. Sup(X)/sup (t) is defined as the confidence of the rule t?X-t where t is the antecedent of the rule and X-t is the consequent of the rule.

Definition 3(length-decreasing support constraint) A  function ( )f l with respect to a transaction database TDB is called a length-decreasing support constraint w.r.t. TDB, if it satisfies ( )0 1 ( ) 1f l f l? + ? ?  for any positive integer l .

An itemset X is frequent w.r.t. length-decreasing support constraint of ( )f l , if sup( ) (| |)X f X? .

Property 1 Smallest Valid Extension[5] (SVE) Given itemsets X and X ? , where X X? ?  and sup( ) (| |)X f X< .

Then the necessary condition which X ?  is a frequent set w.r.t. ( )f l  is that the length of X ?  is greater or equal to  ( )( ) ( ) ( ){ }1 sup min | supf X l f l X? = ? .

Definition 4 (closed itemset) An itemset X is a closed itemset if there exists no proper superset X X? ? such  that ( ) ( )sup supX X? = .

Definition 5 (frequent closed itemset) If an itemset X is a closed itemset and frequent w.r.t. ( )f l , X is called a frequent closed itemset which satisfies the length-decreasing support constraint of ( )f l , in short, X is a frequent closed itemset w.r.t. ( )f l .

B. Related Work  Mining frequent itemsets using the length-decreasing support constraint was first proposed in [5], which also proposed the algorithms with the length-decreasing support constraint (LPMiner and SLPMiner). These algorithms could effectively find all itemsets that satisfy a length-decreasing support constraint. LPMiner used the SVE property to add several pruning methods to compress the search space on the basis of FP-Growth algorithm. However, LPMiner has a number of limitations. Firstly, the efficiency of the algorithm is not high; secondly, when the data is large, the constructed FP-tree is too large to be placed in memory; thirdly, its result set may contain a large number of redundant (i.e., non-closed) itemsets. WLPMiner[7-8] weighted each item in the itemsets differently on the basis of LPMiner algorithm.

To tackle the shortage of LPMiner, BAMBOO[9] was proposed, which is a frequent closed itemsets mining algorithm with length- decreasing support constraint. Taking advantages of the property of length-decreasing support constraint, Invalid Item Pruning method and Unpromising Prefix Itemset Pruning method were proposed to make BAMBOO more efficient than LPMiner. But the overall efficiency was affected due to the functionally overlapping in pruning of aforementioned methods in BAMBOO.



III. ACLCMINER ALGORITHM  A. Length-Decreasing Support Constraint  According to definition 2, the length-decreasing support constraint ( )f l is defined as:  ( ) ( ) ( )  max 1 min  max min min max / max min ( )  min max  min max  s l l  s l l s s l l f l  l l l  s l l  ? ? ? ?  + ? ? ? ?? = ?  < <? ? ??  when  when  when  (1)  Where l is called the length of an itemset, [ ]max, min 0,1 max mins s s s? ?and .lmax is the maximum  length while lmin is the minimum length. When the length of frequent itemsets is smaller or equal to lmin, the threshold of support gets the maximum value smax. When the length of frequent itemsets is greater or equal to lmax, the threshold of support will be the minimum value smin and the support will no longer reduce. Figure 1 gives the function curve diagram for length-decreasing support constraint.

Figure 1. Function curve of the minimum support.

B. Antecedent-Consequent Constraint  Based on the characteristics of the data, we will divide the attributes of the database into decision attributes and non-decision attributes in accordance to these attributes as the different aspects in the rules. Non-decision attribute appears in the antecedent of rule, while the decision attribute appears in the consequent. For the rule X Y, we need to calculate support (Sup(X Y)=P(X?Y))and confidence (conf(X Y =P(Y|X) =P(X?Y)/P(X)), without the need to calculate P(Y) value. So we do not need to generate the frequent itemsets which only contain decision attributes.

Antecedent-consequent constraint restricts each itemset generate only one rule, which greatly reduces the number of generated rules.

Our algorithm adds the constraint of the antecedent and consequence, so we need to encode the attribute values block by block in TDB. For an itemset I={i1, i2,?, in}, we encode the decision attributes those act as the antecedent of rules from zero to m and the non-decision attributes those act as the consequents of rules from m+1 to n.

C. Storage Structure  1) Compound Frequent Pattern Tree: Since the use of length-decreasing support constraint, if the itemset X is frequent, the support of X must satisfy that support threshold corresponding to the length of X, which is (| |)f X .

Therefore, we re-define the structure of f_list to make the determinaton of frequent 1-itemset and the length of transitions containing this itemset correlated. According to the data in Table , we give the structure of f_list, as  1 lmax  minsup  smax=f(lmin)  smin=f(lmax)  lmin Length of itemsets     shown in Figure 2. Itemset is counted according to the length of transactions. Assume the parameters of ( )f l are: max 0.8s = , min 0.4s = , min 3l = , max 5l = . So the function ( )f l is: ( )( ) 0.8 3f l l= ? , ( )( ) 0.6 4f l l= = ,  ( )( ) 0.4 5f l l= ? , it is easy to figure out that {f, e, a, g, h} is the complete set of frequent closed 1-itemsets w.r.t ( )f l .

For {b}, the support of any length does not satisfy ( )f l , so any itemset containing b is not frequent. The reason is given in the invalid item pruning strategy. Therefore, {b} is non-frequent itemsets.

f 4  e 4  a 3  b 3  g 3  h 3  i 1  Figure 2.  Compound f_list structure.

Figure 3.  FP_tree.               Figure 4.   CFP_Tree.

The compound nodes in the compound frequent pattern tree (CFP_Tree) are obtained by merging.

Theorem 1. Given CFP_Tree. For nodes P and C, if each path containing P has C and there is ( ) ( )sup supP C=  in  each branch, C can be merged with P to form a new node and C can be removed from the CFP_Tree.

Proof: If each path containing P in the CFP_Tree has C and there is ( ) ( )sup supP C=  in each branch. Then P and C  always appear together, or both of them do not appear. So all of the closed itemsets generated must include both P and C or neither. Therefore, P and C can be merged.

Figure 3 shows the FP_tree constructed with the data in Table , while Figure 4 is the CFP_Tree corresponding to the FP_tree. CFP_Tree has fewer nodes than FP_tree. Also, in the f_list P and C can be merged into an item. Therefore, CFPTree need smaller space than FP_tree.

2) RTree Structure: To speed up the closure checking, we should choose the tree-like structure to store the generated frequent closed itemsets. This structure has good performance in the search speed and storage space.

Therefore, we propose a structure called RTree to store the generated frequent closed itemsets. Each node stored the item and its support. Figure 5 shows an example which stores totally four closed itemsets: {f,e,a,g,h:2}, {f,e,a,g:3}, {e:4}, {f:4}.

Figure 5.  RTree structure.

In RTree, if the supports of parent node and the supports of child nodes are different, a frequent closed itemsets from the root node to the parent node is then formulated. For example, in Figure 5, the support of {f} node is 0.8 and the support of {e} node is 0.6, then we have a frequent itemset {f: 0.8}. Also, if the current node is a leaf node, a frequent closed itemsets from the root node to the current node is formulated, such as {f, e, a, g, h: 0.4}.

D. Pruning Strategy  In ACLCMiner, an efficient pruning method, Invalid item pruning, is adopted, which is based on the SVE property. Meanwhile, according to the antecedent- consequent constraint and the requirements of closed itemsets, ACLCMiner uses decision attribute pruning, item merger and sub-itemset pruning to reduce the search space.

1) Invalid Item Pruning: Definition 7(Invalid item) Given a conditional database TDB|P w.r.t. a particular prefix itemset P. For any item x which appears in TDB|P, We maintain a total number of max_ xl  supports, denoted as [ ]s 1 max_x xl? , where max_ xl is the maximal length of the transactions which contain item x and [ ] ( )s 1 max_x xk k l? ? records the support of item x in  transactions which the length is k. Item x is called an invalid  item w.r.t. TDB|P if [ ] [ ] ( ) max_  , sup s | | xl  x x j k  k k j f k P =  ? = < +? .

Here is Invalid item pruning. For a particular prefix itemset P and its conditional database TDB|P, there is no hope to grow P with any invalid item w.r.t. TDB|P to get closed itemsets satisfying the length-decreasing support constraint. Invalid Item Pruning is used in the original transaction database and the conditional transaction database.

When building a frequent pattern tree, other than LPMiner algorithm based on smin, the support and the length of transactions are taken into account at the same time. This constructs a smaller CFP-tree. From Figure 2, we know that  f  e  a,g  h  root  f: 0.8  e: 0.6  h: 0.4  a,g: 0.6  e: 0.8  f: 4  e: 3  a: 3  h: 2  g: 3  e: 1  h: 1  f: 4  e: 3  {a,g}: 3  h: 2  e: 1  h: 1  rootroot  Total support count (sup)  6 1  Length of transaction Support count ( [ ]s k )  2  1 6  1   5  2  3  1 6  1   5  2  6  1   5  2  6  1   5  2  3  1 6  1   5  1  2  1 6  1   3 1     [ ]s 5 0.2b = , [ ]s 3 0.2b = and [ ]s 2 0.2b = , then [ ]sup 5 0.2b = ,  [ ]sup 3 0.4b = and [ ]sup 2 0.6b = .We can get  [ ] ( ),supbk k f k? < , so {b} is a invalid items.

2) Decision Attribute Pruning: Thanks to the antecedent-consequent constraint, we do not need calculate the support of itemsets including only the decision attributes. Therefore, we use a pruning strategy for the decision attributes. First, the algorithm scans the transaction database to generate the frequent 1-itemsets and the support counts. Then the frequent 1-itemsets are respectively sorted in descending order of support according to decision attributes and non-decision attributes, and all decision attributes are ranked in front of non-decision attributes.

Hence, the non-decision attributes are the children of the decision attributes in the constructed CFP-tree. In the process of mining frequent itemsets, we only generate conditional CFP-tree for the decision attributes, while the decision attributes are not considered. This makes that there are not itemsets which contain only the decision attributes in the generated frequent itemsets. This will reduce the number of the conditional pattern tree generated and improve the efficiency of algorithm.

3) Unpromising Prefix Itemsets Pruning: Since we are only interested in closed itemsets, we choose two existing pruning methods to stop mining patterns with unpromising prefix itemsets as soon as possible. They have been popurlarly used in several closed itemset mining algorithms, such as CLOSET+.

The first pruning technique is item merging. Given a conditional database TDB|P w.r.t. a particular prefix itemset P. If the itemsets X can be contained in each of the transactions in the TDB|P, X can be merged with P to form a new prefix, and we do not need to mine closed itemsets containing P but no X.

The second pruning technique is sub-itemset pruning. Let X be the frequent itemset currently under consideration. If X is a proper subset of an already found frequent closed itemset Y and sup(X)=sup(Y), then X and all of X?s descendants in the TDB|X cannot be frequent closed itemsets and thus can be pruned.

E. The Algorithm  The ACLCMiner algorithm is shown as follow.

ACLCMiner TDB, smin, smax, lmin, lmax, m INPUT TDB: a transaction database smin, smax, lmin, lmax are the parameters of ( )f l and m: the coding boundary for the decision attributes and the non-decision attributes.

OUTPUT  the complete set of closed itemsets that satisfy the length-decreasing support constraint BEGIN 01. ITree NULL; f(l) F(smin, smax, lmin, lmax) 02. ACLCMiner( , TDB); 03. Out(ITree);  ALGPRITHM aclcminer P, PTDB INPUT P: a prefix itemset PTDB: the conditional database w.r.t.

prefix P.

BEGIN 04. f_list Create_acconstraint_flist(PTDB, m); 05. f_list invalid_item_pruning(f_list, f(l), |P|); 06. S item_merging(f_list); P P S; f_list f_list-S; 07. if ( P ) 08.   if (sup(P) f(|P|) ) 09.     if(sub_itemset_pruning(P, ITree)) 10.       return; 11.     else 12.       insert(ITree, P); 13.   end if 14. end if 15. if (f_list ) 16.   CFPTree Create_cfptree(f_list, PTDB); 17.   for all (x f_list and x m) do 18.      P? P {x}; 19.      PTDB? build_cond_database( P? , CFPTree); 20.       ACLCMiner( P? , PTDB? ); 21.   end for 22. end if END  At first, ACLCMiner uses the input values of smin, smax, lmin, lmax to get the function ( )f l , and then calls the subroutine aclcminer(P, PTDB). The subroutine first builds f_list according to the decision attributes pruning strategy (line 03). Then it applies the invalid item pruning method, and the set of valid items is denoted as f_list (line 04). Next aclcminer() uses the item merging technique to identify the set of items that has the same support as P, and denoted as S, S will be merged with P and removed from f_list (line 05).

While getting a new itemset, the algorithm will first check if it can satisfy the support constraint, if not, the algorithm do not need to check if it is closed and it will not be inserted into the frequent closed itemsets(lines 08-13). If an itemset is a closed itemset that satisfies the support constraint, it will be inserted into the RTree (line 12). Next the conditional CFP- tree is built (line 16), and aclcminer() recursively calls itself to mine the closed itemsets with the length-decreasing support constraint by growing prefix P under the bottom-up divide-and-conquer paradigm (lines 17-25). Finally, according to the RTree, we will output the frequent closed itemsets satisfied requirements. ACLCMiner removed the unpromising prefix pruning strategy in BAMBOO, because it repeats with the invalid item pruning strategy.



IV. EXPERIMENTAL RESULTS  This experiment is carried out on an Intel Pentium PC, 2GB memory, 2.4GHz. All procedures were implemented using JAVA language. Data is the real data from the HIS system database of China-Japan Friendship Hospital, and we selected two groups of data , which are the effectiveness of Traditional Chinese Medicine(TCM) and the herbal nature in the coronary heart disease database. The characteristics of     the data table are shown in Table (the last column shows the average and maximal transaction length). Table shows the two parameters of ( )f l , i.e. sim and smax. For the data of TCM effectiveness lmin is 8 and lmax is 20, and for the data of the herbal nature lmin is 3 and lmax is 8. We ran respectively BAMBOO and ACLCMiner on the same environment, and the experimental results are shown in Table . The comparison results for the two algorithms are shown in Figure 6 and Figure 7.

TABLE II. DATASET CHARACTERISTICS  TABLE III. EXPERIMENTAL RESULTS         support  N um  b er  o f i  te m  se ts  (t ho  us an  d)         R un  tim e  (s )  Figure 6.  Comparison between ACLCMiner and BAMBOO TCM effectiveness .

support  N u  m b  e r  o f i  te m  se ts  (t h  o u  sa n  d )         R u  n tim  e (  s)  Figure 7.  Comparison between ACLCMiner and BAMBOO herbal nature .

From Figure 6 and Figure 7 we could see that ACLCMiner generates a small number of frequent closed  itemsets, and the algorithm runtime is faster than BAMBOO. And as decreasing the value of smin and smax, the effect is increasingly apparent. ACLCMiner?s higher performance stems from two aspects: on the one hands, it adopts the antecedent-consequent constraint to mine much small number of valid itemsets than BAMBOO, on the other hand, it adopts effective pruning methods and storage structure, which greatly improves the running efficiency.

From the comparison of Figure 6 and Figure 7, we can see when the decision attributes are more and the number of items contained in the transaction is large, the effect is particularly evident.



V. CONCLUSIONS  In this paper, according to the characteristics of the medical data, a freqnet closed itemsets mining algorithm based on the antecedent-consequent constraint and the length-decreasing support constraint (ACLCMiner) is proposed. The proposed algorithm greatly saves memory space and improves the efficiency. Meanwhile, the algorithm generates interesting rules and the number of generated rule is small. The rules mined from ACLCMiner can help to improve the accuracy of clinical syndromes, so it has very important theoretical and practical value. At the same time, ACLCMiner can also be extended to other areas.

