Self-Tuned Kernel Spectral Clustering for Large

Abstract?We propose a parameter-free kernel spectral clus- tering model for large scale complex networks. The kernel spectral clustering (KSC) method works by creating a model on a subgraph of the complex network. The model requires a kernel function which can have parameters and the number of communities k has be detected in the large scale network.

We exploit the structure of the projections in the eigenspace to automatically identify the number of clusters. We use the concept of entropy and balanced clusters for this purpose. We show the effectiveness of the proposed approach by comparing the cluster memberships w.r.t. several large scale community detec- tion techniques like Louvain, Infomap and Bigclam methods. We conducted experiments on several synthetic networks of varying size and mixing parameter along with large scale real world experiments to show the efficiency of the proposed approach.

Keywords-kernel spectral clustering, number of clusters, parameter-free spectral clustering

I. INTRODUCTION  In the modern era complex networks are ubiquitous. Their  presence ranges from domains like road graphs, social net-  works, collaboration networks, e-mail networks, trust net-  works, biological networks to financial networks. Complex  networks can be represented as a graph G(V,E), where the vertices V represent the nodes of the graph and the edges  are represented by E. The nodes represent the entities in the  network and the edges determine the relationship between  these entities. Real world networks exhibit community like  structure. This means that the nodes which are part of one  community tend to be more densely connected within the  community and sparsely connected between the communities.

The problem of community detection has received a lot of  attention [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11],  [12] and has often been framed as graph partitioning and  graph clustering [13]. In this paper we refer to clusters and  communities interchangeably.

Spectral clustering methods [9], [10], [11], [12] are widely  used for community detection. Spectral clustering methods  work by performing an eigen-decomposition of a normalized  Laplacian matrix derived from the affinity matrix of the  nodes in the network. The disadvantage of spectral clustering  methods is that they require to compute and store the affinity  matrix. As the size of network increases it becomes in-feasible  to store the N ? N affinity matrix where N is the number of nodes. Also, these methods do not have an out-of-sample  extension property. Hence, they cannot scale for very large  networks (millions of nodes).

Recently, a kernel spectral clustering formulation based on  weighted kernel principal component analysis (PCA) with a  primal-dual framework was proposed in [14]. The method was  extended for community detection in [7] and [22]. The KSC  model requires a kernel function and the number of clusters k  has to be detected. For model selection, the proposed approach  in [7] and [22] uses the concept of Modularity [16]. The  Modularity criteria needs to store a N?N matrix which makes it in-feasible for large scale networks. An approach to apply  KSC for big data networks was proposed in [15]. We use the  concepts proposed in [15] to build the model. In this paper  we propose to make the kernel spectral clustering approach  parameter-free or in other words  ? Use the normalized linear kernel function.

? Automatically detect the number of communities (k) in  the large scale network.

In section II, a description of the KSC method for large  scale networks is given. In section III, we explain the proposed  approach for making KSC parameter-free. In section IV, we  perform experiments on various synthetic and real world  datasets. Section V provides conclusion to the paper.



II. KERNEL SPECTRAL CLUSTERING FOR LARGE SCALE  NETWORKS  The KSC [14] and [7] method consists of three steps. The  first step is building the model on the training data. The second  step is to estimate the model parameters using validation data.

The final step is the out-of-sample extension or community  inference for unseen test nodes.

For building the model, the KSC method selects a sub-  graph which captures the inherent community structure of  the network. Several approaches have been proposed for  sampling a graph including [17], [18], [19], [20]. The Fast  and Unique Representative Subset (FURS) selection approach      was proposed in [18] and used in [15]. We use the FURS  technique for training and validation set selection.

For large scale networks, the training data comprises the  adjacency list of all the nodes vi, i = 1, . . . , Ntr. Let the training set of nodes be represented by Vtr and the training  set cardinality be Ntr. The validation and test set of nodes are  represented by Vvalid and Vtest respectively. The cardinality  of these sets are Nvalid and Ntest respectively. These sets  of adjacency lists can efficiently be stored in the memory as  real world networks are highly sparse and there are limited  connections for each node vi ? Vtr. The maximum length of the adjacency list can be equal to N . This is the case when a  node is connected to all the other nodes in a network.

A. Model Building and overcoming any kernel parameters  For Vtr training nodes, D = {xi} Ntr i=1, such that xi ? R  N .

Given D and a user-defined k (maximum number of clusters in the network), the primal formulation of the weighted kernel  PCA [14] is given by:  min w(l),e(l),bl    k?1?  l=1  w(l) ?  w(l) ?  2Ntr  k?1?  l=1  ?le (l)?D?1? e  (l)  such that e(l) = ?w(l) + bl1Ntr , l = 1, . . . , k ? 1  (1)  where e(l) = [e (l) 1 , . . . , e  (l) Ntr  ]? are the projections onto the eigenspace, l = 1, . . . , k ? 1 indicates the number of score variables required to encode the k communities, D?1? ? R  Ntr?Ntr is the inverse of the degree matrix associated to  the kernel matrix ?. ? is the Ntr ? dh feature matrix, ? = [?(x1)  ?; . . . ;?(xNtr ) ?] and ?l ? R  + are the regularization  constants. We note that Ntr ? N i.e. the number of nodes in the training set is much less than the total number of nodes  in the large scale network. The kernel matrix ? is obtained by calculating the similarity between the adjacency list of  each pair of nodes in the training set. Each element of ?, denoted as ?ij = K(xi, xj) = ?(xi)  ??(xj) is obtained by calculating the cosine similarity between the adjacency lists  xi and xj which has been shown to be an effective similarity  measure for large scale data [25]. Thus, ?ij = x ?  i xj  ?xi??xj?  can be calculated efficiently using notions of set unions and  intersections. This corresponds to using a normalized linear  kernel function K(x, z) = x ?z  ?x??z? [24]. The clustering model  is then represented by:  e (l) i = w  (l)??(xi) + bl, i = 1, . . . , Ntr (2)  where ? : RN ? Rdh is the mapping to a high-dimensional feature space dh, bl are the bias terms, l = 1, . . . , k ? 1.

However, for large scale networks we can utilize the explicit  expression of the underlying feature map and dh = N . Since the KSC formulation is valid for any positive definite kernel,  we use a normalized linear kernel function to avoid any kernel  parameter. The projections e (l) i represent the latent variables  of a set of k ? 1 binary cluster indicators given by sign(e (l) i )  which can be combined with the final groups using an encod-  ing/decoding scheme. The dual problem corresponding to this  primal formulation is:  D?1? MD?? (l) = ?l?  (l) (3)  where MD = INtr -( (1Ntr1  ?  Ntr D ?1 ? )  1? Ntr  D ?1 ? 1Ntr  ). The ?(l) are the dual  variables and the kernel function K : RN ? RN ? R plays the role of similarity function.

B. Encoding/Decoding Scheme  Ideally when the communities are non-overlapping, we will  obtain k well separated clusters and the normalized Laplacian  has k piece-wise constant eigenvectors. This is because the  multiplicity of the largest eigenvalue i.e. 1 is k as depicted in [23]. In case of KSC due to the centering matrix MD the eigenvectors have zero mean and the optimal threshold  for binarizing the eigenvectors is self-determined (equal to  0). So we need k-1 eigenvectors. In the eigenspace every cluster Ap, p = 1, . . . , k is a point and is represented with a unique codebook cp ? {?1, 1}  k?1. To obtain this codebook  CB = {cp} k p=1 we transform the rows of the projected  vector matrix obtained from the training data by binarizing  it i.e. [sign(e(1)), . . . , sign(e(k?1))]. The codebook set CB is obtained by selecting the top k most frequent codebook  vectors. However, in real world networks the communities do  overlap and we do not have piece-wise constant eigenvectors.

For the matrix D?1MD? each eigenvector contains infor- mation about binary clustering. However, using the one versus  all encoding scheme, the number of score variables needed to  encode k clusters is k ? 1. The decoding scheme consists of comparing the cluster indicators obtained in the validation/test  stage with the codebook and selecting the nearest codebook  based on Hamming distance. This scheme corresponds to the  ECOC decoding procedure [26] and is used in out-of-sample  extensions as well. The out-of-sample extension is based on  the score variables which correspond to the projections of the  mapped out-of-sample points onto the eigenvectors found in  the training stage. The cluster indicators can be obtained by  binarizing the score variables for the out-of-sample points as:  sign(e (l) test) = sign(?test?  (l) + bl1Ntest) (4)  where l = 1, . . . , k?1, ?test is the Ntest?Ntr kernel matrix evaluated using the test points with entries ?r,i = K(xr, xi), r = 1, . . . , Ntest and i = 1, . . . , Ntr. This natural extension to out-of-sample points is the main advantage of KSC. In this  way the clustering model can be trained, validated and tested  in an unsupervised learning procedure. For the test set we use  the entire network. If the network cannot fit in memory, we  divide it into chunks to calculate the cluster memberships for  each test chunk in parallel.

C. Model Selection or Estimation of k  In [15], the authors proposed a criterion called Balanced  Angular Fit (BAF ) which tries to estimate the number of  clusters using the validation set of nodes. The BAF criterion  is based on the principle that the nodes which belong to the  same cluster have nearly zero or very small angular distance  in the eigenspace w.r.t. each other in comparison to the nodes  belonging to different clusters. To determine the number of  clusters, the end-user provides a maximum possible number of  clusters (maxk). The KSC model is built using this maxk i.e.

we perform an eigen-decomposition of the D?1MD? matrix and select the top maxk eigenvectors. The BAF criterion is  iteratively evaluated for k = 2 to k = maxk?1. The value of k for which the BAF is maximum is provided to the end-user  as the ideal number of clusters. The method works well but  requires to iterate over all values of k from 2 to maxk ? 1.

Hence, it doesn?t automatically determine the ideal value of k  in a few steps. Another approach to automatically determine  the ideal value of k was proposed in [11] using the structure  of the eigenvectors. However, the underlying assumption is  that the eigenvector matrix Z comprises k piece-wise constant  eigenvectors and there is at most one non-zero entry in every  row. Both these assumptions do not hold for the KSC model  for real world overlapping networks. Algorithm 1 provides a  summary of the Original KSC method.

Algorithm 1: Original KSC Algorithm  Data: Given a graph G = (V,E).

Result: The partitions of the graph G, i.e. divide graph  into k clusters.

1 Convert and store the graph G in sparse format.

2 Select the training set of nodes Xtr (i.e. maximum size  = 5, 000 nodes) using FURS.

3 Select the validation set of nodes (same size as Xtr)  after removing the subgraph S corresponding to Xtr using FURS.

4 Calculate the kernel matrix ? by applying cosine similarity operations on sparse adjacency lists of ?i, j vi, vj ? Xtr.

5 Perform eigen-decomposition of ? to obtain the model i.e. ?(l), bl.

6 Use out-of-sample extension property to obtain the  projection values for the validation set i.e. e (l) valid.

7 Use BAF and e (l) valid to estimate the number of  clusters k.

// This is the step which is automated  by our proposed approach.

8 After estimating k, use the out-of-sample property to  assign clusters to unseen test nodes (We use the entire  network as test data).



III. SELF-TUNED KERNEL SPECTRAL CLUSTERING  In section II-A, we have shown that for large scale networks  the normalized linear kernel function is sufficient. The only  parameter left is the number of clusters to be identified (k). We  propose a novel technique to determine the number of clusters  (k) automatically in a few steps. We exploit the projections  of the validation nodes in the eigenspace. The proposed  approach is based on the simple idea that the projections of  the validation nodes which belong to the same cluster have  very small or nearly zero angular distance from each other in  comparison with the nodes belonging to different clusters.

Before we explain the technique, we mention a few pre-  requisites. All the experiments were performed on a single  machine with 8 Gb RAM, 2.4Ghz Intel I-7 processor. The  maximum size matrix that can be stored in memory is 5, 000? 5, 000. The training set size is fixed as Ntr = min(15% of N , 5000) i.e. minimum between 15% of the total nodes in the network (N ) and 5, 000 nodes. The subset size of 15% of the nodes in the network is chosen as per experimental  analysis in [21]. We set the validation set size Nvalid to be  the same as Ntr. We put the condition that minimum size  of a cluster is MinCsize = max(0.01% of Nvalid,5) i.e.

maximum between 0.01% of the number of nodes in the validation set and 5 nodes. Since for estimation of the number of clusters we use the validation set, we put the minimum size  constraint on the validation set. This constraint is to prevent  the selection of outlier groups of small size as a cluster. Thus,  suppose the training set size is 5, 000 nodes then the minimum size of cluster has to be 5. The maxk value is determined as ? Ntr MinCsize  ?. Thus, if the minimum size of the cluster is 5 and maximum size of the validation set is 5, 000 then the maximum number of clusters that can be detected is 1, 000. The runtime complexity of KSC approach is O(N3tr) + O(Ntr ? Ntest).

However, testing can be done in parallel and the second term  in time complexity can be reduced significantly.

A. Identifying the number of clusters  We obtain the projection of the validation nodes vj ? Vvalid in the eigenspace using the out-of-sample extension and bina-  rize the score variables as follows:  sign(e (l) valid) = sign(?valid?  (l) + bl1Nvalid) (5)  Let the latent variable matrix be represented as E. The size  of the this matrix is Nvalid ? (maxk ? 1). The matrix E is defined as [e1, . . . , eNvalid ]  ?. We then create a new symmetric  affinity matrix A from the latent variable matrix E. Since we  already stated that the maximum size of the validation set is  5, 000 nodes, the maximum size of the A matrix is 5, 000 ? 5, 000. The affinity matrix is build using cosine distance as:  A(i, j) = CosDist(ei, ej) = 1? cos(ei, ej) = 1? e ?  i ej  ?ei??ej?  The CosDist() function can take values between [0, 2]. We use the concept that nodes which belong to the same com-  munity have smaller or nearly zero angular distance w.r.t the  nodes within the community and more angular distance w.r.t  nodes belonging to different communities. Thus, the nodes  which belong to the same cluster will have very small or nearly  zero CosDist(ei, ej), ?i, j in the same cluster.

Figure 1 highlights this idea on a synthetic network of 1, 000  nodes generated by the software proposed in [2] with mixing  parameter ? = 0.1. The mixing parameter implies the extent of overlap in the communities with smaller values representing  less overlap. In Figure 1a, we obtain the projections of the  validation nodes in 2-dimensional space as we set maxk = 3 for the purpose of visualization. Figure 1b showcases the presence of a block diagonal structure for the affinity matrix A.

To obtain this block diagonal matrix we used the groundtruth  cluster memberships for the validation set of nodes. We then  sort the validation nodes using the groundtruth information on     (a) Validation node projections  (b) Affinity matrix A w.r.t. groundtruth  Fig. 1: First two steps for identifying the number of clusters in a given network  the basis of their cluster affiliation and construct the affinity  matrix A. The black shaded regions represent the part of the  affinity matrix where the CosDist(ei, ej) values are zero and nearly equal to zero. The aim is to identify the number of such  block diagonals which determine the number of clusters k in  the given network.

Real world networks have overlapping communities. How-  ever, the extent of overlap in these communities is not known  beforehand. Also the groundtruth is not available in most real  world large scale networks. Thus the affinity matrix A, though  contains a block-diagonal structure, need not necessarily be  ordered in block-diagonal format. We observe from Figure  1a, that projection of nodes which belong to the same cluster  do not necessarily have zero angular distance due to presence  of overlap. But the angle between two projection vectors say  ei and ej belonging to the p th cluster (Cp) is mostly less  than 90? leading to 0 ? cos(ei, ej) ? 1. After little algebraic manipulation we obtain that 0 ? CosDist(ei, ej) ? 1 for two nodes i and j belonging to cluster p (Cp).

Since the extent of overlap in terms of angular dis-  tance is not known beforehand we setup a threshold td = [0.1, 0.2, . . . , 1]. We need to investigate only 10 different threshold values which is much less than iteratively solving  for each k = 2 to k = maxk?1 in increments of 1. Generally as the value of threshold increases the number of clusters  decreases as more validation nodes satisfy the threshold and  fall in the same cluster. For a given value of threshold t ? td, we first identify that validation node (ei) for which the number  of nodes with CosDist(ei, ej) ? t is maximum. We keep the count of this number of nodes in a vector. We obtain the  indices of all those validation nodes and shrink the affinity  matrix A by removing the rows and columns corresponding  to these indices. We repeat this process recursively till it results  in an empty matrix. This procedure is equivalent to identifying  the block diagonal structure. As a result of this procedure  for each value of threshold t, we obtain a vector (SizeCt)  containing information about the size of clusters for that value  of threshold t.

In order to determine the ideal number of clusters k, we  use the notion of entropy and balance w.r.t. the size of the  clusters. For each value of threshold t we have the vector  SizeCt. We also put the constraint that the minimum size  of a cluster is greater than or equal to MinCsize. So we  remove from SizeCt vector all the size of clusters which are  less than MinCsize . The Shannon entropy for a discrete set  of probabilities is defined as:  H(s)t = ? k?  i=1  p(si) t log(p(sti)), ?t ? td (6)  where H(s)t is the entropy corresponding to the vector SizeCt for the threshold t. Here s  t i is the size of the i  th cluster  and p(sti) is defined as ratio between the size of i th cluster  corresponding to threshold t and the size of the validation  set (Nvalid). Entropy values are higher when there is a larger  number of uniformly sized clusters.

Balance is generally defined as the ratio between the  minimum size cluster in SizeCt and the maximum size  cluster in SizeCt. The aim of using balance is to prevent  one or more large community from being superior over all  other communities. Higher balance promotes uniformly sized  clusters and prevents few extremely large communities from  being dominant. This leads to more number of communities.

In this paper, we use the overall balance criterion which is the  sum of balance for all the clusters in SizeCt and defined as:  B(s)t = k?  i=1  sti max(st1, . . . , s  t k)  , ?t ? td (7)  where sti represents the size of the i th cluster corresponding to  threshold t. The denominator in (7) refers to maximum sized  cluster for the threshold t. The higher the overall balance in  the clusters, the closer the value of B(s)t is to k.

We calculate the entropy and overall balance for each of  these SizeCt and use the harmonic mean or an F-measure to  determine the best of both entropy and overall balance as their  values are comparable. The F-measure is defined as:  F (s)t = 2H(s)tB(s)t  H(s)t +B(s)t , ?t ? td (8)  where H(s)t and B(s)t represents the entropy and expected balance for threshold t. We evaluate F (s)t for all values of threshold t ? td and output that value of threshold t say maxt for which F (s)t is maximum. The number of clusters (k) is obtained from the vector SizeCmaxt and is equal to the  number of terms in this vector. Algorithm 2 summarizes the  proposed method for identifying the number of clusters (k) in  a given large scale network.

Algorithm 2: Algorithm to automatically identify k com-  munities  Data: E = [e1, e2, . . . , eNvalid ] Result: The number of clusters k in the given large scale  network.

1 Construct an affinity matrix A using the projection  vectors ei ? P and the similarity function CosDist().

2 Set td = [0.1, 0.2, . . . , 1].

3 for each t ? td do 4 Save A in a temporary variable B i.e. B := A.

5 Initialize the SizeCt as an empty vector.

6 while B is not an empty matrix do  7 Locate the projection of validation node ei for  which the number of nodes with  CosDist(ei, ej) ? t is maximum.

8 Count the number of these nodes and locate the  indices of these nodes.

9 Append the count of these nodes to the vector  SizeCt.

10 Remove rows and columns corresponding to  these indices.

11 end  12 Calculate the entropy H(s)t from SizeCt.

13 Calculate the expected balance B(s)t from SizeCt.

14 Calculate the F-measure F (s)t using H(s)t and  B(s)t.

15 end  16 Obtain the threshold corresponding to which the  F-measure is maximum as maxt.

17 Estimate k as the number of terms in the vector  SizeCmaxt.



IV. EXPERIMENTS  We conducted experiments on several synthetic and real  world networks. The synthetic networks are generated from  the software proposed in [2] with varying the size of the  network and the mixing parameter ?. The mixing parameter  contains information about the extent of overlap between the  communities in the large scale network.

A. Experiments on Synthetic networks  Figures 2,3, 4 and 5 show the result of our proposed  approach for identifying the number of clusters k on synthetic  networks made of 5, 000, 10, 000, 25, 000 and 50, 000 nodes respectively. We show the results for two different mixing  parameters ? = 0.1 and ? = 0.5. For smaller ? the extent of overlap is less in comparison to higher value of ? and  detecting communities is less difficult.

From Figures 2a, 2b, 3a, 3b, 4a, 4b, 5a and 5b, we show  that the proposed method identifies exactly or close to the  exact number of clusters present in the network even when the  extent of overlap is high. The adjusted rand index (ARI) [27]  values are also quite high suggesting that the clusters obtained  via KSC are meaningful. We also observe that the threshold  corresponding to which we obtain the maximum F-measure  is related to the mixing parameter ?. For the smaller mixing  parameter (0.1) the threshold value t for which the F-measure is maximum is less than or equal to the threshold value  corresponding to higher mixing parameter 0.5 as observed from Figures 2c, 2d, 3c, 3d, 4c, 4d, 5c and 5d.

B. Experiments on Real World Datasets  We conducted experiments on several real-world large scale  networks ranging from trust networks (PGPnet), collaboration  networks (Cond-mat), citation networks (HepPh), commu-  nication networks (Enron), social networks (Epinion), web  graphs (Youtube), actor networks (Imdb-actor) to road net-  works (roadCA). Details about these networks are available  at http://snap.stanford.edu/data/index.html. Table I provides a  few key statistics of each dataset.

Dataset Vertices Edges CCF  PGPnet 10,876 39,994 0.008  Cond-mat 23,133 186,936 0.6334  HepPh 34,401 421,578 0.1457  Enron 36,692 367,662 0.497  Epinions 75,879 508,837 0.2283  Imdb-actor 383,640 1,342,595 0.453  Youtube 1,134,890 2,987,624 0.173  roadCA 1,965,206 5,533,214 0.0464  TABLE I: Vertices (V), Edges (E) and Clustering Coefficients  (CCF) for each dataset  We use the entire large scale network as test set in our  experiments. We use the clusters obtained by Louvain [8], In-  fomap [6] and Bigclam [28] methods as groundtruth for these  networks. We compare the clusters obtained via the Original  KSC method w.r.t. these groundtruth clusters. Then we modify  the Original KSC method by automatically determining the  number of clusters (k). We combine our proposed approach  with the original out-of-sample extension property of the KSC  model to obtain the Self-Tuned KSC (ST-KSC) method.

In order to perform a comprehensive evaluation, once we  automatically obtain the value of k, we perform k-means on  the projections in the eigenspace for the validation set to obtain  the centroids. We obtain the projections of the test set in the  eigenspace and assign the cluster membership corresponding  to the centroid to which it is the closest. This method is  referred as Self-Tuned k-means KSC (ST k-means KSC). We     (a) A w.r.t. groundtruth (b) A w.r.t. groundtruth  (c) F-measure vs Threshold (d) F-measure vs Threshold  Fig. 2: Self-Tuned KSC on a synthetic network of 5, 000 nodes for mixing parameter values 0.1 and 0.5  (a) A w.r.t. groundtruth (b) A w.r.t. groundtruth  (c) F-measure vs Threshold (d) F-measure vs Threshold  Fig. 3: Self-Tuned KSC on a synthetic network of 10, 000 nodes for mixing parameter values 0.1 and 0.5  then compare the cluster memberships obtained via ST-KSC  and ST k-means KSC w.r.t. the community affiliations via  Louvain, Infomap and Bigclam method respectively.

For evaluating the cluster memberships we use three infor-  mation theoretic measures: mutual information (MI), variation  of information (VI) and adjusted rand index (ARI). These are  standard cluster quality evaluation metrics given two list of  cluster memberships and are described in detail in [29].

Table II evaluates the clusters obtained via the original KSC  method, the proposed ST-KSC method and ST k-means KSC  methods w.r.t. the clusters obtained by Louvain, Infomap and  Bigclam methods. Since the partition obtained by methods like  Louvain and Infomap method are not unique, we perform 10 randomizations and report the mean VI, ARI and MI values.

Figure 6a and 6b shows the variations for these evaluation  metrics in case of Enron and roadCA networks respectively.

(a) A w.r.t. groundtruth (b) A w.r.t. groundtruth  (c) F-measure vs Threshold (d) F-measure vs Threshold  Fig. 4: Self-Tuned KSC on a synthetic network of 25, 000 nodes for mixing parameter values 0.1 and 0.5  (a) A w.r.t. groundtruth (b) A w.r.t. groundtruth  (c) F-measure vs Threshold (d) F-measure vs Threshold  Fig. 5: Self-Tuned KSC on a synthetic network of 50, 000 nodes for mixing parameter values 0.1 and 0.5  From Table II we observe that the absolute values of ARI  are quite small for most networks as it is heavily dependent on  the number of clusters. For methods like Louvain and Infomap  method, the number of clusters found are much more than  that for KSC methods which lead to smaller ARI. From Table  II we observe that clusters obtained by the Original KSC  method generally have better variation of information (VI)  corresponding to the clusters obtained via Louvain, Infomap  and Bigclam methods. This is because the Original KSC  method using the BAF evaluation metric is biased to produce  small number of clusters as observed from Table II. However,  due to this bias it results in poor ARI and MI values for most  of the real world large scale networks. The clusters obtained  by the two self-tuned methods have better ARI and MI values.

The ST-KSC method results in better quality clusters for  PGPnet, Enron, Imdb-actor and roadCA networks when the     (a) For Enron network  (b) For roadCA network  Fig. 6: Variation of evaluation metrics (VI, ARI and MI) for various methods for Enron 6a and roadCA 6b network. It shows  that ST-KSC method gives best results for Enron network and ST k-means KSC method provides best results for roadCA  network.

clusters are evaluated w.r.t. the clusters obtained by Louvain,  Infomap and Bigclam methods. The evaluation metrics for  which ST-KSC method performs better is ARI and MI. The  ST k-means KSC method results in better quality clusters for  Cond-mat, HepPh and Youtube networks w.r.t to the quality  metrics ARI and MI.

Figure 6 showcases the results of the various quality metrics  for the Enron and roadCA networks. Figure 6a shows that the  clusters obtained by all the KSC methods are dissimilar from  those obtained by Louvain method because of the high VI,  low ARI and low MI values. However, the clusters obtained  by the KSC methods are more similar to the ones obtained by  Infomap and Bigclam methods. We observe this from 6a due  to the low VI, high ARI and better MI values. In case of the  Bigclam method, the clusters obtained are highly similar to  that obtained by all the KSC methods. The clusters produced  by the ST-KSC method in particular with VI (0.91), ARI (0.51) and MI (1.97) is the most similar to the clusters produced by     Original KSC ST-KSC ST k-means KSC  k VI ARI MI k VI ARI MI k VI ARI MI  Louvain 4 4.022 0.004 0.124 30 3.94 0.073 1.102 30 4.08 0.041 0.87  PGPnet Infomap 4 3.583 0.006 0.1 30 3.7 0.1 0.98 30 3.77 0.063 0.78  Bigclam 4 1.57 0.011 0.029 30 3.4 0.12 1.026 303.462 0.07 0.78  Louvain 3 4.021 0.01 0.225 52 3.91 0.011 0.324 52 4.5 0.04 0.68  Cond-mat Infomap 3 3.132 0.05 0.222 52 3.1 0.051 0.32 52 3.63 0.224 0.68  Bigclam 3 0.5 0.49 0.562 52 1.47 0.155 2.49 52 1.22 0.331 1.99  Louvain 3 3.22 0.028 0.093 765.7395 0.014 0.216 76 5.23 0.09 0.64  HepPh Infomap 3 5.72 0.006 0.166 76 7.224 0.014 0.78 76 6.3 0.1 1.4  Bigclam 3 0.95 0.0004 0.0005 76 6.44 0.001 0.51 76 6.25 0.09 0.8  Louvain 3 3.6 0.038 0.274 16 3.57 0.053 0.442 16 3.51 0.0441 0.37  Enron Infomap 3 1.6 0.467 0.261 16 1.73 0.53 0.4 16 1.62 0.51 0.344  Bigclam 3 0.7 0.48 0.51 16 0.91 0.51 1.97 16 0.94 0.50 1.3  Louvain 3 3.1 0.0006 0.005 24 3.08 0.001 0.015 243.451 0.03 0.06  Epinion Infomap 3 4.163 0.017 0.007 24 4.16 0.031 0.013 24 4.47 0.014 0.1  Bigclam 3 0.11 0.1 0.007 24 0.84 0.02 0.02 24 0.74 0.014 0.016  Louvain 3 3.05 0.0005 0.006 59 4.4 0.114 1.09 59 3.84 0.093 0.673  Imdb-actorInfomap 3 2.7 0.0006 0.004 59 4.2 0.11 0.99 59 3.58 0.08 0.63  Bigclam 3 1.1 0.0 0.0 593.0405 0.025 1.07 59 3.14 0.02 0.83  Louvain 3 3.64 0.0003 0.004 21 4.1 0.016 0.047 21 4.55 0.017 0.096  Youtube Infomap 3 3.71 0.0003 0.0035 21 4.16 0.014 0.03 21 4.7 0.016 0.07  Bigclam 3 0.056 0.0 2.6e-06 21 1.6 0.035 0.06 21 1.25 0.09 0.1  Louvain 3 6.12 0.0004 0.141 39 6.38 0.0005 0.41 39 6.42 0.0006 0.242  roadCA Infomap 3 8.0 7.6e-05 0.21 39 7.9 0.0001 0.64 39 8.14 0.000110.40206  Bigclam 3 1.075 0.0 0.0 39 2.62 0.044 1.26 39 1.56 0.17 1.01  TABLE II: Evaluation of the cluster memberships for Original  KSC, ST-KSC and k-means KSC for several real world  networks w.r.t. Louvain, Infomap and Bigclam methods. The  highlighted numbers represent the best results. Original KSC  method performs better w.r.t. VI while the ST-KSC and ST  k-means KSC methods perform well w.r.t. ARI and MI.

Bigclam method.

From Figure 6b we observe that the clusters obtained by  all the KSC methods have high VI and low ARI values in  comparison to the clusters obtained by Louvain and Infomap  method. This is because the mean number of clusters obtained  by Louvain and Infomap method are 3, 000 and 65, 807 respectively over 10 randomizations. This is much larger in  comparison to the number of clusters obtained by Original  KSC (3) and 39 obtained by the proposed approach. How- ever, the Bigclam method identifies 40 clusters. The clusters obtained by Bigclam method have best result w.r.t. VI with  Original KSC method (1.075), w.r.t. ARI with ST k-means KSC (0.17) and w.r.t. MI with ST-KSC (1.26).



V. CONCLUSION  In this paper we proposed an approach to make the kernel spectral clustering method free of parameters for large scale networks. We used the normalized linear kernel for large scale networks and devised an approach to automatically identify the number of clusters k in the given network. For achieving this, we exploit the projections of the validation nodes in the eigenspace by creating an affinity matrix which had block-diagonal structure. We used the concepts of entropy and balance to identify these block-diagonals and obtain the number of clusters. We compared the resulting KSC methods with large scale community detection methods like Louvain, Infomap and Bigclam methods. In future work, we plan to evaluate the clusters obtained by the KSC methods using other likelihood based techniques like BIC and AIC.

Acknowledgements  This work was supported by Research Council KUL, ERC AdG A-DATADRIVE-B, GOA/10/09MaNet, CoE EF/05/006, FWO G.0588.09, G.0377.12, SBO POM, IUAP P6/04 DYSCO, COST intelliCIS.

