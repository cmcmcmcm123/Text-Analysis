AVI:Based on the vertical and intersection operation of

Abstract?Association Rules is the extraction of data mining in the important research, analysis and research in the Apriori algorithm for association rule, the algorithm for computing efficiency is not high, proposing an improved algorithm AVI (Apriori with vertical and intersection operation) that use itemset union and identification intersection, just scanning the transaction database one time that can get the identity of a set of first-order and large set of vertical. K-order designate the options set for this operation as long as the first order of the large itemsets, scanning the database without having to repeat, thereby improving the efficiency of the mining algorithms.

Keywords- data mining; association rules; Apriori algorithm; AVI algorithm

I. INTRODUCTION  Date Mining often referred to as knowledge discovery in databases (KDD), precisely, and KDD of knowledge in the learning stage, known as data mining. Data mining is a very important processing step of KDD, and it is a crossover field of study, Integration of the database, AI, Machine Learning, Statistics, Knowledge Engineering, Object-Oriented Method, High Performance Computing and Data visualization. The latest technology of research results is shown in [1].



II. ASSOCIATION RULES ANALYSIS  An association rule is an important element of Data Mining research. Association rules showing attribute - value is frequently given along with data set conditions for the emergence , can be described as Supposing I is a k-sets composed of a collection of different projects.

A Transaction database D, in which each transaction T is a set consisting of a group items of I, namely T? I, T has the only identification TID. If itemsets X? I and X? T, then transaction set T contains the itemsets X. An association rule is an implication, such as the form X => Y, ? ? and ? ? . Association rule X => Y condition is established: It has degree of support for Supp, namely, there is at least Supp% of the transaction in database D that contains X ? Y. It has degree of confidence for Conf, namely, the transaction database D contains X, there are at least Conf% transaction also contains Y[5].

Association rule mining problem is found to have user- specified minimal support(min_sup) and minimal confidence(min_conf) of association rules. This problem is usually decomposed into two subproblems. One is to find those itemsets whose occurrences exceed a predefined threshold in the database; those itemsets are called frequent or large itemsets. The second problem is to generate association rules from those large itemsets with the constraints of minimal confidence.



III. APRIORI ALGORITHM  The greatest impact of mining association rules algorithm is the Apriori algorithm presented by RakeshAgrawal and others[2]. It is a Boolean association rule mining frequent itemsets algorithm, That is, first generate frequent itemsets (completed by the two-step connection and pruning), and then generate association rules. The basic idea of the algorithm is that scanning the transaction database many times, by using the principle that in a given transaction database D, any frequent itemset is a subset of frequent itemsets; superset of any weaknesses is the weaknesses set.

This principle is scanned many times on the transaction database. The Apriori algorithm uses breadth-first search and a tree structure to count candidate item sets efficiently. It generates candidate itemsets of length i from itemsets of length i-1.

First of all, to identify a group of frequent sets, the collection is recorded as L1, L1 is used to find the set of frequent 2-itemsets L2; L2 is used for L3, If it goes on, until you can not find frequent k-itemsets Lk, Li needs to find a database for each scanning, Li-1 to find out how the Li is divided into two processes composed of connections and pruning. According to the core idea of the Apriori algorithm, the functional block diagram of the Apriori algorithm is shown in Fig.1[3].

By the Apriori algorithm can be seen, Each Ci have to scan the database once, but this time some business have no effect on the generation of frequent itemsets. Thus reducing the number of scanning the database D, For improving the efficiency of the algorithm it is necessary [4].based on the Apriori algorithm is low operating efficiency of existing inadequate, This paper presents an improved association rule mining algorithm - AVI algorithm.

Figure 1. the function block diagram of the Apriori algorithm

IV. AVI ALGORITHM DESIGN AND IMPLEMENTATION  A. A  Algorithm Optimization  Definition 1: For itemset X, t (X) = {tid | tid is the identity of t, t  D and t supports X}; For the identification set Y, i(Y) = y Y itemset(y), itemset (y) that y corresponding to the transaction itemset[4]. For example:  t({ A ,B ,D}) = { 2 , 6} i({2,6}) ={A,B,D} {A,B,C,D,E}={A,B,D}  (See Table )  TABLE I. TRANSACTION DATA SHEET  TID R-listitem TID 1 A,B,E 2 B,D 3 B,C  From Table , we can get: Itemset X = {A}: the vertical itemsets of corresponding  identification sets is {1}; Itemset X = {B}: the vertical itemsets of corresponding  identification sets is {1, 2, 3}; Itemset X = {C}: the vertical itemsets of corresponding  identification sets is {3}; Itemset X = {D}: the vertical itemsets of corresponding  identification sets is {2}; Itemset X = {E}: the vertical itemsets of corresponding  identification sets is {1}; AVI algorithm uses the vertical itemsets, by a second-  order frequent itemsets generated set of candidate processes, Through the key set and operations, identification sets of intersection operations are completed, for example:  A B={1}?{1,2,3}={1}  B. The counting of support and the description of the AVI algorithm  Let the database of transactions D = (T1, T2, ..., Tn) ,it consist of n transactions. Symbol X.sup means the degree of support, itemset X in D, Symbol X.count means the degree of support counts, itemset X in D.

For each candidate k-itemsets, connected with it as long as from the frequent (k-1)-key focus, Optional 2 sets of vertical identity representations are bitwise intersection operations, statistical results and then identify the number of concentrated elements, as a support for counting [6].

Input : ( k-1) - frequent itemsets candidate k-itemsets to connect nodes in R, two-node P and Q identification sets vertical are Pb and Qb;  Output: the counts of support of the candidate itemsets k nodes R.count  Algorithm:  C. The illustration of the AVI Algorithm  Supposing min_sup=2/9=22%, minimum support count is 2, sample data are shown in Table .

TABLE II. TRANSACTION  DATABASE  D  TID Itemset TID Itemset  1 ABE 6 BC 2 BD 7 ACD 3 BC 8 ABCE 4 AB 9 ABC 5 AC  The AVI Algorithm steps are shown as follows: Step 1: Scanning transaction database D, the entry in D  will be converted to table , and to count the number of elements identified focus.

Step 2: Because of min_sup = 2, can get the candidate 1-itemsets. Remove items that are not frequent 1-itemsets, Generate the vertical set of frequent 1-itemsets.(See Table  )  TABLE III. CANDIDATE  1-ITEMSETS VERTICAL  Itemset Identification vertical Sup.count  {A} {145789} 6 {B} {1234689} 7 {C} {356789} 6 {D} {27} 2 {E} {18} 2  Step 3: According to candidate set of trees and are known to frequent 1-itemsets to find frequent 2-itemsets.

Process is as follows: according to the first layer of the tree nodes is frequent 1-itemsets node, that the second layer of each node in each subset is frequent, so generated by the Table  that for any two vertical lines marking sets intersection operations. (See Table  Step 4: Then counting the number of elements and the count as its degree of support, If support ?min_sup, the   1) Rb=Pb Qb 2) For (i=0;i<n and Rb!=0;i++)    do begin 3) If ((Rb ? Qb)! =0) then   //Indicated that Rb and Qb  are included the first same element.

4) R..count++ 5) Shr(Rb,1)    //To shift to the right one 6) End    corresponding combination is a frequent itemset 2-itemsets [8], By frequent 2-itemsets, get a new identification of vertical. (See Table )  TABLE IV. CANDIDATE  2- ITEMSETS VERTICAL  Itemset Identification vertical Sup.count  {A,B} {1489} 4 {A,C} {5789} 4 {A,D} {7} 1 {A,E} {18} 2 {B,C} {3689} 4 {B,D} {2} 1 {B,E} {18} 2 {C,D} {7} 1 {C,E} {8} 1 {D,E} {} 0  TABLE V. FREQUENT 2- ITEMSETS VERTICAL  Itemset Identification vertical Sup.count  {A,B} {1489} 4 {A,C} {5789} 4 {A,E} {18} 2 {B,C} {3689} 4 {B,E} {18} 2  Step 5: Found the frequent 3-itemsets. The first step generate a candidate 3-itemsets, according to candidate set of trees, for the third layer node, if it is the second layer of non- frequent 2-itemsets linked nodes, pruning a link with the second-tier node, the remaining node with the second level linking to third-layer nodes is a candidate 3- itemsets, can get ABE nodes, choose any two of frequent 2-itemsets linked to the identity of the second-tier of nodes in the vertical set of operations that a transaction, count the number of elements, if support ? min_sup then it is a frequent 3-itemsets, or not , by processing we can get  L3={(A B C) (A B E)}.According to the calculations we get a frequent 3- itemsets. (See Table  REQUENT  3-VERTICAL  Itemset Identification vertical Sup.count  {A,B,C} {89} 2 {A,B,E} {18} 2  Step 6: Find frequent 4-itemsets. According to the above steps, there is no candidate 4-itemsets, therefore, there is no frequent 4-itemsets.

Step 7: The process of frequent itemsets found in the end, get all the frequent itemsets:  L1= {A, B, C, D, E} L2={{A,B},{A,C},{A,E},{B,C},{B,E}} L3= {{A, B, C}, {A, B, E}}

V. EXPERIMENT AND RESULT ANALYSIS  After the text edit has been completed, the paper is ready for the template. Duplicate the template file by using the Save As command, and use the naming convention  prescribed by your conference for the name of your paper. In this newly created file, highlight all of the contents and import your prepared text file. You are now ready to style your paper.

In order to facilitate research and experience the AVI algorithm, this article uses Visual FoxPro to achieve the AVI algorithm.

A.  design ideas  In this method, for ease of operation, the introduction of the following data table, And made the following assumptions:  ? Define a transaction data table TRAN.DB, it consists of two fields, field names respectively T, ITEMS which are character [7],In turn behalf of the Panel ID, things included in the project record.

? Create a project data table ITEM.DBF, it contains a field, called 11, Data type is character, used to store all the items, each record in this table represents a project. The number of records in the table is the number of projects.

? Create a number of empty tables ITEM1.DBF,ITEM2.DBF,ITEM3.DBF,ITEM4.DB F,??, these were used to store an item frequent sets,2 project frequent itemsets,3 project frequent itemsets,4 project frequent itemsets. Before you run the program, these tables, only the structure, there is no data.

? Generate a set of frequent. By 1 project frequent, we can construct a candidate itemset tree.

? Generate two frequent sets. The use of ITEM1.DBF And the candidate item set the length of the tree produces two candidate itemsets and the corresponding identification set into the ITEM2.DBF.count the number of identification, namely, the degree of support the candidate itemsets, remove support for less than a given minimum support of all records, Find all the length of two of the frequent itemsets into ITEM2.DBF, The rest of analogy, until you find all the frequent itemsets. If it is found the number of candidate itemsets is zero, then stop operation.

? Finally, the output of all frequent itemsets.The programs need only scan the transaction database one time.

B.  Experimental results  In this paper, AVI algorithms experiments, The execution time of the algorithm were compared, use a real experimental data(March 2008 non-computer professional computer grade test scores library in China University of Mining and Technology ),A total of 1200 experimental database records, in order to facilitate excavation, the number of attributes and types of property data into Boolean types, Transformed total of 20 properties, The length of each transaction 4, taking into account the actual situation, respectively, a minimum degree of support for the 1%,2%,3%,4%,5%and 6%, Tested this algorithm and the Apriori algorithm to improve execution time, shown in Fig.2.

TABLE VI. F  VI    Figure 2.  Different supports for  the algorithm execution time  In order to improve the performance of mining frequent closed itemsets, AVI can be used in the algorithm to sort the child nodes, as well as dynamic optimization strategy. The newly generated child nodes according to their support for the size of the small to large re-sequencing, Thereby reducing the number of branches, speed up the frequent closed itemsets mining process.

This paper uses a comparison with the Apriori algorithm, the experimental data are from http://www.Cs.Rpi.Edu/za- ki/software/. Experiments show that degree of support between the two larger only slight changes, reduce the degree of support when the change will be sharply increased.

In particularly, for the minimum degree of support of 45%, AVI faster than the Apriori is about 8 times, shown in Fig.3.

Figure 3.  AVI performance analysis

VI. CONCLUSION  The AVI algorithm presented in this paper, it is an identity-based set of vertical, through the intersection operations, and generate candidate itemsets tree and the corresponding degree of support. Comparing with classic Apriori algorithm, AVI algorithm only needs to conduct a scan of the transaction database, to significantly reduce I / O frequency, so the AVI algorithm is simple and moderate memory overhead. Through relevant experiments, for different minimum support, to compare the execution time of these two algorithms, the superiority of the AVI algorithm in the larger minimum degree of support is not very clearly, however, with the minimum degree of support reduction, its superiority gradually increases.

