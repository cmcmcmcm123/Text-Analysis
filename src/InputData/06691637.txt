Terabyte-scale image similarity search: experience and best practice

Abstract?While the past decade has witnessed an unprece- dented growth of data generated and collected all over the world, existing data management approaches lack the ability to address the challenges of Big Data. One of the most promising tools for Big Data processing is the MapReduce paradigm.

Although it has its limitations, the MapReduce programming model has laid the foundations for answering some of the Big Data challenges. In this paper, we focus on Hadoop, the open- source implementation of the MapReduce paradigm. Using as case-study a Hadoop-based application, i.e., image similarity search, we present our experiences with the Hadoop framework when processing terabytes of data. The scale of the data and the application workload allowed us to test the limits of Hadoop and the efficiency of the tools it provides. We present a wide collection of experiments and the practical lessons we have drawn from our experience with the Hadoop environment. Our findings can be shared as best practices and recommendations to the Big Data researchers and practioners.



I. INTRODUCTION  In the past decade, data has become ubiquitous. The scale at which data is generated has grown greater than ever before. According to the 2011 Digital Universe study of International Data Corporation (IDC), the information produced in 2011 all over the world, has gone beyond 1.8 Zettabytes(ZB), marking an exponential growth by a factor of nine in just five years. This data explosion has enormously raised the bar for both industry and research: existing systems that acquire, store and process the data collections do not posses the hardware and software capaci- ties to accommodate the data. In this context, ?Big Data? is becoming a hot term used to characterize the recent explosion of data. Big Data describes the unprecedented growth of data generated and collected from all kinds of data sources. This growth can be in the volume of data or in the speed of data moving in and out data-management systems.

It can also be the growth in the number of different data formats in terms of structured or unstructured data.

A massive amount of data produced all over the planet belongs to the multimedia type. Continuously growing mul- timedia collections come from social networks: in 2012, Facebook reported that 300 million images were uploaded every day, accumulating to 7 Petabytes of photo content every month [1]. These huge streams of images coming from all over the Web render multimedia content a significant  instance of Big Data. Multimedia collections check most of the points in the ?5V?s? list. Big Volume is clearly a feature.

Usually, there are a limited number of standardized ways to represent multimedia content, so Variety is not much of an issue. There are scenarios in which Big Variability, Velocity and Value are important for multimedia processing. In a copyright detection scenario, obtaining the right result is as important as the time it takes to get it.

Current state-of-the-art technologies developed for Big Data analytics can be employed to address the extremely large multimedia datasets now available on the Web. MapRe- duce [2] is probably the most efficient and popular tool that enables the processing of large amounts of information using commodity machines. Although it has its limitations, the MapReduce programming model has laid the foundations for higher-level data processing and is definitely a first step to answering many Big Data challenges. In this paper, we focus on the Hadoop [3] project, an open-source implementation of the MapReduce paradigm. By the means of a specific mul- timedia application, i.e., image similarity search, we present our experiences with Hadoop when processing terabytes of data. The scale of the data and the application workload have tested the limits of Hadoop and the efficiency of the tools it provides. Our findings can be shared as best practices and recommendations to the Big Data community.

Our first contribution is adapting the Hadoop environment to multimedia retrieval application workloads, where default settings are no longer efficient. We list the parameters and values that delivered best performance for us. The second contribution is addressing the cluster heterogeneity problem by proposing a platform-aware configuration of the Hadoop environment. The third contribution is introducing the tools, provided by the Hadoop framework, useful for a large class of application workloads, in which a large-size auxiliary data structure is required for processing main dataset. Finally, we present a series of experiments conducted on very large image dataset (biggest among reported in the literature).

In contrast to a number of Big Data reports describing experiments with either small or synthetic data, we report our findings and lessons obtained from a real-life use case, namely, finding images potentially violating copyrights in terabyte-sized collections.

The paper is structured as follows. Section II briefly lists      the challenges of Big Data analytics and presents current trends in this direction, with a focus on Hadoop. Section III gives an overview of MapReduce-based high-dimensional indexing scheme we designed and implemented for the Hadoop runtime environment. In Section IV we present our first results with running the application in the default Hadoop environment, while in Section V we describe further optimizations we achieved through various mechanisms.

Section VI concludes.



II. BIG-DATA PROCESSING  The Big Data notion describes the unprecedented growth of data generated and collected from all kinds of data sources from various research and industry fields. Clearly inherent to the term is the volume of information involved in Big Data processing: its scale easily reaches terabytes and often goes beyond that. Apart from this big volume feature, there are several others that render Big Data processing a challenging field requiring complex and ground-breaking techniques. In this section, we focus on Big Data challenges and the current trends in Big Data analytics.

A. Challenges  Scale. The need for efficiently managing rapidly increas- ing volumes of data has always been there, and it was addressed for the most part. At the core of adjusting to the data size lies parallelism. Whether it is achieved on a single machine through multiple computing units, or across the machines of a distributed system, large data sets are nowadays efficiently handled. However, in the Big Data context, scalability is still an issue. Infrastructures such as clusters(notably HPC clusters), Grids, Clouds(Nimbus [4], Amazon Elastic Compute Cloud [5]), multi-core servers with large memory represent a major step forward, but still have their limitations when accommodating Big Data.

Speed. In practice, the size of the data gives rise to real challenges. Performing actions such as data acquisition, storing, analyzing, visualizing, etc., becomes a problematic issue that needs dedicated approaches. It is obvious that the volume of data is the most immediate challenge for conventional data-management and processing frameworks.

A fundamental change is required in the architecture of systems capable of delivering high throughput.

Heterogeneity. Big data is also about the growth in the number of different data formats in terms of structured or unstructured data. Usually, a pre-processing step is required to bring the data to a form that is understood by the processing framework. Efficient representation, access, and analysis of semi-structured data require further work.

B. Trends  MapReduce [2] is probably the most efficient and popular tool that enables processing of large amounts of information  using commodity machines. Although it has its limita- tions, this programming model has laid the foundations for higher-level data processing and is definitely a first step to answering some of the Big Data challenges. There are several MapReduce implementations (Apache?s Hadoop [6], Twister [7], Disco [8]) on top of which specialized higher- level frameworks were developed. Computer vision-wise, the Mahout [9] project uses Hadoop to provide an extensive list of machine learning algorithms. In the area of content-based image retrieval, there are systems that can handle several million images [10], [11], billions of descriptors [12], [13], or address web-scale problems [14], [15] and the Hadoop- based ImageTerrier platform [16].

C. Hadoop  The MapReduce programming model has been imple- mented by the open-source community through the Hadoop project [3]. Maintained by the Apache Foundation and supported by Yahoo!, the Hadoop project has rapidly gained popularity in the area of distributed data-intensive com- puting. The core of the Hadoop project consists of the MapReduce implementation and the Hadoop Distributed File System (HDFS).

The Hadoop MapReduce framework [6] was designed following Google?s architectural model and has become the reference MapReduce implementation. The architecture is tailored in a master-slave manner, consisting of a single master jobtracker and multiple slave tasktrackers. The job- tracker?s main role is to act as the task scheduler of the system, by assigning work to the tasktrackers. Each task- tracker disposes of a number of available slots for running tasks. Every active map or reduce task takes up one slot, thus a tasktracker usually executes several tasks simultaneously.

When dispatching ?map? tasks to tasktrackers, the jobtracker strives at keeping the computation as close to the data as possible. This technique is enabled by the data-layout infor- mation previously acquired by the jobtracker. If the work cannot be hosted on the actual node where the data resides, priority is given to nodes closer to the data (belonging to the same network rack). The jobtracker first schedules ?map? tasks, as the reducers must wait for the ?map? execution to generate the intermediate data. Apart from data splitting and scheduling responsibilities, the jobtracker is also in charge of monitoring tasks and dealing with failures.

The Hadoop Distributed File System(HDFS) [17] was built with the purpose of providing storage for huge files with streaming data access patterns, while running on clusters of commodity hardware. HDFS implements design concepts commonly used by distributed file systems: data is organized into files and directories, a file is split into fixed-size blocks that are distributed across the cluster nodes. The blocks are called chunks and are usually of 64 MB in size (this parameter specifying the chunk size is configurable). The architecture of HDFS consists of several datanodes and a     single namenode. The nodes in a HDFS cluster that store the chunks are called datanodes. A centralized namenode is responsible for keeping the file metadata and the chunk location. HDFS handles failures through chunk-level repli- cation (3 replicas are kept by default). When distributing the replicas to the datanodes, HDFS employs a rack-aware policy: the first copy is always written locally, the second copy is stored on a datanode in the same rack as the first replica, and the third copy is shipped to a datanode belonging to a different rack (randomly chosen). The namenode decides and maintains the list of datanodes that store the replicas of each chunk.



III. APPLICATION WORKLOAD (DISTRIBUTED INDEXING + SEARCHING)  Our workload is based on the scalable, MapReduce ori- ented, high-dimensional indexing scheme initially presented in [18]. The indexing algorithm uses at its core a hier- archical unstructured quantization scheme, quite similar to the approach proposed by Nister and Stewenius [19]. In a nutshell, high-dimensional descriptors are clustered around randomly picked representative points that are hierarchically organized.

A. Dataset preparation  Our datasets, the collections of high-dimensional descrip- tors, were converted to Hadoop Sequence Files. Each raw descriptor becomes a sequence file record. A record in this file has a key (an image identifier), and a value (a high-dimensional descriptor). Hadoop scatters across many machines the blocks of the sequence file in its distributed file system, HDFS [17]. Thus, the entire descriptor collection is stored as a set of HDFS sequence files; this set of sequence files also represents the input data to the applications de- scribed further.

B. Distributed index creation  To be able to index a huge collection of descriptors using MapReduce, we first perform a phase of building auxiliary data required by the distributed process. This preliminary phase, which we execute on a single machine and usually only once, consists of building the index tree, a hierarchically-organized collection of cluster representatives.

The tree is created by randomly picking from the descriptor collection C points, i.e., the representatives of the visual vo- cabulary dictionary eventually created. These representatives are organized in a hierarchy of L levels for efficiency. The index tree is serialized to a file on disk and sent to all the nodes involved in the distributed index creation.

Creating an index from a very large collection of descrip- tors is a very costly process. Distributing it with MapReduce involves designing the map and reduce phase of the indexing application. Each map tasks receives at startup time (i) a block of input data (a chunk of a Hadoop sequence file)  and (ii) the file containing the index tree. The mapper further assigns the descriptors in its data block to the correct rep- resentative discovered by traversing the tree. The resulting assignments are sent to reduce tasks that write to disks high- dimensional descriptors grouped by cluster identifier. This eventually creates one or more indexed files which contain clustered high-dimensional descriptors. The number of such files is defined by the number of reduce tasks.

The indexing application is highly data-intensive, both in the map phase when reading the descriptors to assign them to cluster representatives, and also in the reduce phase when writing the clusters to disk. The application also generates a substantial amount of output data, slightly larger than the input set. The map stage is CPU-intensive as well, by performing distance calculations for every input record.

The shuffling step intensively utilizes the network, as the intermediate data is sent from mappers to reducers. Finally, distributed indexing is a long-running application on account of its data- and CPU-intensive nature.

C. Distributed search  Search phase is geared toward throughput as it processes very efficiently large batches of queries, typically 104?107  query descriptors. The search also requires a preliminary step, the creation of a lookup table, where all query de- scriptors of a batch are grouped according to their closest representative discovered from traversing the index tree. This lookup table is written to the local disk of all the nodes that will perform search. The MapReduce implementation of the search process is detailed further. Each map task receives (i) a block of data from one of the previously created index files, (ii) the file containing the lookup table. The mapper processes only the descriptors in its assigned chunk of data, that are relevant for the queries. Distance calculations are computed for those descriptors and queries assigned to the same cluster identifier. k-nn results are eventually emitted by the mappers, then aggregated by reducers to create the final result for the query batch.

Our implementation of the distributed search [20] is data- intensive in the map phase where indexed files are read from disk. Unlike the indexing application, the distributed search is short-running since only a part of the indexed descriptors are compared to the queries. This also renders the search application unbalanced, the workload of each mapper depends on whether the chunk they process includes clusters required by the queries. Another aspect that differs in this application as opposed to the index creation, refers to the data shuffled by mappers and generated by reducers, which is insignificant in size, compared to the input data.

Note that both applications require auxiliary data of substantial amount: the index tree/the lookup table have to be loaded by each mapper for every chunk of data.

While our experiences pertain to these two applications, their     Cluster #Nodes #CPU@Freq #Cores  RAM Local  /CPU Disk Cl1 64 2 Intel@2.50GHz 4 32GB 138GB Cl2 25 2 Intel@2.93GHz 4 24GB 433GB Cl3 40 2 AMD@1.70GHz 12 48GB 232GB  Table I CLUSTER CONFIGURATIONS.

characteristics are general enough so that our findings can be easily applied to big data processing.

D. Dataset  In our experiments we used the dataset provided by one of our partners in the Quaero project, http://quaero.org/.

The dataset consists of around 30 billion SIFT descriptors (approximately four terabytes of data) extracted from 100 million images harvested on the Web. Besides using this entire collection, we also used a subset of it containing 20 million images, i.e., 7.8 billion descriptors (about one terabyte). To the best of our knowledge, these datasets are among the largest collections presented to the multimedia community. Details on evaluation of indexing quality and the quality results themselves can be found in [18].



IV. HADOOP IN PRACTICE  Although the Hadoop framework is now widely adopted, processing terabyte-sized datasets require to reconsider many of its design, implementation and execution aspects.

The Hadoop has been shown to scale and efficiently process terabytes of data on thousands of machines: e.g., in Yahoo!, Facebook, etc., but a few research works actually reported Hadoop at such large scales, data- and platform-wise. In fact, one can hardly find real-life recommendations on Hadoop parameter configuration or suggestions on tools to be used for handling such amounts of data. The implementation and the experiments presented in this paper are real applications conducted on real datasets; in contrast to the usual Hadoop benchmarks, real applications raise complex problems and challenges. Luckily, the Hadoop framework contains a few tools to solve some of these problems. However, it is far from being obvious how and when to use them.

We present here our experience with big data processing using Hadoop in a grid environment. Based on the challenges and problems we were faced with, we can recommend best practices (configuration tunning, tools) for dealing with them.

The applications described in Section III were integrated into a framework that we developed to be able to: configure the compute environment, deploy a Hadoop cluster, transfer input data to HDFS from external storage, run indexing and searching jobs and optionally, retrieve the results and analyze logs.

A. Experimental platform  To reach the point where we could process terabytes of data on a Hadoop cluster instance, we have performed a large series of experiments on the Grid?5000 [21] platform. The Grid?5000 project is a widely-distributed infrastructure de- voted to providing an experimental platform for the research community. The platform is spread over 9 geographical sites located on French territory. We conducted our experiments on machines belonging to 3 clusters of the Rennes site of Grid?5000. The clusters configurations are shown in Table I. A Hadoop instance is deployed on these nodes as follows: the namenode and jobtracker are each launched on a dedicated node, while the datanodes and tasktrackers are co- deployed on the remaining nodes; we also allocate a separate machine to act as Hadoop client for job submissions.

B. Experiments with the default Hadoop settings  In a first round of experiments, we performed indexing with the default Hadoop configuration parameters. This gave us a baseline for assessing any improvements we could further obtain through various means.

What we refer to as the default Hadoop deployment is the basic configuration shipped with Hadoop, to which we applied some adjustments, based on the guidelines provided for dealing with big data. The first obvious adjustment was to set the map and reduce slots to accommodate the platform capacities: 8 slots for mapping and 2 slots for reducing per node. We set these values based on our clusters computing power and following the recommendations in Hadoop?s tutorial. The second parameter we modified is the HDFS chunk size; the default value of 64 MB is not suitable for big data, as it entails larger pressure on the namenode (because of filesystem metadata) and a large number of map tasks be executed. After some experiments with varying chunk sizes, we discovered the right chunk size for our workload to be 512 MB. At the level of HDFS, we kept the default replication factor of 3 for the input data. The replication factor is a crucial parameter for performance, not only because it enables fault tolerance, but also because it favors local execution of map tasks, minimizing the amount of data read remotely. The output data however, was written only once, to avoid the substantial overhead of writing replicas remotely. Given the size of our dataset, this cost is considerably high.

With these three basic configurations, indexing 1 TB of data on 106 nodes took 95 minutes, while 4 TB on 100 nodes were indexed in 10 hours in the same environment (Table II).

There are several reasons to explain the significant difference in runtime between indexing 1 TB and 4 TB. In addition to the obviously larger number of tasks to be executed, there is also a larger computational work done by each map task.

The tree of representatives built for indexing 4 TB is a lot larger than the one covering 1 TB: 1.8 GB as opposed to 461 MB. The size of the index tree has two consequences.

Nodes Default Hadoop Tuned Hadoop  1 TB 50 202min 174.7min 106 95min 75min  4 TB 100 10h1min 8h27min  Table II INDEXING TIME.

Parameter Default value Tuned value #Map slots/tasktracker 2 8  #Reduce slots/tasktracker 2 8 Input data replication 3 3 Output data replication 3 1  Chunk size 64 MB 512 MB JVM reuse off on  Map output compression off on Reduce parallel copies 5 50  Sort factor 10 100 Sort buffer 100 MB 200 MB  Datanode max. receivers 256 4096 Namenode handlers 10 40  Table III HADOOP CONFIGURATION TUNING.

First, each mapper has to perform more distance calculations to assign each point to the closest cluster. Second, the index tree takes up more RAM in each map slot, thus we had to reduce the map slots to 4 per node.



V. LARGE-SCALE HADOOP  A. Adjusting to the data size  Understanding the application workload and accurately assessing the scale at which the application runs are key steps to achieving optimal performance. We present here a brief analysis of our workload, which helped us pinpoint the issues to further improve.

Indexing 4 TB of data with Hadoop involves: storing 16 TB of data (input replicated 3 times plus output writ- ten once), shuffling 4 TB of intermediate data, executing approximately 8000 map tasks. Platform-wise, we dispose of 100 nodes, i.e., 800 map slots (400 for 4 TB) and 200 reduce slots, organized in 3 clusters connected to the same network switch. Based on these observations, several optimizations are in order. Since a substantial amount of data is shuffled from the mappers to the reducers, compressing the map output can be beneficial, as it also reduces network usage. Rack-awareness is not useful in this case, since the nodes connect to a single switch; however, if it is the case, configuring rack-awareness according to the network topology is necessary as it allows Hadoop to optimally place tasks on nodes for execution.

Table III sums up these optimizations by listing the Hadoop parameters and the values we used in the indexing process. As mentioned above, enabling map output compres- sion reduced the amount of shuffled data by 30% (from 1 TB to 740 GB). Other low-level parameters refer to the shuffling phase, when the data is sorted on the mapper?s side, copied to the reducer and then merged and sorted on the reducer  node. The last two lines of Table III show options configured on the HDFS nodes: the maximum number of chunk requests (read/write) each namenode can simultaneously handle, and the number of connections the namenode can serve at a time.

Table II shows a comparison of the indexing time using the default Hadoop configuration as well as the optimized one. On the same testbed and with the same slots for mapping and reducing, the customized Hadoop parameters deliver a much faster indexing time. The results in Table II show that setting the right parameters for the Hadoop frame- work can drastically improve Hadoop?s performance. Tuning the parameters of the Hadoop framework is a rather complex task that requires good knowledge and understanding of both the workload and the framework itself. Given the performance gains obtained, tuning the parameters listed in Table III is worth-while; nevertheless, the values delivering best performance are highly dependent on the workload, and thus, tweaking them in order to discover the best values is advisable.

B. Hadoop on heterogeneous clusters  Large-scale processing of big data is most commonly performed in a heterogeneous environment. Resource het- erogeneity can impact the performance since most data- processing systems are designed for homogeneous plat- forms [22]. This is also the case for the Hadoop framework: the configuration settings can be specified only globally, not in a per-cluster manner. The consequence is that the Hadoop deployment is configured according to the least equipped node, at the expense of wasting resources on more out-fitted nodes. Parameters such as the number of map/reduce slots per tasktracker and the amount of RAM memory allocated to each task have to be set to the lowest available values.

The advantage of having a single configuration for the whole platform is that it is easy to manage; the considerable downside however, is that the platform is under-utilized.

To fully exploit the computing power of our three clus- ter environments, we developed a simple mechanism that configures tasktrackers parameters on each cluster based on the cluster?s capabilities. This mechanism consists of three steps:  ? deploy Hadoop on all the nodes using the global con- figuration settings, i.e., adjusting to the lowest available values;  ? create cluster-specific configuration files; ? restart tasktrackers with the new configuration files.

Specifically, considering the clusters description in Ta-  ble I, we deployed Hadoop with 8 map slots and 2 reduce slots per node, and then restarted tasktracker processes on Cl3 to use 20 map and 4 reduce slots. With this platform- aware Hadoop deployment, indexing 1 TB of data on 107 nodes finished in 65.4 minutes, 30 minutes faster than the default Hadoop run and 10 minutes faster than the tuned Hadoop run (see Table II).

0  500  1000  1500  2000  2500  tim e(  s)  task id  map  (a) Default Hadoop          0  500  1000  1500  2000  2500  tim e(  s)  task id  map  (b) Platform-aware Hadoop  Figure 1. Time progress of map tasks when indexing 1 TB on 107 nodes.

Figure 1 shows the map tasks in time when indexing 1 TB with the default Hadoop deployment (Figure 1(a)) and when configuring Hadoop per-cluster (Figure 1(b)). The figures basically depict how the map phase progresses in time. There are about 2050 map tasks to be launched on the 100 node platform. With the default Hadoop run, we could set 8 map slots per tasktracker, giving us a total of 856 slots for mapping at a time. As Figure 1(a) shows, the first wave on map tasks is launched in parallel. Since mappers run on nodes with different performance, there is some variance in map duration times which eventually leads to the degrada- tion of mapper waves. Also, the spikes show that some map tasks are significantly slower than the rest of the wave. This is an effect of using the default Hadoop settings that do not accommodate the large amount of data being processed.

Some tasks have to spill their output to disk and wait for network to send the data to the reducers. Adjusting low-level Hadoop parameters (increasing the size of memory buffers, compressing the map output, etc.) can prevent this behavior, as can be seen in Figure 1(b). In addition, configuring each cluster according to its true capabilities provides 1192 slots for mapping in parallel, and finally, faster execution time.

C. Dealing with large-size auxiliary data  As discussed in Section III, both indexing and search- ing applications use auxiliary information to process the input data. In the case of the distributed index creation, the mappers load the tree of cluster representatives and traverse it for every input point to assign it to the closest cluster. Whereas the search application is concerned, the auxiliary data consists of the batch of query descriptors to be processed. In both cases, the additional information can amount to significant sizes: the index tree corresponding to 1 TB of data is 461 MB in size, while for 4 TB of data, 1.8 GB of auxiliary data are required. We focus here on  tools we found useful in managing auxiliary data.

1) Multi-threaded Mappers: Loading the same piece of information in every mapper is a requirement commonly encountered in MapReduce applications. The way Hadoop addresses this issue is to allow users to adjust the RAM dedicated to each mapper so that it accommodates the size of the resource to load. However, by doing so, one has to take into account the machine?s capacity, i.e., the total available memory: having more RAM per mapper means having less map slots per node, less map operations executed simultaneously, and finally, longer execution time. A way to avoid wasting CPU cores while still allocating more RAM, is to make use of multi-threaded mappers. Implemented in Hadoop, the multi-threaded mapper spans a configurable number of threads, each thread executing map tasks as a nor- mal mapper would. The threads of a mapper share the same RAM memory, thus auxiliary data can be loaded only once and read simultaneously by multiple threads during the map phase. The map computations are executed in parallel, each thread working on different (key, value) pairs. By setting the right number of threads per mapper, the CPU capacity can be fully utilized. The downside of this mechanism is that multi- threaded mappers use synchronization when both reading the input data and then when writing the map output: the threads synchronize when reading each (key, value) pair from the input chunk of data allocated to the mapper; running the map function on that pair is done in parallel by all the threads; however, writing the output of each map operation to the same map output buffer is again synchronized.

The optimal configuration of map slots and number of threads per each slot strongly depends on the application workload. Obviously, the combination of map slots ? threads per slot has to fully utilize the machine?s processing power. The synchronization steps in the multi-threaded map- per implementation incur an inherent overhead when using     Mapper slots Threads per mapper Time(min) 1 8 87 2 4 68 4 2 41 8 1 42  Table IV SMALL-SCALE INDEXING WITH VARYING NUMBER OF MAP THREADS.

this mechanism over the normal Hadoop mapper. Thus, the performance gain of using multi-threaded mappers is only partially dependent on the workload and the configuration.

Our indexing application provides a suitable case for using multi-threaded mappers. For this approach to be beneficial performance-wise, we have to discover the configuration that allows us to overcome the synchronization overhead. In the normal mapper case, the index tree is loaded by each map task, while in the multi-threaded mapper, the tree is loaded once and then shared by all the threads. In both cases, there is an overhead that cannot be avoided: index tree loading vs. synchronization. To discover the optimal number of map slots ? threads per slot, we initially performed a small-scale experiment using 13 nodes belonging to the same cluster of the Rennes site. Disposing of 8 cores per node, Hadoop tasktrackers were configured with various combinations of map slots and threads. In each setup, we performed indexing on 45 GB of data using the multi-threaded mapper imple- mentation; the index in this experiment was 460 MB in size.

The results are displayed in Table IV. In addition to these multi-threaded mapper runs, we also ran the indexing with the default mapper implementation on the same dataset and on the same platform; in this case, the indexing took 40 minutes to complete. It is fair to assume this is the optimal execution time in the given setup, furthermore, we can use it as baseline to compare against. The slowest run is the configuration in which 8 threads are spawned within the same map slot; this is a clear effect of the reading/writing synchronization implemented in Hadoop?s multi-threaded mapper. As we increase the number of slots and reduce the number of threads per slot, the indexing time decreases.

The best configuration that also achieves a running time comparable to the baseline is 4 map slots ? 2 threads per slot.

The benefits of using multi-threaded mappers are revealed when processing the big dataset of 4 TB. When using the default mapper, the size of the index tree (1.8 GB) compelled us to increase the RAM size to 6 GB and limit the map slots per tasktracker to 4, under-utilizing the node?s CPU power.

With multi-threaded mappers, we were able to take full advantage of all the CPU cores, while sharing the memory between the threads of a mapper. Based on the results of the small-scale experiment, we configured the tasktrackers with 4 mapper slots, each mapper running 2 threads. This setting delivered the best execution time for the distributed indexing: 6 hours and 8 minutes.

Query batch(#images) Time(s)  Complete loading 3,000 382 12,000 521 25,000 755  MapFiles 3,000 343 12,000 607 25,000 932  Table V SEARCH TIMES OVER 1 TB ON 100 NODES.

2) MapFiles: In some applications, the mappers require auxiliary data related only to the data chunks they process.

In this case, we can avoid loading all the auxiliary data by loading only the necessary part. This partial loading of auxiliary information can be achieved by the means of Map- Files. Hadoop?s MapFiles can be regarded as common map structures, sets of (key, value) associations that persistently store the data in binary format, as HDFS SequenceFiles.

In addition to the data file that contains the (key, value) mappings in order, a MapFile also has an index file storing keys and for each key, its offset in the data file. The index file can be configured to store only a fraction of the key set.

MapFiles are an efficient tool to load auxiliary data and reduce RAM usage, since only the index file is loaded into memory, while the data file is read from disk whenever necessary. Accessing a MapFile, i.e., getting the value as- sociated to a key, consists of 1) searching the key in the in-memory index and getting the offset of the key on disk, and 2) seeking in the data file to the key?s offset and reading the corresponding value.

We used MapFiles to store the batches of queries in the searching application. We performed searches on 1 TB of data in two scenarios: when the batch of queries is entirely loaded from disk into memory by each mapper, and when the batch is stored as a MapFile and only its index is kept in the main memory. Using a MapFile key interval of 1, we searched over 1 TB on 100 nodes, using three batches of different sizes. The search times shown in Table V prove that MapFiles are an efficient tool for managing auxiliary data.

Apart from delivering comparable performance, MapFiles also reduce RAM usage. For instance, instead of loading approximately 500 MB of queries, each mapper keeps in memory a 4.6 MB index of the keys. This can make a significant difference when the query batch is very large. If the query batch is of the order of gigabytes in size, loading it into memory by each mapper is inefficient and sometimes unfeasible. Even if the total available RAM memory is able to accommodate the size of the batch, this would impact the number of mappers running simultaneously. Although using MapFiles takes longer to process queries, it still allows all the mappers to run at the same time; thus the overhead of reading from disk is eventually outweighed.

D. Observations  Among other tools that Hadoop provides to facilitate data distribution, we found particularly useful the distributed     cache, which transparently transfers auxiliary data to the local disks of all the nodes.

Overall, Hadoop is helpful for achieving scalability. How- ever, the size of the data puts a significant load on the framework. We observed that tasks often fail because of insufficient disk space, communication timeouts, etc. These failures can be avoided or at least minimized through pa- rameter configuration. On the other hand, we encountered a few Hadoop errors that we were hard to localize and fix. At this point, the framework?s fault tolerance mechanisms are crucial for completing the job, which Hadoop manages to achieve, on average.



VI. CONCLUSION  This paper has presented our experience with processing terabytes of multimedia data through the Hadoop MapRe- duce framework. We described a wide collection of exper- iments and the practical lessons we have drawn from our experience with the Hadoop environment. We have shown the performance benefits of understanding the application workload and of tuning Hadoop configuration parameters accordingly. In addition, we addressed issues commonly found in Big Data processing: resource heterogeneity and management of large auxiliary data. The tools we employed are existing tools implemented in Hadoop, that despite the lack of documentation, prove to be very efficient once one understands how and when to use them. Our experiments have shown the benefits of using multi-threaded mappers and MapFiles in reducing memory usage while maintaining the same degree of parallelism.

Although the tools provided by the Hadoop to accom- modate great data scales proved to be essential for the performance of our applications, there is still a room for improvement. For example, the multi-threaded mappers syn- chronize when reading and writing a single key/value pair, negatively affecting the performance. One way to minimize the number of synchronization steps is to have each thread use a separate buffer for reading and writing. While this mechanism is easy to implement for writing, for reading it is less trivial, since it requires modifications in the Hadoop?s input data splitting process. Another interesting direction for future work is to implement our applications using other distributed frameworks and compare them to the Hadoop?s performance. Since our experimental platform can accom- modate our dataset into main memory, we plan to look into in-memory distributed frameworks, such as Main Memory Map Reduce (M3R) [23], a Hadoop variant that maintains the data in the cluster?s main memory.

The implementation and the experiments presented in this paper are real applications conducted on real datasets. The lessons drawn from our work can be shared as best practices and recommendations in the unceasing task of rising to the Big Data challenge.

ACKNOWLEDGEMENTS  This work was partly achieved as part of the Quaero Project, funded by OSEO, French State agency for inno- vation (see http://www.quaero.org/). The experiments pre- sented in this paper were carried out using the Grid?5000/ ALADDIN-G5K experimental testbed (see http://www.

grid5000.fr/).

