Stream Processing of Scientific Big Data on Heterogeneous Platforms ?   Image Analytics on Big Data in Motion

Abstract?High performance image analytics is an important challenge for big data processing as image and video data is a huge portion of big data e.g. generated by a tremendous amount of image sensors worldwide. This paper presents a case study for image analytics namely the parallel connected component labeling (CCL) which is one of the first steps of image analytics in general. It is shown that a high performance CCL implementation can be obtained on a heterogeneous platform if parts of the algorithm are processed on a fine grain parallel field programmable gate array (FPGA) and a multi- core processor simultaneously. The proposed highly efficient architecture and implementation is suitable for the processing of big image and video data in motion and reduces the amount of memory required by the hardware architecture significantly for typical image sizes.

Keywords-component; Big data; image analytic; stream processing;  image processing; heterogeneous  platform; feature  extraction; data  in  motion; component  labeling

I. INTRODUCTION One of the big data challenges are image analytic tools  applicable to photos, surveillance videos and e.g. scientific data from sensors that can reach massive proportions over time [1]. Statistics show that 2.5 quintillion bytes are generated every day and 90% of all data was generated since 2011 [1]. Generally, big data is defined based on its main characteristics which are growing in three dimensions: volume, velocity and variety [2]. In this new concept, the volume of unstructured data is in the scale of petabytes, and creation of them is in the fraction of the second [3]. In the literature big data in motion is defined as continues data streams at high data transfer rate. Such big data represent data sets that cannot be analyzed with conventional algorithms and standard hardware platforms [1, 4]. Due to the typical memory capacity and bandwidth limitations, which determine the overall throughput, the processing of the continuously increasing amount of data is done online and locally on the streamed data. The new scale of volume, velocity and variety requires redesigning a number of infrastructure components and algorithms to support the large-volume, complex and growing data [4, 5].

Modern high performance heterogeneous computing approaches based on field-programmable gate arrays (FPGAs) and general-purpose computing on graphic processing units (GPGPUs) combined with general purpose processors enable the processing of large scale problems in  the field of e.g. genomics, bioinformatics, graph analytics, social network analytics, etc. which was not possible before, see [5], [6]. For instance, Convey Computer proposed a high-performance hybrid core system which pairs Intel processors with a coprocessor of FPGAs which is able to execute data-intensive problems much more effectively [7].

IBM has also established Netezza as an advance high performance data analytical tool which has led to an exponential growth in the field of big data analytics.

Netezza is based on IBM blade architecture and uses FPGAs to filter the input data before processing it [1].

Scientific data are defined in four different types: raw data, structured data, published data and data linked to the publication [5]. First type of the scientific data is raw data which is generated from observation and experiment of different phenomena. For instance biological, climate and life sciences generate massive amounts of scientific data [5, 6]. Images and videos have the highest amount of volume among scientific data, and are analytically prepared to gain additional value [8], [9]. The preparation is done by extracting various properties of the image such as objects and movements.

The first step in analytic image processing for many video-based applications is segmentation which is followed by connected-component labeling [10]. Connected- component labeling performs the task of labeling all connected image pixels in a binarized image in order to identify objects or extract certain features of an object. The throughput of the CCL can strongly influence the performance of the whole image processing system as it is one of the first complex processing steps in image processing applications. For this reason, a parallel CCL algorithm with memory-efficient architecture is proposed here suited for high performance image processing applications such as image analytics. It is based on a scalable single pass CCL algorithm which is memory- efficient and therefore especially suited for FPGAs. By using the proposed architecture and algorithm it is possible to achieve a high processing throughput for performing connected component labeling of streamed images without the need of buffering a full image, which would cause a performance limitation either due to limited FPGA-internal memory or due to limited FPGA-external memory bandwidth.

DOI 10.1109/CSE.2013.142    DOI 10.1109/CSE.2013.142     In the next section, the state of the art and related algorithms which cover memory-efficient and scalable CCL algorithms are presented. In Section III the video and image analytics framework are presented and the proposed algorithm and architecture are discussed. In the fourth Section experimental results are shown and compared to existing approaches in the literature. Finally, the paper is concluded in Section V.



II. RELATED WORK Connected component labeling (CCL) is one of the first  steps in image analytics algorithms. As an alternative to the classical two-pass CCL algorithm more sophisticated single pass algorithms have gained interest for reconfigurable computing recently. The classical CCL algorithms are completed after two scans of the same image and have sequential dependencies. Therefore, a high amount of storage and memory for storing full images is required [11].

In order to store the full image and labels, a memory with the same size as the original image is required. The labels are characterized by the position of the pixel within the image. If the pixel belongs to the background area of the image a special background label is used, otherwise its label is determined by the labels of adjacent pixels.

However, Baily et al. proposed FPGA based single pass architectures [12]. The proposed architecture has two drawbacks: Its worst case memory requirement is related to the height and width of the processed image, and also due to the lack of parallelism one pixel is processed per clock cycle causing a performance bottleneck in stream processing.

Ma et al. reduced the memory requirements significantly [13] of the algorithm described in [14] by reusing the labels.

As a result, the required worst case memory is proportional to the width of the processed image. Kumar et al. proposed a parallel architecture and enhances the single pass algorithm used in [15]. The main idea is to store the whole images in prior to processing. In order to gain performance speed-up, the image is divided in to several slices, and different CCL units have to independently process them. After that, each line from the image slice is fetched sequentially at a time in a round robin manner, and each CCL unit sends a vector to a global FIFO memory which acts as a vector collector. The vector mainly describes the regions of the processed image slice excluding the edges. In the next step a coalescing unit (CU) determines whether two regions of adjacent slices are connected and belong to the same object by processing one or both border of the image slice.

For the merge operation it is essential to connect the CCL units to the CU and perform the merging operation of neighboring image slices in a round-robin manner.

Additionally, to have a successful merging, each edge region needs to have a unique, distinguishable label in its image slice. In order to overcome with the limitations of  [13] a modified version was proposed  [16] which reduces drastically the amount of memory required for processing by  using two types of labels, slice local label and slice global labels. In this paper, a CU was introduced which improve real-time processing of CCL architecture by merging the results of image slices in a memory-efficient way.



III. VIDEO AND IMAGE ANALYTICS FRAMEWORK Image analytics requires the processing of videos and  images to transform pixel level information to object based information for the analysis of certain properties specific for the considered application domain like science, industrial measurement techniques or life science applications. Videos are considered as sequence of images with a specific frame rate. For high-speed scientific applications, the frame rate may reach up to several hundred or thousand frames per second or more, such that the video data of a single image sensor are in the range of 1 to 10 Gigabytes per second or 0.1 to 1 Petabyte per day. To cope with this big data in motion, a high performance reconfigurable computing framework is proposed in this section.

The overall framework is composed of a high-speed input data stream, a heterogeneous platform based on high performance reconfigurable computing devices using field programmable gate arrays (FPGA) and multi-core CPUs to which the image processing functions are mapped jointly. As shown in Figure 1, the input data stream is connected via several high speed links to the FPGAs which are able to acquire and analyze images as well as videos with a very high frame rate in real-time. Therefore, the framework based on the heterogeneous platform is equipped with integrated image processing capabilities such as segmentation, component labeling and feature extraction.

Figure 1. High performance reconfigurable computing framework for proccessing real-time image stream processing  The following image processing steps, which enable a massive data reduction from GBytes of Pixel data to only KBytes of abstract object descriptors - so called feature vectors, are transferred to the multi-core CPU. This reduction enables the framework to output information on every object in every frame even in real-time for very high frame rates.

The amount of data which has to be transferred from the FPGA is reduced by several orders of magnitude in this way.

The image processing architecture realized on the FPGA has the tasks of image segmentation and feature extraction.

For segmentation, which is the process of separating the     Figure 2. Grayscale raw image taken by image sensor and its histogram.

object from the background, thresholding is applied. By using this method all pixels having an intensity value over a certain threshold are considered as an object and converted to black and all pixels below this threshold are converted to white and considered as background. Accordingly, a binary image is generated from the original image. In this case, the threshold value for separating the objects from the background is of major importance. In this work the method proposed in [17] is used for segmentation by generating a histogram of the captured grayscale image of a scientific application in the field of spray process, as seen in Figure 2.

The threshold value is calculated by using the arithmetic mean value of the two peaks detected in the histogram, representing the objects and the background.

The next step is feature extraction based on connected component labeling. The main challenge is processing high image frame rate in real-time which requires high bandwidth in the range of 10 to 100 Gbit/s. To overcome this problem, a highly optimized and sophisticated architecture is used. Due to the limited resources in embedded systems, algorithms which are especially dedicated to FPGA architectures have been proposed [12, 13, 14, 16].

The connected component labeling architecture proposed in this work is based on Bailey [12] and Ma [13] with a parallel processing extension. Figure 4 shows the architecture for sequential processing [13] and it consists of several components which are described as follows: the neighborhood context block provides four registers A, B, C and D containing four previously processed pixels connected to the current pixel. A, B and C contain the label of the previous row and D contains the previous pixel label.

The row buffer block is used for caching the new labels, since they are not saved in a temporary image. The key difference from [11] to [13] is that the labels are reused in every row, and cause the reduction in the need for memory.

The merger table decides any equality as a result of mergers on the previous row. The translation table modifies a label allocated to the pervious row to the current new label. The label for the current pixel is selected in the label selection block based on the labels of its neighborhood.

Figure 3. Segmentation of grayscale image with calculated threshold value.

The data merging unit records the features of each region by monitoring the labels of the pixels in the neighborhood context block. Each region has one entry in the data merging unit indexed by the region's label. Whenever a region is updated, its entry in the data merging unit is updated as well.

Figure 4. Connected Component Labeling Architecture proposed in [13].

A bounding box is defined by two coordinates A and B.

Coordinate A indicates the upper left corner and coordinate B indicates the lower right corner. In order to extract the bounding box for each object the structure and the merging process within the data merging unit is proposed [4 ,6].

The data merging unit has to be changed as shown in Figure 5 to provide bounding box extraction. The modified block has two inputs a and b, one for providing the currently processed pixel?s coordinates and the other one for giving information on the neighbor pixels label. The input data is read from the corresponding data table. For each extracted object a merging unit as well as a data table is necessary. To     find the bounding box for two entries of the data table the following equations are used.

??? = ???(???, ??	)  (1)   ?? = ??? ( ??, ?	)  (2)  ??? = ???(???, ??	)  (3)   ?? = ???( ??, ?	)  (4)  Figure 5. Data Merging Unit for extracting several features of the image objects simultaneously.

In order to handle high data throughput a parallelization approach is proposed. Thus, the image is divided into several slices, and each slice is processed separately in parallel.  In each slices the objects are identified and merged by a central unit called the coalescing unit. As a result several pixels are processed simultaneously and speedup based on the number of image slices can be obtained.

Figure 6 through Figure 8 show the processing steps for feature extraction. Figure 6 shows the binary image after segmentation. In the next step, the image is divided into several image slices for processing in CCL processing units which are working in parallel. As it is shown in Figure 7, the bounding box is extracted by each CCL proceeding unit.

In the last step, the bounding boxes touching the slice borders will be merged together to have the correct result.

This step is depicted in Figure 8.

Figure 6. Binarized image after thresholding.

Figure 7. Extracted object features for all sub-images.

Figure 8. Merged object features for input image.



IV. EXPERIMENTAL RESULTS  In the following, experimental data to examine the potential of heterogeneous systems consisting of reconfigurable logic devices and general purpose processors (GPP) in the context of an image processing system for big data analytics is provided.  For this examination the approaches from [14], [11] and [16] are evaluated on both reconfigurable logic devices and in software on a general purpose processor.

The approach in [16] gains a speedup from dividing the image in several vertical slices using the architecture from [14] as slice processors and merging the results using a coalescing unit. The results for the implementation on a single FPGA where parallelism is inherently used and in software for different image sizes on a single core are given in Table 1.

For the implementation of an array of slice processing units (SPU) consisting of up to 100 SPUS on a single FPGA was realized, where all are processing an individual image in parallel. For the software implementation the classical two-pass algorithm [11] is used to process one image on a single core of a general purpose processor (GPP).

TABLE I. CCL-BENCHMARK CPU VS. FPGA  Hardware Platform Image Size CPU FPGA Result  Width Height Giga-pixels/s Cores # Giga-  pixels/s Area % Speedup  128 128 0,06301 1 15,3 8 242  256 256 0,06277 1 11,3 43 182  512 512 0,06286 1 12,3 81 198  1024 512 0,06189 1 9,1 93 146  2048 1024 0,06062 1 6,9 83 114  When comparing the throughput of the software implementation on a GPP and the dedicated architecture on an FPGA, the bandwidth of the FPGA architecture for CCL is one to two orders of magnitude higher. The hardware architecture is highly optimized to the FPGA structure, while for the presented software solution further research has to be done to enable a proper comparison, but no decrease in speedup by less than one order of magnitude can be expected.  This enables the FPGA architecture to process an image stream consisting of several different image slices simultaneously in real-time. For the case of processing a single image, only one slice processing unit of the SPU array can be used. Still the FPGA architecture accelerates the processing by approximately a factor of 2 compared to a single GPP core. By using the parallelization scheme from [16], several slice processing units can be used to process a single image in parallel enabling a higher throughput. Figure 9 through Figure 11 show the results for this approach where the coalescing unit is realized for FPGAs. Depending on the image size up to 4.5 GPixels/s can be processed. To study the performance of the coalescing process on a GPP, a prototype implementation of a software version of the coalescing unit was realized. The result for the throughput given in Table 2 can be achieved for the case that the feature vectors provided by the slice processing units are already stored in the systems RAM and can be accessed at full memory bandwidth.

TABLE II. SOFTWARE COALESCING: THROUGHPUT IN GIGA- PIXEL PER SECOND  image size / # of slices throughput (Gigapixels/s) 256 ? 256 / 4 6,62  1024 ? 1024 / 8 10,51 2048 ? 1024 / 4 13,40  Under this circumstances the coalescing unit on the GPP achieves up to 13 GPixels/s, which is in the same order of magnitude as the FPGA implementation. In general, the irregular data structures and the sequential algorithm of the coalescing process are better suited for a GPP software implementation. Considering the performance results of both slice processing units and the coalescing unit on a heterogeneous platform, the conclusion is that the slice processing units should be implemented on a FPGAs and  Figure 9. Achievable frame rate for an image with 0.5 Megapixels.

Figure 10. Achievable frame rate for an image with 2.1 Megapixels.

Figure 11. Achievable frame rate of maximum 800 fps for an image with 6.3 MegaPixels and a throughput of 5 Gigapixels per second.

the coalescing unit should be implemented on a GPP in order to achieve maximum performance for big data image analytics. Achieving a throughput of 5 Gigapixel/s is equivalent to 0.4 Petabytes per day, so the implementation on a heterogeneous platform with a single mid-sized FPGA     can be scaled up linearly with a several FPGAs and processor cores under the condition that several different images are processed in parallel. Beyond these performance considerations, mapping the irregular data structure and the sequential part of the algorithm, which is the coalescing algorithm, to the GPP is of advantage because of its higher processing frequency. For the regular data structures used in the slice processing units carrying out the parallel part of the algorithm, the FPGA achieves a higher throughput due to its parallel architecture.



V. CONCLUSION Images and videos have the highest amount of volume among big data and therefore high performance image processing plays an important role for image analytics. In this paper, we have investigated a parallel component labeling algorithm including feature extraction with a broad set of features as an important part of an image analytics framework. It is shown here that the performance of the proposed parallel component labeling algorithm with generalized feature extraction is accelerated and optimized if it is mapped to and executed on a heterogeneous hardware platform based on a fine-grained field-programmable gate array and a multi-core processor. For the parallelized connected component labeling (CCL) algorithm of this paper, the required memory compared to the requirement of algorithms known in the literature is reduced significantly on a heterogeneous hardware platform for typical image sizes even when compared to similar sliced parallel single pass CCL algorithms and architectures such that memory size nor memory bandwidth of the hardware platform has an impact on the performance. The basic structure of the algorithm and architecture is a set of parallel CCL units generating feature data of image slices being coalesced in a separate and subsequent coalescing unit. In order to achieve highest performance, it was shown that the parallel CCL units should be implemented on fine-grained FPGAs and the coalescing unit should be implemented in software on a multi-core processor. With the achieved throughput of 5 Gigapixels per second or 0.4 Petabytes per day the implementation on such a heterogeneous platform with a single mid-sized FPGA can be scaled up linearly with a plurality of FPGAs. Thus, the concept has been proven to be ideally suited for high performance image analytics for big data in motion.

