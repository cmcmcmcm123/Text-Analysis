A Conceptual Infrastructure for Knowledge Discovery Process with Granularity

Abstract?Knowledge discovery has received more and more attention from the business community for the last few years.

One of the most important and challenging problems in it is the definition of discovery process model, which are well understood, efficiency, and quality of outcome. A conceptual infrastructure for knowledge discovery process is proposed with business understanding, model selection and domain knowledge integration in evolving database environment. The corresponding layered architecture creates an automatable process and communication mechanism to incorporate known knowledge and data granules into discovery process, through ontology service facility. A higher order mining method embedded in the process is proposed, to achieve monitoring and identifying changes statically and tracing trends dynamically. Finally, implementation techniques of basic data structure and simplified mining algorithms are discussed.

Keywords-discovery process; conceptual infrastructure; higher order mining

I.  INTRODUCTION The data mining (DM) and knowledge discovery in  databases (KDD) movements have been based on the fact that the true value is not in storing the data, but rather in our ability to extract useful reports and to find interesting trends and correlations. The set of DM processes used to extract and verify patterns in data is the core of the knowledge discovery process. These processes comprise many steps, which involve data selection, data preprocessing, data transformation, DM, and interpretation and evaluation of patterns. During the last a few years, most research in DM focuses on the development of new algorithms or improvement in the speed or the accuracy of the existing ones [1]. Relatively little has been published about theoretical frameworks of knowledge discovery.

In 1996, Fayyad et al. presented a process model that resulted from interactions between researchers and industrial data analysts [2]. The model did not address particular DM techniques, but rather provided support for the complicated and highly iterative process of knowledge generation. Since then, several different process models have been developed.

These process models consist of multiple steps executed in a sequence, which often includes loops and iterations [3,4].

The CRISP-DM (CRoss Industry Standard Process for Data Mining) process model includes six steps: business understanding, data understanding, data preparation, modeling, evaluation and deployment [3]. In general, the CRISP-DM is the most suitable for novice data miners  working on industrial projects, due to the intuitive, industry- applications-focused description, and has been already acknowledged and relatively widely used in both research and industrial communities. One of the main limitations of the CRISP-DM life-cycle representation is that it is essentially sequential and linear.

Agrawal et al. showed an active data mining process, where the mining algorithm was applied to each of the partitioned data set and rules were induced [5]. Spiliopoulou et al. investigated the utility of inducting rules from results of other mining routines [6]. Several researches have concentrated on fusion of domain knowledge with DM system [7]. Wang et al. discussed some recent achievements in domain-oriented data-driven data mining (3DM) model [8]. However, little attention has been paid to establishment of a general infrastructure for supporting share of domain knowledge.

In this paper we attempt to explore knowledge discovery process model to make data mining for evolving database environments, and to incorporate the domain knowledge and data granules with the KDD process. Our conceptual infrastructure for a data mining system is built upon three essential components: Ontology, KDD process model and Knowledge engineering. It also provides a higher order mining method to identify and trace changes in data or patterns.



II. PROCESS MODEL AND INFRASTRUCTURE We present a model and an infrastructure for the KDD  process, which is based on the concept of active, continuous mining and designed to integrate known knowledge and granules in order to support automatic discovery process.

A. Mining ontologies Ontology provides a framework for sharing conceptual  models about known knowledge. A model that allows agents to deal with explicit, declaratively represented ontologies was described in the FIPA Ontology Service [9]. The known knowledge may be is shared and reused by utilizing ontology service.

It is necessary to set up conceptual frameworks into which data and datasets, databases and known knowledge might be classified. The families of available models need to be classified, and the relation of model-families to data-types needs to be made clear. We need abstract models of data and their relations, and of models and their relations. It is by knowing the ontological structure of the data available, and   DOI 10.1109/AICI.2010.260    DOI 10.1109/AICI.2010.260     the models that may be applied to it, that we can reasonably make a choice of the model to use.

A necessary high-level structure to business and data exploration process is the trio of interrelated processes of business-understanding, data-understanding and the availability of a model-ontology. The data ontology includes specifically knowledge of data, data-types and data-source; the model-ontology includes a clear understanding of the data-types associated with various classes of model.

To make use of data granules mechanism (or groups, classes, clusters of a universe) to support discovery process, it need create various levels of data, as well as inherent data structure, by partitioning data attributes into intervals. The different levels of granularities form data granule ontologies.

B. Process model The model, called C-KDD (Cohesive KDD) model, is  shown in Fig. 1. It consists of four stages: planning, session mining, merge mining, and post-processing.

During planning stage, the KDD process begins with business understanding, including business-aims and business-logic. Through interactive exploration and experimentation, discovery goals, business data, and subsequent processes are identified and the specification of discovery task schedule (TS) is generated. The ontological domain knowledge is used to eliminate irrelevant attributes, update the vague prior business factors, inferring other abstract attributes, etc. Moreover, the set of valid data attributes, process steps, and algorithms are composed in suggested order based on the user?s desiderata, by a data mining ontology.

The session mining stage performs select-transfer- premining and achieves partial data mining. It places emphasis on local and static rules induction, and executes induction on incremental data at regular intervals, e.g.

month. As the functions are already specified in the TS, they are periodically repeated on incremental data as per the frequency or trigger condition, and whose outcome forms a rule bin (RB). Ontological knowledge is used to assist in determining the selected features, parameters, etc.

The merge mining is initiated by mining queries or a trigger event. The query contents are listed in consultation with the TS; user can commit them, according to his requests. A trigger event is occurred as the causes of time or rule rising. It places emphasis on overall and dynamic rules discovery, in the interaction paradigm, the rules are merged and refined from several RBs. The parameters and constrains are supplemented with ontological knowledge.

The post-processing stage begins with matching discovered rules and known knowledge, filters useless ones, then classified and ranked automatically interesting results according to interestingness. When a critical point threshold is reached, an alert will be triggered. Meanwhile, the user can review and confirm these findings. It would also integrate new interesting insights with the known knowledge, to perform knowledge evolution and presentation. Then, it forms a close-loop solution that helps to maintain the continuous knowledge discovery process.

Figure 1.  C-KDD Process Model  When unable to satisfy intelligence application or rule review, the process flow goes back to the planning stage to re-explore data.

C. Higher order mining Higher order mining is a knowledge discovery process  concerned with mining over previously induced patterns or models [10]. Its methods and techniques are applied in the C- KDD process model.

In planning stage, data attributes concerned with data mining are partitioned into intervals, called bags. Here, we consider only equi-width bags (the interval size of each bag is the same). For example, Age=20, Age=21, and Age=22 form a bag (20 ? Age ? 22). When a quantitative attribute is partitioned into bags, the bags are mapped to consecutive integers, called BIDs; when attribute values are mapped to bags their value is replaced with the corresponding BID for that bag. For categorical attributes we also map the attribute values to a set of consecutive integers and use these BIDs in place of the categorical values. In the same way, these basis BID may be mapped to upper BIDs. So, a BID hierarchy defines a sequence of mappings from a set of low-level BIDs to higher, larger extent BIDs. It is useful to rule clustering and classification later stages.

In the session mining stage, Induction can be separated into two steps, the first involves the generation of rules upon primary data, with the other performing rule clustering and classifying. Rule clustering is the combination of adjacent attributes values, or adjacent bags of attribute values in rule.

For example, clustering (Age=20) and (Age=21) results in  Session Mining Planning  Schedule Generation  Data Exploration  Business Understanding  Selection  Induction  Transform  Merge Mining  Rule Extraction  Mining Query  Postprocessing  Human-Machine Processing  Machine Processing  Rule Review Filter & Alert  Evolution & Deployment  Grouping     (20 ? Age ? 21). A clustered rule is an expression of the form Xc?Yc. Xc and Yc are items of the form (Attribute = value) or (bagi ? Attribute ? bagi+l), where bagi denotes the lower bound for values in the ith bag. Clustered rules will always have a support and confidence of at least that of the minimum threshold levels.

Fig. 2 shows a unifying framework that uses heuristics for generating the clustered association rules, which we now describe.

While the primary data is read, the minimum support is used along with the minimum confidence to generate a large number of association rules. Clustering rules engine forms clusters of adjacent association rules, supporting by the BID hierarchy, according to grouping criteria. These clustered association rules are then tested for their accuracy (by the verifier) against a sample of tuples from the primary data database. The accuracy is supplied to a heuristic optimizer that adjusts the minimum support threshold and/or the minimum confidence threshold, and restarts the mining process at the clustering rules engine. These heuristic adjustments continue until the verifier detects no significant improvement in the resulting clustered association rules, or the verifier determines that the budgeted time has expired.

Once the clustered association rules are discovered for a particular level of BID, we then go up to try upper BID.

Finally, the clustered rule structures are achieved.

Associative classification uses association rules to build a classifier. The association rules must have the consequent bound to the class label, hence are considered associative classification rules (ACRs). Once a set of ACRs are gathered, they are ranked, with the highest ranked ACRs used to create classifiers.

The framework of generating associative classification rules is similar to Fig. 2. Firstly, association rules must be pruned and ranked during the ACR generation process to reduce the volume of rules generated, and to allow for an informed selection procedure to build the classifiers from the most highly ranked rules. One procedure to evaluate ACRs is to rank them in the order of confidence, support, and precedence, respectively. Then, given a prioritized set of ACRs, a classifier is created based on the set of rules that cover the training set. The classifier is iteratively built where, if a rule covers some set of examples in the training set, it is added and those pertinent examples are removed from further rule evaluations. Each rule, therefore, only covers a particular set of examples, and hence only rules that increase the accuracy of the classifier are included. The default class is determined as the rule which covers the largest number of examples.

In the merge mining stage, higher order mining focuses on frameworks that detect changes upon data that come from a non-stationary distribution, in pattern management suites and, obviously, in temporal mining methods. Not only the data change but their schema also changes as well.

We can detect changes in patterns to identify and quantify the differences between patterns drawn from different snapshots of the dataset, namely RBs. We also trace the trends in association rules or clusters from a sequence of RBs.

Figure 2.  Framework of Clustering Rules  D. Knowledge engineering The processes of model interpretation, validation and  evaluation have rightly been given places of importance in knowledge discovery through data exploration and mining, and in particular in the process model, to ensure the quality of the outcome of KDD. The problems about meaningfulness, significance and importance are a crucial part of trained DM-model understanding. These problems about what we know from DM, and indeed what cannot be known, are essentially part of an as developing ?Knowledge engineering? for KDD.

Without doubt, a formal KDD theory of knowledge needs to be developed. A KDD epistemology will have close links with the data and model ontologies discussed above, and also with representations and storage concerning the prior knowledge and communication.

For C-KDD process model, a hierarchy of processes operating on the constructs at each level of ?Information Hierarchy? involving data, information, knowledge, and sometimes wisdom, may also be defined, and is illustrated in [11]. Corresponding to the data-level of the hierarchy, operations concerned with manipulations and storage of basic data resources we term ?technical?. Information is derived by operations performed on data using the tools and techniques of DM. We refer to the processes at this level as ?contextual?. Then, derived and discovered information becomes knowledge only when it is subjected to processes which put the information in the context of the business problems or opportunities. Hence, the processes which deal with such ?understanding? of information/knowledge are termed ?tactical?. Finally, the processes, which put the integrated knowledge in the context of the whole enterprise and its aims, are referred as ?strategic?.

A parameter estimation method, used to explain the interestingness and evolutional regularity of the rule, has been discussed in [12].

Association rule mining  Primary data  Min. Supp.

Min. Conf.

Association rules  Higher order mining   Clustering rules engine  Grouping criteria  Verifier   Cluster analysis  Test data Clustered association rules  Heuristic Optimizer  Generate clustered rule structure  BID hierarchy

III. LAYERED ARCHITECTURE In implementing the model, the machine processing steps  require developing autonomous components. Each component is an agent, which obtains the known knowledge through ontology service. The discussion of six components is as below.

1. Data selection (Csel) ? to select data from data sources. It decides dynamically the target subset, specified in terms of the location and time span for heterogeneity.

2. Data transformation (Ctra) ? to manipulate data to satisfy Cind needs.

3. Data induction (Cind) ? to execute mining algorithm to induce partial and raw rules from incremental data. It consists of a set of functions for each algorithm.

4. Rule grouping (Cgrp) ? to execute clustering and classifying algorithm to group static and raw rules. It consists of a set of functions for clustering, classifying, verification, and  optimization algorithm.

5. Rule extraction (Cext) ? to achieve overall induction and refinement, only dealing with RBs. It is important to refine the premined results and perform higher order explanation, particularly those describing changes across the session.

6. Rule filters (Cflt) ? to verify mining results. The result that contradicts known knowledge may be considered being interesting, because they can occur due to new knowledge or un-codified simply general knowledge.

7. Knowledge evolution (Cevl) ? to incorporate newly discovered knowledge into the known knowledge. This derives the necessity to support the ontology evolution, involved identification, mapping, and merging etc.

The motivation for the architecture is to synthesize the complete KDD process and to provide a continuous and convenient environment, as shown in Fig. 2.

The top layer, the front-end user layer, composes the end user interface. It provides functionality of human machine processing, and implements the user interface for C-KDD model. The TSs, arranging each mining process and interrelated data, are stored in schedule pool (MSP) and provide monitoring information through ontology services.

The Query Broker determines what TS should be contacted to meet a high-level user request. Rule Manager supports the review process for discovered results. Granularity Processor displays and controls the data granule transformation.

The middle layer, the autonomous component layer, is implemental in machine processing. The autonomous components and the RBs are located in this layer. The components populated the processes, are managed by corresponding to each TS entry. The RBs should be restrictively shared in order to preserve privacy.

Data mining in evolving domains presents several challenges from distributed, dynamic data sources. The storage management layer organizes the data sources by maintaining the ontology and metadata about the concept, term, and physical attribute of each one. It implements ontology service-based information integration. Granularity Manager organizes the data into various granularities according information induction requirement. Resource  Broker determines what data set should be contacted to meet a request specified in TS, by searching through ontologies for concept and metadata for physical location about data that match the request criteria.

All the three layers manipulate and share knowledge and metadata stored in repository segment. The segment maintains structural and semantic information about each data source, recording the relationship between attributes of the data sources with terms from business domain, computing contextual information gleaned from these linkages and other source-related information. Ontology Agent provides mediating knowledge base and ontology services, to support access to their services in a unique way that is well integrated with the FIPA Agent mechanism [9].

Figure 3.  Architecture of C-KDD

IV. IMPLEMENTATION In order to experiment and evaluate the feasibility of the  design, we constructed our architecture in a prototype. We concentrate on meta-model of RB and simplified mining algorithms invoking ontology services.

In the C-KDD model, the mining components are mainly three parts: data induction, rule grouping, and rule extraction, which use the rule bin (RB) to preserve the former results.

Accordingly, design of data structure for RB is a significantly important activity in the implementation.

The meta-model of RB consists of four classes stored in metadata repository. A leading class identifies each RB class table. An attribute class defines the attributes? properties of each RB. A method class stores its data operations and condition. A constraint class describes each event occurrence. They are used to provide continuous and active data mining. The primary attributes of the classes are as follows:  Leading (ClassName, ParentName, Operation); Attribute (AttributesName, ClassName, MethodName,  AttributesType, AssoAttribute);  ?  Granularity Manager  Resource Broker Partitioner  TS Generator  User Manager  Query Broker  Query Processor  Front-end User  Metadata Repository  Storage Management DB1 DB2 DBn  O ntology A  gent  Knowledge Base  Autonomous Component  Csel Ctra Cind Cext Cflt Cevl  Function Library RB1  ? RBm  Data Explorer  MSP  Rule Manager  Component Manager  Repository  Repository Manager Granularity  Processor  Cgrp     Method (MethodName, ClassName, Parameters, MethodType, Condition, Action);  Constraint (ConstraintName, ClassName, MethodName, Parameters, Event, Timing).

Table I is an example of the RB table for association rule, whose attributes are defined by the above attribute class, given a data table Source (A, B, C, D). The record counts of attribute for computing support and confidence of association rule are stored in this RB table, particularly ID of known knowledge is stored in field KK, according to year session. The details of rule extraction algorithm using ontology services (OS) are shown as below. It is shown to have linear scale-up.

TABLE I.  RB TABLE EXAMPLE  Data RuleID Ant_count Rule_count Rule BID KK  2005 R1 Count(B) Count(A, B) B ? A 91 R1_ID  2005 R2 Count(C) Count(A, C) C ? A 81 R2_ID ? ? ? ? ?  ?  2006 R1 Count(B) Count(A, B) B ? A 92 R1_ID ? ? ? ? ?  ?  RuleExtractionAR (TSid,Session,minsup,minconf,OutRule) Begin                //obtain possible interesting rules  RB_table = OS_SelectRBTable (TSid); //obtain RB table using OS  Total_Rec = OS_CountRec (TSid);     //total of records For each No_Session in Session do Begin  No_Rule = Count_Rule (RB_table, No_Session); //count number of rules in the session  For i = 1 to No_Rule do Begin  Support (i) = RB_table.Rule_count (i) / Total_Rec; Confidence (i) = RB_table.Rule_count (i)  // RB_table.Ant_count (i); If Support(i)>=minsup and Confidence(i)>=  minconf  then If OS_IsInteresting (TSid,RB_table.KK(i))  then     //verify interesting using OS Output (OutRule, Support(i), Confidence(i))  //output interesting rule End  End End  The change trend of rules (second order rule) may also be extracted from OutRule. The rule is a R = (ruleClause, statistics), ruleClause has the form: A?B, the statistics may be the support, confidence, and accuracy. A higher order rule is defined as a temporal sequence of R. A sequence of statistics for the same rule R in each session forms a time series; therefore time series analysis can be operated on it.

We will not discuss here in details for tracing the trends in rules.



V. CONCLUSION Implementation of a continuous KDD process has been  attracting more and more interests from various industries.

We have proposed a conceptual infrastructure for the KDD process model that endeavors to separate autonomous mining sub-processes from a full KDD process, and integrate known knowledge using ontology services. The architecture utilizes ontology service to provide an inherent mechanism for share known knowledge and data granules hierarchy, and constitute a knowledge discovery platform extending support for data independence and granularity transformation. We have also proposed a higher order mining method embedded in discovery process, to achieve monitoring and identifying changes statically and tracing trends dynamically. We have also discussed that data attributes concerned with data mining were partitioned into bags, and formed bags hierarchy to support clustering and classifying rules.

