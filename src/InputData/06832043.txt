GPS: A General Framework for Parallel Queries over Data Streams in Cloud

Abstract?Parallel query processing over data streams in cloud  computing environments has attracted considerable attention recently in various fields, due to the huge potential value of analyzing massive data or big data in a large number of streaming applications. Nevertheless, existing studies on queries primarily focus on the algorithms for the specific query types with the lack of the general framework for processing various queries. More- over, existing parallel frameworks in cloud such as MapReduce and its variations are not suitable for many complex queries over complex data streams. In this paper, we extensively discuss the problem of designing the general framework for parallel queries over data streams in cloud. Particularly, we propose and implement a framework called GPS, which can be well adapted to various queries over complex data streams like the uncertain data streams. Furthermore, we further propose a hierarchical and general parallel model for queries over data streams based on the proposed framework, which is more flexible than the MapReduce model. The skyline queries over uncertain data streams based on our proposed framework with real deployment are conducted as an example to verify the performances of our proposals.

Keywords?parallel query; parallel framework; data streams; cloud computing; skyline queries

I. INTRODUCTION  Many complex queries over massive data or big data have received considerable attention recently with the emergence of a large number of practical applications in domains like sensor network, RFID network, data cleaning, location-based service, network traffic analysis, market surveillance and traffic monitoring with radar. Moreover, the data in these scenar- ios are often generated dynamically and continuously, even with uncertainty [1], [2]. For example, in the application of hazardous weather monitoring with radar networks [3], a large amount of meteorological data are collected continuously and fed to the real-time processing system to predict natural disasters like tornados and severe storms [4]. In all these scenarios, it is highly desirable to conduct timely data analysis over the complex data streams.

In the complex streaming environments, the challenge of the query mainly lies in various aspects, such as the compu- tational complexity, the streaming complexity, and the query requirements. First, the computation of the query, such as prob- abilistic skyline query [5] and nearest-neighbor query [6], is CPU-intensive, which requires powerful processing capability.

Second, the fast arriving rate (e.g., 200Mb/sec from a single radar node [4]), as well as the limited storage space forces us to seek efficient algorithms over sliding windows. Third, the  strict limit of the response time and the large sliding window concerned by the users make the query more challenging.

Nevertheless, the existing centralized query techniques [7], [8] are hard to meet the query requirements, which spurs us to explore the parallel processing techniques for the complex queries.

Cloud computing as a new emerging distributed computing paradigm driven by economies of scale, provides abundant computing resources to users with minimal management effort in the data centers. Cloud is a platform or an infrastructure that enables execution of codes (services, applications), in a managed and elastic way. In computational view, cloud com- puting is actually a network of data centers and is described as a powerful, low-cost, and energy-efficient approach to future computing [9], which has the potential to yield fast response times and permit processing of high-throughput input streams.

There has been an increasing interest in parallelizing some specific queries (e.g., skyline queries) in various distributed and parallel environments recently [10], [11]. All the existing approaches share the idea of partitioning the whole dataset to subsets, processing locally the subsets in parallel, and finally merging the results. The correctness of the approaches stems from the additivity of the query operator, which means that Q(S1 ? ? ? ? ? Sn) = Q(Q(S1) ? ? ? ? ? Q(Sn)), where Q(Si) denotes the queries over Si.

However, all the existing studies on queries over data streams are all devoted to the research on a specific query type with the lack of a general framework for various queries.

Although many parallel frameworks based on the cloud com- puting environment are proposed, such as MapReduce [12], HOP [13], Twister [14], HaLoop [15] and Hadoop++ [16].

These frameworks are all not well adapted to the parallel queries over data streams, which mainly stems from the following facts: 1) The frameworks are mainly designed for the parallel processing for various tasks over static data based on the distributed file systems, but not for the queries over data streams without any file management systems; 2) The tasks in many complex queries cannot be partitioned into many individual and independent computational tasks, especially for the queries that do not satisfy the additivity [2], such as the uncertain skyline top-k queries; 3) Many complex queries need interactions of the parallel nodes or even need optimizations with the assistant nodes, but the MapReduce based frameworks can not well support the communication of the parallel nodes.

Motivated by these, we study the problem of parallel query  on Embedded and Ubiquitous Computing  DOI 10.1109/HPCC.and.EUC.2013.161   on Embedded and Ubiquitous Computing  DOI 10.1109/HPCC.and.EUC.2013.161     processing over data streams in cloud computing environments.

Specifically, we propose a General framework for Parallelizing Streaming queries called GPS. To summarize, we provide the following contributions in this paper:  ? For the first time, we propose a general framework GPS for parallel queries over data streams with the window-based data stream model;  ? We further propose a hierarchical and general parallel model for queries over data streams based on the GPS;  ? We conduct the experiments of the skyline queries over uncertain data streams based on our proposed parallel framework with real deployment as an exam- ple to verify the effectiveness and efficiency of our proposals.

The remainder of this paper is organized as follows: Section II presents the proposed framework GPS in detail, which includes the introduction of the GPS architecture, and discussion of the optimization from various aspects. Section III describes the hierarchical and general parallel model based on the GPS, which is exclusively designed for parallel queries over data streams. We extensively evaluate the performances of our proposals from many aspects in Section IV. Finally, we conclude the paper and outline our future work in Section V.



II. GENERAL PARALLEL FRAMEWORK  A. Architecture of GPS  To realize the parallel processing for the queries over data streams, we first propose a general parallel framework for the queries, and the overall structure is summarized in Fig. 1.

From Fig. 1, we can see that there are several layers and modules in the framework. The bottom is the funda- mental communication platform, which is mainly responsible for the communication of the nodes involved in the parallel processing. The platform can be implemented with various middlewares, and in our prototype we adopt Ice?, which is an object-oriented middleware. The upper layer is the module for the input and management of data streams, which mainly consists of three submodules, including the probing module, the preprocessing module, and the caching and storage module.

The probing module is mainly responsible for probing the streaming rate and throughput to optimize the later query processing. Preprocessing mainly includes the work of data cleaning, transformation and loading to adapt to the queries.

The cache and storage module is responsible for optimizing the access and storage of the streaming data.

Based on the underlying management of the streaming data, we can adopt various models for data stream analysis.

The most popular are the window-based models, which mainly contains the landmark model, sliding window model and snapshot model, in which the sliding window model is the most frequently used. Moreover, according to criteria of the physical and logical model, as well as updating time interval, the sliding window model can be divided into count-based sliding window and time-based sliding window, as well as tumbling window, sliding window and jumping window [17], respectively.

?Zeroc. Available: (http://www.zeroc.com/)  After defining the models for data stream analysis, we can develop the module for the parallel processing for vari- ous queries, which is the most important part in the whole framework. To realize the parallel processing, we propose a hierarchical and general parallel model, which will be intro- duced in detail in the next section. Furthermore, to improve the parallel query processing, several optimization modules are developed in the framework, including the indices, nodes organization, load balancing, and fault tolerance. The indices mainly include three aspects: 1) the index for streaming items maintained in the windows, 2) the index for the nodes involved in parallel computing, and 3) the index for the data structures that defined by users to improve the queries. Generally, we can improve the queries over data streams by optimizing the organization of the parallel nodes in two ways, i.e., considering the network structure with various roles of the nodes and the data correlation for parallel nodes.

In GPS we try to develop the load-balance strategies in three aspects: 1) adjusting the input of streaming data, 2) con- trolling the distribution of streaming data, and 3) optimizing the window partition according to the capabilities of the nodes.

The fault-tolerant module is exclusively designed to solve the problems resulted from the failures of the nodes or links. To explore the strategies we can mainly adopt the replication technology and coding techniques.

Finally, according to the specific definitions of various queries over data streams, we can develop different optimiza- tion strategies for the queries. Note that, the framework can support almost all kinds of queries over data streams for the users, including not only the query styles as ad hoc queries and the continuous queries, but also the specific queries like skyline queries, top-k queries, nearest neighbor queries, range queries, key-word queries, aggregate queries, join queries, and frequent items queries over data streams.

B. Considerations for Optimization  According to the introduction of the parallel framework addressed before, we can see that there are many problems needs to be further considered and solved, which are also the directions of our current and future work.

1) Size of Input Streaming Data Block: The characteristics of data streams such as real-time response and single-pass make a huge challenge to the queries over data streams.

Besides, data streams exhibit unpredictability as they may be sometimes slow, but sometimes very fast with high throughput.

Therefore, we need to analyze the blocks of the incoming items according to the real states of the streams, so as to process the streaming data timely and accurately. Thus, to avoid the overflow of memory or the incorrectness deriving from the overflow of the items, we need to take fully into account the size of input streaming data block and adaptively adjust the size according to the capability of the parallel system.

2) Load Balancing for All Nodes: Load balancing is one of the most important goals of the distributed systems, which aims to avoid the case that too heavy a load in some of the nodes while the other nodes are idle. Studying the load balancing strategies not only can avoid the performance degradation or a single point of failure, but also can improve the resource utilization. Thus, we need to consider the loads of the nodes     ?????????? ? ??? ??? ???????  ?????? ? ???? ? ? ?????????? ???? ?  ? ??? ? ? ?? ????? ? ?? ??? ??????? ????? ? ? ??????? ??? ??? ???????  Underlying Data Communication Platform  ???????? ????? ??? ????	?? ???? ??? ??????? ???	????? ???????? ????? ????? ?? ????	??? ???????? ????? ?????  ?????? ??? ??? ?????? ????	?  !? ????" ????? ? ?????? ?????  ??	? ? # ??$????? ???????	? ??? ??  ???????	? ??? ??  ?????????? ?? ??? ????? ?? ????????? ??  ??????? ????	?? ???? ??? ???????  ?"?? ? ????	??  ???%" ????	??  && ????	??  ?? ?? ????	??  '??%$??? ????	??  ???????? ????	??  (?  ????	??  ?%??? ????	?? ?? ? ???? ????	??  Organization of Network Structure  Organization of Data Correlation  ? ??) ?????????? ? ??) ??  ?????? ? ??? Index of  Parallel Nodes Index of  Data Structures  !???%*??? ?? ???????	??  ??? ?????????? ??? ??????????? ???  +????% ?????? ? ???????	??  ??????? ???????????	? ??	???????????  ? ?????????????? !????????????? ???  Organization of Parallel Nodes  Fig. 1: The architecture of GPS  when we assign tasks based on the window partitioning. Be- sides, we can dynamically adjust the loads in each node. Thus, exploring dynamic load balancing algorithms or load shedding techniques, is also a very important aspect for designing the general parallel query framework.

3) Integration of Indexing Structures: A large number of studies indicate that the index structure can effectively improve the query processing [8], [18]. We also believe that the index can improve the parallel queries over data streams.

First, starting from the global data or the nodes, develop the distributed or parallel index for the whole streaming items or the parallel nodes. Second, study the index techniques to optimize the processing in each node, so as to improve the performance of the whole system. Third, since we may adopt some special data structures to optimize the queries, if we can use some index structures to organize or index the data structures, the query can also be significantly improved.

4) Organization of Nodes and Data: The organization of the processing nodes not only refers to the architecture of the parallel query processing, but also relates to the query efficiency. Generally, the organization of nodes includes the organization of the network connection structure and the organization of the nodes according to the correlation of the streaming items. The former is the fundamental structure for parallel query processing, and the latter is an effective way to optimize the parallel query processing. Similarly, we can also organize the streaming items with a certain data mapping  strategies to disseminate the data to the processing nodes to improve the queries. Since the query needs fast response and all the data are maintained in the memory during the query processing, we believe that replication method is more suitable than others like data coding. Therefore, we choose replication method to achieve the fault-tolerance in GPS.

5) Fault-Tolerant Query Processing: Failures are normal rather than exceptional in the cloud computing environments.

For example, Google has reported 5 average worker deaths per Map-Reduce job [12], and at least one disk failure in every run of a 6-hour Map-Reduce job on a cluster of 4,000 machines [19]. Moreover, as the number of components in- creases, so does the probability of failure. Once failures are happened during the query process, the query results would be incorrect and the continuous query may be interrupted, which waste plenty of computing resources and seriously affects the query experience of the users. Therefore, we need to study the fault-tolerant strategies to solve the problems, which include the fault detection, task migration and rapid recovery for the queries. Thus, we need to recover the parallel queries with minimal cost, such as to make full use of the results that have already calculated and avoid the problem of double counting.

6) Evaluation Methods and Metrics: To evaluate the per- formance of a parallel framework for the queries over data streams, a variety of evaluation approaches or metrics must be considered in its design. The approaches or the metrics should have a certain versatility and fairness, as well as can     practically reflect the performance of parallel processing. The typical metrics for evaluation in GPS include: query processing rate, load-balance, scalability, fault-tolerant or robustness, the query complexity in time and space.



III. GENERAL PARALLEL MODEL  A. Structure of Parallel Model  In this section, we propose a hierarchical and general paral- lel model for queries over data streams, which corresponds to the module for parallel processing in our proposed framework.

The structure of the model is shown in Fig. 2.

1P  2P 3P  4P  nP  M Data stream DS e6, e5, e4, e3, e2, e1, e0  1'P  2'P 3'P  4'P 'nP  1-layer  N-layer  Initial Mapping Level  Intermediate Mapping Level  Final Returning Level  Intermediate Aggregation Level  Initial Aggregation Level  Q  Fig. 2: The hierarchical and general parallel processing model for queries over data streams  Generally, there are two ways to translate a centralized processing to parallel processing, i.e., data partitioning and task partitioning, which are fully considered in our parallel model.

First, we parallelize the query by partitioning the whole query tasks into many subtasks and assign each task to a layer, thus the query processing can be vertically parallelized. Second, we further parallelize the query by assigning the processing tasks on the whole streaming data in a layer to many processing nodes, thus all the nodes in a layer are involved in the parallel processing for the tasks on their own local streaming items.

As shown in Fig. 2, the whole general parallel processing model is organized as a hierarchical parallel processing struc- ture. The model includes several layers, and the number of layers is defined by the users. In each layer, we organize the nodes to parallel execute the tasks assigned in the layer. The nodes in the upper layers connect with the nodes in the lower  layers with some mapping rule, and the nodes in each layer can in parallel execute the tasks for the queries.

Generally, all the parallel queries over data streams based on the aforementioned parallel model mainly include the following five processing levels.

? Initial mapping level: we firstly map the active stream- ing data to the nodes in the first layer with a certain mapping rule defined by the users.

? Initial aggregation level: the nodes in this layer exe- cute the query processing in parallel in the first layer, then push the results to the next level.

? Intermediate mapping level: we map the results pro- duced by the upper level with a certain rule to initiate a new round of parallel processing in the layer.

? Intermediate aggregation level: similar to the initial aggregation level, the main task in this layer is to further execute the task that assigned in it.

? Final returning level: this layer is mainly responsible for collecting the final results from the calculation by the final processing layer.

Generally, in many queries over data streams, we may only need to consider for the processing with one layer, but in some complex queries, the query processing may refer to several layers, thus to do the parallel query processing with our proposed framework can significantly improve the queries. In particular, the nodes between two adjacent layers not necessarily with one by one mapping rule, and we only need to design the rules for the specific queries.

?   ?   ?  M  1P 2P nP3P  Q  Input data stream  Output results  Fig. 3: The parallel model with 1-layer  For simplicity, we abstract one layer to explain the general query processing with the proposed parallel model. Specifi- cally, as shown in Fig. 3, there exist three kinds of nodes in our parallel processing settings:  ? Monitor node (M ): the node is responsible for map- ping and delivering the arriving streaming items to the parallel computing nodes;  ? Peer node (Pi (1 ? i ? n)): the nodes are respon- sible for the computation for their maintained local streaming items, which can communicate with M ;     ? Query node (Q): the node is responsible for outputting the final updated query results from all the processing nodes to the users.

Note that, in each layer there actually exist two levels, i.e., a mapping level and an aggregation level. Although this structure is similar to the MapReduce, GPS is more flexible than MapReduce, as all the nodes can directly communicate with others in each layer. Besides, many assistant nodes can also be used in each layer to improve the query if necessary, thus the role of each node can be flexibly defined and not only the map or reduce node.

B. Parallel Model in Each Layer  In the aforementioned general model for parallel query pro- cessing over data streams, each layer is individually designed for different query tasks. In our design of each layer, two essential types of parallel processing models are proposed: centralized parallel model (CPM) and distributed parallel model (DPM).

1) Centralized Parallel Model: As shown in Fig. 4, M can directly communicate with the peer nodes, but the peer nodes need not communicate with each other. When the streaming data comes to node M , M forwards the data to the peer nodes with some user defined rule. Each peer node maintains two sliding windows W and Wi, where W denotes the global slid- ing window andWi is the local sliding window. Note that, each peer node is responsible for processing the queries for the data in the window Wi. In this model, the global sliding window W are divided into multiple sub-windows W1, W2, . . . , Wn, which are assigned to the nodes P1, P2, . . . , Pn, respectively.

Specifically, it satisfies that W =  ?n i=1Wi and Wi ?Wj =? (i ?= j), which means that there is no overlap between any  two sub-windows.

M  1P 2P nP?   ?   ?  e0, e1, e2, e0, e1, e2, e0, e1, e2,  Data stream DS e6, e5, e4, e3, e2, e1, e0  Fig. 4: Centralized parallel model  In this model, since each peer node maintains the global skyline window, each node can ensure the correctness of the querying results and need not communicate with each other.

However, since this model demands that each node maintains two sliding windows in memory and the size of the sliding window can be too large, this model may consume too much memory resources and decrease the processing efficiency when the memory space is not enough. From the analysis, we can see that this parallel model try to exchange the communicate overhead with the memory overhead, which is more suitable  for the applications that are more concerned about the network communication overhead.

Data stream DS e6, e5, e4, e3, e2, e1, e0  1P  2P 3P  4P  nP  M  e0, e5  e1, e6 e2, e7,  e3, e8,  e4, e9  Fig. 5: Distributed parallel model  2) Distributed Parallel Model: To further explore the par- allelism and reduce the memory overhead, we propose a distributed parallel model for the queries over data streams. As shown in Fig. 5, M can directly interact with the peer nodes, and all the peer nodes can also communicate with others. When the data arrives to M , M alternatively forwards the data to the peer nodes in the corresponding layer. Each peer node maintains a local sliding window Wi. Similar to CPM, it also satisfies that W =  ?n i=1Wi and Wi?Wj = ? (i ?= j) in DPM.

In DPM each peer node does not need to maintain any global sliding window, but only a local sliding window is maintained. Peer nodes can execute the parallel queries by communicating with others when the interactions for the queries are needed. Thus, DPM can efficiently realize the parallel query processing with less memory overhead.

Note that, in both CPM and DPM another important problem that we need to address is the load balance. Partic- ularly, in order to achieve the target, we try to assign the partitions of the global sliding window to the peer nodes with different sizes according to the capabilities of the peer nodes in logical. Actually, the capability may be determined by the processing capability, storage capability and network bandwidth. Since each node may run a variety of services in cloud, the remaining resources of each node may constantly change at different times. Thus, to measure the comprehensive capability of each peer node, we try to run the query over multiple sampled uncertain data sets with different scales to evaluate the capability of each node, in order to achieve the load balancing purpose.

As shown in Algorithm 1, we first sample some uncertain datasets with different sizes to test the processing capability of each peer node, which is measured by the processing rate (Line 4), and then get the the average rates by computing the average values for several test times (Line 5). Afterwards, we can further obtain the overall average processing rates with Line 7, and thus we assign the sizes of the windows according to the overall capability weights with Lines 8?9.

Algorithm 1: Load balance oriented window partitioning Input : The length of global sliding window |W |;  the number of peer nodes n; Output: The size of each sub-window |Wi| (1 ? i ? n) begin1  Randomly generate m data sets with different2 scales: S1, S2, ? ? ? , Sm; Each peer node tests the times (t1, t2, ? ? ? , tm) of3 processing the queries on these data sets; Each peer node obtains the processing rates with4 vi = |Si|/ti, (i = 1, 2, ? ? ? , m); Each peer node continuously samples and computes5 it with l times, and gets the average rates as vi = (vi1 + vi2 + ? ? ?+ vil)/l, (i = 1, 2, ? ? ? , m); foreach peer node i (1 ? i ? n) do6  Compute the overall average processing rate7 v?i = (v1 + v2 + ? ? ?+ vn)/n; Calculate the overall capability weight8 wi = (v  ? 1 + v  ? 2 + ? ? ?+ v?n)/n;  Obtain the corresponding size of the9 sub-window as |Wi| = wi ? |W |;  end10  C. Example of Uncertain Skyline Queries  In this subsection, we will provide an example of skyline queries over uncertain streams to explain the general parallel model for the parallel query processing.

Assume that we model the data streams with count-based sliding windows, and the tuples are processed in a first-in-first- out (FIFO) manner. Furthermore, suppose the data stream is denoted byDS and the set of most recent N elements inDS is represented by DSN , which is included in the sliding window W . First, we provide some definitions related to the skyline queries over uncertain streams.

Definition 1: (dominance) For any two objects u and v from the d-dimensional space, u dominates v, denoted by u ? v, iff u.i ? v.i for all dimensions 1 ? i ? d and there at least exists a dimension j satisfies u.j < v.j.

Assume that SKY (W ) denotes the set of elements in W that form the skyline of W . The probability that an streaming item e appears in the skylines of the possible worlds is Psky(e) =  ? e?SKY (W ),W?? P (W ) [18]. Thus, the skyline  probability of e against the data stream DSN can be defined as follows [5], [8].

Definition 2: (skyline probability) Suppose P (e) denotes the existence probability of an item e, then the skyline prob- ability of e over DSN is defined by  Psky(e) = P (e)? ?  a?DSN ,a?e (1? P (a)) (1)  Based on the above definition, we can define the q-skylines in DSN that is represented by SKYN,q as follows:  Definition 3: (q-skyline in DSN ) Given a probability threshold q ? (0, 1], q-skyline in DSN is defined by a subset of DSN , where the skyline probability of each streaming item  in the subset is not less than q. That is, for each element e in SKYN,q , we have Psky(e) ? q.

Problem Definition: Given an uncertain data stream DSN  containing N active items e1, e2, . . . , eN in a sliding window DSN , where N is the window size, query the q-skyline items continuously in DSN .

Take queries with DPM for example, M first transmits e0 to P1, and then transmits e1 to P2, followed by e2 to P3, and so forth. Specifically, if the window of one peer node Pi is full and it receives a new item enew, then it will transmit the new item and the expired item eold to all the other peer nodes Pj (1 ? j ? n, j ?= i). After receiving the items from Pi, Pj computes the dominating probability P jd (enew) of enew against the local window Wj with P jd (enew) =  ? ei?Wj ,ei?enew(1 ? P (ei)), and then  sends the value to Pi. Meanwhile, Pj updates the skyline probabilities for all its maintained items in Wj and delivers the updated skylines to node Q. Thus, when Pi receives the P jd (enew) (1 ? j ? n, j ?= i) values from all the other peer nodes, it can compute the global skyline probability of enew with Psky(enew) = P (enew)?  ?n j=1 P  j d (enew).



IV. EMPIRICAL EVALUATION  In order to verify the effectiveness and efficiency of the proposals, we have implemented the prototype of the general framework and taken uncertain skyline queries for an example to test the performance of the queries with it.

We conduct the experiments with real deployment on a small-sized data center, which includes 48 hosts within 3 clusters. All the hosts are homogeneous, each of which is configured with a dual-core Intel 2.6GHz Xeon CPU, 2G main memory, a 1TB hard disk and gigabit ethernet. In addition, each host is deployed with two virtual machines, each of which corresponds to a peer node in the parallel model. The framework is implemented with Java running on the CentOS operating system. We run each experiment five times and get the average value.

Moreover, we use synthetic data [20] for the experiments, and use Normal distribution to randomly assign an existence probability to each streaming tuple with mean value ? = 0.7 and standard deviation ? = 0.3. Specifically, assume that the layer of the framework is 1 and adopt the CPM and DPM to process the query. Meanwhile, suppose that the default size of the global sliding window is 2 ? 106, and the default dimensionality of the streaming tuple is 4.

A. Performance of Load Balancing  To analyze the load balance of the proposed algorithm, we use 10 peer nodes to carry out the skyline computation and differ the capabilities of the nodes by allocating different re- sources to each virtual machine. According to the Algorithm 1, we assign the size of each local sliding window based on the capability of the peer nodes, which is measured by the overall processing rate for the queries on some sampled data sets.

In this experiment, we have sampled three Independent datasets with sizes of 1M, 10M and 100M, and then we can draw the overall processing rate to get the overall capability     TABLE I: Load Balance Evaluation Peer node ID 1 2 3 4 5 6 7 8 9 10  Capability weight 0.022 0.041 0.062 0.083 0.096 0.105 0.115 0.134 0.163 0.179 Local window size (M) 0.11 0.205 0.31 0.415 0.48 0.525 0.575 0.67 0.815 0.895 Time per update (DPM) 11.11 11.91 11.42 12.03 11.57 11.78 12.34 11.45 11.09 11.89 Time per update (CPM) 17.32 18.03 17.89 17.37 17.44 18.39 18.52 17.61 17.25 17.99  weight, thus we can get the size of the corresponding local sliding window. The Independent streaming data with dimen- sionality d = 4, size of global sliding window |W | = 5M , update granularitym = 102, and number of peer nodes n = 10 are used in this experiment.

Table I gives the exclusive cpu time per update for updating the skyline computation of the 10 peer nodes. As can be seen from the table, the proposed algorithm can achieve good load balance among all the peer nodes in CPM and DPM. In Algorithm 1, the monitor node M divides the data fairly according to the capabilities of the peer nodes, which contributes to the ideal query load balancing.

B. Performance with Update Granularity  Since the streaming data is continuously arrived, and sometimes a batch of items may arrive at a time, thus we need to evaluate the performance with the number m of the arrivals at a time. To evaluate the effect of the numbers, in our experiments we vary m from 1 to 103. As shown in Fig. 6, we can see that the time per update for two models increases as m increases for both the synthetic and real data, due to the time-consuming for the query updating for the larger number of arrivals. Nevertheless, we also find that the time is not linearly increasing as the increase of m. This is probably a consequence of that, when m is small, the communication overhead increases due to the frequent data transmission, whereas when m is large, the vast number of dominating tests increase the computation overhead.

10-1      100 101 102 103  Ti m  e pe  r u pd  at e  (s )  Number of tuples arrived per update  DPM CPM  (a) Independent data  10-1      100 101 102 103  Ti m  e pe  r u pd  at e  (s )  Number of tuples arrived per update  DPM CPM  (b) Anti-correlated data  Fig. 6: Performance vs. Update granularity m  Furthermore, as shown in Fig. 6, we can find that the performance of DPM is better than CPM, which stems from that the computation of the skyline probabilities for the new arrivals is only occupied by a peer node in CPM, while in DPM the tasks are executed by all peer nodes. Thus, the computation is improved, especially in the cloud environment. To be more obviously evaluate the models, we choose 100 as the default value for the remaining tests.

Moreover, from Fig. 6 we can also see that the perfor- mances of CPM and DPM in two types of synthetic data are  close, this is probably the consequence of that we compute the dominating probabilities for new items and update the skyline probabilities based on the complete dominance tests in the models. Therefore, in the following experiments we only use Independent data to evaluate our proposals.

C. Performance with Peer Number  To evaluate the scalability of the models, we test the performances of the models with different number n of peer nodes. From Fig. 7(a), we can see that the time per update for continuous uncertain skyline queries in each model is reduced when the number of peer nodes increases from 1 to 16. The results confirm that the more peer nodes are adopted to parallelize the query, the more improvement will be, especially for the query over the large sliding window. Therefore, we believe that the proposed models are scalable.

1 2 4 8 16  Ti m  e pe  r u pd  at e  (s )  Number of peer nodes  DPM CPM  (a) Update time vs. n    1 2 4 8 16N um  be r o  f i te  m s  up da  te d  pe r s  ec on  d  Number of peer nodes  DPM CPM  (b) Update rate vs. n  Fig. 7: Performance vs. Peer number n  Moreover, from Fig. 7(b) we can see that the performance which is measured by the number of items updated per second is improved as the increase of the number of nodes.

Nevertheless, the growths are relatively slowly compared with the growth rates of peer numbers. The main reason is that the size of each local window decreases gradually with the increase node number, thus the computational overhead on each node decreases, whereas the communication overhead increases, which affects the performance of the models.

D. Performance with Size of Global Sliding Window  To evaluate the adaptivity of the framework for the size of the global sliding window |W |, we vary |W | from 0.5 ? 106 to 5? 106. Moreover, we assign the global window to n local windows with equal sizes |Wi| = |W |/n (1 ? i ? n) to achieve the goal of load balance as far as possible.

From the experiments, we find that framework can effec- tively process the queries even when the sliding window is large. As shown in Fig. 8, the overall time per update increases when we vary the size of global window from 0.1M to 5M .

This is nature as that the larger size of the global window     10-1     0.1M 0.5M 1M 2M 5M  Ti m  e pe  r u pd  at e  (s )  The size of global sliding window  DPM CPM  Fig. 8: Performance vs. Window size  is, the more recent stream items that need to be maintained will be, thus the computation overhead including the cost for updating the existing items and the cost for computing the skyline probabilities for new arrivals increases. Furthermore, as it is expected the performance of DPM is better than CPM, due to the more parallelism for computing the skyline probabilities of the new arrivals.



V. CONCLUSION  Nowadays, parallel query processing over data streams has received considerable attention in academic and commercial areas, but it poses an immense challenge to researchers. In this paper, we present a general framework GPS for parallel queries over data streams for distributed computing environments with high performance [21], [22], especially for the cloud computing environments. To the best of our knowledge, it is the first work to study the problem. Extensive experimental results demonstrate that GPS is effective and practical.

As future work, it is interesting and challenging to improve the framework in several aspects, such as the index techniques and interface standardization of the framework. Moreover, we will test the performance of the framework for other queries, such as uncertain top-k queries and the queries over the graph- ical streaming data. Additionally, we are currently studying the fault-tolerant techniques for parallel query processing over uncertain data streams based on the proposed framework GPS.

ACKNOWLEDGEMENT  This work was supported by the National Grand Fundamental Research 973 Program of China (Grant No.2011CB302601), the National Natural Science Foundation of China (Grant No.61379052), the National High Tech- nology Research and Development 863 Program of China (Grant No.2013AA01A213), the Natural Science Foundation for Distinguished Young Scholars of Hunan Province (Grant No.S2010J5050), Specialized Research Fund for the Doctoral Program of Higher Education (Grant No.20124307110015).

