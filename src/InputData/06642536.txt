Big Data Analysis Using Apache Hadoop

Abstract? The paradigm of processing huge datasets has been shifted from centralized architecture to distributed architecture.

As the enterprises faced issues of gathering large chunks of data they found that the data cannot be processed using any of the existing centralized architecture solutions. Apart from time constraints, the enterprises faced issues of efficiency, performance and elevated infrastructure cost with the data processing in the centralized environment.

With the help of distributed architecture these large organizations were able to overcome the problems of extracting relevant information from a huge data dump. One of the best open source tools used in the market to harness the distributed architecture in order to solve the data processing problems is Apache Hadoop.  Using Apache Hadoop?s various components such as data clusters, map-reduce algorithms and distributed processing, we will resolve various location-based complex data problems and provide the relevant information back into the system, thereby increasing the user experience.

Index Terms?Hadoop,Big data,Map Reduce, Data processing

I. INTRODUCTION  Amount of data generated every day is expanding in drastic manner.Big data is a popular term used to describe the data which is in zetta bytes. Government , companies many organisations try to acquire and store data about their citizens and customers in order to know them better and predict the customer behaviour . social networking websites generate new data every second and handling such a data is one of the major challenges companies are facing. Data which is stored in data warehouses is causing disruption because it is in a raw format ,proper analysis and processing is to be done  in order to produce usable information out of it.

New tools are being used to handle such a large amount of data in short time.ApacheHadoop is java based programming framework which is used for processing large data sets in distributed computer environment. Hadoop is used in system where multiple nodes are present which can process terabytes of data.hadoop uses its own file system HDFS which facilitates fast transfer of data which can sustain node failure and avoid system failure as whole.

Hadoop uses MapReduce algorithm which breaks down the big data into smaller chunks and performs the operations on it  Various technologies will come in hand-in-hand to accomplish this task such as Spring Hadoop Data Framework for the basic foundations and running of the Map-Reduce jobs, Apache Maven for distributed building of the code, REST Web services for the communication, and lastly Apache Hadoop for distributed processing of the huge dataset.

I.



II. RELATED WORK Hadoop framework is used by many big companies like  Google, yahoo, IBM , for applications such as search engine, advertising and information gathering and processing  A. Apache Hadoop goes realtime at Facebook  At Facebook, Hadoop has traditionally been used in conjunction with Hive for storage and analysis of large data sets. Most of this analysis occurs in offline batch jobs and the emphasis has been on maximizing throughput and efficiency. These workloads typically read and write large  700IEEE IRI 2013, August 14-16, 2013, San Francisco, California, USA    amounts of data from disk sequentially. As such, there has been less emphasis on making Hadoop performant for random access workloads by providing low latency access to HDFS. Instead, we have used a combination of large clusters of MySQL databases and caching tiers built using memcached[9]. In many cases, results from Hadoop are uploaded into MySQL or memcached for consumption by the web tier.

Recently, a new generation of applications has arisen at Facebook that require very high write throughput and cheap and elastic storage, while simultaneously requiring low latency and disk efficient sequential and random read performance MySQL storage engines are proven and have very good random read performance, but typically suffer from low random write throughput. It is difficult to scale up our MySQL clusters rapidly while maintaining good load balancing and high uptime. Administration of MySQL clusters requires a relatively high management overhead and they typically use more expensive hardware. Given our high confidence in the reliability and scalability of HDFS, we began to explore Hadoop and HBase for such applications.

B. Yelp : uses AWS and Hadoop  Yelp originally depended upon giant RAIDs to store their logs, along with a single local instance of Hadoop.

When Yelp made the move Amazon Elastic MapReduce, they replaced the RAIDs with Amazon Simple Storage Service (Amazon S3) and immediately transferred all Hadoop jobs to Amazon Elastic MapReduce.

Yelp uses Amazon S3 to store daily logs and photos, generating around 100GB of logs per day. The company also uses Amazon Elastic MapReduce to power approximately 20 separate batch scripts, most of those processing the logs. Features powered by Amazon Elastic MapReduce include:  1. People Who Viewed this Also Viewed 2. Review highlights 3. Auto complete as you type on search 4. Search spelling suggestions 5. Top searches 6. Ads Yelp uses MapReduce. MapReduce is about the  simplest way you can break down a big job into little pieces. Basically, mappers read lines of input, and spit out tuples of (key, value). Each key and all of its corresponding values are sent to a reducer.



III. THE PROPOSED SCHEMES As mentioned earlier we overcome the problem of analysis  of big data using Apache Hadoop . The processing is done in  four steps which includes creating a servers of required configuration using Amazon web services using their own cloud formation language and configuring it using chef . Data on the cluster is stored using MongoDB which stores data in the form of key:value pairs which is advantageous over relational database for managing large amount of data.

Various languages allows writing scripts for importing data from collections in Mongodb to the json file which then can be processed in Hadoop as per user?s requirement .

Hadoop jobs are written in spring framework this jobs implement MapReduce algorithm for data processing. Six jobs are implemented in STS for a data processing in a location based social networking application.

The record of the whole session has to me maintained in log file using aspect (AOP) programming in spring.

The output produced after data processing in the hadoop job, has to be exported back to the database . the old values to the database has to be updated immediately after processing , in order to prevent loss of valuable data.

The whole process is automated by using java scripts and tasks written in ant build tool  for executing executable JAR files.

A. Importing data from database to Hadoop Data is stored in mongo DB which is a NoSQL  database. Its stored in the form of key value pairs which makes it easier for latter updation and creating the index than using traditional relational database.   JSON (JavaScript Object Notation  ) is a data interchangeable format which is easy for humans to read and write and its easy for machines to parse and generate. It?s a text format which is completely language independent  but it is understandable for c, java programmer .

Using json files with Hadoop Is beneficial as it stores information in key value format. Map reduce is designed in such a way that it takes key value pairs as an input. Hence there is no additional cost of conversion .

Groovy scripts can be written to make a connection to remote database  and copy collection to the json file which then will passed to HDFS to perform required operations .groovy scripts are called and run in an application using groovy class loader .

File system of hadoop is different than the file system the OS .it uses single name node and multiple  data nodes to store data and perform jobs. hence json file is to be transferred the HDFS from local file system . java script is included in batch file to automate this process  B. Performing jobs in Hadoop Map/Reduce is a programming paradigm that was made popular by Google where a task is divided into small portions and distributed to a large number of nodes for processing (map), and the results are then summarized into the final answer (reduce).  Hadoop also uses Map/Reduce for data processing. Hence different functions for the processing are written in the form of Hadoop job. A Hadoop job consists of mapper and reducer functions. These jobs are written in spring     framework i.e. STS is used as the integrated development environment (IDE). Six Hadoop jobs are implemented in spring framework. These jobs are required to process data for a location based social networking application. The input to these jobs is a JSON file containing different information fields about various locations.

The jobs and their implementation are explained below: Hadoop jobs are written for calculating:  1.Average Rating:- Ratings for a location are added from different records and average is calculated. Higher rating specifies that a particular location is more popular.

2.Total Recommendations:- Total number of recommendations given by user for a particular location are calculated.

3.Total Votes:- Total number of votes given by user for a particular location are calculated.

4.Total Suggestions:- A user can suggest a location to other users who visit the web site. Total suggestions are calculated.

5.Unique Tags:- Unique tags corresponding to a location are calculated.

C. Exporting data back to the database The output produced after data processing in the  hadoop job, has to be exported back to the database. The old values in the database has to be updated immediately after processing, in order to prevent loss of valuable data.

Exporting of data is done in the following steps:  1.The output of the hadoop job is in a key value format. The key contains the location_id and value is the specific value pertaining to the data processed.

2.There are 6 different jobs, each job will have its own export program, that will copy the specific output to the mongo database. Consider the following example: Calculating the average rating for a particular location(say barbeque nation) is a hadoop job that we have implemented. Now members from all around the globe will post their ratings and comments regarding barbeque nation.

We need to give a final absolute rating to barbeque nation  for our users interest. This comes under the data processing part which will be implemented in the hadoop job.

The final output of the average ratings of all places is produced in a comma separated format output file. This file in turn is used in the Groovy script to update the records in the database.

The key value pairs are extracted from the output file and a query is generated to update the database. Hadoop and mongodb are connected using the hadoop-mongodb connector.

D.Logging and automation The whole session of such a large process has to be  maintained and logged in to a file for a record. Aspect(AOP) Programming allows us to put an aspect on a class(ie to intercept a class)  1. @Before - Run before the method execution 2. @After - Run after the method returned a result 3. @Around - Run around the method execution,  combine all three advices above.

Aspect can be implemented using following steps  1.To add Project Dependencies To enable AspectJ, you need aspectjrt.jar, aspectjweaver.jar and spring-aop.jar in pom.xml file 2.Spring Bean Normal bean, with few methods, later intercept it via AspectJ annotation.

3.Enable AspectJ In Spring configuration file, put ?<aop:aspectj- autoproxy />?, and define your Aspect (interceptor) and normal bean.

Aspect Types :  AspectJ @Before The logBefore() method will be executed before the execution of  a class method  AspectJ @After The logAfter() method will be executed after the execution of class method.

AspectJ @Around The logAround() method will be executed before the class method.

To overcome the shortage of time and to run such a large  process manually the whole process has to be automated.

Ant is a build tool where task can be written to automated various task.

1)Making excuted JAR file 2)Compliation of  java class 3) Clean Ant

IV. CONCLUSION The application performs the operation on big data like counting average ratings, total recommendations, unique tags etc,  in optimal time and producing an output with     minimum utilization of resources .  The data analysis and processing is used in a social networking application. Thus providing the necessary information to the application users with least effort.



V. ACKNOWLEDGMENT We would like to thank Prof. Jyoti Nandimath for guiding  us throughout the selection and implementation of the topic .we would also like to thank Mr.Divyansh Chaturvedi for his guidance and providing the resources for making this paper possible.



VI. REFERENCES [1.] ?Hadoop in Action? by Chuck Lam [2.]?Hadoop the definitive guide? by Tom White publisher: O?  REILLY [3.] ?Pro Hadoop- build scalable distributed applications in the  cloud? by Jason Venner [4.]  Michael G Noll tutorials Applied Research. Big Data.

Distributed Systems. website: http://www.michael-noll.com [5.]  Expert One-on-One J2EE Design and Development? by Rod  Johnson.

[6.]   MK Yong Spring Tutorials [7.]  ?Groovy in Action? by D-ierk Konig with Andrew Glover, Paul  KingguiIllaume Laforge, and Jon Skeet.

