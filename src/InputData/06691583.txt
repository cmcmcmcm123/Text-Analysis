Direct QR factorizations for tall-and-skinny matrices in MapReduce architectures

Abstract?The QR factorization and the SVD are two fundamental matrix decompositions with applications throughout scientific computing and data analysis. For matrices with many more rows than columns, so- called ?tall-and-skinny matrices,? there is a numerically stable, efficient, communication-avoiding algorithm for computing the QR factorization. It has been used in traditional high performance computing and grid com- puting environments. For MapReduce environments, existing methods to compute the QR decomposition use a numerically unstable approach that relies on in- directly computing the Q factor. In the best case, these methods require only two passes over the data. In this paper, we describe how to compute a stable tall-and- skinny QR factorization on a MapReduce architecture in only slightly more than 2 passes over the data. We can compute the SVD with only a small change and no difference in performance. We present a performance comparison between our new direct TSQR method, indirect TSQR methods that use the communication- avoiding TSQR algorithm, and a standard unstable implementation for MapReduce (Cholesky QR). We find that our new stable method is competitive with unstable methods for matrices with a modest number of columns. This holds both in a theoretical performance model as well as in an actual implementation.

Keywords-matrix factorization, QR, SVD, TSQR, MapReduce, Hadoop

I. Introduction The QR factorization of an m?n real-valued matrix A  is: A = QR  where Q is an m?n orthogonal matrix and R is an n?n upper triangular matrix. It is a fundamental subroutine in many advanced data analysis procedures including princi- pal components analysis, linear regression, and general lin- ear models. We call a matrix tall-and-skinny if it has many more rows than columns (m ? n). This case is common in big data applications with billions of data points with only a few hundred descriptors. In practice, this means that it is cheap to distribute O(n2) data to all processors and cheap to perform O(n3) floating point operations in serial. In this paper, we study algorithms to compute a  QR factorization of a tall-and-skinny matrix for nearly- terabyte sized matrices on MapReduce architectures [7].

Previous work by one of the authors gave a fast MapRe- duce method to compute only R [5]. (The details of these are described further in Sec. II.) In order to compute the matrix Q, an indirect formulation is used:  Q = AR?1.

For R to be invertible, A must be full-rank, and we assume A is full-rank throughout this paper. The indirect formu- lation is known to be numerically unstable. Numerically stable algorithms are important because they ensure that the algorithm behaves predictably regardless of the prop- erties of the input. This feature is vital for implementing algorithmic libraries that handle the diversity of data input in real-world applications. We refer readers to the text by Higham for more on the numerical properties of various algorithms [11].

The simple process of repeating the algorithm, which is called iterative refinement, can sometimes be used to pro- duce a Q factor with acceptable accuracy [14]. However, if a matrix is sufficiently ill-conditioned, iterative refinement will still result in a computed matrix Q that is not orthogonal, and hence, is not nearly the QR factorization from any matrix. We shall describe a numerically stable method (Sec. III) that computes Q and R directly in approximately the same time as performing the repetition of the indirect computation for some matrices.

In Sec. IV-A, we present a performance model for our algorithms on a MapReduce cluster, which allows us to compute lower bounds on running times. Real world performance is almost always within a factor of two of the lower bounds (Sec. IV-B).

A. MapReduce motivation The data in a MapReduce computation is defined by  a collection of key-value pairs. When we use MapReduce to analyze tall-and-skinny matrix data, a key represents the identity of a row and a value represents the elements in that row. Thus, the matrix is a collection of key-value pairs. We assume that each row has a distinct key for      simplicity; although we note that our methods also handle cases where each key represents a set of rows.

There are a growing number of MapReduce frameworks that implement the same computational engine: first, map applies a function to each key-value pair which outputs a transformed key-value pair; second, shuffle rearranges the data to ensure that all values with the same key are together; finally, reduce applies a function to all values with the same key. The most popular MapReduce imple- mentation ? Hadoop [19] ? stores all data and intermediate computations on disk. Thus, we do not expect numerical linear algebra algorithms for MapReduce to be faster than state-of-the-art in-memory MPI implementations running on clusters with high-performance interconnects. However, the MapReduce model offers several advantages that make the platform attractive for large-scale, large-data compu- tations (see also [20] for information on tradeoffs). First, many large datasets are already warehoused in MapRe- duce clusters. With the availability of algorithms, such as QR, on a MapReduce cluster, these data do not need to be transferred to another cluster for analysis. In fact, a simple corollary of our analysis is the performance of the algorithms is largely bounded by simply reading and writing the data in the MapReduce cluster, indicating that even using an MPI cluster for the computation would not greatly reduce running time. Second, MapReduce systems like Hadoop provide transparent fault-tolerance, which is a major benefit over standard MPI systems. Other MapReduce implementations, such as Phoenix++ [18], LEMOMR [9], and MRMPI [15], often store data in memory and may be a great deal faster; although, they usually lack the automatic fault tolerance. Third, the Hadoop computation engine handles all details of the distributed input-output routines, which greatly simplifies the resulting programs.

For the majority of our implementations, we use Hadoop streaming and the Python-based Dumbo MapReduce in- terface [2]. These programs are concise, straightforward, and easy-to-adapt to new applications. We have also investigated C++ and Java implementations, but these programs offered only mild speedups (around 2-fold), if any. The Python implementation uses about 70 lines of code, while the C++ implementation uses about 600 lines of code.

B. Success metrics  Our two success metrics are speed and stability. The differences in speed are examined in Sec. IV-B. To analyze the performance, we construct a performance model for the MapReduce cluster. After fitting two parameters to the performance of the cluster, it predicts the runtime to within a factor of two. We study stability in an expanded online version of this manuscript.1 These results show that only our new direct TSQR method produces a matrix Q that is numerically orthogonal.

1Available from http://arxiv.org/abs/1301.1071.



II. Indirect QR factorizations in MapReduce One of the first papers to explicitly discuss the QR  factorization on MapReduce architectures was written by Constantine and Gleich [5]; however many had studied methods for linear regression and principal components analysis in MapReduce [3]. These methods all bear a close resemblance to the Cholesky QR algorithm we describe next.

A. Cholesky QR The Cholesky factorization of an n ? n symmetric  positive definite real-valued matrix A is:  A = LLT  where L is an n?n lower triangular matrix. Note that, for any A that is full rank, AT A is symmetric positive definite.

The Cholesky factor L for the matrix AT A is exactly the matrix RT in the QR factorization as the following derivation shows. Let A = QR. Then  AT A = (QR)T QR = RT QT QR = RT R.

Since R is upper triangular and L is unique, RT R = LLT .

The method of computing R via the Cholesky decompo- sition of AT A matrix is called Cholesky QR.

Thus, the problem of finding R becomes the problem of computing AT A. This task is straightforward in MapRe- duce. In the map stage, each task collects rows ? recall that these are key-values pairs ? to form a local matrix Ai and then computes ATi Ai. These matrices are small, n?n, and are output by row. In fact, ATi Ai is symmetric, and there are ways to reduce the computation by utilizing this symmetry. We do not exploit them because disk access time dominates the computation; a more detailed performance discussion is in Sec. IV. In the reduce stage, each individual reduce function takes in multiple instances of each row of AT A from the mappers. These rows are summed to produce a row of AT A. Formally, this method computes:  AT A = #(map tasks)?  i=1 ATi Ai  where Ai is the input to each map-task.

Extending the AT A computation to Cholesky QR sim-  ply consists of gathering all rows of AT A on one processor and serially computing the Cholesky factorization AT A = LLT . The serial Cholesky factorization is fast since AT A is small, n ? n. The Cholesky QR MapReduce algorithm is illustrated in Fig. 1.

It is important to note the architecture limitation due to the number of columns, n. The number of keys emitted by each map task is exactly n: 0, 1, ... n? 1 (one for each row of ATi Ai), and the total number of unique keys passed to the reduction stage is n. Thus, the row sum reduction stage can use at most n tasks.

Alternatively, the reduce function can emit a key-value pair where the key represents the row and column index     A  A1 A1TA1  map emit  A2 A2TA2  map emit  A3 A3TA3  map emit  A4 A4TA4  map emit  shuffle  (A1TA1)1 (A2TA2)1 (A3TA3)1 (A4TA4)1  (ATA)1  (A1TA1)2 (A2TA2)2 (A3TA3)2 (A4TA4)2  (ATA)2  (A1TA1)3 (A2TA2)3 (A3TA3)3 (A4TA4)3  (ATA)3  (A1TA1)4 (A2TA2)4 (A3TA3)4 (A4TA4)4  (ATA)4  emit  emit  emit  emit  ATA RTR emit  reduce  reduce  reduce  reduce  Local ATA Row Sum Cholesky  Fig. 1. MapReduce Cholesky QR computation for a matrix A with 4 columns.

of a given entry of ATi Ai, and the value is the given entry.

This increases the number of unique keys to n2 (or, by taking symmetry into account, n(n ? 1)). It is also valid to use more general reduction trees where partial row sums are computed on all the processors, and a reduction to n processors accumulates the partial row sums. The cost of this more general tree is the startup time for another map and reduce iteration. Typically, the extra startup time outweighs the benefit of additional parallelism.

Each of these variations of Cholesky QR can be de- scribed by our performance model in Sec. IV-A. For exper- iments, we use a small cluster (where at most 40 reduce tasks are available), and these design choices have little effect on the running times. We use the implementation where the reduce function takes in rows of AT A as it is the simplest.

B. Indirect TSQR One of the problems with Cholesky QR is that the  matrix AT A has the square of the condition number of the matrix A. This suggests that finite precision computations with AT A will not always produce an accurate R matrix.

For this reason, Constantine and Gleich studied a succinct MapReduce implementation [5] of the communication- avoiding TSQR algorithm by Demmel et al. [8], where map and reduce tasks both compute local QR computations.

This method is known to be numerically stable [8] and was recently shown to have superior stability to many standard algorithms [13]. Constantine and Gleich?s initial implementation is only designed to compute R. We will refer to this method as ?Indirect TSQR?, because Q may be computed indirectly with Q = AR?1. In Sec. III, we extend this method to also compute Q in a stable manner.

We will now briefly review the Indirect TSQR algorithm and its implementation to facilitate the explanation of the more intricate direct version. Let A be a matrix that ? for simplicity of explanation ? has 8n rows and n columns,  which is partitioned across four map tasks as:  A =  ? ???  A1 A2 A3 A4  ? ??? .

Each map task computes a local QR factorization:  A =  ? ???  Q1 Q2  Q3 Q4  ? ???  ? ?? ? 8n?4n  ? ???  R1 R2 R3 R4  ? ???  ? ?? ? 4n?n  .

The matrix of stacked upper triangular matrices on the right is then passed to a reduce task and factored into Q?R?. At this point, we have the QR factorization of A in product form:  A =  =Q? ?? ?? ???  Q1 Q2  Q3 Q4  ? ???  ? ?? ? 8n?4n  Q????? 4n?n  =R????  R????? n?n  .

The above construction generalizes to the case that A is not partitioned evenly. If A is m ? n, in the first step, a QR decomposition is computed on the block of A that is streamed to a map task. The Indirect TSQR method ignores the intermediate Q factors and simply outputs the n ? n factors Ri in the intermediate stage and R? in the final stage. Fig. 2 illustrates each map and reduce output.

We do not need to gather all R factors onto a single task to compute R?. Any reduction tree computes R? correctly.

Constantine and Gleich found that using an additional MapReduce iteration to form a more parallel reduction tree could greatly accelerate the method. This finding differs from the Cholesky QR method, where additional iterations rarely helped. In the Sec. III, we show how to save the Q factors to reconstruct Q directly.

C. Computing AR?1  Given the matrix R, the simplest method for computing Q is computing the inverse of R and multiplying by A, that is, computing AR?1. Since R is n?n and upper-triangular, we can compute its inverse quickly. Fig. 3 illustrates how the matrix multiplication and iterative refinement step cleanly translate to MapReduce. This ?indirect? method of the inverse computation is not backwards stable (for example, see [17]). Thus, a step of iterative refinement may be used to get Q within desired accuracy. However, the indirect methods may still have large errors after iterative refinement if A is ill-conditioned enough. This further motivates the use of a direct method.

S(1)  A  A1  A2  A3  A3  R1 map  A2  emit R2 map  A3  emit R3 map  A4  emit R4 map  shuffle  S1  A2  reduce  S2 R2,2  reduce  R2,1 emit  emit  emit  shuffle  A2 S3 R2,3  reduce emit  Local TSQR  identity m ap  A2 S(2) R reduce emit  Local TSQR Local TSQR  Fig. 2. MapReduce TSQR computation with a 2-stage reduction tree. S(1) is the matrix consisting of the rows of the Ri factors stacked on top of each other, i = 1, 2, 3, 4. Similarly, S(2) is the matrix consisting of the rows of the R2,j factors stacked on top of each other, j = 1, 2, 3.

A  A1 R-1 map  R  Q1  A2 R-1 map  Q2  A3 R-1 map  Q3  A4 R-1 map  Q4  A  A  A  A  A  emit  emit  emit  emit  distribute  TS Q  R   Q  Q1 R1-1  map  R1  Q1  Q2 R1-1  map  Q2  Q3 R1-1  map  Q3  Q4 R1-1  map  Q4  Q  Q  Q  Q  Q  emit  emit  emit  emit  distribute  TS Q  Q  R distribute  Local MatMul Local MatMul  Iterative Refinement step  Fig. 3. Indirect MapReduce computation of Q with iterative refinement.

D. Pseudo-Iterative Refinement A variety of fast, randomized algorithms have recently  been developed for least squares problems [1], [12], [16]. A key idea from this work is that the R factor from the QR factorization of a small, random subset of the rows of A is a first-order approximation to the R factor of the entire matrix. For tall-and-skinny matrices, the number of rows sampled grows as approximately 100n log n.

The standard iterative refinement procedure will com- pute the R factor of two m ? n matrices: the original matrix A and the approximate Q factor AR?1. We take a sampling approach to compute the R factor of only one m ? n matrix and one smaller matrix. To approximate random sampling, we read a single block of the matrix from Hadoop Distributed File System (HDFS). Given a sample of rows, As, we compute its R factor, Rs. We then compute the approximate Q factor via Q1 = AR?1s . Next the R factor of Q1, R1, is computed. Finally, the refined Q factor is given by Q1R?11 . In the implementation, the  computations of Q1 and R1 are performed simultaneously to avoid writing the disposable factor Q1 to disk. The refined Q factor is computed by (AR?1s )R?11 , and the R factor is given by R1Rs. We call this method Pseudo- Iterative Refinement. The standard iterative refinement procedure is then the special case of Pseudo-Iterative Refinement where As = A.



III. Direct QR Factorizations in MapReduce  One of the textbook algorithms to compute a stable QR factorization is the Householder QR method [10]. This method always produces a matrix Q where ?QT Q? I?2 is on the order of machine error. We have implemented the algorithm in MapReduce and discuss it in the on- line version (see previous footnote). However, the House- holder method involves changing the entire matrix once for each column. Writing the updated matrix to disk is prohibitively expensive in MapReduce and our perfor- mance data showed that Householder QR is an order of magnitude slower. Thus, we begin our discussion with our new, stable algorithm, Direct TSQR.

A. Direct TSQR  We finally arrive at our proposed method. Here, we directly compute the QR decomposition of A in three steps using two map functions and one reduce function, as illustrated in Fig. 4. This avoids the iterative nature of the Householder methods but maintains the stability properties [8], [10], [13]. For an example, consider again a matrix A with 8n rows and n columns, which is partitioned across four map tasks for the first step:  A =  ? ???  A1 A2 A3 A4  ? ??? .

The first step uses only map tasks. Each task collects data as a local matrix, computes a single QR decomposition, and emits Q and R to separate files. The factorization of A then looks as follows, with Qi,1Ri the computed factorization on the i-th task:  A =  ? ???  Q1,1 Q2,1  Q3,1 Q4,1  ? ???  ? ?? ? 8n?4n  ? ???  R1 R2 R3 R4  ? ???  ? ?? ? 4n?n  .

The second step is a single reduce task. The input is the set of R factors from the first step. The R factors are collected as a matrix and a single QR decomposition is performed. The sections of Q corresponding to each R factor are emitted as values. In the following figure, R? is the final upper triangular factor in our QR decomposition     of A: ? ???  R1 R2 R3 R4  ? ???  ? ?? ? 4n?n  =  ? ???  Q1,2 Q2,2 Q3,2 Q4,2  ? ???  ? ?? ? 4n?n  R????? n?n  .

The third step also uses only map tasks. The input is the set of Q factors from the first step. The Q factors from the second step are small enough that we distribute the data in a file to all map tasks. The corresponding Q factors are multiplied together to emit the final Q:  Q???? 8n?n  =  ? ???  Q1,1 Q2,1  Q3,1 Q4,1  ? ???  ? ?? ? 8n?4n  ? ???  Q1,2 Q2,2 Q3,2 Q4,2  ? ???  ? ?? ? 4n?n  =  ? ???  Q1,1Q1,2 Q2,1Q2,2 Q3,1Q3,2 Q4,1Q4,2  ? ???  ? ?? ? 8n?n  A = QR?.

One implementation challenge is matching the Q and R factors to the tasks on which they are computed. In the first step, the key-value pairs emitted use a unique map task identifier (e.g., via the uuid package in Python) as the key and the Q or R factor as the value. The reduce task in the second step maintains an ordered list of the keys read. The k-th key in the list corresponds to rows (k ? 1)n + 1 to kn of the locally computed Q factor. The map tasks in the third step parse a data file containing the Q factors from the second step, and this redundant parsing allows us to skip the shuffle and reduce. Another implementation challenge is that the map tasks in the first step and the reduce task in the second step must emit the Q and R factors to separate files. For this functionality, we use the feathers extension of Dumbo.

The thin singular value decomposition (SVD) of an m? n real-valued matrix is A is:  A = U?V T  where U is an m ? n orthogonal matrix, ? is a diagonal matrix with decreasing, non-negative entries on the diago- nal, and V is an n?n orthogonal matrix. To compute the SVD of A, we modify the second step and add a fourth step. In the second step, we also compute R = U?V T .

Then A = (QU)?V T is the SVD of A. Since R is n ? n, computing its SVD is cheap. The fourth step computes QU . If Q is not needed, i.e., only the singular vectors are desired, then we can pass U to the third step and compute QU directly without writing Q to disk. In this case, the SVD uses the same number of passes over the data as the QR factorization. If only the singular values are needed, then only the first two steps of the algorithm are needed along with the SVD of R. However, in this case, it would be favorable to use the TSQR implementation from Sec. II-B to compute R only.

B. Extending Direct TSQR to a recursive algorithm A central limitation to the Direct TSQR method is the  necessity of gathering all R factors from the first step onto one reduce task in the second step. As the matrix becomes fatter, this serial bottleneck becomes limiting.

We can cope by recursively extending the method and repeating the computation on the output R from the first step. The algorithm is outlined in Alg. 1, and the performance benefits of the algorithms are empirically analyzed in Sec. IV-C.

Algorithm 1 Recursive extension of Direct TSQR function DirectTSQR(matrix A)  Q1, R1 = FirstStep(A) if R1 is too big then  Assign keys to rows of R1 Q2 = DirectTSQR(R1)  else Q2 = SecondStep(R1)  end if Q = ThirdStep(Q1, Q2) return Q  end function

IV. Performance Experiments We evaluate performance in two ways. First, we build  a performance model for our methods based on how much data is read and written by the MapReduce cluster.

Second, we evaluate the implementations on a 10-node, 40-core MapReduce cluster at Stanford?s Institute for Computational and Mathematical Engineering (ICME).

Each node has 6 2-TB disks, 24 GB of RAM, and a single Intel Core i7-960 3.2 GHz processor. They are connected via Gigabit ethernet. After fitting only two parameters ? the read and write bandwidth ? the performance model predicts the actual runtime within a factor of two.

Although the cluster is small, we emphasize that the algorithms scale with the number of map tasks launched, not the number of nodes. Therefore, these algorithms scale to larger clusters. This is covered in detail in our performance model, and the numbers of map tasks used by the algorithms are listed in Table III.

All matrices used in the experiments are synthetic. The matrix dimensions are chosen to reflect problems in model reduction [6] and fast robust linear regression [4].

We do not perform standard parallel scaling studies due to how the Hadoop framework integrates the com- putational engine with the distributed filesystem. This combination makes these measurements difficult without rebuilding the cluster for each experiment.

A. Performance model Since the QR decomposition algorithms have more out-  put data (Q and R factors) than input data (the matrix A), we choose a performance model that emphasizes read     A  A1 R1 map  emit Q11 emit  A2 R2 map  emit Q21  emit  A3 R3 map  emit Q31  emit  A4 R4 map  emit Q41  emit  First step  R1  R2  R3  R4  Q12  Q22  Q32  Q42  R  reduce  emit  emit  emit  emit  emit  Second step  Q11 Q12  emit Q1 map  shuffle  Q21 Q22  emit Q2 map  Q31 Q32  emit Q3 map  Q41 Q42  emit Q4 map  Q12 Q22  Q32 Q42 2 distribute  Third step  Fig. 4. Direct MapReduce computation of Q and R.

and write volume. Our performance model targets Hadoop so that we can more accurately evaluate the performance of the algorithms for a popular MapReduce framework.

Let Mj and Rj be the number of map and reduce tasks for step j, respectively. Let Mmax and Rmax be the maximum number of map and reduce tasks that can run concurrently on the cluster. Both Mmax and Rmax are fixed in the Hadoop configuration, and Mmax + Rmax is usually at least the total number of cores. Let kj be the number of distinct input keys passed to the reduce tasks for step j. We define the map parallelism for step j as pmj = min{Mmax,Mj} and the reduce parallelism for step j as prj = min{Rmax,Rj , kj}. Let Rmj , W mj be the amount of data read and written in the j-th map step, by all map tasks, respectively. We have analogous definitions for Rrj and W rj for the j-th reduce step. Finally, let ?r and ?w be the inverse read and write bandwidth, respectively.

After computing ?r and ?w, we can provide a lower bound for the algorithm by counting disk reads and writes. The lower bound for a job with N iterations is:  Tlb = N?  j=1  Rmj ?r + W mj ?w pmj  + Rrj ?r + W rj ?w  prj .

We use streaming benchmarks to estimate ?r and ?w for the 40-core ICME cluster, and the results are in Table I.

On this cluster, Mmax = Rmax = 40. Table II provides the number of reads and writes for our algorithms, and Tables III and IV provide the information for computing pmj and prj . The keys for the matrix row identifiers are 32-byte strings. The computed lower bounds for our algo- rithms are in Table V. In Sec. IV-B, we examine how close the implementations are to the lower bounds.

TABLE III Values of Mj and Rj needed to compute pmj and prj . M1,M3, and M5 are dependent on the matrix size. Other listed data are not. The values Mj and Rj for Cholesky and  Indirect TSQR are the same.

Matrix Cholesky, +PIR +IR Direct Dimensions Indir. TSQR  4.0B ? 4 M1 1200 3 1200 2000 2.5B ? 10 1680 7 1680 2640 600M ? 25 1200 3 1200 1600 500M ? 50 1920 3 1920 2560 150M ? 100 880 44 880 880  M2 Mmax 1 Mmax Mmax 4.0B ? 4 M3 1200 1200 1200 2000 2.5B ? 10 1680 1680 1680 2640 600M ? 25 1200 1200 1200 1600 500M ? 50 1920 1920 1920 2560 150M ? 100 880 880 880 880  M4 ? Mmax Mmax Mmax 4.0B ? 4 M5 ? 1200 1200 ? 2.5B ? 10 ? 1680 1680 ? 600M ? 25 ? 1200 1200 ? 500M ? 50 ? 1920 1920 ? 150M ? 100 ? 880 880 ?  R1 Rmax Rmax Rmax Rmax R2 1 1 1 1 R3 ? Rmax Rmax ? R4 ? 1 1 ?  B. Algorithmic comparison  Using one step of iterative refinement yields numerical errors that are acceptable in a vast majority of cases. In these cases, performance is our motivator for algorithm choice. Tables VI and VII show performance results of     TABLE I Streaming time to read from and write to disk. Performance is in inverse bandwidth, so larger ?r and ?w means slower  streaming. The streaming benchmarks are performed with Mmax map tasks.

Rows Cols. HDFS Size read+write read ?r/Mmax ?w/Mmax  (GB) (secs.) (secs.) (s/GB) (s/GB)  4,000,000,000 4 134.6 713 305 2.2660 3.0312 2,500,000,000 10 193.1 909 309 1.6002 3.1072 600,000,000 25 112.0 526 169 1.5089 3.1875 500,000,000 50 183.6 848 253 1.3780 3.2407 150,000,000 100 109.4 529 151 1.3803 3.4552  TABLE II Number of reads and writes at each step (in bytes). We assume a double is 8 bytes and K is the number of bytes for a row  key (K = 32 in our experiments). The amount of key data is separated from the amount of value data. For example, 8mn + Km is Km bytes in key data and 8mn bytes in value data. For iterative refinement, psamp is the probability of  sampling a row. psamp = 1 for standard iterative refinement.

Cholesky Cholesky Indirect Indirect Direct + I.R TSQR TSQR + I.R. TSQR  Rm1 8mn + Km psamp(8mn + Km) 8mn + Km psamp(8mn + Km) 8mn + Km W m1 8M1n2 + 8M1n 8M1n2 + 8M1n 8M1n2 + 8M1n 8M1n2 + 8M1n 8mn + 8M1n2 + Km + 64M1 Rr1 8M1n2 + 8M1n 8M1n2 + 8M1n 8M1n2 + 8M1n 8M1n2 + 8M1n 0 W r1 8n2 + 8n 8n2 + 8n 8R1n2 + 8R1n 8R1n2 + 8R1n 0 Rm2 8n2 + 8n 8n2 + 8n 8R1n2 + 8R1n 8R1n2 + 8R1n 8M1n2 + KM1 W m2 8n2 + 8n 8n2 + 8n 8R1n2 + 8R1n 8R1n2 + 8R1n 8M1n2 + KM1 Rr2 8n2 + 8n 8n2 + 8n 8R1n2 + 8R1n 8R1n2 + 8R1n 8M1n2 + KM1 W r2 8n2 + 8n 8n2 + 8n 8n2 + 8n 8n2 + 8n 8M1n2 + 32M1 + 8n2 + 8n Rm3 8mn + Km 8mn + Km 8mn + Km 8mn + Km 8mn + Km  + M3(8n2 + 8n) + M3(8n2 + 8n) + M3(8n2 + 8n) + M3(8n2 + 8n) + M3(8M1n2 + 64M1) W m3 8mn + Km 8M3n2 + 8M3n 8mn + Km 8M3n2 + 8M3n 8mn + Km Rr3 0 8M3n2 + 8M3n 0 8M3n2 + 8M3n 0 W r3 0 8n2 + 8n 0 8R3n2 + 8R3n 0 Rm4 ? 8n2 + 8n ? 8R3n2 + 8R3n ? W m4 ? 8n2 + 8n ? 8R3n2 + 8R3n ? Rr4 ? 8n2 + 8n ? 8R3n2 + 8R3n ? W r4 ? 8n2 + 8n ? 8n2 + 8n ? Rm5 ? 8mn + Km ? 8mn + Km ?  +2M5(8n2 + 8n) 2M5(8n2 + 8n) W m5 ? 8mn + Km ? 8mn + Km ? Rr5 ? 0 ? 0 ? W r5 ? 0 ? 0 ?  TABLE V Computed lower bounds for each algorithm. psamp is the sampling probability for Pseudo-iterative refinement.

Rows Cols. psamp Cholesky Indirect Cholesky Indirect Cholesky Indirect Direct TSQR +PIR TSQR+PIR +IR TSQR+IR TSQR  Tlb (secs.)  4,000,000,000 4 0.0025 1803 1803 1821 1821 2343 2343 2528 2,500,000,000 10 0.0042 1645 1645 1655 1655 2062 2062 2464 600,000,000 25 0.0025 804 804 812 812 1000 1000 1237 500,000,000 50 0.0016 1240 1240 1250 1250 1517 1517 2103 150,000,000 100 0.0500 723 723 735 735 884 884 1217  TABLE VI Times to compute QR on a variety of matrices with seven MapReduce algorithms; only the DirectTSQR method is  guaranteed to be numerically stable.

Rows Cols. HDFS Size Cholesky Indirect Cholesky Indirect Cholesky Indirect Direct  (GB) TSQR +PIR TSQR+PIR +IR TSQR+IR TSQR job time (secs.)  4,000,000,000 4 134.6 2931 3460 3276 3620 4365 4741 6128 2,500,000,000 10 193.1 2508 2509 2887 3354 3778 4034 4035 600,000,000 25 112.0 1098 1104 1275 1476 1645 2006 1910 500,000,000 50 183.6 1563 1618 1772 1960 2216 2655 3090 150,000,000 100 109.6 1023 1127 1146 1304 1400 1652 2076     TABLE IV Values of kj needed to compute pmj and p  r j .

Chol. Chol. Indir. TSQR Indir. TSQR Direct +PIR,+IR +PIR,+IR  k1 n n M1n M1n M1 k2 n n M2n M2n M1 k3 0 n 0 M3n 0 k4 ? n ? M4n ? k5 ? 0 ? 0 ?  TABLE VIII Fraction of time spent in each step of the Direct TSQR algorithm (fractions may not sum to 1 due to rounding).

Rows Cols. Step 1 Step 2 Step 3  4,000,000,000 4 0.72 0.02 0.26 2,500,000,000 10 0.61 0.04 0.34 600,000,000 25 0.56 0.06 0.38 500,000,000 50 0.55 0.07 0.39 150,000,000 100 0.47 0.15 0.38  Cholesky QR and the the Indirect and Direct TSQR methods for a variety of matrices.

In our experiments, we see that Indirect TSQR and Cholesky QR provide the fastest ways of computing the Q and R factors, albeit ?QT Q? I?2 may be large. For all matrices with greater than four columns, these two methods have similar running times. For such matrices, the majority of the running time is the AR?1 step, and this step is identical between the two methods. This is precisely because the write bandwidth is less than the read bandwidth.

For the matrices with 10, 25, and 50 columns, Direct TSQR is competitive with the indirect methods with iter- ative refinement, albeit slightly slower. The performance is the most similar for smaller number of columns (e.g., with ten columns). However, when the matrix becomes too skinny (e.g., with four columns), Cholesky QR with iterative refinement is a better choice. When the matrix becomes too fat (e.g., with 100 columns), the local gather in Step 2 becomes expensive. Table VIII shows the amount of time spent in each step of the Direct TSQR compu- tation. Indeed, Step 2 consumes a larger fraction of the running time as the number of columns increases.

Table IX shows how each algorithm performs compared to its lower bound from Table V. We see that Direct TSQR diverges from this bound when the number of columns is too small. To explain this difference, we note that Direct TSQR must gather all the keys and values in the first step before performing any computation. When the number of key-value pairs is large, e.g., with the 4,000,000,000 ? 4 matrix, then this step becomes limiting and this is not accounted for by our performance model. Thus, the model predicts the runtime of Cholesky QR and Indirect TSQR with iterative refinement more accurately than Direct TSQR. Although their lower bounds are greater, the empirical performance makes these algorithms more attractive as the number of columns increases. If guaran-  0 50 100 150 200     number of columns  ru nn  in g  tim e  (s )  150M rows      0 50 100 150 200 250      number of columns  ru nn  in g  tim e  (s )  100M rows      0 50 100 150 200 250 300     number of columns  ru nn  in g  tim e  (s )  50M rows      no recursion recursion  no recursion recursion  no recursion recursion  Fig. 5. Running time of Direct TSQR with and without recursion.

The recursive version takes only one recursive step.

teed stability is required, Direct TSQR is the best method, and the performance cost of stability is quite small for a modest number of columns.

C. Recursive Direct TSQR  In the preceding performance analysis, we used Di- rect TSQR without the recursive extension described in Sec. III-B. Fig. 5 shows the performance benefits for the recursive extension as the number of columns increases for matrices with 50, 100, and 150 million rows. In these experiments, a single recursive step is taken. For these matrices, the recursive version of the algorithm is faster once the number of columns is approximately 150.



V. Conclusion If numerical stability is required, the Direct TSQR  method discussed in this paper is the best choice of algorithm. It is guaranteed to produce a numerically or- thogonal matrix. It usually takes no more than twice the time of the fastest, but unstable method, and it is often competitive with conceptually simpler methods.

Our code for this paper is openly available, see: https://github.com/arbenson/mrtsqr  This software runs on any system with Hadoop streaming.

In the future we plan to investigate mixed MPI and  Hadoop code. The idea is that once all the local mappers     TABLE VII Floating point operations per second on a variety of matrices with four MapReduce algorithms.

Rows Cols. 2?rows?cols2 Cholesky Indirect Cholesky Indirect Cholesky Indirect Direct TSQR +PIR TSQR+PIR +IR TSQR+IR TSQR  2?rows?cols2/sec 4,000,000,000 4 1.280e+11 4.367e+07 3.140e+07 3.907e+07 3.536e+07 2.932e+07 2.700e+07 2.089e+07 2,500,000,000 10 5.000e+11 1.994e+08 1.993e+08 1.732e+08 1.491e+08 1.323e+08 1.239e+08 1.239e+08 600,000,000 25 7.500e+11 6.831e+08 6.793e+08 5.882e+08 5.081e+08 4.559e+08 3.739e+08 3.927e+08 500,000,000 50 2.500e+12 1.599e+09 1.545e+09 1.411e+09 1.276e+09 1.128e+09 9.416e+08 8.091e+08 150,000,000 100 3.000e+12 2.933e+09 2.662e+09 2.643e+09 2.338e+09 2.143e+09 1.836e+09 1.393e+09  TABLE IX Performance of algorithms as a multiple of the lower bounds from Table V.

Rows Cols. Cholesky Indirect Cholesky Indirect Cholesky Indirect Direct TSQR +PIR TSQR+PIR +IR TSQR+IR TSQR  multiple of Tlb 4,000,000,000 4 1.626 2.261 1.799 1.988 1.863 2.023 2.424 2,500,000,000 10 1.525 1.525 1.744 2.027 1.832 1.956 1.638 600,000,000 25 1.366 1.373 1.570 1.818 1.645 2.006 1.544 500,000,000 50 1.260 1.305 1.418 1.568 1.461 1.750 1.469 150,000,000 100 1.415 1.559 1.544 1.746 1.584 1.848 1.770  have run in the first step of the Direct TSQR method, the resulting Ri matrices constitute a much smaller input.

If we run a standard, in-memory MPI implementation to compute the QR factorization of this smaller matrix, then we could remove two iterations from the direct TSQR method. Also, we would remove much of the disk IO associated with saving the Qi matrices. These changes could reduce runtime by at most a factor of 4.

