The Application of a Top-Down Algorithm in  Neighboring Class Set Mining

Abstract?This paper focuses on character of present frequent neighboring class set mining algorithms which is suitable for mining short frequent neighboring class set, and introduces a top-down algorithm in frequent neighboring class set mining.

This algorithm is suitable for mining long frequent neighboring class set in large spatial data according to top-down strategy, and it creates digital database of neighboring class set via neighboring class bit sequence. The algorithm generates candidate frequent neighboring class set via top-down search strategy, namely, it gains k-neighboring class set as candidate frequent items by computing k-subset of (k+1)-non frequent neighboring class set.

The mining algorithm computes support of candidate frequent neighboring class set by digit logical operation. The algorithm improves mining efficiency through these two methods. The result of experiment indicates that the algorithm is faster and more efficient than present algorithms when mining long frequent neighboring class set in large spatial data.

Keywords- spatial data mining; neighboring class set; top-down strategy; bit sequence; long frquent itemsets

I.  INTRODUCTION As we all know, geographic Information Databases is an  important form of spatial database in spatial data mining, mining spatial association rules from Geographic Information Databases is one important part of spatial data mining and knowledge discovery (SDMKD), which is also known as spatial co-location pattern as in [1]. Spatial co-location pattern are some implicit rules expressing construct and association of spatial objects in Geographic Information Databases, and also expressing hierarchy and correlation of different subsets of spatial association or spatial data in Geographic Information Databases as in [2]. Nowadays, in research of spatial data mining, there are mainly three kinds approaches of mining spatial association rules as in [3], such as, layer covered based on clustering as in [3], mining approach based on spatial transaction as in [2, 4, 5, and 6] and mining approach based on non-spatial transaction as in [3]. The first two methods may be also used to mining frequent neighboring class set, The spatial association as in [4, 5, and 6] is quite single, because they only express spatial association among these objects which are all close to objective. However, neighboring class set expresses another spatial association among these objects which are close to each other. MFNCS as in [2] uses the similar method of Apriori to search frequent neighboring class set, which gains some right instance of (k+1)-neighboring class set only through  connecting right instance of k-neighboring class set according to down-top strategy, and so this algorithm is only suitable for mining short frequent neighboring class set according to character of Apriori. Hence, this paper introduces a top-down algorithm in frequent neighboring class sets mining, denoted by TDA, which is suitable for long frequent neighboring class set according to top-down strategy.



II. PRELIMINARY KNOWLEDGE According to these representations as in [2], every object in  spatial domain forms a spatial data set, which is expressed as this data structure, denoted by <Object Identify, Class Identify and Spatial Location>. Here, identify of different class in spatial data set is denoted by Class Identify, identify of different object instance in the same class is denoted by Object Identify, location coordinate of object is denoted by Spatial Location. We regard an object as an instance of corresponding class, and so spatial data set is made up of these instances of spatial Class Identify. Sets of Class Identify are thought as a class set, denoted by C = {C1 C2 ?, Cm}, means there are m different classes.

A. Definition and Property Definition 1 Neighboring Class Set, it is a subset of spatial  class set in spatial data set, which is expressed as {Ct1 Ct2 ? Ctk} (tk ? m), denoted by NCS.

Let I = {it1 it2 ? itk} be an instance of neighboring class set denoted by NCS={Ct1 Ct2 ? Ctk}, here, itj is an instance of Ctj (j 1, 2, ?, k).

Example, let {V, X, Y, Z} be a NCS, and I = {V4, X7, Y6, Z3} is an instance of NCS.

Definition 2 Neighboring Class Set Length, its value is equal to the sum of class set contained in neighboring class set.

If the length of NCS is equal to k, it is denoted by k-NCS.

Example, let {V, W, Y} be a NCS, and its length is 3.

Definition 3 Right Instance of Neighboring Class Set, let I={it1 it2 ? itk} be an instance of NCS, if ?  ip and iq (ip ,iq ? I), and distance (ip iq) ? d, and then we think I be an right instance of NCS. Here, d is the minimal distance used by deciding two spatial objects are close to each other, Euclidean distance is expressed as distance (ip iq).

This work was fully supported by science and technology research projects of Chongqing Education Commission (Project No. KJ091108), and it was also fully supported by science and technology research projects of Chongqing Three Gorges University (Project No. 10QN-22, 24 and 30).

___________________________________     Definition 4 Neighboring Class Set Support, it is equal to the sum of right instance of neighboring class set, which is denoted by support (NCS).

Definition 5 Frequent Neighboring Class Set, its support is not less than the minimal support given by user.

Property Let (k+1)-NCS is frequent neighboring class set, and k-NCS is also frequent neighboring class set. Here, k-NCS ? (k+1)-NCS.

B. Problem Description According to above definition, property and these  representations as in [2], we describe frequent neighboring class set mining as follows:  Input:  (1) Class set is denoted by C = {C1 C2 ?, Cm}, instance set is denoted by I = {i1 i2 ?, in}, each ik (ik ? I) is expressed as above defined data structure.

(2) Minimal distance is denoted by d.

(3) Minimal support is denoted by s.

Output:  Frequent neighboring class set.



III. THE TOP-DOWN ALGORITHM  IN FREQUENT NEIGHBORING CLASS SET MINING  A. Forming Digital NCS Database In order to form digital database of neighboring class set,  we need find corresponding NCS of every right instance, and turn NCS into a digit. The course of turning used by the algorithm is expressed as follows:  Step1, Firstly, we define the order of class as C = {C1, C2? Cj?Cm}, Here j is defined as bit sequence, and so each class existing in right instance has bit sequence denoted by BSj.

Step2, and then, we use an exponent to denote every class, if bit sequence of this class is denoted by BSj, and then this exponent is equal to 12 ?jBS .

Step3, we will gain a digit by computing this value, denoted  by? =  ? L  j  BS j   12 , here L is equal to Neighboring Class Set Length,  and namely, it is the sum of class existing in this neighboring class set.

According to this method, the algorithm will gain a digit by scanning every right instance and this digit also denote this corresponding NCS of right instance.

Now we save these digits by this data structure expressed as follows:  Structure neighboring class set {  Int Digit; // saving digit expressed as NCS of right instance  Int Count; // saving the sum of same digit expressed as NCS} NCS  Example, here class set is expressed as C = {V, W, X, Y, Z}, the first three right instances are express as I1 = {W2, X4, Y3, Z1}, I2 = {V1, W5, Y2}, I3 = {W3, X5, Y4, Z2}.

Now we use the previous introductive method to create digital NCS database, this course is expressed as follows:  NCS of I1 is expressed as {W, X, Y, Z}, and the bit sequence is expressed as {2, 3, 4, 5}, and then digit = 2(2-1) + 2(3-1) + 2(4-1) + 2(5-1) =30, NCS [0]. Digit =30, NCS [0].Count=1.

NCS of I2 is expressed as {V, W, Y}, and the bit sequence is expressed as {1, 2, 4}, and then digit = 2(1-1) + 2(2-1) + 2(4-1) =11, NCS [1]. Digit =11, NCS [1].Count=1.

NCS of I3 is expressed as {W, X, Y, Z}, and the bit sequence is expressed as {2, 3, 4, 5}, and then digit = 2(2-1) + 2(3-1) + 2(4-1) + 2(5-1) = 30, because NCS [0] has already saved this information, and only NCS[0].Count=NCS[0].Count +1=2, .......

B. The method of generating candidate frequent NCS The algorithm generates candidate frequent neighboring  class set according to top-down strategy. Namely, it uses digit logical operation to generate k-candidate frequent itemsets of digit form by (k+1)-digit non-frequent itemsets. The algorithm adopts search strategy which is similar to B_UDMA as in [7] and ITDASN as in [8]. The course of generating is expressed as follows:  Suppose here are three non-frequent neighboring class sets and two frequent neighboring class sets, their length are all equal to 4, and they may generate some candidate frequent neighboring class set, their length ought to be 3.

Non-frequent neighboring class set:  (1)NFNCS1=21-1+22-1+23-1+25-1=23;  (2)NFNCS2=21-1+22-1+23-1+24-1=15;  (3)NFNCS3=21-1+23-1+24-1+25-1=29;  Frequent neighboring class set:  (1)FNCS1=22-1+23-1+24-1+25-1=30;  (2)FNCS2=21-1+22-1+24-1+25-1=27;  Generating candidate frequent neighboring class set:  No. 1 NFNCS1=21-1+22-1+23-1+25-1=23, these subsets of non-frequent itemsets are expressed as follows:  C1 = 21-1+22-1+23-1=7.

C* = 21-1+22-1+25-1=19, which is deleted by FNCS2.

C2 = 21-1+23-1+25-1=21.

C* = 22-1+23-1+25-1=22, which is deleted by FNCS1.

No. 2 NFNCS2=21-1+22-1+23-1+24-1=15, these subsets of non-frequent itemsets are expressed as follows:  C* = 21-1+22-1+23-1=7. (Duplication)  C* = 21-1+22-1+24-1=11, which is deleted by FNCS2.

C3 = 21-1+23-1+24-1=13.

C* = 22-1+23-1+24-1=14, which is deleted by FNCS1.

No. 3 NFNCS3=21-1+23-1+24-1+25-1=29, these subsets of non-frequent itemsets are expressed as follows:  C* = 21-1+23-1+24-1=13. (Duplication)  C* = 21-1+23-1+25-1=21. (Duplication)  C* = 21-1+24-1+25-1=25, which is deleted by FNCS2.

C* = 23-1+24-1+25-1=28, which is deleted by FNCS1.

C. The method of computing support Accordingly to chapter A and digit logical operation, we  may gain this property as follows:  Property Let p and q express two right instances, let Cp be a NCS denoted by p, let Cq be a NCS denoted by q, then Cp ? Cq ? p & q=p.

This algorithm uses logical operation to compute support according to this property. The process is expressed as follows:  Suppose class set is expressed as C = {U, V, X, Y, Z}, there are 5 neighboring class sets as follows:  NCS1 = {U, X, Y, Z};  NCS2 = {V, X, Z};  NCS3= {U, V, X, Y, Z};  NCS4 = {V, X, Y, Z};  NCS5 = {X, Y, Z};  Their digits denoting neighboring class set of right instance are expressed as {29, 22, 31, 30 and 28}. Suppose a candidate is 20 which is denoted by C-NCS = {X, Z}, and then 29&20 =20, 22&20=20, 31&20=20, 30&20=20, 28&20=20. Because of this, we write 5 to support (C-NCS).

D. The process of mining frequent neighboring class set Input:  (1) Spatial class set is denoted by C = {C1 C2 ? Cm}.

(2) Instance set is denoted by I = {i1 i2 ? in}.

(3) The minimal distance is denoted by d.

(4) The minimal support is denoted by s.

Output: Frequent neighboring class set.

Step1: Firstly, the algorithm computes the entire right instances denoted by I? from instance set in spatial database as I by the minimal distance as d.

Step2: And then, forming neighboring class set database as NCS after scanning once right instance set as I? via the introductory method in chapter A of III.

Step3: Using top-down strategy to generate candidate frequent NCS according to this chapter B of III. Namely, if a k- candidate is frequent neighboring class set, and it is written to FNCS which saves frequent neighboring class set, otherwise,  the algorithm will compute (k-1)-subset of k-candidate, and then continue to search frequent neighboring class set.

Step4: Output FNCS according to chapter A of III.



IV. THE ANALYSIS AND COMPARING OF CAPABILITY At present, there are very little documents of research  frequent neighboring class set. Here we compare MFNCS as [2] with TDA which is introduced by this paper as follows:  A. The Analysis of Capability MFNCS: this algorithm uses idea of Apriori to find  frequent neighboring class set, which is made of three stages, firstly, computing all the frequent 1-NCS, secondly, generating all the 2-NCS by range query, and generating all the k-NCS (k>2) by iteration. The algorithm has some repeated computing and superfluous candidate frequent neighboring class set, which gains some right instance of (k+1)-neighboring class set only through connecting right instance of k-neighboring class set. As this algorithm adopts down-top strategy to generate candidate frequent neighboring class set, it is only suitable for mining short frequent neighboring class set.

TDA: this algorithm adopts top-down strategy to generate candidate frequent neighboring class set, which is made of three stages, firstly, computing the 1st m-candidate frequent neighboring class set which contains all classes, and then generating (m-1)-candidate frequent neighboring class set, let (m-1) be k, and generating all (k-1)-frequent neighboring class set (k>3) by iteration. But the algorithm is only suitable for mining long frequent neighboring class set for top-down strategy.

B. The comparing of experimental result Now we use experiment to testify above analyses and  comparison. Two mining algorithms are used to generate frequent neighboring class set from 12267 right instances, whose class sets are expressed as digit from 3 to 8191, neighboring class set does not include any simple class, namely, it has two classes at least, the number of spatial class set is denoted by m=13, the number of right instance included by these neighboring class set observe the discipline which is expressed as follows:  NCS of digit denoted by 8191 has one right instance.

NCS of digit denoted by 8190 has two right instances.

NCS of digit denoted by 8189 has one right instance.

NCS of digit denoted by 8188 has two right instances.

?  Our experimental circumstances are expressed as follow: Intel(R) Celeron(R) M CPU 420  1.60 GHz, 1.24G, language of the procedure is Visual C# 2005.NET, OS is Windows XP Professional.

The experimental result of two algorithms is expressed as Fig.1, where support is absolute. The runtime of two algorithms is expressed as Fig.2 as support and length of neighboring class set change.

Figure 1.  The experimental result        1000(3) 500(4) 200(5) 100(6) 50(7) 20(8) 10(9) 5(10)  Support(Length)  R un  tim e(  M ill  is ec  on d  MFNCS  TDA    Figure 2.  The comparing of runtime

V. CONCLUSION This paper introduces a top-down algorithm in neighboring  class sets mining, which is suitable for long frequent neighboring class set. The result of experiment indicates that the algorithm is faster and more efficient than present algorithms when mining long frequent neighboring class set in large spatial data.

