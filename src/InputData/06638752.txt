RANK MINIMIZATION FOR SUBSPACE TRACKING FROM INCOMPLETE DATA

ABSTRACT  Extracting latent low-dimensional structure from high-dimensional data is of paramount importance in timely inference tasks encoun? tered with 'Big Data' analytics. However, increasingly noisy, het? erogeneous, and incomplete datasets as well as the need for real-time processing pose major challenges towards achieving this goal. In this context, the fresh look advocated here permeates benefits from rank minimization to track low-dimensional subspaces from incomplete data. Leveraging the low-dimensionality of the subspace sought, a novel estimator is proposed based on an exponentially-weighted least-squares criterion regularized with the nuclear norm. After re? casting the non-separable nuclear norm into a form amenable to on? line optimization, a real-time algorithm is developed and its con? vergence established under simplifying technical assumptions. The novel subspace tracker can asymptotically offer the well-documented performance guarantees of the batch nuclear-norm regularized esti? mator. Simulated tests with real Internet data confirm the efficacy of the proposed algorithm in tracking the traffic subspace, and its superior performance relative to state-of-the-art alternatives.

Index Terms- Low rank, online algorithm, matrix completion.

1. INTRODUCTION  Nowadays ubiquitous e-commerce sites, the Web, and Internet-friendly portable devices generate massive volumes of data. The undeniable consensus is that tremendous economic growth and improvement in quality of life can be effected by harnessing the potential benefits of analyzing this large volume of data. As a result, the problem of extracting the most informative, yet low-dimensional structure from high-dimensional datasets is of paramount importance [6]. The sheer volume of data and the fact that observations are acquired se? quentially in time, motivate updating previously obtained 'analytics' rather than re-computing new ones from scratch each time a new datum becomes available. In addition, large-scale datasets are often incomplete, and prone to corrupt measurements as well as communi? cation errors. In this context, consider the following streaming data model with incomplete observations at time t  (1)  where Xt E IRP is the signal of interest, and v, accounts for the noise. The set Wt C [1,2, ... , P] contains the indices of available observations, and the corresponding sampling operator Pw, ( -) sets the entries of its vector argument not in w, to zero, and keeps the rest unchanged. Suppose that the signal sequence {X'}?l lives in a low-dimensional (<< P) linear subspace Lt. Given the incom? plete observations {P WT (y T ), WT } ?=1' this paper deals with online (adaptive) estimation of L" and reconstruction of the signal Xt as a byproduct.

Work was supported by an AFOSR MURI grant no. FA 9550-10-1-0567  Relation to prior work. Subspace tracking has a long history in signal processing. An early noteworthy representative is the projec? tion approximation subspace tracking (PAST) algorithm [14]. Re? cently, an algorithm (termed GROUSE) for tracking subspaces from incomplete observations was put forth in [1], based on incremental gradient descent iterations on the Grassmannian manifold of sub? spaces. PETRELS is a second-order recursive least-squares (RLS)? type algorithm, that extends the seminal PAST iterations to handle missing data [4]. As noted in [5], the performance of GROUSE is limited by the existence of barriers in the search path on the Grass? manian, which may lead to GROUSE iterations being trapped at 10- cal minima; see also [5]. Lack of regularization in PETRELS can lead to unstable behaviors, especially when the amount of missing data is large. Relative to all aforementioned works, the algorithmic framework of this paper permeates benefits from rank minimization to low-dimensional subspace tracking (Section 3), and offers theo? retical performance guarantees (Section 4).

Contributions. Leveraging the low dimensionality of the underly? ing subspace, a novel estimator is proposed based on an exponentially? weighted least-squares (LS) criterion regularized with the nuclear norm of the unknown signal of interest. Upon recasting the non? separable nuclear norm into a form amenable to online optimization, a real-time algorithm for susbspace tracking is developed and its con? vergence is established under simplifying technical assumptions. In? terestingly, under mild assumptions the proposed online algorithm attains the global optimum of the batch nuclear-norm regularized problem, whose quantifiable performance has well-appreciated mer? its [2, 3]. As a byproduct, the proposed online algorithm offers a viable approach to solving large-scale matrix completion problems.

Simulated tests with Internet traffic data corroborate the effective? ness of the proposed algorithm for traffic estimation, and its superior performance relative to state-of-the-art alternatives [1,4].

Notation: Operators (.) ', (9, AminO, and amax(-) will denote trans? position, Kronecker product, minimum eigenvalue, and maximum singular value, respectively; I . I is the magnitude of a scalar and II 112 the ?2-norm of a vector. For matrix X, IIXIIF denotes the Frobenius norm, and X :::: 0 means that X is positive semidefinite.

The n x n identity matrix will be represented by In, while On will stand for the n x 1 vector of all zeros, and Onxp := OnO?.

2. NUCLEAR?NORM REGULARIZATION  Collect the indices of available observations up to time t in the set Ot := U?=lW', and the actual observations in the matrix Po, (Y,) := [PW1 (Yl), . . .  ,Pw, (y,)] E IRPxt. Likewise, introduce matrix X, containing the signal of interest. Since x, lies in a low-dimensional subspace, X, is a low-rank matrix. A natural estimator leverag? ing the low rank property of X, attempts to fit the incomplete data Po, (Yt) to X, in the LS sense, as well as minimize the rank of X,. Unfortunately, albeit natural the rank criterion is in general NP-hard to optimize [12]. Typically, the nuclear norm IIX, II. :=     Lk ak (Xt) (ak is the k-th singular value) is adopted as a surrogate to rank(Xt). Accordingly, one solves [3]  A {I 2 } (Pl) Xt := argm?n '2 IIPrlt(Yt - X)IIF + AtIIXII.

where At is a (possibly time-varying) rank-controlling parameter.

Albeit convex, (Pl) is a non-smooth optimization problem (the nu? clear norm is not differentiable at the origin). In addition, scal? able algorithms to impute missing entries for streaming observations should effectively overcome the following challenges: (c I) the prob? lem size can easily become quite large, since the number of opti? mization variables is Pt; (c2) existing iterative solvers for (PI) typ? ically rely on costly SVD computations per iteration; see e.g., [2]; and (c3) different from the Frobenius-norm, (columnwise) nonsepa? rability of the nuclear-norm challenges online processing when new columns {PWt (Ytn arrive sequentially in time. In the following subsection, the 'Big Data' challenges (cl) and (c2) are dealt with to arrive at an efficient online algorithm in Section 3.

2.1. A separable low-rank regularization  To address (cl) and reduce the computational complexity and mem? ory storage requirements of the algorithm sought, it is henceforth assumed that the dimensionality of the underlying subspace Lt is bounded by a known quantity p. Accordingly, it is natural to require p :::: rank(Xt). As argued in Remark I, the smaller the value of p, the more efficient the algorithm becomes. Because rank(Xt) ::; p, (PI),s search space is effectively reduced and one can factorize the decision variable as X = LQ', where L and Q are P x p and t x p matrices, respectively. It is possible to interpret the columns of X (viewed as points in IRP) as belonging to the low-rank subspace, spanned by the columns of L. The rows of Q are thus the projections of the columns of X onto the subspace. Adopting this reparametriza? tion of X in (P 1) one arrives at a non convex problem where the num? ber of variables is reduced from Pt in (Pl), to p(P + t). The savings can be significant when p is small, and both P and t are large.

To address (c2) [along with (c3) as it will become clear in Sec? tion 3], consider the following alternative characterization of the nu? clear norm [12]  IIXII. := min -2  {IILII} + IIQII}}, s. t. X = LQ'. (2) {L.Q} The optimization (2) is over all possible bilinear factorizations of X, so that the number of columns p of L and Q is also a variable. Lever? aging (2), the following reformulation of (PI) provides an important first step towards obtaining an online algorithm:  (P2) min -2 1IPrlt(Yt - LQ')II} +  A t  {IILII} + IIQII}}? {L,Q}  As asserted in [8, Lemma I], adopting the separable Frobenius-norm regularization in (P2) comes with no loss of optimality relative to (PI), provided p :::: rank(Xt). By finding the global minimum of (P2) [which could have considerably less variables than (Pl)], one can recover the optimal solution of (Pl). However, since (P2) is nonconvex, it may have stationary points which need not be glob? ally optimum. Interestingly, the next proposition shows that under relatively mild assumptions on rank(Xt) and the noise variance, sta? tionary points of (P2) qualify as global optimum solutions of (Pl).

Proposition 1. f8} Let {Lt, Q'} be a stationary point of(P2J. If amax[Prlt (Yt - LtQ;)] ::; At, then Xt := LtQ; is the globally optimal solution of (P 1).

3. ONLINE MATRIX IMPUTATION  As stated in Section 1, the goal is to recursively estimate Xt at time t from historical observations {PWr (y r), Wr }?=1' naturally placing more importance on recent measurements. To this end, one possible adaptive counterpart to (P2) is the exponentially-weighted LS esti? mator found by minimizing the empirical cost (Q := [q1, ... , qt])  where.\t := At/ L!=l fJt-T, and 0 < fJ ::; 1 is the so-termed for? getting factor. When fJ < 1, data in the distant past are exponentially downweighted, which facilitates tracking in nonstationary environ? ments. In the case of infinite memory (fJ = 1), the formulation (3) coincides with the batch estimator (P2). This is the reason for the time-varying factor.\t weighting IlL II}?  3.1. Subspace tracking from incomplete data  Towards deriving a real-time, computationally efficient, and recur? sive solver of (3), an alternating-minimization (AM) method is adopted in which iterations coincide with the time-scale t of data acquisition.

A justification in terms of minimizing a suitable approximate cost function is discussed in detail in Section 4.1. Per time instant t, a new datum {PWt (Yt), wt} is drawn and qt is estimated via  q[t] = arg mm - IIPWt (Yt - L[t - l]q) 112 + -llql12 .

. [ 1 2 At 2]  q 2 2 (4)  Updating (4) entails an ?2-norm regularized LS (ridge-regression) problem, that admits closed-form solution  q[t] = (AtIp + L'[t - l]ntL[t - 1]) -1 L'[t - I]PWt (Yt) (5)  where diagonal nt E IRPx P is such that [nt]p,p = 1 if p E Wt, and is zero elsewhere. In the second step of the AM scheme, the updated subspace matrix L[t] is obtained by minimizing (3) with respect to L, while the optimization variables {qT }?=1 are fixed and take the values {q[T]}?=l' namely  L[t]=arg m?n [ ?t IILII}+ ??t-T ? IIPWT (YT-Lq[T]) II?] . (6) Notice that (6) decouples over the rows of L which are obtained in parallel via  for p = 1, ... , P, where Wp,T denotes the p-th diagonal entry of nT? For fJ = 1 and fixed At = A, Vt, subproblems (7) can be effi? ciently solved using the RLS algorithm [13]. Upon defining sp[t] := L!=l fJt-T Wp,T(YP,T )q[T], Hp[t] := L?=l fJt-T Wp,Tq[T]q'[T] + AtIp, and Mp[t] := H;l[tJ, one simply updates  sp[t] = sp[t - 1] + Wp,tYP,tq[t]  M [t] = M [t - 1] _ W Mp[t - l]q[t]q' [t]Mp [t - 1] p P p,t 1 + q' [t]Mp [t - l]q[t] and forms Ip[t] = Mp [t]sp [tJ, for p = 1, ... , P.

However, for 0 < fJ < 1 the regularization term (At/2 )IIIII? in (7) makes it impossible to express Hp [t] in terms of Hp [t - 1] plus a     Algorithm 1 : Subspace tracking from incomplete observations input {PWT (YT)' WT };"'=1, {AT };"'=1, and (3.

initialize Gp[O] = Opxp, sp[O] = Op, P = 1, ... , P, and L[O] at random.

fort = 1, 2, ... do  D[t] = (AtIp + L/[t - l]OtL[t - 1])-1 L/[t - 1].

q[t] = D[t]Pwt (Yt).

Gp[t] = (3Gp[t - 1] + Wp,tq[t]q[tJ', p = 1, ... , P.

sp[t] = (3sp[t - 1] + w1'.iYP,tq[t], p = 1, ... , P. lp[t] = (Gp[t] + AtIp) sp[t], p = 1, ... , P.

return Xt := L[t]q[t].

end for  rank-one correction. Hence, one cannot resort to the matrix inversion lemma and update Mp[t] with quadratic complexity only. Based on direct inversion of Hp [t], p = 1, ... , P, the overall recursive algorithm for subspace tracking from incomplete data is tabulated under Algorithm 1.

Remark 1. [Computational cost] Careful inspection of Algorithm 1 reveals that the main computational burden stems from p x p inver? sions to update the subspace matrix L[t]. The per iteration complex? ity for performing the inversions is O(pp3) (which could be further reduced if one leverages also the symmetry of Gp ltD, while the cost of multiplication as well as additions is O(pp2). The overall cost of the algorithm per iteration can be safely estimated as O(P p3), which is affordable since p is typically small (cf. the low rank assumption).

Remark 2. [Tuning Ad In practice, to tune At one can resort to the heuristic rules proposed in [3], which build upon the following reasonable assumptions: i) Vp,t ? N(O, a2), ii) elements of Ot are independently sampled with probability Jr, and iii) P and t are large enough. Accordingly, one can pick At = (VP + 00) vna which naturally increases as time evolves, and where te := L?=l (3t-T is the effective time window.

4. PERFORMANCE GUARANTEES  This section studies the performance of Algorithm 1 for the infinite memory special case i.e., when (3 = 1. In the sequel, to make the analysis tractable the following assumptions are made:  AI) {W,}?l and {PWt (Yt)}?l are independent and identically distributed (i.i.d.) random processes;  A2) {PWt(Yt)}?l is uniformly bounded; and A3) Iterates {L[t]}?l are in a compact set.

To clearly delineate the scope of the analysis, it is worth com? menting on the assumptions AI)-A3) and the factors that influence their satisfaction. Regarding AI), the acquired data is assumed sta? tistically independent across time as it is customary when studying the stability and performance of online (adaptive) algorithms [13].

While independence is required for tractability, AI) may be grossly violated because the observations {PWt (Yt)} are correlated across time (cf. the fact that {x,} lies in a low-dimensional subspace).

Still, in accordance with the adaptive filtering folklore e.g., [13], as (3 -+ 1 the upshot of the analysis based on i.i.d. data extends ac? curately to the pragmatic setting whereby the observations are cor? related. Uniform boundedness of PWt (Yt) [cf. A2)] is natural in practice as it imposed by the data acquisition process. The bounded  subspace requirement in A3) is a technical assumption that simpli? fies the analysis, and has been corroborated via extensive computer simulations [9].

4.1. Convergence  The convergence of the iterates generated by Algorithm 1 is estab? lished first. Upon defining the function  1 2 At 2 9t(L, q) := '2 IIPWt (Yt - Lq)112 + 211ql12  in addition to et(L) := minq 9t(L, q), Algorithm 1 aims at mini? mizing the following average cost function at time t  (8)  Normalization (by t) ensures that the cost function does not grow un? bounded as time evolves. For any finite t, (8) is essentially identical to the batch estimator in (P2) up to a scaling, which does not affect the value of the minimizer. Note that as time evolves, minimiza? tion of Ct becomes increasingly complex computationally. Hence, at time t the subspace estimate L[t] is obtained by minimizing the approximate cost function  in which q[t] is obtained based on the prior subspace estimate L[t - 1] after solving [ef. (4)] q[t] = argminq9t(L[t - l], q). Obtain? ing q[t] this way resembles the projection approximation adopted in [14]. Since Ct(L) is a smooth convex function, the minimizer L[t] = arg minL Ct(L) is the solution of the quadratic equation \7Ct(L[t]) = Opxp.

So far, it is apparent that the approximate cost function Ct (L [t]) overestimates the target cost Ct(L[t]), for t = 1, 2, .... However, it is not clear whether the subspace iterates {L[t]}?l converge, and most importantly, how well can they optimize the target cost func? tion Ct. The good news is that Ct(L[t]) asymptotically approaches Ct(L[t]), and the subspace iterates null \7Ct(L[t]) as well, both as t -+ 00. The latter result is summarized in the next proposition, whose proof is inspired by [11] and can be found in [9].

Proposition 2. Suppose At = A 'tit, and Amin[\72Ct(L)] 2': cfor some c > O. Then limt-texo \7Ct(L[t]) = Opxp almost surely (a.s.), i.e., the subspace iterates {L[t]}?l asymptotically coincide with the stationary points of the batch problem (P2).

The sampling set Ot plays a key role towards satisfying the Hes? sian's positive semi-definiteness condition in Proposition 2. Intu? itively, if the missing entries are somehow uniformly spread across time, the likelihood that \72Ct(L) = fIpp+ t L?=l (q[T]q/[T])@ OT t cIpp holds is higher.

4,2, Optimality  In line with Proposition 1, one may be prompted to ponder whether the online estimator offers the performance guarantees of the nuclear? norm regularized estimator (PI), for which stable/exact recovery results are well documented e.g., in [2, 3]. Specifically, given the learned subspace L[t] and the corresponding Q[t] [obtained via (4)]     over a time window of size t, is {X[t] L[tlQ'[t]} an optimal solution of (PI) when t ---+ oo? This in turn requires asymptotic analysis of the optimality conditions for (PI), and is established in the next proposition. Additionally, numerical tests in Section 5 cor? roborate that the online estimator attains the performance of (PI) after reasonable number of iterations.

Proposition 3. For the iterates generated by Algorithm 1, if there ex?  ists a subsequence {L[tk], Q[tk]} for which c1) limk-+= 'VCtk (L[tk]) = Opxp a.s., and c2) kamax[Prltk (Ytk - L[tklQ'[tk])l ::; )f,; hold, then the sequence {X[k] = L[tklQ'[tk]} satisfies the optimal? ity conditions for (P 1) [normalized by tkl as k ---+ 00 a.s.

Regarding the convergence condition cl ), even though it holds for a time invariant rank-controlling parameter A as per Proposi? tion 2, numerical tests indicate that it also holds for the time-varying case (e.g., when At is chosen as suggested in Remark 2). Accord? ing to A2) and A3), amax[Prlt (Yt - L[t]Q'[t])] ? O( Vi), which implies that the quantity on the left-hand side of c2) cannot grow un? bounded. Moreover, upon choosing At ? O( Vi) as per Remark 2 the term in the right-hand side of c2) will not vanish, which suggests that the qualification condition can indeed be satisfied.

5. NUMERICAL TESTS  The convergence and effectiveness of Algorithm I is assessed in this section via computer simulations.

5,1. Synthetic data tests  The signal Xt = UWt is generated from the low-dimensional sub? space U E lRpxr, with i.i.d. entries Up,i ? N(O, liP), and pro? jection coefficients Wi.t ? N(O, 1). The noise Vi.t ? N(O, a2) is i.i.d., and the entries of Yt are sampled uniformly at random with probability 1l' to form the diagonal sampling matrix nt. The obser? vations at time t are then generated as PWt (Yt) = nt(Xt + Vt). Fix r = 5 and p = 10, while different values of 1l' and a are examined.

The evolution of the average cost Ct(L[t]) in (8) for different per? centages of missing data and noise variances is depicted in Fig. l (a).

For validation purposes, the optimal cost [normalized by the window size t] of the batch estimator (PI) is also shown. It is apparent that Ct (L[t]) converges to its batch counterpart (PI), which corroborates that Algorithm I can attain the performance of (PI). This obser? vation together with the low-complexity of Algorithm l's iterations [ef. Remark 1], make it a viable alternative for solving large-scale matrix completion problems.

Next, Algorithm 1 is compared with two state-of-the-arte sub? space trackers, namely PETRELS [4] and GROUSE [I]. These two algorithms require and estimate of the dimensionality of the under? lying subspace, which is denoted by K. Set A = 0.1, f3 = 0.99, and consider an abrupt change in the subspace at t = 104 to evalu? ate the tracking performance of the algorithms. The figure of merit is the average estimation error et := t 2:::=1 IIXi - xi112/11xi112' which is depicted in Fig. I(b). It is observed that if the subspace dimensionality is chosen as K = p, Algorithm I attains a better esti? mation accuracy than PETRLES and GROUSE (a constant step size 0.1 was adopted for the latter). Even though PETRELS works well for K = r, if one overestimates the rank PETRELS exhibiting erratic behaviors for 25% available observations. As expected, for the ideal choice of K = r all three schemes achieve similar estimation accu? racy. The smaller error exhibited by PETRELS (relative Algorithm I) might be due to a suboptimum choice of A. Still, Algorithm I is more stable numerically when the amount of missing observations  (a) (b) Fig. 1. Performance of Algorithm 1. (a) Evolution of the average cost Ct(L[t]) versus the batch counterpart. (a) Relative estimation error for different schemes when 1l' = 0.25 and a2 = 10-3.

?  (a) (b) Fig. 2. Performance for Internet-2 data if K = P = 10 and f3 = 0.95.

(a) Average estimation error for various amounts of missing data. (b) Estimated (red) versus true (blue) 00 flow traffic for 1l' = 0.25.

is large, thanks to the regularization terms in (3). The price paid by Algorithm 1 is in terms of higher complexity per iteration. Note that the complexity for PETRELS is O(PK2), and only O(PK(l + 1l'K)) for the first-order algorithm GROUSE.

5.2. Tracking Internet-2 network traffic  In IP networks accurate estimation of the origin-to-destination (00) flow traffic is a task of paramount interest. Typically, the data avail? able is a measured traffic via NetFlow [7], for a small subset of 00 flows. Several studies have demonstrated that 00 flow traf? fic exhibits a low-intrinsic dimensionality, which is mainly due to common temporal patterns across 00 flows and periodic behaviors across time [7]. In this example, OD-f1ow traffic-levels are col? lected from operation of the Internet-2 network (Internet backbone across USA) [7]. The measured 00 flows contain spikes (anoma? lies), which are removed as detailed in [9] to end up with an anomaly? free data stream {y,}. A subset of entries of Y t are then selected independently with probability 1l', to yield the input of Algorithm 1.

The evolution of the average traffic estimation error (et) is depicted in Fig. 2(a), for different schemes and various amounts of miss? ing data. It is observed that the estimation accuracy and subspace learning speed degrades gracefully as the NetFlow sampling rate de? creases. Algorithm 1 outperforms competing alternatives when At is tuned adaptively as per Remark 2, for a2 = 0.1. When only 25% of the total OD flows are sampled by Netflow, Fig. 2(b) depicts how the representative 00 flows are accurately tracked using Algorithm 1.

6. REFERENCES  [1] L. Balzano. R. Nowak. and B. Recht. "Online identification and tracking of subspaces from highly incomplete information," in Proc. of Allerton Conference on Communication, Control. and  Computing, Monticello, USA, Jun. 2010.

[2] E. J. Candes and B. Recht, "Exact matrix completion via convex optimization," in Found. Comput. Math., vol. 9, no. 6, pp. 717- 722,2009.

[3] E. J. Candes and Y. Plan, "Matrix completion with noise," in Proceedings of the IEEE, vol. 98, pp. 925-936, 2009.

[4] Y. Chi, Y. C. Eldar, and R. Calderbank, "PETRELS: Subspace estimation and tracking from partial observations," in Proc. of  Processing, Kyoto, Japan, Mar. 2012.

[5] W. Dai, O. Milenkovic, and E. Kerman, "Subspace evolu? tion and transfer (SET) for low-rank matrix completion," IEEE Trans. Signal Process., vol. 59, pp. 3120--3132, Jul. 2011.

[6] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Sta? tistical Learning. Springer, Second edition, 2009.

[7] A. Lakhina, K. Papagiannaki, M. Crovella, C. Diot, E. D. Ko? laczyk, and N. Taft, "Structural analysis of network traffic flows," in Proc. of ACM SIGMETRICS, NY, USA, Jul. 2004.

[8] M. Mardani, G. Mateos, and G. B. Giannakis, "In-network sparsity regularized rank minimization: Applications and al? gorithms," IEEE Trans. Signal Process., 2012, see also arXiv:1203.1570vl [cs.MA].

[9] M. Mardani, G. Mateos, and G. B. Giannakis, "Dynamic anomalography: Tracking network anomalies via sparsity and low-rank," IEEE 1. Sel. Tpoics in Signal Process., 2012, see also arXiv:1208.4043vl [cs.NI].

[10] G. Mateos and G. B. Giannakis, "Robust PCA as bilinear de? composition with outlier-sparsity regularization," IEEE Trans.

Signal Process., vol. 60, pp. 5176-5190, Oct. 2012.

[11] J. Mairal, J. Bach, J. Ponce, and G. Sapiro, "Online learning for matrix factorization and sparse coding," in 1. of Machine Learning Research, vol. 11, pp. 19-60, Jan., 2010.

[12] B. Recht, M. Fazel, and P. A. Parrilo, "Guaranteed Minimum? Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization," in SIAM Rev., vol. 52, pp. 471-501, 2010.

[13] Y. Solo and X. Kong, Adaptive Signal Processing Algorithms: Stability and Performance, Prentice Hall, 1995.

[14] M. B. Yang, "Projection approximation subspace tracking," IEEE Trans. Signal Process., vol. 43, pp. 95-107, Jan. 2012.

[15] http://internet2. edu/observatory /archive/data-coll ecti ons.

