Associating IDS Alerts by an Improved Apriori Algorithm

Abstract?Among a large number of association rule mining algorithms, Apriori algorithm is the most classic one, but the  Apriori algorithm has three deficiencies, namely: the need for  scanning databases many times, generating a large number of  Candidate Anthology, as well as frequent itemsets iteratively.

The paper presents a method that solves the maximal frequent  itemsets through one intersection operation. The degree of  support is obtained through the times of intersection without  having to scan the transaction database, by numbering some of  the properties to reduce memory space and search the  candidate set list easily, thereby enhancing the efficiency of the  algorithm. Finally, it can generate association rules for  Intrusion Detection System. Experimental results show that  the optimized algorithm can effectively improve the efficiency  of mining association rules.

Keywords-data mining ? association rules ? Apriori  algorithm?itemsets

I. INTRODUCTION  Data mining [1] is the most dynamic emerging research in today's database technology and artificial intelligence research; the main goal is to dig out valuable models from a large database for users. In the transaction database, mining association rule is a very important research topic in data mining field. Intrusion Detection System (IDS) is the core of security technology. It can effectively cooperate with the other network security products and provide proactive and real-time protection for the network. Data mining  technology may be used to discover the knowledge and establish attack behavior model from original IDS alerts.

Firstly, some basic concepts and issues of association rules are presented in the following.

Association rules discovery is an aspect of data mining.

An association rule is a rule that describes relationship between the data items in the database. Mining objects are generally transactional database.

The following is the formal description of association rule mining problems: Set I = (i1, i2? im) ij is an item. Set D as a transaction database, T ? I, T has an identity TID.

Association rule is the form like "X  Y", if X ? I, Y ? I, X ? Y = ?. Conditions that association rule X Y is set up: ?  with support for S. Namely, in the transaction database D, there are at least S% of the transaction that contains X ? Y, itemset (X ? Y) is called the support rate for the support rate of association rules. Denoted by: S (X  Y) = S (X ? Y). ? with a confidence level C.

Namely, in the transaction database D contains X's affairs at least C% of the transaction and contains Y. Association rule X  Y level of confidence recorded as: C (X  Y) = s (x ? y) / s (x) * 100%; if S (X  Y) ? min_sup (minimal support) and C ( X  Y) ? min_conf (minimal confidence level), claimed that stronger rules of association rules, otherwise known as the weak rule.

Maximal frequent itemsets [2]: all of its direct supersets are not frequent itemsets.

Closed itemsets [3]: If all the direct supersets of an itemset do not have the same degree of support with it.

Frequent closed itemsets [3]: a closed itemset is frequent.

Third International Symposium on Intelligent Information Technology and Security Informatics  DOI 10.1109/IITSI.2010.47     Association rule mining problem is to find out all the strong association rules with a given degree of support and confidence in the D. Therefore, the mining association rules can be divided into two sub-issues:  According to the minimum support rate to find out all the frequent itemsets in the database D, if S (X) ? min_sup, claims X as a frequent itemset.

Generate association rules. For each frequent itemset X, Y, if X ? Y, X ? ?, S (X) / S (Y) ? min_conf, the association rule is Y  (Y-X).

As the second step is relatively easy, the current research focuses on the first step, that is, to find frequent itemsets. As the number of different itemsets can be up to 2m and the scale of database is large, it is almost impossible calculating the support of all items.



II. RELATED WORK  A. Apriori Algorithm  Apriori algorithm is proposed by R. Agrawal and R.

Srikant in 1994. To be used in IDS, the algorithm may be improved from the following aspects:  If some candidate itemsets are considered as non-frequent ones, they should not be generated.

There are many duplicated comparing operations on the same items in the connection operation.

Each time after the candidate itemsets are generated, it scans the database again to determine whether these candidates are frequent ones. The procedure includes many duplicated and unnecessary scanning operation.

To be used in IDS process, it only considers the association rules between the maximal frequent itemsets and all the other ones can be ignored.

B. Improved Apriori algorithm  Many researchers have focused on improving the Apriori algorithm, and they have put forward a number of ideas. All the previous research may fall in the following categories: partition-based technology, matrix compression, the weighted attributes [4] etc. Valdes [5] improved the algorithm by cutting the candidates of non-frequent sequences in the L1 sequence, and identifying sequential models from the remaining candidates. In order to reduce the  number of scanning and quickly discover user behavior models, Yan [6] used the ratio of support to expected support to denote the conditional probability from item X to item Y. Treinen [7] proposed an improved one by applying priority attributes to items with length l (l ?k) to determine the candidate k-th itemsets so that the scanning times is reduced.



III. IMPROVEMENT ON NON-ITERATIVE APRIORI ALGORITHM  Traditional Apriori algorithms solve candidate itemsets by iteration. They are improved by decreasing candidate itemsets and reducing scanning times. But in essential, the bad performance of the algorithm mainly results from the iteration of self-join operations. The paper proposes a non-iterative improved Apriori algorithm which is mainly used in IDS.

A.  The algorithm basis  The characters and lemma of association rules include: 1) Subsets of any frequent itemset are also frequent  itemsets. 2) The super sets of any non-frequent itemset are non-frequent.  3) The subsets of any maximum frequent itemset must be frequent, and super sets of it are non-frequent.4) The super sets of any closed itemsets are non-frequent.Lemma 1: Any maximum frequent itemset can be obtained by the intersection operation. Lemma 2: Intersection operation can get all the frequent itemsets.

Lemma 3: The value of support degree relates to the number of intersection operations.

The following is the demonstration procedures of the lemmas (min_sup value with 1 is not considered).

1) ???? supmin_)(,( XsIXX  sup))min_)(,,( <??? YsIYYXY =>s(X)?min_sup?2  => ),(ji Xji TT ji =??? ?  Conclusion: Any maximum frequent itemset must be supported by at least two items.

2) sup)min_)(,( ??? XsIXX => ?<???? sup)min_)(,,( YsIYYXY  sup)min_)(,,( <??? YsIYYXY     => ),(ji TT jiXji ?????  Conclusion: Any frequent itemset is a maximal frequent itemset, or a subset of a maximum frequent itemset.

3) Define a function count(x) which denotes the times of the intersection to get itemset x. Let x is supported by n items, if there is an intersection between any two items, the total number of intersections is count(x).The relationship between s(x) and count(x) is that count(x) = CS(X)2  Conclusion: the support degree of an itemset is determined by count(x) without scanning the transaction database D.

During IDS alert mining procedure, only the maximum frequent itemsets are considered, since association rules with low-dimension have no practical effects. Since most candidate itemsets with high degree of support are obtained by intersection operation, and they are probably the maximal frequent ones, the paper proposes a method that replaces iteration with intersection operation, discarding useless sub-frequent itemsets. Intersection operations can select the candidate itemsets whose support degree are not less than two and be sure to get all the maximum frequent itemsets.

The support degree of the candidate itemsets is obtained by counting the number of intersection operations, not by re-scanning transaction database. In order to simplify the intersection operations, in the pre-processing stage, some of the attributes are indexed separately.

B.  The algorithm procedure  The algorithm procedure is depicted in the following: Step 1: The conversion phase makes indexes for alert  attributes such as protocol type, alert type, and attack category so as to simplify intersection operation.

Step 2: Doing intersection between transactions to get the maximal itemsets. If the results are found in the list of the candidate itemsets, the counter is added by 2, otherwise, the result is inserted into the list and the counter is reset as 2.

The counter denotes the support degree for the transaction.

Step 3: Obtaining all frequent itemsets. The counters in the list are converted into the value of the support degree. A candidate itemset is a frequent one if its support degree is not less than predefined min_sup.

Step 4: Generating Association rules. In our experiments, an association rule only involves attack type, and the algorithm only generates the association rule like X Y, where Y is the attack behavior. If the confidence of an association rule is greater than or equal to min_conf, it is a strong one.

The formal description of Algorithm is described as below:  Input: transaction database D = (TID1, TID2 ... TIDn), the minimal transaction support threshold min_sup.

Output: maximal frequent itemsets L / / Pre-processing for (each i,j i<n&&j<m){ if (itemj? TIDi)  convert(itemj); // the last three properties  will be encoded } // Core Algorithm count= 0;   //the number of itemsets elements for each TIDi? D?TIDj? D?i? j?{  temp = TIDi ? TIDj;   // Seek common ground  if(temp!=? ){  // Search for candidate itemset list Index = Serchitem(temp); // If it already exists, then the automatic counting, otherwise, this  //candidate set to join the candidate list.

if(index== -1)  hash[index]++; else{ itemsets.add(temp);  hash[Count++]=1; }}} For each itemsets.get(k) (k?Count){  if(hash[k]? min_sup-1){ Add itemsets.get(k) to L // the candidate set K is a frequent candidate set }} // generating association rules For each Li?L { Y = Attacks?         X = Li ?Y; If(conf(X Y)?min_conf)  // it is a strong //association rules?

Associate(X Y)     // Generating a new rule }  C.  An example  Table I shows the original database D from AllElectronics [8], which includes nine items and five attributes. The minimum support degree is 2.

TABLE I. THE TRANSACTIONAL DATABASE OF A BRANCH OF  ALLELECTRONICS.

id items  T1 I1,I2,I5  T2 I2,I4  T3 I2,I3  T4 I1,I2,I4  T5 I1,I3  T6 I2,I3  T7 I1,I3  T8 I1,I2,I3,I5  T9 I1,I2,I3  The Apriori algorithm works in the following procedure: Error! Reference source not found.Scanning database 5 times to get five 1-frequent itemsets {I16, I27, I36, I42, I52}.Error! Reference source not found.10 times intersection operation for the candidate itemsets, and 10 times scanning operation to get six 2-frequent itemsets {I1I24, I1I34, I1I52, I2I34, I2I42, I2I52}.Error! Reference source not found.30 times intersection operation and 6 times scanning operation to get two 3-frequent itemsets {I1I2I32, I1I2I52}.Error! Reference source not found.One intersection operation and one scanning operation to get nothing.

Apriori needs 41 times intersection operation and scanning database 22 times to get frequent itemsets {{I1, I2, I3, I4, I5}, {I1I2, I1I3, I1I5, I2I3, I2I4, I2I5}, {I1I2I3, I1I2I5}}.

The improved works like the following.

Since the transaction database has only one attribute, the  pre-processing stage is unnecessary. In this case, after the intersection operation repeats C92 times and scanning candidate itemset list 9 times to count the values of support degree, the result list of the frequent itemsets is like {{I1,I2,I3  },{ I1I2, I1I3, I2I3, I2I4}, {I1I2I3, I1I2I5}}. It is obvious that itemsets I2I4?I1I2I3?I1I2I5 are maximum frequent itemsets and others are their subsets.

If either intersection operation or the comparison operation is a basic unit operation, Apriori algorithm needs 41+22?9=239 units, but the improved algorithm needs only C92+9=81 units. The non-iterative algorithm is obviously more efficient than Apriori.

D.  Experimental Analysis  MIT Lincoln Laboratory DARPA99 and DARPA2000 [9] data sets are chosen to evaluate the effect of the algorithm. Inside-network alerts of the third week are used off-line with Snort 2.4.2 running as NIDS mode under Red hat Linux 9.1, opening all the detection capabilities, using the default full alert mode.

Table II shows the original DARPA99 and DARPA2000 data. The first and second column denotes the week day on which the alerts were generated. The third column means the total number of the captured packets and the forth column points out the number of the types of the alerts. For example, on Wednesday, there are 39 kinds of alerts and the total number of alert is 3465.

TABLE II. DARPA ORIGINAL DATA SHEET  data source alert number alert type  2000year inside 1883 21  DMZ 486 19  99Mon inside 3407 40  99Tue inside 3887 43  99Wen inside 3465 39  99Thu inside 3840 36  99Fri inside 4151 44  TABLE III. ALERT CORRELATION RESULTS  data source association  number  attack  type  2000 inside 18 4  DMZ 46 4  99Mon inside 9 3  99Tue inside 10 3     99Wen inside 11 3  99Thu inside 10 4  99Fri inside 6 3  Firstly a format conversion module converts the original alerts to the normalized ones, which take source IP, source port, destination IP, destination port, protocol type, alert classification, and attack type as mining attributes. We use the normalized alerts as the input. The implementation is based on the java platform. Threshold values are configured as min_sup = 0.01 and min_conf =0.1.

Table III shows the mining association results. The algorithm generates some useful association rules. The first two column are as same as table 1.The third column points out the number of the strong association rules of the alerts and the forth column denotes the number of the attack type of the alerts. For example, on Wednesday, there are 3 kinds of attack types and the total number of strong association rules is 11.

Experimental results denote the analysis of data sets in the third week. Only 0.4% alerts are mined as strong association rules, which include "CHAT IRC message", "Potential Corporate Privacy Violation" originated by attacking the port 6667 of address 192.168.1.20 frequently.

The rule "INFO TELNET access? also shows that telnet service on IP address 172.16.112.50 is frequently accessed.

There is an important rule named "SNMP public access udp" which denotes 635 duplicated alerts on Thursday, since IP 192.168.1.30 sends data packets to port 161 on IP 172.16.112.100 through port 32775 every 1 ~ 2 minutes.

Obviously, the algorithm is useful to reduce the duplicated alerts and help the administrator improving the efficiency.

The inside network dataset of Darpa2000 is also used. In the dataset, there is an attack chain composed of five stages.

Four of the five stages are denoted correspondingly by the alerts "RPC portmap sadmind request UDP", "Decode of an RPC Query", "RPC sadmind query with root credentials attempt UDP", "RPC port map Solaris sadmin port query udp portmapper sadmin port query attempt". The algorithm successfully mined all the four rules, which show the full intension of the attacker. The attacker starts from 202.77.162.213 attempting to find the host with "RPC  sadmind" vulnerability. Then, he sends a decoding of RPC requests. After that, he launches an exploit to obtain administrator privileges. Finally, he calls rsh to get a remote Shell. This result demonstrates that the algorithm may help the administrator finding out the intension of the attacker.



IV.  CONCLUSION  The iterative self-join operations used in traditional algorithms include many duplicated and unnecessary scans which result in the low performance. The paper proposes an improved way which removes the iteration procedure and uses one intersection operation to generate maximal frequent itemsets. Experiments demonstrate that the improved one can quickly generate some strong association rules. These rules may denote the targets of the attacks and the intention of the attackers so that they may help the administrator to find out valuable information.

By now, the improved algorithm mines association rules with IDS alerts not with alert attributes, so it may not be extended to broad excavation. The time attribute of the alerts are not considered yet. In future work, alert attributes are to be involved in the association rules which help constructing the attack scenarios in the form of IDMEF [10].

REFERENCE [1] Hu Kan, Xia Shao2wei. Data mining based on large data ware2house: survey [J]. Journal of Software, 1998, 9 (1): 53262.

[2] R.Bayardo. Efficiently Mining Long Patterns from Databases. In Proc.

Of 1998 ACM-SIGMOD Intl.conf.on Management of Data, pages 85-93,Seattle,WA,June 1998 [3] M.J.Zaki and M,Orihara.Theoretical foundations of association rules.

In Proc. Of the 1998 ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery, seattle, WA , June1998.

[4] G.D.Rankumar, S.Ranka, and S.Tsur.Weighted Association Rules: Model and Algorithm. http://www.cs.ucla.edu/~czdemo/tsur/,1997 [5] A Valdes, K Skinner. Probabilistic Alert Correlation [C] . 4th International Workshop on the Recent Advances in Intrusion Detection (RAID?2001), Davis , USA ,2001.

[6] X.Yan, J.W.Han, R.Afshar. CloSpan: Mining Closed Sequential Patterns in Large Databases. In SDM'03, San Francisco, CA, May 2003.

[7] J.J.Treinen, R.Thurimella. A Framework For The Application Of Association Rule Mining In Large Intrusion Detection Infrastructures.

Recent Advance In Intrusion Detection 2006, LNCS 4219, Berlin: Springer Verlag 2006:1-18.

[8] Jiawei Han and Micheline Kamber, Data Mining: Concepts and Techniques [M], 2nd edition, Morgan Kaufmann, 2006, Page 151.

[9] MIT Lincoln Labs. 1999 DARPA intrusion detection evaluation [EB /OL ]. [ 2007 - 03 - 15 ]. http: //www. ll.mit.edu /IST/ideval/ in2dex. html.

[10] POPPI S. Snort IDMEF plugin [EB /OL ]. [2005 - 11 - 15 ].http:// sourceforge.net/projects/ snort2idmef.

