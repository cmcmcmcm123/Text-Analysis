ADIOS Visualization Schema:   A First Step towards Improving Interdisciplinary Collaboration in High Performance

Abstract?Scientific communities have benefitted from a significant increase of available computing and storage resources in the last few decades. For science projects that have access to leadership scale computing resources, the capacity to produce data has been growing exponentially. Teams working on such projects must now include, in addition to the traditional application scientists, experts in various disciplines including applied mathematicians for development of algorithms, visualization specialists for large data, and I/O specialists.

Sharing of knowledge and data is becoming a requirement for scientific discovery; providing useful mechanisms to facilitate this sharing is a key challenge for e-Science. Our hypothesis is that in order to decrease the time to solution for application scientists we need to lower the barrier of entry into related computing fields. We aim at improving users? experience when interacting with a vast software ecosystem and/or huge amount of data, while maintaining focus on their primary research field.

In this context we present our approach to bridge the gap between the application scientists and the visualization experts through a visualization schema as a first step and proof of concept for a new way to look at interdisciplinary collaboration among scientists dealing with big data. The key to our approach is recognizing that our users are scientists who mostly work as islands. They tend to work in very specialized environment but occasionally have to collaborate with other researchers in order to take full advantage of computing innovations and get insight from big data. We present an example of identifying the connecting elements between one of such relationships and offer a liaison schema to facilitate their collaboration.

Keywords?collaboration; schema; visualization; simulation; experiments

I. INTRODUCTION Collaboration between scientists in leadership-scale science  has grown to include a diverse collection of scientists as the computing and storage resources have increased, thus allowing researchers to tackled larger, more complex physical problems.

Progress in technology and computing has also increased the amount of data acquired and analyzed from sensors, medical equipment or larger instruments such as the neutron beams at the Spallation Neutron Source at Oak Ridge National Laboratory or the Tokamak Fusion Test Reactor at Princeton  Plasma Physics Laboratory. Our own experience in the field of fusion, during the CPES project [1], is an example of such complex collaboration between scientists from various fields and institutions with the common goal of successfully running, analyzing and getting knowledge from fusion simulations. Our experience is not uncommon across universities and national laboratories. The Scientific Discovery through Advanced Computing (SciDAC) Office of Science program?s mission is to accelerate progress in scientific computing in order to deliver breakthroughs in various scientific topics. It does so by partnering with experts from national laboratories and universities to leverage a wide range of expertise and ensure that results from this effort benefit the wider research community. In this context we have developed an end-to-end framework to support collaboration among groups of scientists working around fusion simulations [2]. We describe our work in more detail including lessons learned from our experience that lead to a refreshed approach to collaboration around e- Science data.

We start by exploring the related work and the current state of scientific collaboration. We then make a case for a decoupled collaboration in e-Science by illustrating common scenarios in leadership scale computing. Finally, we focus on the relationship between the application scientist and the visualization expert, presenting the ADIOS visualization schema as a facilitator for expertise and data exchange between the two. Our goal is to allow scientists to focus on their area of training and interest, while minimizing time wasted in acquiring skills in related fields and reducing the need for collaborators who are experts in these fields.



II. SCIENTIFIC COLLABORATION  A. Scientific collaboration The most common, and perhaps the most tangible way to  study collaboration in research is to measure and deduct collaborations through co-authorship [3],[4],[5]. Sonnenwald presents a comprehensive study of scientific collaboration and influencing factors in [6]. The study warns of the challenges of collaboration for its own sake.  The scientists? perspective in the similar work of Hara [7] also shows that successful   DOI 10.1109/eScience.2013.24     collaboration in research is not easily found. Indeed it is a complex problem that has to take into account collaboration between individuals, groups, institutions, sectors or nations; as well as access to required infrastructure and social networks and persona. However these studies, along with various documented success stories of scientific collaboration in domains including education, health care and computer science, indicate that collaboration has the potential to solve complex scientific problems. It can lead to new discoveries as well as new ways to conduct science. The emergence of scientific social networks is demonstrating that scientists are increasingly leveraging a wider range of tools to tap into this potential. The first attempts at establishing new networks have been largely focused on publications. Sites like LinkedIn1 and ResearchGate2 help researchers make professional connections and stay up-to-date on the latest publications and development in their fields. While scientists do not actually work (online) using these sites, the availability of Web 2.0 technology has also heralded the emergence of gateways and virtual research environments. These technologies foster the formation of more closely knit communities of researchers who work together using portals and common tools in their daily work routines.

Therefore, we note that as a research topic, scientific collaboration is a broad subject that can be tackled from various angles: statistics, quality, social character, temporal, geospatial considerations, impact of technology etc.

Interdisciplinary collaboration adds a level of complexity as participants have different expertise and training. For example, different perspectives on the same general science topic often result in confusion due to mismatched nomenclature and incompatible software.

As Hara et al. [7] suggest, getting a further understanding of scientific collaboration requires an in-depth understanding of researchers? daily work practices, leading to an understanding of how integrative scientific collaboration may be facilitated, perhaps through supporting technologies. This paper presents an analysis of collaboration in e-Science. We build on the example of the High Performance Computing community (HPC) where interdisciplinary collaboration is increasingly required.

B. Interdisciplinary collaboration in e-Science As computers are allowing for generation and exploration  of larger datasets, several application scientists (chemists, physicists etc.) have two options if they want to stay up-to-date in their respective fields: collaborate with experts in various sub-fields of computer science, or become knowledgeable in these areas themselves. The first one seems more feasible and reasonably achievable as the latter takes away from the time and energy focused on their main area of interest. Sites and software tools like MyExperiment [8] and ManyEyes [9] make it easy for scientists to use, share and reuse workflows, visualizations and datasets. Nevertheless, researchers dealing with large-scale data still face many challenges and often require some knowledge in computing and data management in order to manipulate that data. The extreme case of High   1 LinkedIn: http://www.linkedin.com/ 2 ResearchGate: http://www.researchgate.net/  Performance Computing where increase in computing power has allowed scientific problems to grow in complexity and scale highlights these challenges. For a theoretician, making sense of such large amount of data involves writing codes that run on supercomputers, managing data output, analyzing that data, and visualizing the results. It is becoming impossible for one single scientist to correctly and efficiently perform all of the necessary tasks without collaborating with other experts.

Extracting insight from simulations running on supercomputers typically involves pen-and-paper scientists, computation scientists, applied mathematicians (modeling algorithms, data mining algorithm etc.), I/O and performance experts, and visualization experts. Concrete examples of this are presented in the next section.



III. THE HIGH PERFORMANCE COMPUTING (HPC) CASE In HPC, collaboration has indeed become an essential piece  of scientists? daily lives. Utilizing leadership scale computers certainly requires more than acquiring programming skills.

New software like GlobusOnline [10] and Web groups like HPCWeb3 indicate that more scientists are getting access to large amount of data and organizing themselves for better sharing of resources, data and expertise. GlobusOnline helps researchers move, sync and share big data through web browsers and avoid time consuming, error prone IT tasks.

HPCWeb is a community group that is focused on making the computing and data resources that underlie simulation science, scientific computing, and data-centric science easily accessible through web browsers. In this section we describe a specific fusion project and other similar science collaboration projects around universities and national laboratories.

A. Example in Physics During the five year CPES [1] project, the goal of our  computer science team has been to support fusion scientists as they ran and monitored the XGC-1 code [11] on supercomputers at Oak Ridge National Laboratory and the National Energy Research Scientific Computing Center.

Application scientists are often unable to take full advantage of currently available computational resources. They may lack formal training in software engineering and often get their training from other scientists in a team. Code development methods, software tools and other such considerations are made within a group based on previous scientists? familiarity and/or preferences. A study by Basili et al. [12] gives a software engineering perspective on HPC common practices and discusses some of these issues. Our role in the CPES project has been to provide knowledge about the infrastructure and related software in order to bridge this knowledge gap.

Our close interactions with physicists have helped us identify ways in which we could not only provide them with solutions but also improve their experience. Our resulting end-to-end technologies included three main elements: a fast Adaptable I/O System (ADIOS) [13], a workflow management system and an electronic simulation monitoring portal (eSiMon4) [14].

3 HPCWeb: http://www.w3.org/community/hpcweb/ 4 eSiMon: http://www.olcf.ornl.gov/center-projects/esimmon/     With ADIOS we abstract away many decisions about where and when the I/O is done and which resources are used.

Such decisions are important to attain satisfactory I/O performance and while achieving high performance should be a concern for end-users, they are generally not trained to make performance decisions. ADIOS has been designed to simplify this process. First it uses a simple programming API that does not express the I/O strategy but simply declares what to output, which is the primary concern of the application scientist. Second, the XML-based external description of output data and selection of I/O strategy allows users to make changes to their I/O choices without changing their code as they change environment or new I/O methods become available. Third, ADIOS offers multiple transport methods selectable at runtime. Finally, the self-describing, log-based file format combined with buffered writing insures excellent write performance [15]. These design decisions allows I/O scientists to conduct research and deliver software while minimally impacting the application scientist who only wishes to output simulation results.

Similarly, our KEPLER monitoring workflow [16] automates a number of mundane and repetitive data management tasks for the XGC-1 users. When users launch the fusion workflow, they are unaware of the heavy lifting done in the back-end on multiple systems; they simply launch their preferred web-browser and log in to eSiMon. While workflow tasks include moving files from computational resources to visualization clusters, pre-processing and archiving raw data files, creating images and movie, eSiMon is a lightweight portal that can be launched from any machine using a web browser to monitor the initial results. Our conscious effort to help researchers think of science instead of files and directories is embedded in the eSiMon design. Rather than focusing on a file browsing interface, the main simulation view displays familiar simulation variable names evolving through time as shown on Fig. 1. The tree-view on the left shows the scientific variables that can be dragged and dropped onto the canvas to display the movies. We use a provenance system [17] to link graphics to raw data for downloads and analysis.

Fig. 1. eSiMon main simulation view  B. Similar cases and different approaches Our data management team has worked with various  applications including combustion (S3D [18]), fusion (GTC [19], GTS [20], XGC-1 [11]), and supernova (Chimera [21]) and more. We have observed a variety of different collaboration techniques. Depending on the size of the project and the size of the group involved, scientists assign more or less time, energy and resources to addressing the data management issues they encounter while doing their work.

Smaller groups may attempt to gain some knowledge in related computing fields in order to get work done. They tend to focus on a direct solution to the task at hand, and may not take performance considerations into account, especially in the beginning phase of their project. Researchers are willing to live with performance problems, as long as their work does not come to a complete halt. This approach is tedious and time consuming when done correctly, but when rushed can result in fragile code and excessive dependence on the scientists who wrote the initial program.

Medium-sized groups may have a staff dedicated to developing software tools and solutions for the entire team.

For instance Bellerophon [22] has been designed and implemented to perform automated verification, visualization and management tasks while integrating with other workflow systems utilized by the CHIMERA [21] development team at ORNL. Similar to our workflow-dashboard system for fusion scientists, this approach is tailored to the needs of this specific team of astrophysicists. In an attempt to get results, developers find themselves delivering code and team specific solutions that work but that are not directly transferrable to other application teams. In these projects, like in our own, users become dependent on computer science experts and their knowledge for introducing changes to the workflow, fixing bugs or adding new functionality.

Larger teams with access to large infrastructure and financial resources may benefit from groups such as the Scientific Computing Group at ORNL where each member acts as a ?liaison? to projects running on the supercomputers.

A liaison is an application scientist with notable computational training able both to understand the science and also to provide valuable input on best practices for using supercomputers. Liaisons help scientists with useful software and point them to existing tools.

Despite this assistance, larger teams still need to develop their own data management solutions. In the past few decades a number of large projects consisting of collaborators from both laboratories and universities have turned to gateways. A science gateway is usually a community-developed set of tools, applications, and data that are integrated via a portal to meet the needs of that community. Gateways provide easy- access to tools and data; they enable researchers to focus more on their scientific goals and less on assembling cyber infrastructure. Gateways can also foster collaboration as a core team of supercomputing-savvy developers can build and deploy applications that become services available to the     community at large. One example is the Earth Science Grid Federation 5  (ESGF), which stores and distributes terascale data sets from multiple coupled ocean-atmosphere global climate model simulations.

Even at large scale, gateways tend to cater to specific communities. Considerable amount of time and effort from cyber infrastructure specialists is invested to serve the need of specific domain scientists. In [23] Wilkins-Diehr examines the lessons learned from the TeraGrid [24]. This work suggests that often the greatest benefit to users was easy access to data and the tools that can filter and mine it. In other words gateways have mainly addressed the need for easy, integrated access to commonly used tools and datasets within a particular scientific field. The focus had been primarily on providing functionality; user-experience and collaboration are still to be improved in future generation gateways. Indeed, despite the substantial effort required to build gateways end-users increasingly expect the same ease of use as commercial social networks for examples.



IV. OUR APPROACH From professional website to complex gateways, scientists  have started to leverage the advances of Web 2.0 technology and rapidly growing advances in computing resources. As a group, we have also tried to take advantage of these advances.

We have used our knowledge in computational science and our close relationship with application scientists to identify the obstacles preventing researchers from focusing on their science. Our work has resulted in the development and release of eSiMon and ADIOS. eSiMon, our one-stop-shop to fusion simulations, hides the complexity of the workflow and its multiple data management tasks from the users and presents them with scientific content. Similarly ADIOS provides sophisticated techniques that take full advantage of current research and resources without burdening the users.

While ADIOS is currently used in a variety of applications, the workflow-dashboard system has been more difficult to generalize across applications. Building, running, debugging and modifying our Kepler workflow remains a complex task that only a workflow expert can tackle. In our experience, despite training and tutorials, application scientists have not easily mastered the workflows and typically rely on the assistance of an expert for any modification. This represents a single point of failure for the group; it is a dependency that can slow down, or completely stop other group members from working.  If and when the knowledgeable person leaves, time and energy has to be invested in training a new group member before usual work can continue its course. This problem caused us to reevaluate our solution, and research and implement a better way to connect application scientists and computer scientists.

The entire robust monitoring workflow with its mundane and tedious tasks cannot be easily simplified.  Workflow tasks tackle issues such as user login and credentials, data movement across platforms, file archiving, etc. Generalizing   5 ESGF: http://esg.nersc.gov/esgf-web-fe/  the workflow involves a tremendous amount of effort, as a diverse set of requirements and constraints has to be taken into account for each application. On the other hand, making application specific workflows also involves considerable knowledge, expertise and effort to adapt to a different group.

Therefore, instead of erroneously trivializing the workflow tasks, we identify points where certain workflow tasks can be more easily connected to fit different application.

We describe this quest for improved connections, a search for a decoupled collaboration. Our hypothesis is that good simulation outputs (or results from other inter-disciplinary collaboration) are the product of a lot of individual knowledge, expertise, and work as well as some collaborative effort. Our interviews and meetings with application scientists suggest to us that it is important to identify where the work of these related but different scientists intersect and specifically target these areas for collaboration improvement. Our goal is to connect expertise where needed in order to allow each researcher to focus on their own area of interest. One example of such a connection is that of the application scientist and the visualization specialist. We asked the question: what pieces of the workflow connect the scientist running his or her simulation code and the experts and tools that can display the large amount of output data. We build on ADIOS and eSiMon for I/O and visualization purposes to design and implement ADIOS Visualization Schema.

A. ADIOS Visualization Schema In the teams we have experience with, notably the fusion  scientists and their most recent project EPSi6 (Center for Edge Physics Simulation), scientists often write their own analysis scripts or utilize a collection of scripts from the community.

As in computing in general, their improvised visualization skills are not always scalable, hence the need for computer scientists in the project. Indeed using visualization tools such as VisIt7, ParaView8, AVS9, etc. requires a certain level of expertise as well as information on the code data structures.

Visualization experts need to know where and how to access the data to be visualized. In other words they need metadata about files and physical variables to be visualized. For visualization experts, an understanding of the science is not essential; rather they want to know about scalars, vectors, arrays and meshes for instance. This is information that application scientists are familiar with and can point to in their code without knowing protocols and methods expected by common visualization tools. Therefore there is a need for translation between the language of the application scientist and the visualization expert.

Matching code, data and scalable visualization software is not a new problem. HPC scientists have long used self- describing data formats such as HDF5 10  and NetCDF 11  to   6 http://www.scidac.gov/fusion/fusion.html 7 VisIt: https://wci.llnl.gov/codes/visit/ 8 ParaView: http://www.paraview.org/ 9 AVS/Express: http://www.avs.com/ 10 HDF5: http://www.hdfgroup.org/HDF5/ 11 NetCDF: http://www.unidata.ucar.edu/software/netcdf/     <?xml version="1.0"?> <adios-config host-language="Fortran" schema- version="1.1">  <adios-group name="field3D"> <var name="n_n" type="integer"/>  <var name="n_t" type="integer"/> <var name="values" gwrite="coord"  path="/coordinates" type="real*8" dimensions="nspace,nnodes"/> <var name="node_connect_list" gwrite="nodeid" path="/cell_set[0]" type="integer" dimensions="nodes_per_elem,ncells"/> <mesh name="xgc.mesh" type="unstructured" file="xgc.mesh.bp" time-varying="no" /> <points-single-var value="values"/> <uniform-cell count="n_t" data="node_connect_list" type="triangle"/>  </mesh> <var name="nphi" type="integer"/> <var name="iphi" type="integer"/> <global-bounds dimensions="nphi,nnode"  offsets="iphi,0"> <var name="dpot" type="real*8" dimensions="1,nnode" mesh="xgc.mesh" hyperslab=": 0,: 1,: nphi"/>  </global-bounds> ?  <global-bounds dimensions="nphi,nnode" offsets="iphi,0"> <var name="iden" type="real*8" dimensions="1,nnode" mesh="xgc.mesh" hyperslab=": 0,: 1,: nphi"/>  </global-bounds> </adios-group>  write large data files.  But these formats do not insure standard organization, or consistent naming convention of particular visualization elements. Therefore, straightforward visualization of the data by visualization collaborators often requires a closer connection to the science and the scientist who wrote the data. Popular tools like VisIt and ParaView develop plugins and readers for common self-describing data formats. Development of readers for particular codes requires investment of time and energy from domain scientists and visualization/software experts. This may seem like a reasonable approach as long as codes, platforms and tools do not evolve, however this is rarely the case in bleeding edge research and development.

The concept of schemas, even visualization schemas, is not new. XDMF 12  (eXtensible Data Model and Format) was created out of the need for a standardized method to exchange data between HPC codes and tools.

XDMF (eXtensibleData Model and Format) uses XML to store Light data and to describe the data Model. HDF5 is used to store Heavy data. Similarly VizSchema [25] attempts to link common data formats to common visualization tools.

Both of these schemas are tightly coupled with the HDF5 file format. Even though we embed our schema as attributes in the ADIOS XML and binary files, the key difference is not the vehicle for the schema, but rather our persistent focus on collaboration and separation of expertise. Indeed the purpose of the schema is not to link the data format, or simply the metadata to the data; rather it is a description of the visual understanding of the data: content that can be processed by scientists? eyes and brains.

By focusing on meshes and physical variables, the visualization process becomes independent of code and developer. If the visual representation of a variable changes, a scientist describes that change in the ADIOS XML file. Even though the visualization expert may need to edit the reader for the resulting binary data, his or her work is thus decoupled from that of the application scientist. Instead of having a reader for each code, programs such as VisIt and Paraview will require only a single reader to visualize any ADIOS data file. If and when changes need to be made, and readers have to be revisited and adapted, visualization experts are able to do so without requiring an understanding of changes in the applications or discussions with domain scientists. Time and effort invested in changes are now tied to the data and no longer associated with specific codes or developers. A physicist, in our case, can simply describe and modify the mesh and the variables without having to understand the corresponding analysis and visualization scripts and software.

They can share scientific content with colleagues without knowledge of their preferred tools and naming conventions.

Data can be shared and visualized independently of users? expertise, familiarity, personal preference or availability of software packages. Furthermore we provide performance and flexibility of I/O as a bonus by embedding the schema into ADIOS XML file.

12 XDMF: http://www.xdmf.org/index.php/Main_Page.

B. Implementation XML tags and attributes provide a convenient way to  organize data in a human readable way. XML is also an intrinsic part of ADIOS; it is already the case that if users change the way they write data, they can simply edit their ADIOS XML file. We now extend this approach by giving the scientists a way to describe the mesh structure, variables that compose the mesh and related variables. The goal is to require minimal information from a domain scientist as possible and still be able to visualize the data on popular HPC tools. When data representation changes in the code, or new variables are added, users do not have to change their code, they simply shuffle variables and meshes around in the XML.

Visualization software including our own reader then interprets the changes and accurately display images on eSiMon for example.

Fig. 2 shows a sample of an ADIOS XML file.

Fig. 2. Sample ADIOS XML file with additions corresponding to the schema underlined.

string   /adios_schema/version_major      attr   = "1" string   /adios_schema/version_minor     attr   = "1" string   /dpot/adios_schema                 attr   = "xgc.mesh" string   /iden/adios_schema                   attr   = "xgc.mesh" string   /adios_schema/xgc.mesh/type attr   = "unstructured" string   /adios_schema/xgc.mesh/time-varying attr   = "no" string   /adios_schema/xgc.mesh/mesh-file attr   = "xgc.mesh.bp" string   /adios_schema/xgc.mesh/nspace   attr   = "/nspace" string   /adios_schema/xgc.mesh/points-single-var attr   = "/coordinates /values" double   /adios_schema/xgc.mesh/ncsets attr   = 1 string   /adios_schema/xgc.mesh/ccount attr   = "n_t" string   /adios_schema/xgc.mesh/cdata attr   = " /cell_set[0]/node_connect_list " string   /adios_schema/xgc.mesh/ctype     attr   = "triangle"  In the case of XGC-1, variables on the mesh and variables composing the mesh are stored in separate files. Variables are also written as compounds of 2D slices of the tokomak. To support such representations and that of other common representations in HPC, the ADIOS Visualization Schema has to be flexible and expandable. Therefore we allow referring to external files for mesh variables as well as the concept of hyper slabs for super sets or sub sets of variables. For example a variable could be a subset of another variable, or as it is in the case in our example, a 3D variable could be visualized as a set of planes using a 2D mesh.  ADIOS processes the newly added users? tags and attributes to generate an annotated binary output file describe thereafter. This file is then used by visualization experts and software to locate, associate and display the meshes and variables in the output data. Fig. 3 shows the schema attributes generated in the output BP file (ADIOS file format) as a result to the users? additions in the  XML file.

Fig. 3. Example of schema attributes generated from sample ADIOS XML  C. Evaluation Collaboration is often a critical component of research of  big machines and big data; however it remains difficult to measure via usual methods of observation, interviews and questionnaires. Studies of co-authorships offer a perspective but not a complete and general picture. In inter-disciplinary studies, particularly in fields such as HPC where diverse disciplines are required to make sense of a tsunami of data, more factors need to be taken into account. We argue that connectors such as the ADIOS Visualization Schema should be components of the complex workflows that process output from large simulations. This type of facilitated juncture insures that scientists are spending their valuable time and energy where they see it fit.

Before using ADIOS schema, Applications scientists and visualization experts would have to discuss and agree on data representation, metadata and I/O patterns ? establishing dependencies on specific persons, codes and visualization tools. Prior to using the schema, collaborators sharing data  within the same field would also often have to impose a single tool for convenience within a team ? inhibiting some users due to limited access to software or varying degrees of experience.

Without our standardized schema, modifications to software such as changes in data representations, changes in infrastructure requiring different I/O, additional variables for debugging, etc. would require modifications in the code as well as changes to visualization readers. This development work would have to be well coordinated and repeated for each change.

To further understand the considerable amount of development involved with implementing readers and writers for various codes and file formats we examined an assortment of VTK13 readers. We found 133 classes declaring themselves as reader and 84 declaring themselves as a writer. All together, this code amounts to about 18% of the VTK code. VisIt and ParaView visualization packages are both based on VTK and expectedly have extensive lists of readers and writers in their documentation14, 15. Table 1 shows a number of simulations that have been used by members of our team, and the number of lines of codes in the corresponding VisIt readers. As can be seen in the table, custom readers typically require thousands of lines of additional code to implement.

TABLE I. CODE-SPECIFIC READERS AND LINES OF CODES IN VISIT  File Format Lines of code  S3D 3114  Chombo 10520  M3D 4643  M3DC1 9888  GTC 3478  Pixie 8528  Grand Total 40171  The amount of collaborative work is not only reflected through lines of code. Our group has been directly involved with writing VisIt readers for codes such as GTC and Pixie3D [26]. Based on our past experience, this work requires visualization expertise and application knowledge and would be significantly simplified using our standardized schema along with a generic reader. Therefore we anticipate that the schema will consolidate numerous file formats and simulation code readers in HPC visualization tools. This task has the potential to notably impact the ratio of time implementing readers over the time spent doing application and visualization research.

As a proof of concept, in the EPSi project we have implemented our own schema reader and plotting method for   13 VTK Visualization toolkit: http://www.vtk.org/ 14 VisIt list of readers: http://visitusers.org/index.php?title=Detailed_list_of_file_for mats_VisIt_supports 15 ParaView list of readers: http://paraview.org/Wiki/ParaView/Users_Guide/List_of_read ers     Monitoring workflowSimulation code  ADIOS XML  edit  launch  visualize  ADIOS BP file  write  Visualization Expert  Physicist  XGC-1 code users. We find that the automatically generated 2D diagnostic images allow physicists to directly dig into the science. This visualization portion of the monitoring workflow replaces a painstaking process of running a series of standard analysis routines before focusing on interesting subsets of the data. For the application scientists the added value of our modified workflow task resides in the flexibility and separation of work via the ADIOS XML file. Researchers can be enthusiastic about learning new skills outside of their area of training; however, this more in-depth section of analysis is where they would rather focus. The scripts and methods they use to make real discoveries may only be used once and cannot be automated. The intangible metrics we target are time spent outside of area of expertise/interest, and time to solution in primary research area. In other words, how quickly and efficiently can we bring scientists to the research aspect of their work, the part that cannot be automated and leads to scientific breakthrough? In the meantime, I/O scientists and visualization scientists are also conducting research independently of the application as the collaboration has been decoupled. The visualization expert can now focus on research issues regarding visualization at scale instead of implementing and debugging readers for applications as discussed earlier.

Each scientist can do what they are trained to do on their own and come together only through this intermediate schema we have implemented. This scenario example is illustrated in Fig.

4.

Fig. 4. Decoupled collaboration between physicist and visualization expert  This proof of concept is a specialized and partial evaluation. For a more in-depth assessment of our work we need measurements of the time scientists spend outside of areas of expertise before and after integrating our ADIOS schema. In addition to interviews, targets for improvements will be tailored to each type of scientists for a more complete perspective of the changes in their daily routines. For example, in the case of visualization experts, the consolidation of simulation code readers and writers gives additional support to scientists? self-reported measurements of time spent outside of research.



V. CONCLUSION In this paper we have presented our perspective of the HPC  community and its answers for inter-disciplinary collaboration around large infrastructure and big data. We have replaced part of our fusion-specific monitoring workflow by a standardized visualization schema and interpreting tool to de-couple  collaboration between two types of scientists: application scientists and visualization experts. We present this work as an example of a new approach to multi-disciplinary collaboration.

While scientific collaboration is generally encouraged, it can be detrimental to research when time and energy are spent outside of one?s area of training and interest. We minimize such waste of time by identifying and improving connections between two types of experts through our visualization schema.

We claim that the schema accelerates discovery by separating scientists? work according to their research area. The schema decreases repetitive interactions between researchers who are connected but do not speak the same scientific language.  More tangible measurements are needed for further analysis. This work advocates encouraging inter-disciplinary collaboration by emphasizing discipline-specific research and making transition from one scientific field of another seamless.

