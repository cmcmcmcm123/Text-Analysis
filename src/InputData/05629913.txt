The authors wish to thank the 085 Innovation Projects Foundation of  Shanghai Education Committee for contract J20902 under which the

Abstract?This article mines the air cargo customer information and establishes the decision tree of the ID3 association rules from the ID3 algorithm. It finds frequent customer information using Apriori algorithm and finally gets the best combination of customer information. The combination has some significance on the development of air cargo.

Keywords-ID3 algorithm; Apriori algorithm; customer information

I. INTRODUCTION During the recent years, great achievements of the research  on data mining application have been achieved, whose broad application prospect have attracted many researchers and business organizations. A batch of data mining systems have been developed, and have been applied in commerce, economy, financial management and other fields among which the most representative systems are the Quest System researched by R. Agrewal of the IBM Almaden and the data mining prototype system DB Miner developed by the team led by the famous international data mining professor Han Jiawei who comes from Canada, Simon Fraster university. The successful application of the commercial software?s greatly arouse the enthusiasm for data mining technology in the academic circle and promote the application of data mining technology in industry, such as ERP system and various management information system, geographic information system, etc.

However, as for domestic airlines, they have already introduced some advanced management experience, but still in the beginning stages. They haven?t applied it yet, but with the increasingly fierce competition, to use the data mining technologies like ID3 algorithm and Apriori algorithm in airline industry will definitely become a trend of the air freight company development.



II.        DECISION TREE ALGORITHM  Decision tree is the most widely used method for data classification. Decision tree classification is a method to deduce the classification rules of the decision tree expression forms from disorder and irregular sample sets. It adopts the top-down recursive method, which first make comparisons of the property values between the inner nodes, then judge the branch followed and finally draw conclusions at the leave node. Therefore, there is a choice rule to correspond to the path  from the root to the leave node, and thus a set of classification rules to correspond to the decision tree.

Among all the decision tree classification algorithms, the most influential one is ID3 algorithm, the main ideas of which are as follows:  Every non-leave node corresponds to a non-category property, whose value is represented by the branch. A leave node represents the category that corresponds to the path between the root to the leave, and it?s property value is recorded by the leave.

In the decision tree, every non-leave node associates with the most informative non-category property.

Entropy is a term to measure the amount of information that a non-leave possesses.

Here is the definition for Information Entropy:  The entropy of discrete random variables: It reflects expected value of the variable. In general, there are two kinds of entropy: unconditional entropy and conditional entropy.

Unconditional entropy, entropy for short, can be expressed in the following formula:   ?  =  ?= n  i ii xpxpXH  2 )(log)()(  (1)  In this formula,  (i = 1,2 ..., n) stands for X value, and n stands for the number of values; expresses the Probability of ?  in which 2 is the substrate of the logarithmic function because entropy is encoded in binary. H (X) reflects the expectation of X with a certain value, that is to say, it stands for the uncertainty of X.

The conditional entropy of the discrete random variable X to a given discrete random variable Y can be referred to as the conditional entropy X to Y and recorded as H (X|Y),  which can be expressed using the following formula:  ?? = =  ?= n  i  m  j jiji yxpyxpYXH  1 1 2 )|(log),()|(  (2)  In this formula, ,  is Joint probability of ji yYxX == , ; ji yx |  stands for Conditional probability  of Y y , X x  , and n, m, respectively stand for the values   DOI 10.1109/iCECE.2010.993    DOI 10.1109/iCECE.2010.993        number of X, Y.  Similarly  2 is the substrate of the logarithmic function because entropy is encoded in binary.

H (X|Y) reflects the uncertainty of variable X under certain conditions. It can be proved that X can effectively lower its uncertainty if certain conditions are added. Therefore,  H(X|Y)?H(X)                                   (3)  So generally we use the conditional entropy value of X so as to lower its uncertainty.

Through the characteristics showed in the data collected, we can find an accurate description or model for each category.

What?s more, we can use the category description it generates to classify the future test data. Therefore, we will have a better understanding of each category in the data. In other words, we obtain some knowledge about this category.



III.     ASSOCIATION RULES  A. Mining Algorithm Definition 1: Mining Association Rules data sets recorded  for the D (Generally, it is a transaction database); D= , , ? , , ? , , , , ? , , ? ,   ,  k=1,2,?,n) is called Transaction, and i m=1,2, ?p? is called item.

Definition 2: Suppose I?{ i , i , ? , i  } is a collection made up of all the items in D, therefore X , any subset of I, is called an itemset of D, and |X| k  means collection X is a k itemset. Suppose t  and X respectively are Transaction and Itemset in D. If   exists, Transaction t  will contain itemset X. Each Transaction has a unique identifier, called TID.

Definition 3: The number of itemsets X contained in data set D is called the supporting number of itemset X, recorded as ? . The degree of supporting of itemsets X is recorded as support (X):  support(X) | |=?100? ?or support(X)= | |? In this formula, The number of Transaction in the data sets  is|D|; If support (X) is not less than the minimum support specified by users, X is called frequent itemset (or ?Large Itemset?), otherwise, X is non-frequent itemset (or ?Small Itemset?).

Definition 4: If X, Y are Itemsets, and X Y , the implication type X Y  is called the association rule, in which X, Y are respectively known as association rule premise and conclusion of the association rule. The support degree of itemsets is called the degree of association rules, recorded as: support(X Y ), support(X Y )=support(X Y). Association rules X Y .Confidence level of the association rules is recorded as confidence( X Y  ) ? confidence  100 . Usually the minimum confidence level specified by users? mining need is recorded as minconfidence.

Support and confidence levels are two important concepts which describe association rules; the former is for measuring the importance of the association rules in the entire statistical data, and the latter is used to measure the credibility of the association rules. Generally speaking, only the association rules of high support and confidence levels can be attractive and useful to users.

The task of mining association rule is to dig out all of the strong rules in D. Strong rules X Y  of the corresponding itemsets ? X Y  ? must be the ?Frequency Itemsets?; Association rules X Y  of the confidence level which is derived by ?Frequency Itemsets?? X Y ?can be calculated by the level support of ?Frequency Itemsets? X and ?X Y ? . Therefore, mining association rules can be divided into the following two sub-questions:  1)According to the smallest degree of support, we can find out all ?Frequency Itemsets? in data sets of D.

2) According to ?Frequency Itemsets? and the minimum ?Confidence Level?, we can generate association rules.

The basic model of mining Association Rules is shown in Figure 1.In this figure, D is data sets; Algorithm-1 is search algorithm of the ?Frequent Itemsets?; Algorithm-2 is the Generated algorithm of the ?Association Rules?; R is the mined association rules set. Users by specifying minsupport, minconfidence separately with the algorithm Algorithm-1, Algorithm-2 interaction, and through the results of interaction with R excavation interpretation and evaluation.

Figure 1. The basic model of mining association rules  B. Discovery Algorithm of Association Rule In order to find out all the full level of support and  confidence of association rules, we can divide this question into the following two sub-questions:  Looking for all combinations of items which are from those services support more than minimum support, these combinations are called large itemsets, while other combinations are called small itemsets.

The necessary rules which are generated by the large itemsets. The general idea: If ABCD and AB are large itemsets, we can calculate the ratio of r which is equal to the number of supporting for (ABCD) the number  k X t?  X Y?        of supporting for (AB), and determine whether the rules are strong or not. Only r minconfidence, it is strong rules. Attention to this rule, it has the minimum level of support, because ABCD is the largest.



IV.    CASE ANALYSIS  Suppose here is the customer information of a certain air cargo company (See Table 1).

Table 1 The information of the customers of an air cargo company  Customer Type Customer value Customer  satisfaction Lost or not  Regular cargo owner Low High No  Individual cargo owner Low High No  Individual cargo owner High High No  Regular cargo owner Low Average No  Regular cargo owner High Average No  Individual cargo owner High Average Yes  Individual cargo owner Low Average Yes  Regular cargo owner Low Low Yes  Individual cargo owner High Low Yes  Individual cargo owner Low Low Yes    Now suppose:  1) D1={A,B},A stands for regular cargo owner, and B for Individual cargo owner; D2={C,D,E,F,G},C?D?E?F?G stands for Customer category 1?Customer category 2? Customer category 3?Customer category 4?and Customer category 5 respectively(See Table 2).

Table 2 Customer classification  Customer category Type Meaning  Category 1 Potential customers Patronize one or two times or even no  patronage  Category 2 Disloyal customers Patronage occasionally  Category 3 General customers Patronize at a comparatively high frequency  Category 4 Loyal customers Patronize at an extremely high frequency  Category 5 Disconnected customers Used to patronize at a high frequency,  but no patronage now  D3={H,I,J},H stands for Door-to-door delivery service, I for expedited service and J for booking service.

2) There are 20 Services in the database,namely, |D|=20  3) The smallest support is 3, and minconfidence is 60?.

A.  The application of ID3 algorithm 1) Calculation process  a) Calculate the initial entropy  Let lost or not be the category property, therefore there are only two categories: Yes or No, that is to say, m=2.The number of the sample sets N=10, thus the number of the No category N ?=5 and the number of the Yes category N2=5. According to the previous formula:  H (Loss)= 5/10 5/10 1 b) Calculate the entropies of the non-category properties  respectively  ? Calculate property A, namely Customer type  There are 4 regular cargo owners, so n11=4, and the number of the No category n11(1)=3 and that of the Yes category n11(2 )= 1.

There are 6 individual cargo owners, so n12=6, and the number of the No category n12(1)=2 and that of the Yes category n12(2)=4.

Now A?s conditional entropy can be calculated:  H(Loss|A)= 0.876 ? Calculate property B, namely Customer value  There are 6 low customer values, so n2=6, and the number of the No category n21(1)=3 and that of the Yes category n21(2)=3  There are 4 high customer values, so n22=4, and the number of the No category n22(1)=2 and that of the Yes category n22(2 )=2  Now such conclusion H(Loss |B)=1 can be drawn using the same method.

? Calculate property C, namely Customer satisfaction  There are 3 high customer satisfaction, so n31=3, and the number of the No category n31(1)=3 and that of the Yes category n31(2)=0  There are 4 average customer satisfaction, so n32=4, and the number of the No category n32 (1)=2 and that of the Yes category n32 (2)=2  There are 3 low customer satisfaction, so n33=3, and the number of the No category n33 (1)=0 and that of the Yes category n33 (2)=3  So H(Loss |C)=0.4  ?        c) Calculate the increased information value of each property, and select the highest as the root node of the decision tree.

G(A)=H(Loss)- H(Loss |A) =1-0.876=0.124  G(B)=H(Loss)- H(Loss |B) =1-1=0  G(C)=H(Loss)- H(Loss |C) =1-0.4= 0.6  Obviously, what causes the entropy to reduce the most is the attribute ?Customer satisfaction?, therefore this property is made the root of the decision tree.

B.   The application of Apriori algorithm 1) TID list in the database  {{A,C,I,H},{A,C,H},{A,C,I},{A,D,I},{A,E,I},{A,E,H},{A,F,I, J},{A,F,J},{A,F,J,H},{A,G,I},{B,C,H},{B,C,I},{B,D,I},{B,E,I},{ B,E,J},{B,E,I,J},{B,F,H},{B,F,H,I},{B,F,J},{B,G,I}}  In order to get  by , we can apply Apriori algorithm.

The specific process is as follows:  Connect.

Prun.

Compare the candidate support count and minimum  support count, and delete the designate option which does not meet minimum support count, and we can get .

?{{ A,F },{ A,H },{ A,I },{ A,J },{ B,E },  { B,F },{ B,I },{ B,J },{ C,H },{ E,I }}  This step is repeated until cannot get , and stop mining association rules.

Once frequent itemsets are obtained in the transactions of database D, it is straightforward to generate strong association rules by them (Strong association rules satisfy minimum support and minimum confidence). Confidence level can be calculated from Definition 4. The following will be generated by the association rules:  Each frequent itemset  L generates its all nonempty sets.

For each of L?s non-empty subset s, if confidence (A B) min, conf, ?s (l-s)? is the output rule, in which min_conf  stands for minconfidence.

The frequent item set deduced above is L={A,F}, therefore {A},{F} are L?s non-empty subsets. Thereby, the following connection rule can be derived from L.

A F, confidence =3/10=30%  F A, confidence =3/6=50%  The minconfidence is 60%, so no rule can be deduced.

A H, confidence =5/10=50%  H A, confidence =5/7=71%  The minconfidence is 60%, so only the second rule can be deduced.

Similarly the results shown in the following Table are obtained.

Table 3  Conclusions drawn by connection rule algorithm  Rules obtained Measures taken  (services offered)  H A Generally those who want  door-to-door delivery service are regular cargo owners  Offer door-to-door delivery service to the regular cargo  owners, but only to high value customers because not all of them can bring high profits  E B General customers are individual cargo owners  No measures taken because it has nothing to do with the  subject  B I Individual cargo owners  usually demand expedited service  The profits individual cargo owners bring are not high, therefore offer expedited  service to loyal or high value individual cargo owners  C H Potential customers usually  demand door-to-door delivery service  Offer door-to-door delivery service to high value potential  customers only  E I General customers usually demand expedited service  The profits general customers bring are low, therefore no expedited service offered

V.   CONCLUSIONS  Now that decision tree for the customers of a certain air cargo company, we can see from it:  It is more likely to keep the customers with high customer satisfaction and lose those with low customer satisfaction.

If the customer satisfaction is average, it is more likely to keep the regular cargo owners and lose individual cargo owners.

According to the decision tree knowledge and Table 3, we can obtain the specific and necessary measures to solve different problems we are face with. For example, individual cargo owners generally will require expedited services, and we can provide expedited services for the customers with average customer satisfaction to keep those who are liable to lose.

