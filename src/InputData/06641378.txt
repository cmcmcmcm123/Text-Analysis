July 1 ? July 5, 2013, Helsinki, Finland

ABSTRACT Large and diverse data result in challenging data management problems that researchers and facilities are often ill- equipped to handle. I propose a new approach to these problems based on the outsourcing of research data management tasks to software-as-a-service providers. I argue that this approach can both achieve significant economies of scale and accelerate discovery by allowing researchers to focus on research rather than mundane information technology tasks. I present early results with the approach in the context of Globus Online.

SPEAKER BIOGRAPHY AND PHOTO Ian Foster is Director of the Computation Institute, a joint institute of the University of Chicago and Argonne National Laboratory. He is also the Arthur Holly Compton Distinguished Service Professor of Computer Science, and an Argonne Distinguished Fellow. Ian's research interests are in distributed, parallel, and data- intensive computing, and the application of these methods in domains such as biomedicine and economics. He has also a long record of contributions in open platforms, software, and standards for distributed systems and information sharing. He has published six books and over 300 articles and technical reports in these areas. His work has been recognized by numerous awards, including the Lovelace Medal, R&D Magazine's Innovator of the Year, and DSc Honoris Causa from the University of Canterbury. Ian Foster is a fellow of the American Association for the Advancement of Science, the British Computer Society, and the Association for Computing Machinery.

July 1 ? July 5, 2013, Helsinki, Finland     WEDNESDAY KEYNOTE I    ?Killer-Mobiles: The Way Towards Energy Efficient High Performance Computers??     Mateo Valero Barcelona Supercomputing Center  Spain    ABSTRACT It is widely recognized that Exascale systems will be constrained by power. The Mont-Blanc project aims to build an alternative approach towards Exascale based on aggregating parts from the embedded and mobile market, which offer a better FLOPS/Watt ratio and a lower unit cost, at the expense of lower peak performance per chip. HPC systems built from these parts will require a higher number of processors, or resort to extensive use of compute accelerators. Using a higher number of chips increases the available memory bandwidth, alleviating the bandwidth wall, but increases the pressure on the interconnection network.

The use of a high number of processors and accelerators, and the increased pressure on the interconnect require extensive code optimizations to achieve strong scaling, point to point synchronizations, and overlap data transfer with computation. The role of the OmpSs Parallel Programming Model is paramount, as the key enabling technology that hides the complexity from the programmer, and transparently performs all the required optimizations.

In this talk, we will review the design philosophies of several vendors, including HPC compute accelerators, and ARM- based mobile application processors in terms of peak performance, memory bandwidth, and energy efficiency; and we will review how the OmpSs programming models exploits the benefits of the Mont-Blanc approach while overcoming the drawbacks.

SPEAKER BIOGRAPHY AND PHOTO  Mateo Valero, http://personals.ac.upc.edu/mateo/, is a professor in the Computer Architecture Department at UPC, in Barcelona. His research interests focuses on high performance architectures. He has published has given more than 400 invited talks. He is the director of the Barcelona Supercomputing Centre, the National Centre of Supercomputing in Spain.

Dr. Valero has been honoured with several awards. Among them, the Eckert-Mauchly Award, Harry Goode Award, ACM Distinguished service,  the ?King Jaime I? in research and two National Awards on Informatics and on Engineering. He has been named Honorary Doctor by the University of Chalmers, by the University  of Belgrade, by the Universities of Las Palmas de Gran Canaria, Zaragoza and Complutense de Madrid in Spain and by the University of Veracruz in Mexico.  "Hall of the Fame" member of the IST European Program (selected as one of the 25 most influents European researchers in IT during the period 1983-2008. Lyon, November 2008)  In December 1994, Professor Valero became a founding member of the Royal Spanish Academy of Engineering. In 2005 he was elected Correspondant Academic of the Spanish Royal Academy of Science, in 2006  member of the Royal Spanish Academy of Doctors, in 2008 member of the Academia Europaea and in 2012 Correspondant Academic of the Mexican Academy of Sciences. He is a Fellow of the IEEE, Fellow of the ACM and an Intel Distinguished Research Fellow.

July 1 ? July 5, 2013, Helsinki, Finland     WEDNESDAY KEYNOTE II    The UberCloud HPC Experiment ? Paving the Way to HPC as a Service     Wolfgang Gentzsch Executive HPC Consultant, Chairman of the ISC Cloud'13,  Chairman of the UberCloud HPC Experiment Germany     ABSTRACT There are several million of small and medium-size manufacturers around the world, most of them using workstations for their daily design and development work. However, there is often the need for more computing.

Buying an expensive compute cluster is usually not an option, especially for small and medium enterprises, and renting computing power from the Cloud still comes with severe roadblocks, such as the complexity of the applications and their implementation itself, intellectual property and sensitive data, expensive data transfers, conservative software licensing, performance bottlenecks from virtualization, user-specific system requirements, and missing standards and lack of interoperability among different clouds.

On the other hand, the benefits of using remote computing resources are extremely attractive: no lengthy procurement and acquisition cycles; shifting some budget from capex to the more flexible opex; gaining business flexibility by getting additional resources on demand, at your finger tip; and scaling resource usage automatically up and down according to your actual needs.

The UberCloud Experiment has been designed to reduce many of the barriers mentioned above. By participating and moving the engineering application onto a remote computing resource, end-users can expect a long list of real benefits, such as: UberCloud is vendor neutral; no hunting for resources in a crowded Cloud market; professional match-making of end-users with suitable service providers; free, on-demand access to hardware, software, and expertise during the experiment; carefully tuned end-to-end, step-by-step process to accessing remote resources; learning from the best practices of other participants; no-obligation, risk free proof-of-concept: no money involved, no sensitive data transferred, no software license concerns, and the option to stay anonymous.

With these benefits, the experiment is leading the way to increasing business agility, competitiveness, and innovation, and participants are not getting left behind in the emerging world of Cloud Computing. Last but not least, all participants are encouraged to make use of the interactive UberCloud Exhibit, a directory of professional cloud services to the wider CAE, Life Sciences, and Big Data communities.

This presentation will focus on all the aforementioned topics in further detail and provide some real use cases from small and medium enterprises in digital manufacturing.

SPEAKER BIOGRAPHY AND PHOTO Wolfgang Gentzsch is consultant for HPC, Grid and Cloud; Co-founder of the UberCloud Experiment; Advisor to the EU funded project EUDAT; and the Chairman of the ISC Cloud Conferences. Previously, he was an Advisor to the EU project DEISA, directed the German D-Grid Initiative, and was a member of the Board of Directors of the Open Grid Forum, and of the US President's Council of Advisors for Science and Technology, PCAST.

Before, Wolfgang was a professor of computer science and mathematics at several universities in the US and Germany, and held leading positions at the North Carolina Grid and Data Center in Durham, Sun  Microsystems in California, the DLR German Aerospace Center in Gottingen, and the Max-Planck-Institute for Plasmaphysics in Munich. In the 90s, he founded HPC software companies Genias and Gridware, the latter developing what is now Grid Engine.

July 1 ? July 5, 2013, Helsinki, Finland     THURSDAY KEYNOTE    High Performance Fault Tolerance / Resilience at Extreme Scale     Franck Cappello Argonne National Laboratory  INRIA-Illinois Joint Laboratory on PetaScale Computing fci@lri.fr, cappello@illinois.edu     ABSTRACT In 2008-2009, many projections about extreme scale systems raised critical concerns about the applicability of existing fault tolerance approaches at Exascale. Five years later huge progresses have been made in many aspects of fault tolerance and resilience for HPC applications. Can we consider the problem as solved? Certainly not but we have serious reasons to be much more confident on the feasibility of Exascale systems and in particular on advanced fault tolerance techniques for them.

In this Keynote, we will first expose the main reasons that made the community thought that Exascale executions could not complete successfully. We will then review the progresses in several key areas of fault tolerance: Advanced checkpointing, fault tolerance protocols, failure prediction. We will show that despite huge improvements, there are still many attractive research problems. Whatever is the outcome of this research, progresses have already went so far that fault tolerance/resilience techniques for HPC in 2020 will be dramatically different compared to techniques commonly used in 2010!

SPEAKER BIOGRAPHY AND PHOTO  Franck Cappello is Program Manager and Senior Computer Scientist at Argonne National Laboraoty. Since 2009, he is co-director with Marc Snir of the INRIA-Illinois Joint-Laboratory on PetaScale Computing where he is also leading the Resilience/Fault Tolerance effort. He is leading the roadmaping effort on Resilience/Fault Tolerance for EESI2 (European Exascale Software Initiative) and led similar effort for IESP (International Exascale Software Project) and EESI1. He is the main PI of the G8 ECS (Enabling Climate Simulation at Exascale) project gathering researchers from USA, France, Germany, Japan, Canada and Spain with the objective of identifying scalability, performance and resilience solutions for running the CESM climate model at extreme scale.

