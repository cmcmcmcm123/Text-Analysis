Mining User Similarity Based on Users Trajectories  Wei Li

Abstract-In this paper, we first extract latent topics of users? checkin by topic model. In order to learn the geographic features of topics, we analyze the distribution and correlation of the venues of each topic.

Based on the assumption that each latent topic, to some extent, reflects some users? habits or behavior patterns in certain area, we use the latent topics to mining user similarity. When use the latent topics to mining user similarity, we first calculate the entropy of each topic, and then use it to analyze which topics will be salient when calculating user similarity. And we define two sets of topic. One is common topic set. The other is variability topic set. Subsequently, we propose two models to calculate user similarity. One is single-layer model. The other is two-layer model. Two-layer model is improved to single model. Finally, we use the proposed models to calculate user similarity in all topic set, common topic set and variability topic set. And then we verify our models by experiment.

Keywords-Location-based social networking; user similarity; topic modeling; check-in; geographic feature.

1. INTRODUCTION  Currently, with the rapid development of the fourth generation mobile communication networks, as well as map services and embedded GPS module interface supports for smart phone, making it easy to identify their location for mobile users which prompts more and more users use location-based social networks(LBSN)[6] and share their location information. By discovering or creating venues, user can checkin and leave tips or comments at where he visits. Many LBSN, such as Foursquare, Facebook Places and Sina WeiBo, etc, have taken different mechanism to attract users and encourage them to checkin and share.

Recently, some researchers have begun to focus more on the large amount of geo-spatially tagged data being produced from users? check-ins, for instance, approaching questions of societal- level interest in an entirely data-driven manner[6][7][8][9], finding patterns of human mobility[10][11][12], predicting friendship on social networking sites based on location data[13][14], and better understanding different aspects of cities[15][16].

In [7] and [17], authors use topic-modeling to get geographic topics by using users? check-ins. They have adopted the topic model Latent Dirichlet Allocation (LDA) for data analysis. In [17], authors extract latent topics by LDA and analyze their geographic  features. Experiment results show that some topics? venues are geographic close, and some are not. Meanwhile, authors also analyze the relationship of venues in each topic. Experiment results show that some topics? venues belong to the same type, such as school, but some topics? venues are not, such as mall, restaurants, etc. In [7], authors focus on getting the factors that drive user to checkin. By analysis the results, authors thought the factors that drive user to checkin mainly include interest factor, community factor and user type factor.

Based on the following two reasons, we use LDA topic-model to extract the latent topic from users? checkin records, and use the extracted topics to calculate user similarity. One reason is that when using LDA topic-model to extract topics, (1) no pre-defined topics; (2) extracted topics represent a meaningful probability distribution of words and document. The other is that extracted topics can be interpreted as users? habits or mobility patterns [7].

In this paper, we first analyze the extracted topics geographic features, as well as the relationships of venues under the topic. We believe that not every topic is suitable for calculating user similarity. Therefore, we adopted the information entropy of topic in each user to select appropriate topic set to calculate user similarity. We obtain two topic sets by topic entropy value. One is the top 10 topics of high entropy which we call it as variability topic set. The other is the top 10 topics of low entropy which we call it as common topic set. And then we use variability topic set, common topic set and all topic set to calculate user similarity.

Experiment results validate our conjecture:? not every topic is suitable for calculating user similarity?. Finally, we use nDCG[4](normalized discount cumulative gain) to evaluate the effectiveness of calculating user similarity method.

2. RELATED WORK  In the past decade, calculating user similarity has become an important research direction of information retrieval and recommendation system. By calculating user similarity, we can recommend friend and location, and predict activity, etc. Recently, several studies calculate user similarity by trajectory similarity [19] [20]. In [19], authors explore a geometric approach to trajectory similarity by exploiting three types of distance, that is: vertical distance, parallel distance and angular distance. While it limit to the geospatial realm, overlooking the types of activities and social information related to the activity location. In [22], authors focused on the spatial components of user trajectories. Their method employs hierarchical trajectory sequence matching to determine similar user.

When measuring user similarity based on geospatial aspects of user trajectories, we believe that an understanding of the   DOI 10.1109/CLOUDCOM-ASIA.2013.74    DOI 10.1109/CLOUDCOM-ASIA.2013.74     semantics of activity space is essential. In [23], authors investigate the concept of semantic annotations for venue categorization. In developing a semantic signature for a categorized place based on check-in behavior, similar, uncategorized places could be discovered. This concept of semantic signatures may also be applied to assessing user similarity through semantic trajectories.

In [18], authors measure semantic similarity between user trajectories in order to develop a friend recommendation system.

This work focuses on the type of activities completed by each user and the sequence in which these activities take place. Similarly, in [22], authors focus on stay cells and obtaining a semantic understanding of the types of activities conducted within the cells.

And semantic trajectory has become a measure of user similarity.

Recently, in [24], authors present a method for determining user similarity based on LBSN data. While authors also made use of checkin information, they concentrated on the hierarchy location categories supplied by Foursquare in conjunction with the frequency of check-ins to determine a measure of similarity.

By comparison, our approach is novel in that besides considering checkin information and frequency, we also extract the latent topics of users? checkin records and then calculate user similarity based on the extracted topics.

3. METHODOLOGY  In this section, we describe the data collection, topic extraction, and methodology used for developing our user similarity measures.

3.1 DATA SET  We first obtain 902 users? id in the surrounding of Beihang University by Sina Weibo API. And then we crawled all these users? checkin list until May 2013. Of these, 675 users have checkin records, that is, 72.828% of users have checkin records.

In our data set, there are 13769 check-ins and 4189 venues. We carried out the following processing of our data set: (1) remove those users whose check-ins is less than 5; (2) remove those venues that have less than 5 users to checkin; (3) also remove some interpret information of title. After the above processing, there are 415 users left, and 9639 check-ins and 3917 venues.

Sina Weibo defines a venue system hierarchy, involving 20 major categories including Sights, Auto Repair, Street Furniture, Science&Education&Culture, Finance&Insurance, Living Services and Food Services, etc.

From Figure 1, we can see that the check-ins in Science&Education&Culture and Business&residential categories are obviously higher than other categories. Figure 2 shows the distribution of users checkin, horizontal axis represents the number of users check-ins. From Figure 2, we can see that 90% users? check-ins is less than 50. The average user? check-ins is probably about 30.

Sig hts  Au to R  epa ir  Str eet  Fu rni  tur e  Sci enc  e& Ed  uca tion  &C ultu  re  Fin anc  e& Ins  ura nce  Liv ing  Se rvi  ces  Pub lic  Fac ilit  es  Au to S  erv ice  s  Ac com  mo dat  ion Se  rvi ces  Ca r S  ale s  Spo rts  Ser vic  es  Bu sin  ess &R  esi den  tial  Foo d S  erv ice  s  Me dic  al S erv  ice s  Ad dre  ss S erv  ice s  Go ver  me nt&  Co mm  uni ty  Co mp  any  Tra ffic  Se rvi  ces  Ma ll S  erv ice  s  Mo tor  cyc le S  erv ice  s       nu m  category  num  Figure 1. Check-ins distribution in 20 venues categories      -3  -2  -1   Distribution of Users Checkin Number  Users Checkin Number  P( X>  x)  Figure 2. Distribution of users check-ins  3.2 LDA MODEL  Based on the following two reasons, we use LDA topic-model to extract the latent topic from users? checkin records, and use the extracted topics to calculate user similarity. One reason is that when using LDA topic-model to extract topics, (1) no pre-defined topics; (2) extracted topics represent a meaningful probability distribution of words and document. The other is that extracted topics can be interpreted as users? habits or mobility patterns [7].

While numerous topic models are discussed in the literature, LDA is a state-of-the art generative probabilistic topic model that can be used to infer the latent topics in a large textual corpus in an unsupervised manner [2]. A topic is a multinomial distribution over terms, where the distribution describes the probabilities that a topic will generate a specific word. LDA models each document as a mixture of these topics based on a Dirichlet distribution.

Several mature implementations of LDA with improvements exist, we employ the JGibbsLDA version.

Figure 3. Graphic model represents of LDA[1]  The graphic model of LDA is shown in Figure 3. ? ?  and ? ?  are parameters of the Dirichlet prior on the per-document topic, respectively. m?  ? is the topic distribution for document m . k?  ? is  the word distribution for topic k . ,m nw is the thn word in  document m and ,m nz is the topic for ,m nw . And K is the number of topics, M is the number of documents and mN is the number of words of thm document.

We employ the LDA to identify topics in our data set. The basic unit is word in the LDA and, in this paper, the venue in a single checkin record represents a word. A user? trajectory consisting of all the venues of his check-ins represents a document, which is a set of words. Every user?s trajectory can be described as a mixture of the geographic topics that are essentially distributions over the venues. Therefore, the LDA can be applied to the mobility data in this way. In this paper, we set the number of topics 30K ? , 50 / K? ?  ? , 0.2? ? ?  . It is worth to note that why we set 30K ? . From Figure 3, we can see that most of users? checkin times is in the range 20-50, and the average probably is 30.

3.3 ANALYSIS OF TOPICS GEOGRAPHIC FEATURES  In this section, we will analyze the overall topics? geographic features. We will analyze the geographic features of each topic, i.e.

whether the venues of each topic are close to each other or not?

Whether there are any spatial relations among the venues in the same topic?

116 118 120 122     lantitude  lan titu  de  lontitude  Figure 4. (a) The venues latitude and longitude distribution of topic 1  111 114 117 120     lantitude  lan titu  de  lontitude  Figure 4. (b) the venues latitude and longitude distribution of topic 13  After analysis, geographic features of topics mainly have the following two cases: (1) there is an obvious concentrated area, that is, a considerable portion or substantially all venues in geographic space is close, but a small part of venues scatter in distant locations; (2) there are two or more concentrated area.

Figure 4(a) shows the venues latitude and longitude distribution of topic 1. From Figure 4(a), we can see that this topic belongs to the first case that is having an obvious concentrated area. And the Figure 4(b) shows the venues latitude and longitude distribution of topic 13, which belongs to the second case that is having two or more concentrated areas. Due to the space limit we only give two typical distribution diagrams of topic.

Belonging to the first case also has topic 2, 3,4,5,6,8,9,10,14,16,17,18,19,20,22,23,24,26,27,28,29. The rest topics are belonging to the second case. Generating the first distribution may be due to: many users concentrated in this region together to work or study, and occasionally, some users go to relatively distant places for sightseeing or working. While generating the second distribution may be due to: some users often travel or work from one place to another. By analysis the detail information of check-ins in the first case, it can be divided into the following three cases: (1) most venues in the topic are related to residential, such as, topic 1, 2, 6, etc. The reason for this situation might be the mutual residential relocation of people in these communities. (2) Most venues are related to school, residential areas, surrounding park, shopping mall, catering and hotels. This distribution is mainly reflected the activity patterns of students or residents in these communities. (3) Most venues are related to recreation places. In contrast, the second case is relatively easy to understand. Because the venues under this type are relative dispersion and have the obvious characteristics, that is, involving railway station, airport, bus stations, etc. We also can infer whether the user was for work or for travel according to the other venues? information.

Through the above analysis, we can get the following conclusions: (1) topics can be interpreted as users? common characteristics or lifestyle or habits; (2) each topic in some certain extent reflects the similarity of users in habits or interests.

3.4 ENTROPY OF TOPICS  We believe that not every topic is suitable for calculating user similarity. Therefore, we adopted the information entropy of topic in each user to select appropriate topic set to calculate user similarity. For any topic t, the probability of this topic belonging to document d is defined as P (t|d). So the topic?s entropy over all documents is defined as Equation 1.

( | ) 2( ) ( | ) log P t d  t T d D E t P t d  ? ?  ? ? 	 (1)     In [25], authors thought that when two objects are compared for similarity, the set of objects from which the two objects are selected has the effect of making some properties more or less salient in the similarity judgment. The properties that are more salient are termed to be more ?diagnostic?. Authors argued that two factors contribute to the diagnosticity of a property. The first is variability, which finds that the properties that vary across the elements of the context set are used more to determine the similarity (or dissimilarity) of two objects. The second factor commonality is the opposite that properties that are shared by most elements of the context set are the important properties, because they help explain what is important in the domain of discourse.

Based on the above ideas, we obtain two topic sets by topic entropy value. One is the top 10 topics of high entropy which we call it as variability topic set. The other is the top 10 topics of low entropy which we call it as common topic set. And then we use variability topic set, common topic set and all topic set to calculate user similarity.

3.5 SIMILARITY CALCULATION MODEL  In this section, we will describe the detail of the similarity calculation models. We specify two models. One is single-layer model. The other is two-layer model.

3.5.1 Single-layer model  Assuming T is the selected topic set. Then extract each user? weight vector Uf based on the selected topic set. And we will calculate the weight of user in a particular topic with Equation 2.

( ) ( )Tk k  Ti  U U k Tn  U i  NUM f T U  NUM ?  ?  ?   ( 2)  U represents a user. kT represents a topic. TkUNUM represents  the number of user U check-ins in the topic kT . ( )kTU?  represents the decay function of user U in the topic kT . The purpose of introducing decay function is to prevent that only consider the weight of topic but ignoring the quantitative of users? check-ins. After we get the users? topic weight vector, the similarity of two users ( , )u vU U is the comparison of these vectors using the cosine similarity and Pearson correlation coefficient.

cos ( , ) | || | u v  u v  U U ine u v  U U  f f SIM U U  f f ? (3)  2 2  ( )( ) ( , )  ( ) ( )  u u v vT Tk T k T k  u u v vT Tk T k T k k  U U U U T T  pearson u v  U U U U T T T T  NUM NUM NUM NUM SIM U U  NUM NUM NUM NUM ?  ? ?  ? ? ?  ? ?     (4)  uT UNUM and vTUNUM represents the average check-ins of  user uU and vU  3.5.2 Two-layer model  Two-layer model is the improvement of single-model, which calculates user similarity in fine-grained. Compared with single- layer, two-layer model first calculate the similarity of two users in each topic, and then sum to represents this two users? similarity.

During summation, different users will be different weighting each topic. This weight is based on the importance of topic to user.

Assuming 1 2{ , ,..., }k nT venue venue venue? is one topic.

nvenue represents a venue. Then extract each user? weight vector ( )U kf T in topic kT . And we will calculate the weight of user in a  venue with Equation 5.

( ) ( )venuek k  venuei  U U k venuen  U i  NUM f venue U  NUM ?  ?  ?   ( 5)  U represents a user. kvenue represents a venue. ( )kvenueU?  represents a decay function.

venuekU  NUM represents that the  checkin times of user U in kvenue . The purpose of introducing decay function is to prevent that only consider the weight of venue but ignoring the quantitative of users? check-ins. After we get the users? weight vector in a topic, the similarity in the topic of two users ( , )u vU U is the comparison of these vectors using the cosine similarity and Pearson correlation coefficient.

cos  ( ) ( ) _ ( , | )  | || |  u v  k  u v  U k U k venue T  ine u v k U U  f venue f venue T SIM U U T  f f ??   (6)  2 2  ( )( ) _ ( , | )  ( ) ( )  u u v vvenue venuei T i Tk k i  u u v vvenue venuei T i Tk k i i  U U U U venue T  pearson u v k  U U U U venue T venue T  NUM NUM NUM NUM T SIM U U T  NUM NUM NUM NUM ?  ? ?  ? ? ?  ? ?     (7)  Then the user similarity in the selected topic set is expressed Equation 8:  _ ( , ) _ ( , | ) ( | ) ( | ) k  u v u v k k u k v T T  T SIM U U T SIM U U T P T U P T U ?  ? 	 (8)  In Equation 8, we can use Equation 6 or Equation 7 to replace _ ( , | )u v kT SIM U U T . ( | )k uP T U represents the weight of topic kT for uU , and ( | )k vP T U  represents the weight of topic kT for vU  3.6 EVALUATION  Our approach is evaluated as an information retrieval problem.

According to Sina Weibo API, we can obtain users? real relationship matrix in our data set. So we can use normalized discounted cumulative gain (nDCG) [4] to evaluate the search results.

Since Sina Weibo API constraints, we obtain the relationship between the users? settings shown in Table 1.

Table 1. User relationship settings  Relevance level Relationship  2 Bidirectional  1 Unidirectional  0 No-Relation  For instance, when iU queries the top p users that with similarity, the similarity calculation model will be returned to  iU a list of users which size is p. According to the real relationship matrix we can get a relevance vector V. Then we can use nDCG to evaluate the search results. nDCG is used to compute the relative- to-the-ideal performance of information retrieval techniques. The discounted cumulative gain of V is computed as follows: (In our experiments, b=2)   ?  ?   ? ?   ? ?    log ib  V i if i  DCG i DCG i V i if i b  V i DCG i if i b  ? ? ? ? ? ??  ? ? ??  (9)  Given the ideal discounted cumulative gain IDCG, then  nDCG@p can be computed as ?  ?  @ DCG p  nDCG p IDCG p  ? . For example,  1,0, 2,1, 2V ?? ? , ?  ?  @5 0.7062  DCG nDCG  IDCG ? ? .

3.7 EXPRIMENTAL RESULTS  In this section, we mainly through the experiments to validate our proposed similarity calculation model effectiveness, stability.

First, we explain our experiments setup. And then we will show and discuss our results.

We conduct three experiments for each similarity calculation model. In the first experiment, we use all extracted topic set to calculate user similarity. In the second and third experiments, we use the common topic set and variability topic set to calculate user similarity, respectively.

Figure 5. The nDCG variation with the change of parameter p of single-layer model  First, we introduce the experiments for single-layer model.

Figure 5 shows the nDCG variation with the change of parameter p. The experiment has five groups, which uses p-value to group.

When the p-value equal to, we can observe the comparing results  of calculating user similarity by cosine and pearson. From Figure 5, we can see that the nDCG will decrease with the increment of p.

This explains, as more users to obtain, the model performance will decline. For each experiment, we can see that when using the common topic set and the variability topic set to calculate user similarity, the performance are better than using all extracted topic set. This may be that there is some noise for the extracted topic set by LDA model. In this paper, by selecting topics with topic entropy remove the noise topics. Experiments show that noise can be eliminated through the topic information entropy. In each experiment, we can find that the performance is basically the same by cosine and pearson. We also can see that when using cosine to calculate user similarity, the performance is slightly higher by using the common topic set than using the variability topic set, except p = 25. While using pearson to calculate user similarity, the performance is just opposite with using cosine. This explains that cosine is suitable to calculate similarity on the common topic set and pearson is suitable on variability topic set.

Two-layer model is the improvement of single-model, which calculates user similarity in fine-grained. Compared with single- layer, two-layer model first calculate the similarity of two users in each topic, and then sum to represents this two users? similarity.

During summation, different users will be different weighting each topic. This weight is based on the importance of topic to user.

Figure 6. The nDCG variation with the change of parameter p of two-layer model  From Figure 6, we can visually see that each group? nDCG value will decrease with the increment of p-value. However, compared with the single-model, the magnitude of reducing is smaller, which illustrates the two-layer model is more stable and faster convergence. For each group of experiments, When using cosine to calculate user similarity on the variability topic set, the performance is worst, and the other two topic sets are substantially uniform. When using pearson to calculate user similarity on the variability topic set, the performance is best. This explains when using two-layer model, different calculation methods may produce significant different results.

In summary, when calculating similarity using common topic set and all extracted topic set, the performance of two-layer model is higher than single-layer model. And with the increment of p- value, the two-layer model is more stable and faster convergence.

But when using the variability topic set the performance of two- layer model vary greatly.

4. CONCLUSIONS  In this paper, we propose a novel method to calculate user similarity. We first extract latent topic of users? checkin by topic model. In order to learn the geographic features of topics, we analyze the distribution and correlation of the venues of each     topic. Based on the assumption that each latent topic, to some extent, reflects some users? habits or behavior patterns in certain area, we use the latent topics to calculate user similarity. By analysis the geographic features of extracted topics, we verify the assumption that each latent topic, to some extent, reflects some users? habits or behavior patterns in certain area. And when using the extracted topic from LDA model to calculate user similarity, these topics are noisy which should be removed. In this paper, we use the information entropy of topic to remove noisy. And this approach is proved to be effective by experiments. Our approach is evaluated as an information retrieval problem. We find that when calculating similarity using common topic set and all extracted topic set, the performance of two-layer model is higher than single-layer model, and with the increment of p-value, the two-layer model is more stable and faster convergence. While using variability topic set, pearson outperforms.

In the future work, we will first expand our data set for further experiments. Meanwhile, we will be to consider the time factor. In this paper, we did not consider the time factor. At the same time, our real relationship is too sample, in the further work, we will enrich it. Also we will consider other ways to remove noisy of the extracted topics.

