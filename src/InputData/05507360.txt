A Tree-based Approach for Efficiently Mining  Approximate Frequent Itemsets *

Abstract-The strategies for mining frequent itemsets, which is the essential part of discovering association rules, have been widely studied over the last decade. In real-world datasets, it is  possible to discover multiple fragmented patterns but miss the  longer true patterns due to random noise and errors in the data.

Therefore, a number of methods have been proposed recently to  discover approximate frequent itemsets. However, a challenge of  providing an efficient algorithm for solving this problem is how  to avoid costly candidate generation and test. In this paper, an  algorithm, named FP-AFI (FP-tree based Approximate Frequent  Itemsets mining algorithm), is developed to discover approximate  frequent itemsets from a FP-tree-Iike structure. We define a  recursive function for getting the set of transactions which fault?  tolerant contain an itemset P. The patterns in the fault-tolerant supporting transactions of P are represented by the conditional AFP-trees of P. Moreover, to avoid re-constructing the tree structure in the mining process, two pseudo-projection  operations on AFP-trees are provided to obtain the conditional  AFP-trees of a candidate itemset systematically. Consequently,  the approximate support of a candidate itemset and the item  supports of each item in the candidate are obtained easily from  the conditional AFP-trees. Hence, the constrain test of a  candidate itemset is performed efficiently without additional  database scan. The experimental results show that the FP-AFI  algorithm performs much better than the FP-Apriori and the  AFI algorithms in efficiency especially when the size of data set is  large and the minimum threshold of approximate support is  small. Moreover, the execution time of FP-AFI is scalable even  when the error threshold parameters become large.

Keywords - Approximate frequent itemset mining; FP-tree structure.



I. INTRODUCTION  Among the various data mining applications, mining association rules is an important one [2]. The strategies for mining frequent itemsets, which is the essential part of discovering association rules, have been widely studied over the last decade such as the Apriori [ I], DHP[ll], and FP? growth [6]. In the traditional frequent itemsets mining algorithms, a strict definition of support is used to require every item in a frequent itemset occurring in each supporting transaction. However, in real-world datasets, it is possible to discover multiple fragmented patterns but miss the longer true patterns due to random noise and errors in the data .

? This work was partially supported by the R.O.C. N.S.C. under Contract No.

98- 222 1 -E-003-0 1 7  and NSC 98-263 1 -S-003-002.

Vi-Lang Tu Department of Computer Science and  Information Engineering National Taiwan Normal University  Taipei, Taiwan  Motivated by such considerations, the problem of mining approximate frequent item sets (or called fault-tolerant frequent itemsets) have been studied recently in many works [3] [4] [5] [8] [9] [lO] [12] [16]. In these approaches, it is allowed that a specified fraction of the items in an itemset can be missing when counting support of the itemset in a transaction database.

This problem was defined and solved in [12] by proposing the FT-Apriori algorithm. In this work, the number of errors allowed in a supporting transaction is a fixed constant value.

Besides, an item support threshold is used to enforce the minimum fraction that each item of the frequent itemset occurs in its supporting transactions. The FT-Apriori algorithm was extended from the Apriori algorithm, in which the downward closure property among the fault-tolerant frequent patterns is applicable. Similar to the Apriori-like algorithms, FT-Apriori algorithm suffered from generating a large number of candidates and repeatedly scanning database. Besides, this approach does not consider that the longer item sets should be allowed more missing items than the shorter ones.

To avoid repeated scans of the database, an algorithm, called VB-FT-Mine (Vector-Based Fault-Tolerant frequent itemsets Mining), was proposed in [8] for speeding up the process of fault-tolerant frequent patterns mining. In this approach, the fault-tolerant appearing vector was designed to represent the distribution that a candidate item set is contained in the data set with fault-tolerance. The VB-FT-Mine algorithm applies a depth-first pattern growing method to generate candidate itemsets. The fault-tolerant appearing vectors of the candidate itemsets are obtained systematically. Besides, whether a candidate is a fault-tolerant frequent itemset could be decided quickly by performing vector operations on the appearing vectors.

Yang et. al proposed two error tolerant models for discovering the error-tolerant itemsets (ETIs) [16]. For a strong ETI, the row error threshold was used to place constraints on the fraction of errors permitted that an error-tolerant itemset is contained in a supporting transaction. On the other hand, without setting the row error threshold, an itemset is named a weak ETI if the fraction of errors in the set of its supporting transactions is below a certain threshold.

The problem of approximate frequent itemsets was defined in [9], which combined the requirements of a strong ETI [16] and the item support threshold defined in [12]. In addition to the row error threshold, another parameter named column error threshold was used to set criteria on the fraction of errors allowed for each item of the approximate frequent itemsets in its supporting transactions.

According to the given row error threshold, the number of errors allowed for an item set in its supporting transaction is proportioned to the length of the itemset. Consequently, the anti-monotone property of support measure does not hold, which makes the construction of an efficient algorithm for discovering approximate frequent itemsets very challenging.

Moreover, [3] considered that a large number of uninteresting patterns, which seldom occurs together in reality, may be discovered as a result of the relaxed pattern mining. Not only slowing down the mining process, it is not easy for users to distinguish the interesting patterns from the uninteresting ones.

Therefore, a core pattern factor is defined in [3] to require that at least some transactions must contain all the items in an approximate frequent itemset. An algorithm called AC-Close was proposed to discover the approximate closed itemsets from the core patterns. The experimental results show that the AC? Close algorithm gets higher precision than the AFI algorithm when noisy data is introduced to the data set.

The frequent-pattern tree (FP-tree) proposed in [6] is an extended prefix-tree structure for storing compressed and crucial information in transactions about frequent itemsets.

Based on the tree structure, efficient algorithms [6] [ 15] were proposed for mining frequent itemsets However, the FP-tree structure was not applied to discover approximate frequent item sets in the previous works.

According to the discussion of the related works, the purpose of this paper is to develop an algorithm for mining approximate frequent itemsets from a FP-tree-like structure efficiently. To prevent from discovering the uninteresting patterns, in addition to the row error threshold, column error threshold, and the minimum threshold of the approximate support, our approach adopts the core pattern factor defined in [3] to be one of the constraints.

There are two noticeable advantages of the FP-tree based mining algorithms. First, the FP-tree is a highly compact database structure which is usually substantially smaller than the original dataset. When the size of database is large, the costly database scans in the mining process can be reduced effectively. Moreover, to avoid costly candidate generation and test, the FP-tree based mining algorithms perform pattern growing by successively concatenating frequent l-itemset found in the conditional FP-trees. This ensures that the unnecessary candidates which are not in the database are never generated.

In this paper, an algorithm, named FP-AFI (FP-tree based Approximate Frequent Item sets mining algorithm), is proposed to discover approximate frequent itemsets, which inherits the advantages of the FP-tree based mining algorithms. The FP? AFI algorithm applies a depth-first search strategy to generate candidates of approximate frequent itemsets. We define a recursive function for the set of transactions which fault-  tolerant contain an itemset P. In other words, if the set of transactions which fault-tolerant contain the prefix of P is known, the set of transactions which fault-tolerant contain P could be obtained from the defmed function. The patterns in the fault-tolerant supporting transactions of P are represented in one or more FP-tree-like structures, which are named the conditional AFP-trees of P. Moreover, to avoid re-constructing the tree structure in the mining process, two pseudo-projection operations on the AFP-trees are provided to obtain the conditional AFP-trees of a candidate itemset systematically.

Consequently, the approximate support of a candidate itemset and the item supports of each item in the candidate are obtained easily from the conditional AFP-trees. As a result, the constraint test of a candidate item set is performed efficiently without additional database scan.

In the experiments, the FP-AFI algorithm is compared with two related works: the FT-Apriori [6] and the AFI algorithms [2]. For getting the same result with the FP-AFI, these two algorithms are modified to include the pruning strategy according to the constraint of core pattern factor. The experimental results show that the FP-AFI algorithm performs much better than the FP-Apriori and the AFI algorithms in efficiency especially when the size of data set is large and the minimum threshold of approximate support is small. Moreover, the execution time of the FP-AFI algorithm is scalable even when the error threshold parameters become large.

This paper is organized as follows. The problem of approximate frequent itemsets mining is defined in Section 2.

The modified FP-tree structure, named AFP-tree, and the corresponding projection operations are introduced in Section 3. Section 4 gives the details of our proposed FP-AFI algorithm. The performance evaluation on the proposed algorithm and two related works is reported in Section 5.

Finally, Section 6 concludes this paper.

I I. PROBLEM DEFINITION Let I = {i 1, i2, ... , im} denote the set of items in a specific  application domain. A set of items is called an itemset; an itemset containing k items is called a k-itemset. Let TDB denote a database of transactions, where each transaction Ti in TDB is a non-empty subset of 1. For a given itemset P, we say that a transaction Ti contains itemset P if and only if Pr:;::;,Ti. The support count of an itemset P in TDB, denoted as sup(P), is the number of transactions in TDB containing P. Given a minimum support threshold min_sup E [0,1], an item set P is called a frequent itemset in TDB if sup(P) ? I TDB I X min_sup.

Definition 1 (Fault-tolerant contain): Let Cr and Cc denote the row error threshold and column error threshold, respectively, where Cr and Cc E [0, 1]. Given an itemset P and a transaction Ti = (i, Si), if there is a subset of Si, S', where S'r:;::;,P, and there does not exist a proper superset of S', S", such that S'c S"r:;::;,P, the transaction '0 is said to contain itemset P with ( IP I - IS' I) errors. Let f.1p denote the value of UPlxcrJ, which is called the maximum row error of P. '0 is said to fault-tolerant contain (abbreviated as FT-contain) P if and only if '0 contains itemset P with k errors and k ::; f.11"    Transaction ID Items Tl AF  T2 AB T3 ABCE  T4 ABCDE TS ACD T6 CDE T7 BCDE T8 ABCDEG  Figure I. The sample database TDB.

Definition 2 (Approximate support and Item support): Let T/{;.Lp) denote the set of transactions in a database TDB which FT-contain an itemset P. Besides, I T/{;.Lp) I denotes the number of members in T/{;.Lp). The approximate support of the itemset P, denoted as sUPa(P), is defined to be the number of transactions which FT-contain P; that is, supaCP) is equal to IT/{;.Lp)l. For each item x in the itemset P, the number of transactions in T/{;.Lp) which contain item x is called the item support of x, denoted as SUPilemP(X).

Definition 3 (Approximate frequent itemset): Given a core pattern factor a, a row error threshold Cn a column error threshold Cn and a minimum support threshold min?sup. An itemset P is called an approximate frequent itemset in the transaction database TDB iff 1) supa CP) ? ITDBlx min-sup ; 2) for each item x in P, sUPuemP(x) ? r suPaCP)x(l-cc)l ; and 3) sup(P) ? ITDBlx (min-sup x a) .

[Example 2.1] Fig. 1 shows an example of transaction database TDB. Let P denote the item set {A, B, C, 0, E}. Since the transactions T4 and T8 contain P, sup(P) is 2. We assume that Cr is 0.2, so the maximum row error of P, f.i  P , is 1. In other  words, when checking whether a transaction Ti FT-contains P, Ti must contain at least 4 items (1P1-LiPlxcrJ=s-LSx0.2J=4) in P.

Among the transactions in TDB, there are four transactions: T3, T4, T7, and T8 which FT-contain P. Consequently, T:( I) = {T3, T4, T7, T8} and SUPa(P) is 4. Among the members in T:(1), the item A is contained in T3, T4, and T8. Thus, sUPiteml'(A) is 3. Similarly, the item supports of the other items in P are sUPilemP(B)=4, sUPilemP(C)=4, SUPilemP(D)=3, and SUPitemP(E)=4. Suppose the parameters min-sup, cc, and a are set to be 0.3, O.S, and O.S, respectively. The itemset P is an approximate frequent itemset because it satisfies all the three requirements of Definition 3. On the other hand, let Q denote the itemset {B, C, 0, E, G}. In this case, /1Q is also 1. The transactions T4, T7, and T8 in TDB FT-contain Q; thus, SupaCQ) is 3. Although SupaCQ) is larger than ITDBlxmin-sup(=8xO.3), the itemset Q is not an approximate frequent itemset because the value of SUPitem Q(G), I, does not satisfy the second requirement of Definition 3. Furthermore, the itemset Q violates the third requirement in the definition because the value of Sup(Q), I, is less than ITDBlx(min-sup x a) .

Header Table item count link  A 6 ---  B 5  C 6  D 5  E 5  Figure 2. The constructed AFP-tree for the transaction database TDB.



III. TREE-BASED STORAGE STRUCTURE  A. Approximate Frequent Pattern Tree The frequent-pattern tree (FP-tree) proposed in [6] is an extended prefix-tree structure for storing compressed and crucial information in transactions about frequent itemsets. In our approach, the FP-tree is modified to support the mining of approximate frequent itemsets.

By following the FP-tree structure provided in [6], in addition to the children links, each node except the root node in our proposed FP-tree-like structure consists of three fields: item-name, count, and node-link, where item-name records the item represented by this node, count records the support counts of the patterns represented by the path reaching this node, and node-link links to the next node in the tree structure which represents the same item. Here, the root node of the tree structure is modified to include a field named count, which is used to record the total number of transactions maintained in the tree. Furthermore, the header table of the tree structure consists of three fields: (1) item-name, (2) count, which is used to accumulate the support count of the item, and (3) link, which is a pointer pointing to the first node of the linked list for chaining all the nodes with the item-name.

Because of the anti-monotone property of support, if an itemset is not a core pattern, any one of its supersets can never be a core pattern. Consequently, the FP-AFI algorithm first constructs the FP-tree-like structure in two scans of the database. In the first scan, the count for each item is accumulated. In the second scan, when constructing nodes into the tree structure, the items in a transaction with support counts less than ITDBlx(min-supxa) are ignored. Besides, the items in each transaction are sorted in lexicographically order. The constructed tree is called the approximate frequent pattern tree (abbreviated as AFP-tree) of the transaction database.

[Example 3.1] Suppose min-sup is set to be O.S and a is set to be O.S. Besides, the transaction database is shown as Fig. 1.

After the first scan, the items with their support counts are obtained as the following: A:6, B:S, C:6, D:S, E:S, F:l, and G:1.

Among the items, the supports count of items F and G are less than ITDBlx(min?supxa)=8xO.SxO.S=2. Therefore, items F and G in the transactions are ignored when constructing the AFP? tree. The constructed AFP-tree for the transaction database is shown as Fig. 2.

B. Approximate Support Computation Function  Let T/(i) denote the set of transactions in TDB which contain the itemset P with i errors. Besides, let j and k denote two distinct non-negative integers. If a transaction contains the itemset P with j errors, it is impossible that the transaction contains P with k errors simultaneously. In other words, for any itemset P, T/,(j) and T/(k) are disjoint if j is not equal to k.

According to this property, the approximate support of an itemset P in a transaction database TDB can be obtained by the following function:  (Approximate support of an itemset P 1  If /1P=O, supaCP) =IT/(O)I = sup(P) else if /1P = !P I, sUPa(P) = ITDBI  else SUPa(P)= IT:(/1p)I= IQ?p(i)l= ? 1?P(i)1 (1)  (2)  (3)  The challenge is how to get I T/(i) I for 0 ::; i ::; /1p. Let P denote an n-itemset consisting of more than one item, i.e.

P= {pj, P2, ... , Pn}, and P' denote the itemset consisting of the first n-J items in P. We call P' the (!p I-l)?refix of P. When a transaction contains the itemset P with i errors, where 1::; i::; !P I, it is possible that the transaction contains the (!P I- I  )?refix of P with (i-I) errors and does not contain {Pn}. Another possibility is that the transaction contains the ( IP I- I )?refix of P with i errors and contains {Pn}. Therefore, the recursive function for getting T/'(i), where 0::; i ::; !P I, is defined as follows:  (Recursive Function of T/'(i)l  Ifi::;!P1 T/(i) = (T/"(i-I)nTe{l'n}(1))u(T/'(i)nTe{l'n}(O)) (4)  for !P I> 1 and z>O;  = T/"(O) n Te{l'n}(O)  for !P I> 1 and i=O;  = TDB - T/(O)  for !P I= 1 and i= 1 ;  = T/(O)  for IP I= I and i=O; Otherwise, T/,(i) = cp.

(5)  (6)  (7)  [Example 3.2] Suppose t:r is set to be 0.5 in this example. Let P denote the itemset {A, B}. Hence, the value of /11' is 1.

According to the definition of the function for getting the approximate support of an itemset, sUPa(AB) is computed by performing the following equation:  supaCAB) = IT/B(O)I+IT/B(I)I (8)  = IT/(O)nT/(O)I+IT/(O)nT/(1)I+IT/(I)nT/(O)I. (9)  The first term I TeA(O)nTeB(O)1 in the equation indicates that the number of transactions which contain both items A and B.

In the transaction database shown in Fig. 1, I TeA (O)n T/(O) I = I {T2, T3, T4, T8}1=4. The second term means the number of  transactions which contains item A but do not contain item B, that is I {Tl, T5}1=2. Similarly, the third term represents the number of transactions which contains item B but do not contain item A, i.e. 1 {T7} I=1. Consequently, sUPa(AB) = 4+2+ 1 =7 is obtained.

C. Approximate Supports Counting from AFP- trees  In this section, according to the recursive function of T/'(i), we will introduce how to get I T/ (i) I for an item set P by constructing the required conditional AFP-trees systematically.

It is worth noticing that, when constructing an AFP-tree, the items in a transaction are sorted in lexicographically order.

Besides, the proposed FP-AFI algorithm adopts the prefix? based and depth-first approach to generate candidate itemsets.

That is, the algorithm will generate candidate itemsets starting from a specific item. If the item set P is an approximate frequent itemset, only the items whose lexicographically order are larger than the ones in P are appended to P to generate longer candidate itemsets. For reducing the storage requirement of the transactions in T/(i), when getting T/(i) for a pattern P, only the set of itemsets which follow P appearing in any transaction within T/(i) is maintained. It is called the conditional pattern base of P with error i and denoted as PB/'(i). According to the defmition of PB/(i), there is a one? to-one correspondence between the patterns in PB/(i) and T/'(i). Thus, IPB/'(i) I is equal to IT/(i)I.

In the constructed AFP-tree of the transaction database, all the itemsets in the transactions which contain item A are maintained in the paths starting from the node of item A.

Therefore, the set of patterns stored in the subtrees of the node of item A corresponding to PBeA(O). By constructing an AFP? tree to represent the patterns in PB/(O), as shown in Fig. 3, the tree is called the A-conditional AFP-tree.

In order to save both memory requirement and execution time, we adopt the concept of TD-FP-growth algorithm [15], in which it is not necessary to construct additional pattern bases and sub-trees. On the contrary, only the conditional header table and the root node of the conditional AFP-tree are constructed. We call such operation a pseudo projection.

Let P' denote the ( IP I- I )?refix of P and x denote the last item in P. Then the P-conditional AFP-tree is obtained by performing a projection on the P'-conditional AFP-tree with respect to x. The process of performing a projection on the P'? conditional AFP-tree with respect to x is as follows.

J) Perform a projection on the P'-conditional AFP-tree of with respect to x:  Look for the entry of x in the P '-conditional header table.

By following the link-list pointed by the Link field, all the nodes of x in the P' -conditional AFP-tree are reached. A root node Rx is constructed with Rx. count=O and Rx.children=null initially. For each node of x in the P '-conditional AFP-tree, denoted as Nx, the value of Nx. count is added into Rx. count; besides, all the children of Nx are inserted into the children links of Rx. In addition, a P-conditional Header Table is constructed by accumulating the count of each item and set the    A-conditional Header Table  Item B  C  D  E  Count Link  , ,  3 , , 3 , , Figure 3. The A-conditional AFP-tree.

A B-conditional Header Table  Item Count Link  C 3 -- -  D 2 - --  E 3 - --  ",- ... , , \ I ... '"  Figure 4. The AB-conditional AFP-tree.

Figure 5.

A C-colUlitional Header  Table ",- ...

I 4 "  -------)?- ? :  ? Item D E  Coulll Link  3 - - -  3 8? '- E:2 Figure 6. The AC-conditional AFP-tree.

link to point to the fIrst node with the corresponding item in the P-conditional AFP-tree.

[Example 3.3] According to the A-conditional AFP-tree shown in Fig. 3, after performing a projection with respect to B, the AB-conditional AFP-tree is shown as Fig. 4. According to the count value stored in the root node of AB-conditional AFP? tree, the value of [PBeAB(O)[ is obtained. Similarly, the AC? conditional AFP-tree is shown in Fig. 5. In the A-conditional AFP-tree, there are two nodes representing item C. Therefore, the patterns occurring with C are maintained in the sub-trees of the two nodes of item C. To avoid re-constructing the AC? conditional AFP-tree, the two sub-trees rooted from the nodes of item D are not merged. All the children nodes of the two nodes representing item C become the children of the new root node. Although the result is not the most compact form of the AC-conditional AFP-tree, both the computing and storage cost of constructing a new AFP-tree is saved.

To get the values of [PB/B( I)[, another pseudo projection operation named a complement projection is defIned. A complement projection on the P'-conditional AFP-tree with respect to x is to get the patterns which co-occur with P' but do  item  B  C  D  E  A conditional Header Table  count link  5 - - --  6 . ?  5 ? .

5 ? . .

.

.

,. --" ? 2 \ ;:J: ... ? __ ... , c:, \ I S:\ \ ... A'??"\;'  ..

,?  ......... -......

? E:1 D:1  .... .. ....

Figure 7. The A-conditional AFP-tree.

not occur with x from the P '-conditional AFP-tree. The result is denoted as the P' X -conditional AFP-tree.

For example, according to Fig. 2, after performing a complement projection on the AFP-tree with respect to A, we would get the patterns CDE and BCDE. The constructed A? conditional header table and AFP-tree is shown as Fig. 6. To avoid destroying the original node links, the root node and the fIrst node on each projected path is re-constructed as the nodes shown in dashed boundary. Moreover, the count fIeld in the root node keeps the number of transactions which don't contain A. The value of the count fIeld can be obtained by subtracting the count of A kept in the header table, 6 in this case, from the count stored in the root node of the FP-tree, 8 in this case. The process of performing a complement projection on the P '? conditional AFP-tree with respect to x is as follows.

2) Perform a complement projection on the P'-conditional FP-tree with respect to x:  A root node Rx' is constructed with Rx?.count=O and Rx,. children=null initially. Let Yb Y2, ... , Yk denote the items whose lexicographical order are larger than x and Yl< Y2<'" <Yk.

The Link fIeld of Yl in the P '-conditional header table is fetched to access each node of Yl. For each node of Yl on the P '? conditional AFP-tree, denoted as Ny}, the content of Ny} is copied into a new node, new ? Ny}, if there is not any ancestor of Ny} contains x. The node new ? Ny} is then assigned to be a child node of Rx" If there is more than one node of Y}, these newly generated nodes are linked via the node link. For item Yi, where 2-:;'i-:;'k, the node of Yi, denoted as Nyi, is also checked one by one. However, the same process as for Nyi is performed on Nyi only when none of the ancestors of Nyi contains Yj for l-:;' j< i.

Let m denote the count value stored in the root node of the P '? conditional AFP-tree and n denote the support count of x stored in the conditional header table of P '. Then Rx" count is set to be m-n. Similarly, a P' x -conditional Header Table is constructed by accumulating the count of each item and set the link points to the fIrst node with the corresponding item in the P' x - conditional AFP-tree.

[Example 3.4[ For getting the value of [PB/B(l)[, a complement projection on A-conditional AFP-tree with respect to B is performed. The obtained AS -conditional AFP-tree is shown in Fig. 7. Besides, to perform a projection on the A? conditional AFP-tree with respect to B will get the AB-    A B -conditiol/al Header Table  Item COllnt Link  C I - --  D I ---  Figure 8. The AB-conditional AFP-tree.

AB -conditional Header Table  Item Count Link  C I - -  D I - --  E I - - - .

Figure 9. The AB - conditional AFP-tree.

conditional AFP-tree as shown in Fig. 8. Accordingly, when lOr is set to be 0.5, sUPa(AB) is computed by add IPB/B(O)I and IPB/B(l)I, which can be obtained from the roots of the AB? conditional AFP-tree, and the sum of the values of the root nodes in the AB -conditional AFP-tree and As -conditional AFP-tree, respectively. Therefore, the conditional AFP-trees of the candidate itemset AB include these three AFP-trees.



IV. THE FP-AFI ALGORITHM  A. Storage Structure for Candidate Itemsets  As the example shown in [Example 3.4], the approximate support of a candidate itemset can be obtained from one or more conditional AFP-trees. Consequently, during the mining process of the FP-AFI algorithm, a storage structure is constructed for a candidate item set P to store the required conditional header tables and the root nodes of the conditional AFP-trees which are used to get the approximate support of P.

The storage structure of a candidate itemset P consists of the following five fields:  1) Itemset: it stores the candidate item set P = {P], P2, ... , Pn}; 2) AFP-tree list: it maintains the list of root nodes of the  required conditional AFP-trees of P; 3) HTable list: it keeps the header table for each conditional  AFP-tree in the AFP-tree list; 4) id vector list: for each conditional AFP-tree in the AFP-tree  list, a binary vector with length IPI is assigned to identify the tree, where the ith bit is set to be I if the conditional AFP-tree is generated from a projection with respect to Pi; otherwise, the conditional AFP-tree is generated from a complement projection with respect to Pi and the ith bit is set to be O.

5) Parent: it points to the storage structure of the (IPI-I) prefix of P.

The storage structure of an itemset P is maintained only when it is used to generate longer candidate itemsets. After the  Itemset AB A F T-tree list [0] m. [2]  H Table list  id vector list  parent  , - -- - , , , ... - -- ...

, 4 , ,- 2 " 1 "  G ? ? G? 0 : 1 0:1 E:I 1. :2 I C I-  EffiH I C  e 3 - e I _ e I D 2 - D I - D I E 3 - E I  <1, 1> <1,0> <0,1> points to the storage structure of A  Figure 1 0. The storage structure of itemset AB.

l- -.

-?  -.

test on all the candidate itemsets with P as their prefix is fmished, the storage structure of P will be deleted.

1 Example 4.11 To continue the running example, when lOr is set to be 0.5, the maximum row error of the candidate item set AS is I. Therefore, the storage structure of AB is constructed as Fig. 9, where the AB-conditional AFP-tree identified by <1,1> represents ..!.he patterns in PB/B (0). The All-conditional AFP? tree andAB -conditional AFP-tree are identified by <1,0> and <0,1>, respectively; these two conditional AFP-trees jointly represent the patterns in P B/B(l). Let i denote the number of bits with value 0 in the id vector of a conditional AFP-tree. It is indicated that the corresponding transactions in the tree contain the candidate item set with i errors. Moreover, the lost items which cause the errors are implied by the positions of the bits with value O. As a result, for the items in the candidate itemset AB, their item supports can be obtained easily by the following formula: <1,1 >x4+<1 ,0>x2+<0, 1 >x 1 =<6,6>, where each term in the summation corresponds to the multiplication of the count value in the root node and the id vector of a conditional AFP? tree in the AFP-tree list of AB. The result shows that both the SUPilem AB(A) and SUPilemAB(B) are 6.

B. Constructing the Conditional A FP-Trees Systematically  In the proposed FP-AFI algorithm, if an approximate frequent item set P' is discovered. The next candidate item set P is generated by appending an item x to P, where x is in the P '? conditional header table with count ? ITDBI X min _sup X a. It is indicated that P is a core pattern. Then, for checking the other requirements of an approximate frequent itemset, the conditional AFP-Trees of P are constructed from the ones of P , as the following.

When the maximum row error of P is equal to the one of P , (;.ip=J.1p'), case I is performed; otherwise, case 2 is performed.

? <case 1>: For computing supaCP), according to equation (3), we have to get I T/,O) I for i = 0 to f../p. The conditional AFP-trees corresponding to PB/'(i) (0 $; i$; f../p.) have been constructed in the storage structure of P '. According to the recursive function defined in    equation (4), the conditional AFP-trees corresponding to PB/'(i) are constructed by performing a projection on the conditional AFP-trees corresponding to PB/'(i) with respect to Pm as well as a complement projection on the conditional AFP-trees corresponding to PB/'(i- 1) with respect to Pn.

? <case 2>: In this case, the maximum row error of P is 1 larger than the one of P '(Jip=f.1p ,+ 1). Since only ?he conditional AFP-trees corresponding to P B/ (i) (O?i?f.1p) were available for computing supaCPj, the conditional AFP-trees corresponding to PB/'(;ip) was not constructed previously. In order to get the conditional AFP-trees corresponding to PB/'(;ip) , we have to go back to the storage structure of the prefix Pk of P with IPkl =/lpfor getting PB/,k(;ip). Let PI denote the (IPkl-l)?refix of Pk and y denote the last item in Pk. The conditional AFP-trees corresponding to PB/k{Jip) are obtained by performing a complement projection on the conditional AFP-trees corresponding to PBt{Jip - 1 ) with respect to y. We don't have to perform a projection on the conditional AFP-trees corresponding to PB/I(;ip) with respect to y because P B/,I(;ip ) must be an empty set when I PI I is less than /lp. The similar process goes forward until the conditional AFP-trees corresponding to PB/'(;ip) is generated. Finally, the conditional AFP-trees corresponding to PB/'(i) (i=0, . . .  , /lp) are constructed by performing the same operations described in <case 1>.

According to the conditional AFP-trees constructed in the storage structure of P, SupaCP) is obtained by adding the count values in these conditional AFP-trees. The item support of each item Pi in P is also obtained easily from the conditional AFP? trees corresponding to P. For each conditional AFP-tree corresponding to P, suppose the count value stored in the root node is c and its id vector is <bI, b], ... , bn>; consequently, the frequency of item Pi appearing in this tree is bi x c. By summarizing the frequencies of Pi in all the conditional AFP? trees of P, SUPiteml'(Pi) is obtained.

C. FP-AFI Algorithm  The FP-AFI algorithm for mining approximate frequent item sets using AFP-tree is proposed as the following.

Algorithm FP-AFI: Input: A transaction database TDB, a core pattern factor a, a  row error threshold t:" a column error threshold t:c, and a minimum support threshold min_sup.

Output: The complete set of approximate frequent itemsets.

begin Initialization(P};  Pattern ?rowth(P};  end.

Transaction ID Items Tl AE T2 AB T3 ABC T4 ABCD T5 BCD  Figure Il. Sample database TDB2.

Item COl/lit Link  A 4 '--  B 4 , - -  C 3 \ , -  D 2 \ , -  Figure 12. The constructed AFP-tree of TDB2.

Procedure Initialization(P} (1) {Scan TDB to accumulate the support count of each item; (2) The items with support count less than ITDBlx min_supxa  are ignored; (3) Scan TDB to construct the AFP-tree and Header Table of  TDB; (4) P' =?;}  Procedure Pattern _growth(P ') (1) { For each item x in P '-conditional header table  (2) Ifx.count ? ITDBlxmin_supxa (3) then {P = P'u {x}; (4) Construct the conditional AFP-trees required by P; (5) Get SupaCP) from the conditional AFP-trees; (6) If SUPa(P) ? ITDBlx min-sup (7) then {Get SUPiten/' (P;) for each item Pi in P; (8) If (each item Pi in P  SUPitemP(Pi) ? IsUPaCP)X(I-t:c)l) (9) then {output(P); (10) P'=P; (11) Pattern_growth(P};} (12) } (13) (14)  IExample 4.21 The transaction database TDB2 shown in Fig.

lO is used to explain the mining process of the FP-AFI algorithm. Here, min _sup is set to be 0.8, both t:" t:c are set to be 0.5, and ais set to be 0.4.

First, the item E is ignored because its support does not satisfy the requirement of the given core pattern factor. The constructed AFP-tree of TDB2 is shown as Fig. 11. From the header table of the AFP-tree, the candidate itemset P={A} is    A  <1> "".-  -  -  ...

... , \ , 4 , \  ,  <1,1>  , ,- -- ... ,  ? 3 ? ,  ?J b? , \  "  B  ?  <1,0> ,--, ...

I \ ... I  ..... _-;  <0,1> ,-  - - .... , , \ , I  Figure 13. The constructing process of the required conditional APP-trees of AB  ... - -- ....

, , , 3 \ \ ,  ? 0 : 1 <1,0>  ,--, ...

...

.... _-;  <0,1>  \ I  C ?J c  ?  , ... -  -- ......

'? b  (0  ,- -- ... , ? \ , 2 , \  \ \  \ \ G  <1,1,0> \ ....

, , --? SUPa(ABC) = 4 , \ , \  \ I  I I  I <0 1 1> I ,?-'- ... I , 1 <\  G  l <1,1,1>*2+ <1,1,0>*1+ <0,1,1>*1 <3,4,3>  ABC(A)-" SUPilem -.) SUPilemABC(S)=4  ABC(C)_" SUPilem -.)  Figure 14. The constructing process of the required conditional APP-trees of ABC.

<1> ... -  --  -  , , \ , 4 , \ ,  <0> ...... -- ....

, , , \ \ I , ,  <1,1> C ,  ,?  -  - -  , ?J' 2 ?  ?  C  y  \ ,  G <10>  ,--, ...

2 \ I ...

.... _-""  <0,1> ,-  -  -  -  , ,  , \ I  e  \ \  \ \  \ ....

--? SUPa(AC) = 5 ?  I <1,1>*2+ I <1,0>*2+  I <0,1>*1 I <3,3>  SUPilemAC(A)=3 SUPilemAC(C)=3  Figure 15. The constructing process of the required conditional AFP-trees of AC.

generated with f./p=O. Consequently, by performing a projection on A from the original AFP-tree, the A-conditional AFP-tree, whose id vector is <1>, is obtained as shown in Fig.

12. According to the count value stored in the root node of the A-conditional AFP-tree, sUPa(A)=ITeA(O)I= IPBeA(0)1=4; A is an approximate frequent itemset. In the A-conditional header table, B is the first item with support larger than or equal to ITDB2Ixa; so P is set to be the next generated candidate item set {A, B}. The maximum row error allowed by {A, B} is different from the error allowed by {A}. Thus, the <case 2> for constructing the conditional AFP-Trees of the candidate item set is performed. In order to get supiAB) I T/R(O)I+IT/R(l)1 = IPB/R(O)I+IPB/R(1)I, a complement projection on the original AFP-tree with respect to A is performed to get the conditional AFP-tree corresponding to pBeA (1) which is identified by vector <0> in Fig. 12. From the conditional AFP-tree identified by vector <1>, a projection with respect on B is performed to get the AFP-tree identified by vector <1,1> which corresponds to PBeAB(O). On the other side, the conditional AFP-trees with vectors <1,0> and <0,1>, which correspond to PB/R(1), are obtained from the AFP? trees with vectors <I> and <0> by performing a complement projection and a projection with respect to B, respectively. By adding the count values in the root nodes of these three AFP? trees, the value of supiAB), 5, is obtained. By summarizing the scalar product of the id vector and the count value stored in the root node of the three conditional AFP-trees: <1,I>x3 +<I,O>x l+<O,I>x 1 =<4,4>, SUPitem AB(A)=4 and SUPitem AB(S)=4 are obtained. Consequently, AS is discovered to be an approximate frequent itemset.

Next, P' is set to be AB and the Pattern?rowthO procedure is performed recursively. Item C is the first item in the conditional header table of AB with support larger than or equal to ITDB2Ixa. Therefore, the next generated candidate itemset P is {A, B, C}. As shown in Fig. 13, the conditional AFP-trees required by candidate item set {A, B, C} are constructed from the ones corresponding to {A, B}. The conditional AFP-trees with id vector < 1,1,1>, as shown in Fig.

13, corresponds to PB/BC(O). Moreover, the conditional AFP? trees with id vectors <1,1,0> and <0,1,1> corresponds to PB/Bc(I). According to the obtained result, ABC is an approximate frequent itemset.

When trying to generate a longer candidate from item set ABC, D is the only item in the conditional header table of ABC.

However, the support of D in the conditional header table of ABC is less than ITDB2Ixa. As a result, the process returns back to generate another longer pattern from AB. Since the support of D in the conditional header table of AB is less than ITDB2Ixa, the process returns back to append item C to pattern A for generating candidate item set AC. The conditional AFP? trees correspond to PBeAC(O) and PBeAC(l) are constructed as shown in Fig. 14. According to the obtained value of supaCAC), that is 5, AC is an approximate frequent itemset.

Since the supports of D in the conditional header table of AC and A are both less than ITDB2Ixa, the process of discovering the approximate frequent itemsets starting with prefix A is finished. The same process is performed repeatedly to discover the approximate frequent itemsets starting with other items.



V. PERFORMANCE EVALUATION A systematic study is performed to evaluate the performance  efficiency of the FP-AFI algorithm. In the experiments, the FP? AFI algorithm is compared with two related works: the FT? Apriori [6] and AFI algorithm[2]. The FT-Apriori[6] and AFI algorithms [2] were proposed previously for mining the approximate frequent itemsets, which do not need to satisty the requirement of core patterns. For getting the same result with the FP-AFI algorithm, these two algorithms are modified to include the pruning strategy according to the threshold value of a core pattern factor. All of these algorithms were implemented using Microsoft Visual C++ 6.0. The experiments have been performed on a 3.40Hz Intel Pentium IV machine with 20 megabytes main memory and running Microsoft xp Professional. Moreover, the data sets are generated from the IBM data generator [1].

In each experiment, one of the setting among the given threshold values at run time and the parameters of the data generator is varied to observe the changing trend of execution time. A dataset T10150D20K [1], which is generated with 20K transactions, 50 distinct items and an average of 10 items per transaction, is used in the first part of experiments.

Fig. 15(a) shows the running time of the three algorithms by varying min_sup with cr=O.2, cc=0.5, and a=0.8. As shown in the figure, FP _ AFI runs much faster than both AFI and FP? Apriori especially when min_sup is smaller. As minJup decreases, more candidates are generated in the process of approximate item set mining such that the computation time  increases. Nevertheless, the execution time of the FP-AFI algorithm is shown to be very efficient when min _sup is set to as low as 0.01, which is about 1113 of the ones of the AFI and FT-Apriori. The reason is that the FP-AFI avoids illustrating the candidate itemsets exhaustively.

Fig. 15(b) presents the run-time performance of the algorithms by varying the core pattern factor u with min_sup =0.02, t;.=0.2, and Ec=0.5. The number of the set of core patterns becomes larger as u decreases. Therefore, more candidates are generated in the process of approximate frequent item sets mining; the execution time also increases.

Nevertheless, the FP-AFI algorithm is shown to be much more efficient than the other two algorithms when a is set to as low as 0.3. When u becomes larger, the number of candidates satistying the constraint of the core pattern factor a will reduce. By adopting the anti-monotone property of support to prune candidate itemsets, FT-Apriori is better than AFI when ais set to as high as 0.5.

We also tested the scalability of the algorithms as the parameter t;. is varied with minJup =0.02, u=0.8, and Ec=0.5.

The result is shown in Fig. 15( c). When the setting of t;.

becomes larger, a larger maximum row error is allowed when checking whether a transaction FT -contains a transaction.

Accordingly, more approximate frequent item sets are discovered. As shown in Fig. 15(c), the running time of AFI increases very quickly as lOr increases while the efficiency of both the FT -Apriori and FP-AFI is not affected much by t;..

When the maximum row error changes frequently as the length of candidate itemsets glow, more cost is required in the FP-AFI to perform the back-and-forward process for constructing the required conditional AFP-trees. That is the reason why the execution time of the FP-AFI increases slightly from cr =0.3. However, the running time of the FP? AFI is still less than FT-Apriori when Cr is set to as high as 0.6.

The running time of the three algorithms by varying tc with min_sup =0.02, u=0.8, and tr=O.2 is shown in Fig. 15(d). The result indicates that the effect of changing tc on the running time of the three algorithms is nearly unnoticed.

On the other hand, the scalability tests by varying the size and the number of distinct items in the transaction database are performed. With 50 distinct items and an average of 10 items per transaction when generating the transaction dataset, Fig.

15( e) shows the running time of the three algorithms by varying the number of transactions, where min_sup =0.02, Cr =0.2, Cc = 0.5 and a= 0.8 are used at rum time. All the running time of the three algorithms increases as the number of transactions increases. The running time of the FT -Apriori algorithm increase most significantly because the FT-Apriori suffers from the disadvantage of repeatedly scanning the data set. Although it is unnecessary for the AFI to scan the database repeatedly, the cost of performing the union or intersection operations on the transaction id lists also increases. The reason is that the transaction id lists used in the AFI algorithm will become longer as the number of transactions increase. On the other hand, the AFP-tree structure provides a compact representation of the transaction database which helps to    Varying min_sup  :0-c: 1000 0 --+- FP-AFJ u OJ ? 800 _AFJ Il.) E - 600 -+- FT-Apriori OJ)  .= c: 400 c: :I ....

2 3 4 5 6  min_sup(%)  (a)  Varying a 2S00  :0-c: ? 2000  --+-FP-AFJ ? OJ .,? ISOO _AFJ OJ) -+- FT-Apriori .= c: 1000 c: :I ....

SOO   0.3 0.4 O.S 0.6 0.7 0.8 0.9  a value  (b)  Varying Gr  :0-c: 0 2500 u OJ ?  2000 OJ E - OJ) ISOO c:  ?c I:: 1000 :I ....

-- --+-FP-AFJ p-  I _AFJ I -+- FT-Apriori I  J  .-.-  o 0.1 0.2 0.3 0.4 O.S 0.6 c,. value  (c)   :0- ? 400 u OJ '" '[ 300 - OJ) .? 200 c: :I ....

,--, 800 "0 c: 700 0 u ? 600 Il.) E 500 -  . w 400 c: c: 300 :I ....

:g 2S00 o u OJ  ? 2000 OJ E .? 1500 c:  ?c I:: 1000   o   VaryingGc  ? ? ? ? ?  .Ai ? ? ? ..

? ? ? ? ?  o 0.1 0.2 0.3 0.4 Cc value  (d)  Varying databse size  L /  ? / .......

? ?  ?? -+  S 10 15 20 2S  number of transactions(K)  (e)  Varying number of items  10 SO 100 ISO 200 2S0 300  number of items  (I)  --+- FP-AFJ _AFJ  -+- FT-Apriori  --+- FP-AFJ _AFJ  -+- FT-Apriori  --+-FP-AFJ  _AFJ  -+- FT -Apriori  Figure 16. The results of performance evaluation.

efficiently generate candidate itemsets and perform approximate support counting. Therefore, the execution efficiency of the FP-AFI is much better than the other two algorithms when the size of the database is set to as high as 25K.

With an average of 10 items per transaction, the number of distinct items in the database is varied to generate 10 transaction databases of size 20K. The running time of the three algorithms is shown in Fig. 15(f). When the number of distinct items in the database increases, the database becomes sparse. The case is good for the AFI. Therefore, the AFI becomes the most efficient one when the number of distinct items is set to be 250. For the FP-AFI algorithm, the constructed AFP-tree becomes bushier when the number of distinct items in the database increases. Consequently, the performance of FP-AFI degrades slightly because it requires more time to traverse the AFP-tree and construct the conditional AFP-trees. However, the running time of the FP? AFI keeps comparable with the AFI when number of distinct items is set as high as 300.



VI. CONCLUSION  In this paper, the FP-AFI algorithm is developed to discover approximate frequent item sets which inherits the advantages of the FP-tree based mining algorithms. We extended the FP-tree structure to be the AFP-tree for storing compressed and crucial information in transactions about approximate frequent itemsets. By deriving the recursive relationship between the set of supporting transactions of an itemset and the one of its prefix, two pseudo-projection operations on AFP-trees are provided to obtain the conditional AFP-trees of an item set systematically. Consequently, the approximate support of a candidate itemset and the item supports of each item in the candidate are obtained easily from the conditional AFP-trees. As a result, the constraint test of a candidate itemset is performed efficiently without additional database scan. The experimental results show that the FP-AFI algorithm performs much better than the FP-Apriori and AFI algorithms in efficiency especially when the size of data set is large and the minimum threshold of approximate support is small. Moreover, the execution time of FP-AFI algorithm is scalable even when the error threshold parameters become large.

The FP-tree-like structure is suitable for maintaining the transactions within a sliding window in a data stream environment. Therefore, to apply the FP-AFI algorithm for mining recently approximate frequent itemsets in data stream provides a good solution for this problem, which is under our investigation currently.

