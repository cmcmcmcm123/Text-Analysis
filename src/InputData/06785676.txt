The Role of Network Analysis and Mining in the Era of Big Data: Panel Summary

Abstract?This paper reports the conclusions of the panel on ?The role of network analysis and mining in the era of big data? held during the 2013 ASONAM conference. The panel was charged with the responsibility of determining one or two big open problems that require a significant investment of time, effort, and resources over a five year period.



I. EXECUTIVE SUMMARY  During the 2013 ASONAM conference (Niagara Falls, Canada, August 2013), the author was charged with the task of holding a panel discussion on the role of network analysis and mining during the era of big data. This brief paper summarizes the final conclusions of the panel.



II. CHARGE TO THE PANEL  The panel was directed to answer a single question. As social media proliferates throughout the world (over 400M tweets per day, over 4.5B likes per day on Facebook, over 100 hours of video uploaded per minute on YouTune), there is an ever-growing interest in leveraging this rich treasure trove of user-provided data for commercial, military, and government purposes. For instance, these channels were used during the recent Arab Spring revolutions that overthrew dictators in Tunisia and Libra. These same channels are used by marketers to understand who likes or dislikes their products and why, while political analysts use this same data to identify how well political candidates are doing and which issues resonate with voters. Government agencies watch these social media channels to identify planned protests, sympathizers of terrorist groups, and other planned criminal activity.

The charge to the panel was simple: Identify one or two major roadblocks that prevent us from effectively analyzing this enormous quantity of big data and leveraging it as well as we could. What single roadblocks remain from a technological perspective in enabling a wide range of applications that could benefit from this data? We asked panelists to identify major scientific problems ? those that will, in all likelihood, take a community of researchers a total of approximatelt 5 years to solve assuming a strong and directed effort to fund such research programs.

In order to answer this question, the panel was asked to deliberate on the following questions:  1) What applications of network analysis and data min- ing have you seen applied to ?big data? style prob- lems in business and government?

2) In such applications, how did the organizations col- lect the ?big data? ? Did they have them already? Or did they have to pro-actively collect them?

3) What kinds of benefits did these organizations get from network analysis and mining of?big data??

How did they establish a value proposition for their customers and management?

4) Are there cases in which network analysis and data mining did not deliver stellar results ? and if so, why?

5) What are organizations looking for in terms of net- work analysis and data mining that current technol- ogy falls short on? What are the critical obstacles to be overcome?

The panel consisted of the following individuals:  1) Prof. Kathleen Carley (Carnegie Mellon University) 2) Prof. Christos Faloutsos (Carnegie Mellon Univer-  sity) 3) Prof. Huan Liu (Arizona State University) 4) Prof. Jaideep Srivastava (University of Minnesota) 5) Dr. Mohammed Rifaie (Royal Bank of Canada)

III. PRINCIPAL RECOMMENDATION  The principal recommendation of the panel was to establish targeted, well funded research programs to answer the Identify Fact or Fiction (IFF) problem.

As the world becomes awash in ever-increasing quantities of data generated by crowds, there is a huge potential for the world to also become awash with:  1) Inadvertently erroneous data. For instance, ac- cording to NBC News, false rumors on Wikipedia included ridiculous statements saying a recent British Prime Minister loved Hitler, and a famour living US senator had died 1. While these comments did not cause grave harm, they can be expected to com- promise the integrity of applications that use social media.

2) Deliberately false and misleading data. For in- stance, in 2011, a false rumor that a leading CNN personality was suspended lit up Twitter. In 2013, a hacked Associated Press account was used to spread the deliberately false rumor that an explosion had  1http://www.nbcnews.com/id/32588168/ns/technology and science-tech and gadgets/t/biggest-wikipedia-blunders/#.Ujber3ev-68  myday Text Box xli    rocked the White House, leading to huge amounts of panicked financial transactions in world stock markets.

The Identify Fact or Fiction (IFF for short) problem is one that performs the following steps:  1) Gather relevant data. Given a proposition p (e.g.

?Person X was suspended today by CNN?), the IFF problem would first identify all data Data(p) about the proposition p available through the big data collections (e.g. Facebook, Twitter) available.

2) Assemble evidence. From the relevant data about p, two pieces of evidence would be assembled: evidence supporting p and evidence invalidating p. Appropriate structures would need to be developed in order to identify, store, and manipulate such evidence.

3) Assess evidence. The evidence would need to be assessed in order to understand the strength and weakness of each body of evidence and then arrive at a probability that the proposition p in question is correct.

In order to execute these steps, a huge amount of re- search would be needed in social network analysis, in natural language processing and machine translation, in sentiment analysis, in database management systems, in artificial intelli- gence, and all of these pieces would need to come together in a seamless ?whole? in order to address such problems effectively.

The panel accordingly recommended that the IFF problem be made an urgent priority for research and development, both within academia and industry. The panel further recommends that appropriate national funding agencies fund such an effort through the creation of targeted research programs.

