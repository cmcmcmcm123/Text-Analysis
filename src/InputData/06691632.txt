Modeling Heterogeneous Time Series Dynamics to Profile Big Sensor Data in

Abstract?While a massive amount of time series can now be collected in many physical systems, it is a challenge to build an analytic model that can correctly profile the data because those time series usually exhibit various behaviors. In this paper we propose an integrated method to address the heterogeneity issue in modeling big time series data. We first extracts relevant features to summarize the underlying dynamics of those series. We present both linear and nonlinear feature extraction techniques, as well as a procedure to determine the right extraction method for individual time series. Given extracted features, our method further models the trajectory pattern of time series in the feature space. Both a regression based and a density based method are presented to profile different types of feature trajectories. Experimental results in a real power plant illustrate that our feature extraction and trajectory model are effective to profile various time series.

Our method has been used to successfully detect anomalies in  the system.

Keywords-Time series, Trajectory model, Anomaly detection

I. INTRODUCTION  With the decreased hardware cost and increased sensor  capability, traditional physical systems undergo revolution-  ary changes recently in their computing, communication and  storage capabilities. They are now equipped with a large  network of sensors distributed across different parts of the  system, which leads to a tremendous amount of time series  data available to system operators. It is important to build  effective models to profile those time series so that we  can better understand the underlying dynamics of system  operation and hence monitor its status to detect anomalies.

However, different time series in the collected data usually  demonstrate totally different behaviors due to the diversities  in system components and their functionalities. Figure 1  presents some examples of time series collected from a  real power plant system. As we can see, they are dramat-  ically different in terms of shape, trend, seasonal variation  and periodicity. Some series exhibit deterministic periodic  behaviors, whereas others show irregular curves in their  evolutions. While there already exist a number of time  series modeling techniques in the literature, current methods  This work was performed while the first author was an intern at NEC Laboratories America, Princeton, NJ.

usually rely on certain assumptions, and are limited for  the category of time series that follow those assumptions.

For example, the commonly used autoregressive(AR) model  [1] assumes that current observation in time series linearly  depends on its previous observations. While the support  vector regression(SVR) [2] has been proposed to handle  nonlinear behaviors in time series, it requires the knowledge  of correct window size to determine the number of predictors  in the regression. In addition, some time series may fit poorly  in regression based models, and can only be profiled by  nonparametric techniques such as density based methods [3].

Given a massive number of heterogeneous time series, it is  necessary to have a more general method to treat different  time series differently, so that we can identify the best profile  for each time series based on its characteristics.

To this end, this paper proposes an integrated method to  model the dynamics of time series so that the heterogeneous  behaviors in big sensor data can be correctly profiled. Given  a time series x = {x1, x2, . . . , xt, . . .}, our method first extracts related features that best summarize its dynamics,  and then builds a model to capture the evolutionary trajectory  of extracted feature vectors. In the feature extraction step,  we apply a sliding window on x to derive a sequence of  state vectors zt = [xt?d+1, ? ? ? , xt?1, xt], which leads to a state matrix Z = [z1, z2, . . . , zn]  T . Compact features are  then extracted by applying subspace decomposition from  Z . We use the singular value decomposition (SVD) to extract linear features, whereas a kernel based method is  presented to extract features from nonlinear time series. We  also present an intrinsic dimension based test [4], [5] to  determine whether an linear or nonlinear feature extraction  is appropriate for a given time series.

Since the extracted features summarize the underlying  dynamics of time series, their trajectory reflects the evolution  of time series. We will show that for many time series their  evolution trajectories demonstrate stronger regularities in the  feature space than in the original data space. Therefore,  we further propose two techniques to model the trajectory  traces in the feature space. Two techniques are proposed  for that purpose. The vector-autoregressive (VAR) method,  which has been shown be effective to describe a variety of  trajectory shapes [6], is presented to model feature trajectory      0 500 1000 1500 2000 2500 3000    0 500 1000 1500 2000 2500 3000 22.5   23.5  0 500 1000 1500 2000 2500 3000   0 500 1000 1500 2000 2500 3000    0 500 1000 1500 2000 2500 3000    0 500 1000 1500 2000 2500 3000    0 500 1000 1500 2000 2500 3000    0 500 1000 1500 2000 2500 3000 ?2    0 500 1000 1500 2000 2500 3000    0 500 1000 1500 2000 2500 3000  1.5   Figure 1: Heterogeneous time series from a physical system.

with regular patterns. We also present a density based  approach to model feature trajectories with less regularities.

Since our feature extraction and trajectory modeling are  performed on each individual time series, our method can be  easily parallelized to handle data from a massive amount of  sensors. That is, we divide the time series into several groups  and each machine builds the profiles of series from a specific  group. The learned profiles can be used to monitor the online  status of time series. Given a new observation xt of series x, we retrieve the model of x from the corresponding machine, compute the value of its feature vector, and compare it with  that derived from the feature trajectory estimation. An alert  will be issued if a large deviation is observed. Experimental  results have demonstrated the good performance of our  method in the system anomaly detection.

Contributions. This paper has made the following contri-  butions: (1) We propose an integrated approach to profile  heterogeneous time series collected from a large amount of  sensors in physical systems. It first extracts relevant features  from time series to represent the dynamics, followed by  modeling the trajectory of feature values along the time.

Our method has correctly profiled all the time series in a real  power plant. It outperforms other state-of-the-art approaches  in the application of anomaly detection. (2) In extracting  features from time series, we have made significant improve-  ments over current methods such as SSA to handle general  behavior of time series. In addition to the linear subspace  decomposition, our method also includes nonlinear feature  extraction to model nonlinear time series. (3) Rather than  focusing on individual feature values, we propose to model  the evolution trace of time series features. Two trajectory  modeling techniques are presented to handle different types  of feature trajectories.



II. EXTRACTING FEATURES OF TIME SERIES DYNAMICS  Figure 2 illustrates the process of extracting relevant  features from time series. We first use the sliding window  based technique to construct a sequence of state vectors,  i.e., the vectors that contain series? dynamics along the  time. After that, we discover non-redundant features from  state vectors. Both linear and nonlinear extraction methods  Figure 2: Feature extraction workflow.

are presented to cover various behaviors of time series.

In addition, we also propose a statistical test procedure to  determine the linearity of a given time series and hence  choose the appropriate extraction method.

A. State Vector Construction  The time series in most physical systems are generated  from underlying physical processes that can be modeled by  some differential equations. That is, the current value xt at time t is produced by a hidden function on its previous samples. While the underlying physical processes are usually  unknown, that fact guides us to construct a state vector at  each time t to cover the dynamics of time series at that moment. It is represented as a d-dimensional vector zt ? R  d  that contains the current observation xt as well as its past d? 1 samples.

zt = [xt?(d?1), ..., xt?1, xt]. (1)  As long as the number of delayed samples in zt is greater  than the order of underlying physical process, the state zt contains the full dynamics of x at time t. By constructing the vector zt at every sample time, we can get the whole picture of the evolution in time series.

If the true value of d is known, the state vector can be directly used as the feature of time series. In practice,  however, that information is unavailable. A common way  to address that is to relax the d value at the beginning and then discover the compact representation of features from  state vectors. In order to achieve that, we stack all the state  vectors into a matrix Z = [z1, z2, ...zn] T , which is called the  Hankel matrix [7], and identify a low dimensional subspace     that contains all the information in Z . The projection of each state vector into that subspace is then regarded as the feature  of time series x.

There are different ways to discover the signal subspace  from matrix Z . If time series x is linear, i.e., the current sample xt is linearly dependent on its past observations, the singular value decomposition (SVD) can be directly applied  to identify the subspace. However, nonlinear methods need  to be utilized if the behavior of x violates the linear  assumption. Before performing the corresponding feature  extraction, we first propose a procedure the discover the  linear or nonlinear characteristics of time series.

B. Intrinsic Dimension Based Test  A statistical test based on the concept of intrinsic  dimension[4] is described here to discover the linear or  nonlinear behavior of time seiries. Given a sequence of  d-dimensional state vectors zt, t = 1, ? ? ? , the intrinsic dimension r describes the number of underlying variables that are needed to represent the state vector. Its estimation  is based on the observation that for an r-dimensional data set, the number of pairs of points closer to each other than  ?, as described in equation 2, is proportional to ?r  Cn(?) =  n(n? 1)  n? i<j  1||zi?zj ||<? (2)  where 1 is the indicator function of the event A.

Then the intrinsic dimension r is defined as r = lim??0 limn??  logCn(?) log ? . We compute Cn(?) for different  ?i and fit a line through [log ?i, logCn(?i)] to derive r.

Based on the estimated dimension r, we test whether  the linear subspace decomposition is sufficient for time  series 1. Given the state matrix Z from time series x, we  first apply the singular value decomposition Z = U?V T , where U and V represent the column and row spaces of Z respectively, and ? are the singular values ? = diag(?1, ..., ?r, ?r+1, ..., ?k), k = min{d, n}. We then check whether the linear subspace with dimension r covers enough variances of the original space by computing  ? =  [ ?21 + ?  2 + ...+ ?  r  ?21 + ? 2 + ...+ ?  k  ]1/2 (3)  If the ? value is larger than a predefined threshold, we use the linear subspace decomposition to extract features.

Otherwise, we use the nonlinear feature extraction method.

C. Linear Feature Extraction  The linear subspace of matrix Z directly comes from the SVD results, i.e., the first r columns of U form the bases of signal subspace Us = [u1, u2, ..., ur]. Given a state vector z, we can obtain its low-dimensional representation  in the signal subspace as yt = U T s zt. Since yt effectively  summarizes the information in zt, we use it as the feature  of time series x at time t.

D. Nonlinear Feature Extraction  For time series with nonlinear behaviors, we use a nonlin-  ear function ?(.) to transform the state vector zt into ?(zt), and then identify the signal subspace in the transformed  vector space. The transformed vector space is also called  ?feature space? in the literature, but we do not use the term  here to avoid the confusion with the feature space described  in Section III.

The base vectors {u?1, ? ? ? , u?r} of signal subspace from transformed vectors ?(Z) = [?(z1),?(z2), ...,?(zn)]  T can  be obtained by the eigen-decomposition of covariance matrix  C = 1n ?n  j=1 ?(zj)?(zj) T , which can be solved by using  the kernel trick [8]. That is, instead of computing the  mapping ?(zj), we use common kernel functions such as the Gaussian kernel and polynomial kernel to represent  their dot product Ki,j =< ?(zi) ? ?(zj) > in the eigen- decomposition, which turns into solving K? = ??, where matrix K = [Ki,j ] is called the kernel matrix, and ? is its eigen-vector associated with eigen-value ?.

It was shown in [9] that the kth base vector u?k of the signal subspace, k = 1, ? ? ? , r, can be derived from ?k, i.e., the eigen-vector associated with kth largest eigen value, u?k =  ?n j=1 ?  k j?(zj), where ?  k j is the jth element in ?  k.

Given a transformed state vector ?(zTt ), its projection on that base vector is u?Tk?(z  T j ) =  ?n j=1 ?  k j?(z  T j )?(z  T j ), which can  be computed by the kernel function ?n  j=1 ? k jK(zi, zj). The  nonlinear features of time series are the projections of ?(zTt ) on all the r base vectors in the transformed vector space.



III. MODELING SYSTEM DYNAMICS  The profiling of data samples in the projected feature  space have been studied for general data types [5], [10]. For  example, [5] used the statistic of Hotelling T 2 to describe the distribution of projected features. However, since this paper  deals with time series, we expect that feature vectors also  exhibit temporal patterns in their evolutions. As an example,  given a time series in Figure 3(a), its extracted 3-dimensional  features in Figure 3(c) demonstrate strong regularity in the  evolution path. We also plot the 3-dimensional state vectors  (xt, xt?1, xt?2) in Figure 3(b). Clearly the points in feature space demonstrate strong trajectory patterns. This is because  the feature vector includes all the dynamics of time series.

By modeling the trajectory of feature vectors, we can build  a profile of time series dynamics.

Two techniques are proposed to model the feature vector  trajectory. The Vector-Autoregression(VAR) is used first to  capture trajectories with strong regularities. We will show  that VAR model can cover a majority of time series we  encountered in real systems. However, for some time series  whose feature trajectories do not fit the VAR model well, we  also provide a density based model to describe their behav-  iors. By combining those two complementary techniques,  we can build the profile for all time series received in real  applications.

0 500 1000 1500 2000 2500 3000 ?0.6  ?0.4  ?0.2   0.2  0.4  0.6  0.8  t  x(t)  ?1 ?0.5  0.5   ?1  ?0.5   0.5  ?1  ?0.5   0.5   x(t)x(t-1)  x(t-2)  ?40 ?20    ?60  ?40  ?20   ?20  ?10       y1 y2  y3  (a) Time series (b) State vectors (c) Feature trajectory  Figure 3: An example of (a) the original time series from a real physical system; (b) the trajectory of 3-dimensional state  vectors; (c) the trajectory of extracted features.

A. Vector-Autoregression Model  It has been shown that VAR model is effective to describe  trajectories with strong regularities [6]. Given a sequence  of feature vectors y0, ? ? ? , yt, VAR describes the relation between current feature yt and its past p fetaures:  yt = c +  p? i=1  ?i ? yt?i + ?(t) (4)  where ?i is a r? r transition matrix, and c is the intercept vector. The noise ?(t) reflects the influence of external randomness on the physical system.

We transform equation 4 into a compact expression yt = Bwt+?(t), where B is a r? (rp+1) parameter matrix B = [c,?1, ...,?p] and wt = [1  T , yTt?1, ..., y T t?p]  T . We assume  the noise follows Gaussian distribution ?(t) ? N (0,?), and use the least square method to estimate transition matrices  and the intercept vector.

B?k?(kp+1) =  [ N? t=1  ytw T t  ][ N? t=1  wtw T t  ]?1 . (5)  We use the Bayesian Information Criteria (BIC) to de-  termine the optimal lag p in equation 4. After obtaining model parameters {c?, ??1, ??2, ..., ??p}, we can predict the next feature value based on past p features by equation 4.

Note that VAR model may not always lead to a good  profile of time series. For some feature trajectories with a lot  of randomness, it will produce large errors ?(t) and hence is not reliable. Here we use the R-square(R2) statistic to measure the goodness of fit in VAR, which is computed as  R2 = 1?  ?N t=1 ||yt ? y?t||  2?N t=1 ||yt ? y?||  (6)  where yt is the feature vector, y?t is its estima-  tion based on equation 4, and y? is the mean y? =?N t=1 yt/N . Equation 6 can be interpreted as R  2 = 1 ? (residual sum of squares)/(total sum of squares). A large R2 indicates that the model provides a good fit to the data.

B. Density Estimation Model  A non-parametric method is also proposed here to handle  feature trajectories with low fitness in VAR model. It relies  on the density distribution of feature vectors in the trajectory.

However, instead of modeling feature vectors directly, we  model the increment of feature vector ?yt = yt+1 ? yt to reflect the temporal coherence in the trajectory. The kernel  density based regression is used to describe the feature  increment: ?yt = ?Nn(yt)  k=1 w(yt, yk)?yk, which leads to  yt+1 =  Nn(yt)? k=1  w(yt, yk)(yk+1 ? yk) + yt (7)  where w(yt, yk) represents the weights of ?yk in the regres- sion, and yk is one of the Nn(yt) nearest neighbors of yt.

That is, given all the feature vectors, we search the nearest  neighbors of yt and include the closest Nn(yt) features in the density model. The motivation here is that if two feature  vectors are similar, their path increment ? should also be similar. Therefore we only include the most similar features  in estimating the path increment.

In addition, among the selected feature yks, those that  are closer to yt should contribute more to the estimation of  yt+1 in equation 7. We use the kernel function w(yt, yk) = Kh(||yt?yk||)  ?Nn(yt) k=1 Kh(||yt?yk||)  to determine the weight value, where  Kh(b) = hK  ( b h  ) , K(?) is a kernel function, and h is the  bandwidth of the kernel. In the experiment, we use K(?) as the Gaussian kernel and the bandwidth h is determined by employing a simple plug-in rule described in [11].

1) Comments.: We have presented two complementary  strategies to model the feature trajectory. While the VAR  model is effective in capturing the shape of trajectory,  the density based approach is applicable to general feature  distributions. We use VAR here to get an accurate prediction  of features based on past feature values, whereas the density  model is used to improve the model coverage for various  time series collected from real systems. From the results in a  real power plant system, we observe that around 93% of the  997 time series can be modeled by VAR model with a high  fitness score. The remaining time series are then handled by  the density model.



IV. ANOMALY DETECTION IN TIME SERIES  Our proposed method is used to detect anomalies in  time series, which are data points that significantly deviate  from the normal pattern. The anomaly detection includes  a training stage and a detection stage. In the training, we     0 500 1000 1500 2000 2500 3000     t  x(t )  ?40 ?30 ?20 ?10 0 10 20 ?5    y  y 2  0 500 1000 1500 2000 2500 3000    t  x(t )  ?5 0 5 10 ?2    y  y 2  0 500 1000 1500 2000 2500 3000    t  x(t )  ?1500 ?1000 ?500 0 500 1000 1500 ?1000     y  y 2  0 500 1000 1500 2000 2500 3000     t (a)  x(t )      ?150 ?100 ?50 0 50 100 150 ?200    y  (b)  y 2  Figure 4: Four time series (left) and their extracted feature  trajectories (right).

apply our method to normal time series xt, t = 0, ? ? ? , t. As a result, we obtain the extracted features and a model that  correctly profiles the evolution of feature vectors.

In the detection stage, given a new observation xt+1, we perform the following steps to determine whether it is  normal or abnormal. (1) Construct the state vector zt+1 from  xt+1 and its past observations; (2) Compute the new feature value yt+1 based on the method in Section II; (3) Use past  feature vectors to predict the new feature y?t+1 based on  the method described in Section III; (4)If there is a large  deviation between yt+1 and y?t+1, i.e., ?y?t+1 ? yt+1? ? ?, xt+1 is regarded as abnormal and an alarm is generated.

The threshold ? for anomaly detection is determined based on the trade-off between false positives and negatives in  different applications. In our experiments, the threshold ? is determined as the maximum residual obtained from the  training process.



V. EVALUATION  We apply the proposed method to model a large number  of time series from a real power plant, and use it to detect  anomalies in the system. In the following, we first describe  the collected data in Section V-A. Section V-B then presents  the training results from the data. The results of detection  are described in Section V-C. Section V-D then evaluates the  the effect of different sliding window size d, described in Section II-A, on the performance of proposed algorithm. In  addition, we further look into the results and investigate the  contribution of nonlinear features in the model performance,  which will be described in Section V-E.

A. Data Description  The data contain 997 time series collected from sensors located at various positions in a real power plant. The sensors  collected the operational data every 30 seconds. Our data  cover a whole day of system operation, in which each time  series contain 2880 samples. Some of the time series are  illustrated in Figure 1. We can see that those time series  vary dramatically in terms of shape, variance, covariance,  trend, seasonal variation and periodicity.

0.8 0.85 0.9 0.95 1         R2  Figure 5: Histogram of R2 values generated by VAR model.

We divide the time series into training and test sets. The  training data contains 1920 samples of all time series, and  the test set takes the remaining samples. We use the training  data to build a model for each time series, and then use the  test data to evaluate the performance of the model.

B. Model Construction  We follow the steps described in previous sections to  build a profile for each time series. In the feature extraction  stage, we compare the ? value in Equation (3) with 0.9 to determine the intrinsic dimension. Figure 4 plots some time  series and their extracted feature vectors. As we can see, the  evolution of feature vectors demonstrate strong regularities  with different shapes such as polynomial, sinusoid, circle  and ellipses. Note that here we only plot the first two  dimension of feature vectors in Figure 4(b), whereas in  reality feature vectors can have more than two dimensions.

Since many time series demonstrate regular feature trajec-  tories, our VAR based technique can correctly model feature  trajectories from a majority of time series. Figure 5 presents  the histogram of VAR model fitness R2s from all time series.

It shows that most time series have a high fitness score on  the VAR model. Here we choose the threshold 0.893. As long as the R2 value is greater than the threshold, we use VAR to model the feature trajectory of time series. It turns  out that VAR can cover 922 time series, around 92.48% of total series. For remaining time series, we use the density  based approach to model their behaviors.

C. Anomaly Detection  Two commonly used metrics to evaluate a model in  anomaly detection are the detection rate and false alarm  rate. Since all the experimental data are collected from the  system normal operations, the test time series can only be  used to evaluate the false alarm rate of proposed method.

In order to evaluate the detection rate, we manually inject  some anomalies in the test data to mimic some faulty  situations in system operations. By tuning different values  of detection threshold ? as described in Section IV, we can obtain a receiver operating characteristic (ROC) curve to  fully describe the performance of the algorithm.

In order to demonstrate the advantage of our proposed  method, we compare three different algorithms to model  time series and then detect anomalies: 1) the autoregressive  (AR) model [1], 2) the singular spectrum analysis (SSA)  [12], and 3) our proposed method. We compare our method     AR d=10 d=50 d=100 model SSA Proposed SSA Proposed SSA Proposed  # false alarms 881 181 5 1864 9 3632 9  Table I: The total number of false alarms generated by  the AR model, SSA model and our proposed method with  different sliding window sizes.

anomaly AR d=10 d=50 d=100 level model SSA Proposed SSA Proposed SSA Proposed  weak 0.8107 0.8867 0.9007 0.8933 0.9187 0.9000 0.9413  middle 0.8607 0.9067 0.9227 0.9067 0.9360 0.9133 0.9567  strong 0.8993 0.9133 0.9460 0.9133 0.9567 0.9400 0.9807  Table II: The detection precision rate generated by the AR  model, SSA model and our proposed method with different  sliding window sizes.

with AR model because AR is one of the most widely used  methods for time series modeling. The SSA model is used  here for comparison because it also uses a sliding window  to extract state vectors from time series and decomposes  the state matrix into different subspaces. However the SSA  method does not model feature vectors, but instead recon-  structs the original time series by removing the noise part  in the decomposition.

Since both SSA and our proposed method use the slide  window to construct state vectors, the size of sliding window  d may affect the performance of algorithms. In the experi- ments we have tried different window sizes, and the methods  are compared under each individual d value. In Section V-D we will further explain the effect of window size to the  model performance.

1) False Alarm Rate: In the testing period, we use the  maximum residual in the training process as the threshold  to flag an alarm if ?y?t+1 ? yt+1? ? ?. We count the number of alarms generated in the test time series. Since the  original tets data are all from normal system operations, the  number of observed alarms are false alarms. Table I shows  the total number of false alarms generated by the AR model,  SSA model and the proposed method with different sliding  window sizes.

As we can see, our proposed method can significantly  reduce the false alarms in anomaly detection. Compared with  hundreds of alarms generated by other methods, our method  only produce less than 10 false alarms.

2) Detection Rate: As all the data are from system  normal operations, we randomly add synthesized anomaly  event values. We randomly select a time t, and we inject 5 consecutive noise by choosing the value from {(1 + ?)max{xt}  t+w k=t?w, (1??)min{xt}  t+w k=t?w} for window size  w. According to noise amplitude parameter ?, we compare the anomaly detection performance for different level of  anomalies (failures), namely weak, middle and strong. It is  more difficulty to detect a weak anomaly as the failure value  would be more similar to normal data. We count it a success  of anomaly detection when it flags one alarm during [t, t+4].

0 0.2 0.4 0.6 0.8 1 0.75  0.8  0.85  0.9  0.95   False positive  T ru  e po  si tiv  e      AR SSA Proposed  Figure 6: The ROC curves of anomaly detection generated  by the AR model, SSA model and our proposed model,  under the middle level anomaly and d = 50.

We repeat this process K = 10 times.

We define the detection precision =  K  [?K i=i  ( N  ?N j=1 I(dij)  )] where I(?) is an index  function of detection success defined as  I(dij) =  ?? ?  1 if anomaly detection success at experiment i for time series j,  0 otherwise  We compare the results with three failure levels: week,  middle and strong, and changing the embedding size d.

Table II shows the detection rate of our linear subspace  decomposition based method compared to AR model and  SSA model. From Table II and Table I, for middle level  anomaly with d = 50, we can see that compared with AR model, only linear subspace decomposition based method  can achieve 7.53 percent improvement of detection rate, while significantly remove the false alarms (reduced from  881 to 9); compared with SSA model, our method can achieve 1.33 percent improvement of detection rate, and can reduce false alarms from 1864 to 9. Also we set different normalized anomaly flag threshold and we got the ROC  curves as shown in Figure 6. Actually, our method can  improve the two baselines even more when the anomaly level  is weak, which make the anomaly detection more difficult.

For example, for weak level anomaly with d = 50, our method improves AR model by 10.8 percent, and improves SSA model by 1.8 percent respectively.

D. Effects of Sliding Window Size  The sliding window size d in Section II-A determines the dimension of state vectors, which will affect the performance  of anomaly detection. A large window can contain more  complete time series dynamics and hence make the model  more accurate. However increasing the window size will also  introduce more noise and a lot of redundant information,  which may lead to an overfitted model. As shown in Table I  and II, with the increase of d, the anomaly detection rate is improved, whereas more false positives are also introduced.

However, compared with the SSA method, our method does  not have a significant increase of false alarms for large ds.

This is probably because the feature trajetory model can  filter out irrelevant dynamics to make the model more robust.

0 500 1000 1500 2000 2500 3000 0.02  0.021  0.022  0.023  0.024  0 500 1000 1500 2000 2500 3000 -0.2  -0.18  -0.16  -0.14  -0.12  t  t  x(t)  x(t)  -6 -4 -2 0 2 4  x 10 -3  -5   x 10  -3  y(t)  y( t-  3)  -0.06 -0.04 -0.02 0 0.02 0.04 0.06 -0.1  -0.05   0.05  0.1  y(t)  y( t-  3)  y1  y1  y2  y2  -3 -2 -1 0 1 2 3 -4  -2     y(t)  y( t-  3)  -2 -1 0 1 2 3 -4  -2     y(t)  y( t-  3)  y1  y1  y2  y2  (a) Time series (b) Linear (c) Nonlinear  Figure 8: Examples of feature trajectory extracted by linear and nonlinear subspace decomposition. (a) original time series;  (b) feature trajectory extracted by linear subspace decomposition; (c) feature trajectory extracted by nonlinear subspace  decomposition.

10 20 30 40 50 60 70 80 90 100     d  Fa ls  e al  ar m  10 20 30 40 50 60 70 80 90 100 0.7  0.75  0.8  0.85  0.9  0.95   d  D et  ec tio  n ra  te  (a) False alarm. (b) Detection rate.

Figure 7: Anomaly detection precision versus the sliding  window size.

Since the number of false positives generated by our  method is quite small, we mainly investigate the effect of  sliding window size on the anomaly detection rate. We set  different values of d and perform experiments to get the corresponding detection rates, which is shown in Figure 7.

It illustrates that detection rate improved with the increase  of s. However, In the experiments we use 10.

E. Do Nonlinear Features Really Help?

We further look into the results to pick those nonlinear  time series and check whether our nonlinear feature ex-  traction in Section II-D contributes to the improvement of  detection performance compared with linear method. Among  the 997 time series, we get 9.3% of them whose feature  vectors are obtained by nonlinear extraction. We apply linear  feature extraction on those time series as well, and compare  the performance between linear and nonlinear features.

It turns out that nonlinear features can produce more  regular shapes in their evolution trajectories. Figure 8 plots  two nonlinear time series and their features extracted by  linear and nonlinear methods respectively. While it is hard  to find strong geometry patterns in the trace of linear features  in Fig. 8(b), the trajectories of nonlinear features are much  more smooth as shown in Fig. 8(c). This is because nonlinear  features correctly embed the dynamics of time series.

We compare the anomaly detection rate of those time  series with linear and nonlinear features. Table III illustrates  Anomaly level weak middle strong Method linear nonlinear linear nonlinear linear nonlinear  Detection rate 0.5524 0.8000 0.7079 0.8413 0.8619 0.9206  Table III: Comparison of anomaly detection rate using linear  and nonlinear features applied to nonlinear time series.

the results with different levels of anomalies. For the middle  level anomaly, nonlinear method can improve linear method  by 13.34% in the detection. For the weak level anomaly nonlinear method can improve linear method by 24.76%.

Such performance improvements are significant enough to  conclude that the nonlinear feature extraction is a necessary  part in our proposed method, which can better profile nonlin-  ear time series and enhance the detection rate. Although in  the system we studied, the nonlinear time series only occupy  around 10% of total series, we believe that more nonlinear  time series will be encountered in other complex systems.



VI. RELATED WORK  Our work is related to two areas of interest: time series  modeling, and the anomaly detection for complex physical  systems. Time series modeling is an active research area in  several communities including data mining and a number  of methods have been proposed. The most widely used  techniques include the auto-regressive (AR), moving aver-  age (MA), and ARMA models, which assume the current  observation of time series is linearly dependent on its past  values or some noise items [1]. In order to handle nonlinear  behaviors, the paper [2] provides a survey on using support  vector regression (SVR) to model time series. Kalman filters  and state space models [1] have also been widely used for  sequence and time series data, and has a wide range of  applications.

Understanding and capturing the underlying system dy-  namics have been widely investigated in time series analysis  [13], [14]. For example, the delay coordinate embedding [14]  was proposed to study the chaotic behavior of time series.

However, that method requires expensive computations to  obtain a reliable delay parameter as well as the embedding  dimension, which is not well suited to handle a massive  number of time series. The singular spectrum analysis (SSA)  [12] avoids such an issue by setting the delay as one and  using a fixed embedding dimension. But SSA decomposes  time series based on the least square criterion, which is  optimal only when the underlying dynamics is linear. In  addition, SSA mainly focuses on the decomposition and  reconstruction of time series to study its individual com-  ponent such as trend and periodic items. It does not model  the whole trajectory of time series in the embedded feature  space. Compared with those methods, our approach handles  both linear and nonlinear dynamics of time series, which can  properly address the various behaviors in big sensor data.

Trajectory mining is also attracting a lot of attentions  recently due to the increasing popularities of spatial temporal  data in many applications. For instance, [15] proposed a  trajectory clustering algorithm to group similar trajectories.

The paper [16] estimated the traffic density given a lot of  vehicle trajectories. In this paper we borrow the idea from  trajectory mining to model the evolution path of time series  in the feature space. However, our method does not compare  different trajectories, but rather model the trace of each  individual trajectory. We use the VAR model to describe  trajectories with strong regularities, and also propose a  density based technique to model irregular trajectories. As  a result, different evolution patterns of time series features  can be captured in our method.

Anomaly detection has been studied for a long time in  data mining community. [17] presented an excellent and  comprehensive survey. Some anomaly detection methods  for time series data include [18]. The anomaly detection  methods have been applied to a wide range of application  domains including spacecraft systems [19], system health  management[20], automobile [21], power system [22], and  so on. With the increasing number of sensors installed  in physical systems, we will see more data mining based  applications to facilitate the system management tasks.



VII. CONCLUSIONS  This paper has proposed a method to model a large num-  ber of time series with heterogeneous behaviors. We have  presented a technique to extract appropriate features from  time series that can represent its underlying dynamics. We  have also proposed two techniques to model the evolution  pattern of feature vectors along the time. By combining the  feature extraction and trajectory model, we have correctly  profiled all the time series in a physical power plant. We  have demonstrated the effectiveness of the proposed method  to detect anomalies in that system.

