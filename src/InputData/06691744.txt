Approximate triangle counting algorithms on Multi-cores

Abstract?Counting triangles in a large network is an important research task because of its usages in analyzing large networks. However, this task becomes expensive when runs on large networks with millions of nodes and millions of edges. For efficient triangle counting on such networks, researchers in recent years have adopted approximate counting or have proposed parallel or distributed solutions. In this work, we propose an approximate triangle counting algorithm, that runs on multi-core computers through a multi-threaded implementation. We show that for a given speedup factor, our method has a better approximation accuracy; further, the multi-threaded implementation that we propose is much superior to the Hadoop based distributed methods that earlier algorithms propose.



I. INTRODUCTION  A key task in network analysis is to count the frequency of various small subgraphs to discover network motifs? subgraphs that are significantly more frequent in a network relative to their occurrence in a randomized network of identical degree distribution [1]. Researchers have shown that network motifs are basic building blocks of different networks, including social networks, molecular interaction networks, and transportation networks [2], [3], [1]. To obtain effective algorithms for finding such motifs, researchers have developed a number of methods for counting the frequency of small subgraphs in a large networks [4], [5], [6]. Besides exact methods, approximate counting algorithms are also considered [7], [8], [9].

Triangle, a complete graph of size three is an impor- tant network motif in various networks, including social networks, information networks, and molecular interaction networks. The reason for triangle being a motif is that, in these networks interactions among entities exhibit transitiv- ity property. For example, in social networks, people who have common friends tend to be friend themselves [10], which justifies the unusual high count for triangles in a social network compared to the same count in a randomized networks of similar scale. This phenomenon also prompted social science researchers to define two important metrics for graph analysis: clustering coefficient and transitivity ratio [11], [12]. Counting triangles is a sub-routine to obtain the values of these metrics. Given the importance of triangle count in a network, in recent years, researchers in the data mining community have shown an overwhelming interest in the problem of triangle counting [13], [14], [15], [16], [17], [18], [19]. Although this is an old problem in graph theory,  the renewed interest in this problem in mainly due to the fact that gigantic networks with millions of nodes and billions of edges are being available in recent years, on which the existing triangle counting methods perform poorly.

In the existing literatures, the best practical algorithm for counting triangles exactly has a cost of O(m3/2), where m is the number of edges in the graph [20]. The algorithm iterates over the edges of the graph, and counts the number of triangles in which each of the edges contributes. Such an algorithm is known as EDGEITERATOR method. NODEIT- ERATOR, a method which is dual to the EDGEITERATOR, iterates over the nodes and counts triangle in O(d2max ? n) time, where n is the number of nodes, and dmax is the maximum degree of a node in the graph. Most of the existing methods belong to the EDGEITERATOR category. However, in this work we show that for counting triangles in most of the real-life graphs, the EDGEITERATOR shows a better performance than the NODEITERATOR.

In terms of time complexity, the fastest triangle counting methods are based on fast matrix multiplication. Alon et al. [21] gave such an algorithm that has a time complexity of O(m  2?  ?+1 ) where ? is the matrix multiplication cost; so the cost of this algorithm becomes O(m1.41), using the best known ? value which is 2.37 at present. However, the methods that are based on matrix multiplication require large amount of memory, and hence they are not suitable for counting triangles in very large graphs.

For today?s large network with millions of vertices and edges, all the exact methods for triangle counting can be deemed as expensive; so, the majority of the recent efforts of triangle counting either adopt a method for approximate counting, or design a parallel or distributed framework for solving the counting task effectively. For approximate count- ing, [13] proposes DOULION , which uses a probabilistic sparsification technique to obtain a sparser graph; then, it computes the exact triangle count on the sparse graph, from which it extrapolates an approximate triangle count of the original graph. Authors of this work also offer a Hadoop (An implementation of MapReduce[22]) based solution for this work. Hadoop is also used in an exact triangle counting which is recently proposed by Suri and Vassilvitskii [23].

A linear algebraic method is also proposed for approximate triangle counting [14],  In this paper, we propose a variant of EDGEITERATOR method for approximate triangle counting. Our method has       a surprisingly high accuracy, with a generous speedup. On large real-life graphs with millions of nodes and edges, the single processor version of our algorithm consistently achieve a 30-fold speedup (compared to the best exact method) with an accuracy that is around 99%. The most attractive feature of our method is that, both the speedup, and the accuracy of our method improve as the input graph becomes larger, so it is particularly suitable for very large graphs.

The simplicity of our algorithm also allows a simple multi-threaded implementation of our method for execut- ing on today?s multi-core architecture?this improves the speedup even further without harming the counting accuracy.

We indeed found that for all the real-life graphs that we encountered, the multi-core version that we propose is a better choice by a wide margin than all the Hadoop based solutions. For a specific example, on a Wikipedia graph with 1.63 millions vertices, and 18.5 millions of edges; using 32- threads out method obtains a whopping 837-fold speedup with an accuracy of 98.2%. None of the Hadoop based solution reports a speedup that is as high as this work.

Below we list the contribution of this work.

? We propose a simple, yet powerful, method for ap- proximate triangle counting. It has a surprisingly high accuracy and a high speedup factor; both the metrics observably improve as the graph grows larger.

? We develop two variants of thread-based multiprogram- ming solutions of our approximate triangle counting algorithm. The parallel implementation of these algo- rithms are simple, and effective.

? We compare the performance of our methods with those of the state-of-the-art approximate triangle counting methods that are available at present, on a collection of large real-life networks to validate the superiority of our methods.



II. METHOD  Assume, G(V,E) is a graph, where V is the set of of vertices and E is the set of edges. Each edge e ? E is represented by a pair of vertices (vi, vj) where, vi, vj ? V .

A graph is called simple, if it does not contain a self loop, and at most one edge exists between two of its vertices. In this work, we consider simple, connected, and undirected graphs. We denote the adjacency list of a vertex v by adj(v), which contains all the vertices that are adjacent to v. In our implementation, all the adjacent lists are sorted in the ascending order of the vertex-id. Since the graph is undirected, for an edge (u, v) u appears in v?s adjacency list and vice-versa. A triangle is represented by a triple of (u, v, w), where u, v, w ? V and there exists an edge between every pair of vertices in the triple.

The approximate triangle counting method that we pro- pose in this work is based on the EDGEITERATOR algorithm that we discuss below.

Algorithm 1 EXACTEI Require: Large network G(V,E)  1: count = 0 2: for each edge (vi, vj) ? E do 3: adj1 = {x|x ? adj(vi), x > max(vi, vj)} 4: adj2 = {x|x ? adj(vj), x > max(vi, vj)} 5: counte = |intersection(adj1, adj2)| 6: count+ = counte 7: end for 8: return count  Algorithm 2 APPROXEI Require: Large network G(V,E),  :Sample factor p 1: count = 0 2: Ep = sample p ? |E| edges from E uniformly 3: for each edge (vi, vj) ? Ep do 4: adj1 = {x|x ? adj(vi), x > max(vi, vj)} 5: adj2 = {x|x ? adj(vj), x > max(vi, vj)} 6: counte = |intersection(adj1, adj2)| 7: count+ = counte 8: end for 9: count = count/p  10: return count  A. Exact triangle count by EXACTEI  An edge iterator algorithm iterates over each edge e(vi, vj) ? E for counting the total number of triangles for which e is a participating edge. Let?s call the number of triangles incident to the edge e, the partial triangle count with respect to e, and represent it by counte. Since a triangle is composed of 3 edges, a triangle will appear in exactly 3 of these partial counts. EXACTEI can obtain the total count of triangles in G by simply adding the partial counts of all the edges followed by a division by 3. The triple counting of a triangle in the above method can be avoided by imposing a restriction that the third vertex of the triangle (say, vk) has an id which is larger than the id?s of both the vertices vi and vj of the edge e; this yields a more efficient version of EXACTEI; a pseudo-code for which is shown in Algorithm 1. It computes the counte for an edge e = (vi, vj) as follows: it takes the adjacency lists of the contributing vertices vi (adj(vi)) and vj (adj(vj)).

Then, it finds the subsets adj1 and adj2 to ensure that the id of the possible third vertex (vk) is strictly higher than the ids of both vi and vj , and then it finds the number of vertices that are common in both adj1 and adj2 i.e., counte = |intersection(adj1, adj2)|. More than 50% of execution time can be saved using this method.

B. Approximate triangle count by APPROXEI  We design an approximate triangle counting algorithm, called APPROXEI, by computing the partial count, counte, only for a fraction of edges. For this, APPROXEI takes     Graph EXACTEI APPROXEI APPROXEI2 time Accuracy speedup Accuracy speedup (sec) (%) (%)  Wiki-1 124 98.5 5.81 99.62 3.83 Wiki-2 273 99.57 5.12 99.86 3.33 Wiki-3 299 99.67 5.18 99.75 3.24 Wiki-4 361 99.75 4.66 99.93 3.12 Zewail 0.043 95.64 6.02 97.04 5.39 Flikr 38.92 99.57 9.54 99.76 6.02 EN 0.72 98.51 7.62 99.52 6.01 EAT RS 0.37 97.56 7.22 99.45 5.3  Table I EXECUTION TIME FOR EXACTEI. AVERAGE ACCURACY AND SPEEDUP  OF APPROXEI AND APPROXEI2. HERE, p = 0.1. SPEEDUP IS WITH RESPECT TO EXACTEI. STATISTICS OF THE GRAPHS ARE IN TABLE II.

an additional parameter, a sample factor p ? [0, 1], which defines the fraction of edges in E for which we compute the partial count. A pseudo-code is shown in Algorithm 2.

APPROXEI first chooses (Line 2) a set of p fraction of edges from the set E uniformly. Then, it computes the partial count of each of the chosen edges (Line 4 ? 6). Finally, the sum of the partial count is divided by p to obtain an approximate count of triangles in the graph G (Line 9).

A second version of approximate counting algorithm can be obtained by lifting the constraint (imposed by Line 4?5) that the id of the third vertex (vk) of a triangle is larger than the ids of both vi and vj . As a consequence, we may end up counting each triangle at most thrice. So, the final count has to be normalized by 3?p (at Line 9 of Algorithm 2). We will call this version, APPROXEI2. As expected, this version is slower than the earlier version of approximate counting algorithm as it performs more works when computing the partial counts by intersecting a pair of unfiltered adjacency lists. However, this version has better sampling performance, as the sampling space of this version is less restricted. So, APPROXEI2 typically achieves a better counting accuracy than APPROXEI. For a comparison, see Table I.

C. Parallel algorithm, PARAPPROXEI  In this section, we discuss the parallel version of the APPROXEI, which we call PARAPPROXEI; the idea for this is quite simple. Since, the APPROXEI algorithm only per- forms read operations on the graph data structures, multiple threads can access these data structures without requiring any exclusive access. So, PARAPPROXEI splits the p ? |E| edges, and assign each part to several threads so that each thread can compute the partial count of the edges in its part independently. Each thread th maintains it?s own counter (say countth) which contains the sum of all the counte processed by it. After every thread has done its share of com- putation, PARAPPROXEI sums the partial triangle counts from each thread (countth) to get the countpartial. Then, it divides the countpartial by appropriate normalization factor (p for APPROXEI, and 3 ? p for APPROXEI2) to get the approximate triangle count. The process is illustrated in Figure 1.

1) Optimization of PARAPPROXEI: As shown in Figure 1, PARAPPROXEI provides each of the threads approxi- mately |E|?p/tc edges from the set Ep, with the expectation that all the threads are assigned the same amount of work for parallel processing. But, most often it is not the case. Even though all the threads have to process the same number of edges, the computation associated with the edges can be significantly different based on the size of the adjacency lists of the vertices incident to an edge. As a result, when we execute PARAPPROXEI (as shown in Figure 1), most of the threads finish their computation within a short span of time; however, there exist some of the threads that take significantly longer time to finish their share of computation; thus, resulting a longer overall execution time (see, Figure 2). This problem is also discussed in earlier works with the phrase ?the curse of last reducer? in the context of Hadoop based parallelization [23].

Ep  Ep1 Ep2 Eptc  T1 T2 Ttc  Figure 1. Illustration of parallel workload distribution among tc number of threads. Here, Ti indicates thread i. Ep is the set of edges to be processed.

Epi ? Ep is a disjoint set of edges assigned to the thread i.

0 5 10 15 20 25 30 35  0.4  0.8  1.2  1.6   Threads  W ai  tin g  tim e  (s ec  )  Figure 2. Thread waiting time over 32 threads of PARAPPROXEI.

Execution with AtomicWorkLoad = (|Ep|/32) for graph ?Wiki-4?  To work around the above limitation, we propose a different variant of the algorithm PARAPPROXEI. In this     Ep  Epi Epi+1 Epi+tc?1  T1 T2 Ttc  Figure 3. Illustration of parallel workload distribution among tc number of threads using queue. Here, Ti indicates thread i. Ep is the set of edges to be processed, each small box in Ep is a packet of edges that is assigned to a thread at a given iteration. Dark packets of Ep are the edges that have already been processed by some thread. Gray packers are being processed, and finally the white packets will be assigned to the next available thread.

variant, each thread takes a small fraction of the total job at the beginning of the execution and upon finishing the execution, it takes additional fractions of job in subsequent iterations, until the entire job is finished. To ensure that different threads work on different fractions of the total job, a queue is used for storing the list of edges that are yet to be processed; this queue mediates the job allocation to different threads. At the beginning of every iteration of job allocation, each thread acquires an exclusive access of this queue to request new job. The process is explained in Figure 3.

Though the above variant of PARAPPROXEI distributes the total work among different threads more evenly, it also suffers from a bottleneck caused due to the exclusive access to the job queue. So, there is a trade-off that is based on the size of job (edges) assigned in response to a job request. A large job assignment (the highest possible is, AtomicWorkLoad = (|Ep|/tc) edges assigned to each thread) ensures that no time is spent for enforcing mutual exclusion but it yields larger waiting time for finishing the last thread. On the other hand, a smaller job assignment results smaller waiting times for finishing the last thread, but it suffers from a high waiting time in the semaphore queue due to the large number of exclusive accesses to the job allocation queue. In our experiments, we find that the differences in speed-up factors vary within a range of 5% to 10% based on the choice of packet size.



III. EXPERIMENTS  We perform several experiments to observe the perfor- mance of approximate triangle counting. For this we use a collection of real-life graphs available from http://www.

cise.ufl.edu/research/sparse/matrices/. We choose the largest graphs used in [13]. The statistics of the graphs are shown  Graph Vertices Edges Wikipedia 2005-11-05 (Wiki-1) 1, 634k 18, 540k Wikipedia 2006-09-25 (Wiki-2) 2, 983k 35, 048k Wikipedia 2006-11-04 (Wiki-3) 3, 148k 37, 043k Wikipedia 2007-02-06 (Wiki-4) 3, 566k 42, 375k Zewail 6k 54k Flikr 820k 6, 625k Epinions network (EN) 75k 405k Edinburgh Associative Thesaurus (EAT RS) 23k 305k  Table II GRAPH USED IN EXPERIMENTS (SOURCE:  HTTP://WWW.CISE.UFL.EDU/RESEARCH/SPARSE/MATRICES/).

in Table II. To reflect the performance of an approximate counting algorithm, we use two metrics: speedup and accu- racy (%). The speedup of a method M defines the ratio of the execution time between EXACTEI and the corresponding algorithm, M and accuracy defines the counting accuracy of the algorithm M , in percentage. All experiments are executed on a 64 core 2.3GHz AMD machine.

A. EDGEITERATOR vs NODEITERATOR  In this experiment we compare the performance of PARAPPROXEI with that of PARAPPROXNI. PARAP- PROXNI algorithm is a NODEITERATOR algorithm to ap- proximate triangle count of a network using multiple threads.

Both the PARAPPROXEI and PARAPPROXNI algorithms are given same set of parameters (a network G(V,E), sampling factor p, thread count tc). A NODEITERATOR algorithm works by iterating over the nodes in V . For, each node v ? V the PARAPPROXNI computes partial count of triangles incident on node v (countv). Finally, the sum of all the partial counts gives the total triangle count count ( count =  ?  v?V  countv ). For approximate triangle count with  sampling factor p, we sample a set Vp, where |Vp| = p? |V | and each node has equal probability to be selected. Then, the approximate triangle count using PARAPPROXNI will  be, count =  ?  v?Vp  countv  p . The method is very similar with that of PARAPPROXEI as explained in Section II-C. The most significant difference here is the process of computing partial triangle count countv. To compute countv of node v we need to go over all the possible pairs of nodes from adj(v) and check if the pair represents an edge (countv = |{(x, y) : x ?= y and x, y ? adj(v) and (x, y) ? E}|).

For this experiment we consider tc = 16 and p = 0.1 for both the algorithms. Each approximation is executed 10 times. The average and variance of execution time and accuracy is reported in table IV. From Table IV, we can see that, EDGEITERATOR algorithm is almost always better than NODEITERATOR in execution time and accuracy.

NODEITERATOR algorithm is also shows higher variance on execution time and accuracy. The reason behind it is that, in many graphs node degree is exponentially distributed     Variance APPROXEI PARAPPROXEI Speedups Graph Sample accuracy of speedup threads  Factor p (%) accuracy 4 8 16 32 Wiki-1 0.1 99.21 0.40 4.49 24.23 42.84 68.57 91.68  0.01 98.2 2.36 33.54 239.94 418.75 664.37 837.74 Wiki-2 0.1 99.56 0.10 4.14 20.1 35.6 55.43 63.42  0.01 98.95 0.49 32.42 199.56 351.02 548.01 614.27 Wiki-3 0.1 99.61 0.12 4.21 19.87 35.54 54.22 60.96  0.01 98.72 1.93 32.85 198.44 341.15 515.91 592.26 Wiki-4 0.1 99.60 0.09 4.33 19.54 34.95 52.55 56.84  0.01 98.28 1.61 33.71 197.13 346.92 504.8 547.29 Zewail 0.1 98.45 0.08 4.29 11.35 12.46 10.0 6.19  0.01 92.26 12.28 9.92 40.14 30.01 15.14 10.03 Flikr 0.1 99.74 0.07 5.18 28.92 51.46 77.67 96.67  0.01 99.43 0.19 33.26 277.44 501.83 730.21 796.85 EN 0.1 99.03 0.62 4.93 22.69 33.1 40.6 38.71  0.01 97.03 5.82 18.83 147.4 164.77 143.46 97.96 EAT RS 0.1 98.21 1.66 4.15 17.74 24.79 27.92 23.62  0.01 96.64 3.88 13.62 101.7 111.67 86.82 56.3  Table III AVERAGE ACCURACY WITH RESPECT TO SAMPLE FACTORS AND SPEEDUPS WITH RESPECT TO SAMPLE FACTORS AND TOTAL THREADS USED.

Graph Execution Time Accuracy Average (s) Variance Average (%) Variance  EI NI EI NI EI NI EI NI Wiki-1 3.13 11.99 0.01 79.78 96.17 96.39 0.19 8.87 Wiki-2 7.89 23.00 0.06 132.43 98.83 97.03 0.03 2.75 Wiki-3 8.60 45.90 0.03 2824.02 98.80 96.36 0.06 9.69 Wiki-4 10.68 47.98 0.08 3056.87 99.07 96.02 0.05 26.47 Zewail 0.00 0.00 0.00 0.00 99.65 95.47 0.02 12.56 Flikr 0.61 1.24 0.00 0.03 99.86 93.40 0.01 9.68 EN 0.02 0.03 0.00 0.00 99.35 92.97 0.28 60.87 EAT RS 0.01 0.01 0.00 0.00 99.39 93.52 0.54 13.28  Table IV AVERAGE AND VARIANCE OF EXECUTION TIME AND ACCURACY OF PARAPPROXEI AND PARAPPROXNI. (tc = 16,p = 0.1)  (Power Law model [24]). That is, number of nodes with low degree is very high compared to the number of nodes with high degree. In general, high degree nodes contribute higher in triangle count. Consequently, when we are performing uniform sampling from nodes in V , we are sampling from a very skewed distribution of partial-count countv; as opposed to EDGEITERATOR which samples from a less skewed distribution of partial-count counte. As can be observed from the Table IV, execution time of NODEITERATOR is also higher than that of EDGEITERATOR.

B. Performance of APPROXEI and PARAPPROXEI  We show the performance of APPROXEI and PARAP- PROXEI in Table III. For both the methods we show results for both p equal to 0.1 and 0.01; for these two cases, the sampler samples 10% and 1% of edges of the original graph.

For PARAPPROXEI we set AtomicWorkLoad = 50edges, and thread-count tc = 4, 8, 16 and 32. For every parameter setting we execute the algorithm for 5 times and show the average of the counting accuracies and the speedups. As we can see, the speedup factor increases as we increase the number of threads for PARAPPROXEI . However, the speedup does not increase linearly (see Figure 4) with the  number of threads, due to the bottleneck, as explained in Section II-C1. In ideal case, where there is no bottleneck for gaining mutual exclusion the speedup factor should be increased linearly with tc. For example, in case of ?Wiki- 1? graph and p = 0.1, speedup with tc = 4 is 24.23.

So, for tc = 8 ideally the speedup should be close to 48.46. But, it is 42.84 for our experiment. Similarly, for tc = 16 speedup is 68.57 instead of 85.68. The more number of threads competing for mutual exclusion the higher the waiting time would be. As a result, increasing the thread count indefinitely does not ensures steady speedup. For example, the speedup of PARAPPROXEI decreases when we increase total threads from 16 to 32 for graph ?EAT RS? for p = 0.1 (see Table III). Figure 4 shows this relation graphically. The speedup increases with thread count up to certain point, but eventually increased number of thread damages the performance. Also, important to note that our approximate algorithm has shown better approximation accuracy for larger graphs.

0 5 10 15 20 25 30 35          Numbers of threads  S pe  ed up     Wiki?2005?11?05 Wiki?2006?09?25 Wiki?2006?11?04 Wiki?2007?02?06 Zewail Flikr Epinion EAT  Figure 4. Speedup Vs Thread count tc for PARAPPROXEI  Mutex Speedup Access Count 1 32.5 2 31.71 4 32.4 8 33.14 16 33.53 32 33.96 64 33.99 128 34.34 256 34.2 512 34.64 1024 34.16 2048 34.2  Table V SPEEDUP WITH RESPECT TO MutexAccessCount FOR GRAPH  ?WIKI-4? USING 16 THREADS  C. Comparing the performance of PARAPPROXEI with dif- ferent MutexAccessCount  For this experiment, we define an additional parameter MutexAccessCount. MutexAccessCount = 1 identifies that, every thread will try to obtain mutual exclusion (job assignment) only once. Where MutexAccessCount = 10 indicates that on average every thread will try to ob- tain new job assignment 10 times. Increased value of MutexAccessCount indicates smaller portion of job as- signed to a thread at single access to mutually exclusive portion of program.

AtomicWorkLoad =  |Ep| MutexAccessCount?tc  In this experiment, we execute EXACTEI with differ- ent MutexAccessCount for graph ?Wiki-4?. For a con- stant number of threads tc, as we increase the value of MutexAccessCount, the waiting time of all other threads for last thread to finish decrease but the waiting time to get exclusive access to job queue Ep increases. The result is presented in Table V. As we can see, for a good span of possible value of MutexAccessCount the speedup is  Graph DOULION APPROXEI Accuracy(%) Speedup Accuracy(%) Speedup  Wiki-1 55.2 19.61 98.2 33.54 Wiki-2 88.52 20.54 98.95 32.42 Wiki-3 90.13 20.72 98.72 32.85 Wiki-4 94.31 21.64 98.28 33.71 Zewail 92.41 3.61 92.26 9.92 Flikr 99.04 17.69 99.43 33.26 EN 97.4 7.16 97.03 18.83 EAT RS 53.5 4.94 96.64 13.62  Table VI AVERAGE ACCURACY AND SPEEDUP OF OUR IMPLEMENTATION OF DOULION (p = 0.1) (NOT PARALLEL) AND APPROXEI. p = 0.01  Graph time(sec) GP Parallel  Exact web-Berk-Stan 102 12.04 as-Skitter 124.8 9.75 LiveJournal 654 26.46  Table VII AVERAGE EXECUTION TIME OF GRAPHPARTITION AS STATED BY [23]  AND PARAPPROXEI WITH p = 1 AND tc = 32  approximately same.

D. APPROXEI vs DOULION  In this experiment we compare the performance of AP- PROXEI with that of DOULION [13]. For that, we repeat the APPROXEI with p = 0.01 and DOULION with p = 0.1 for all the graphs from Table III. The speed up is with respect to EXACTEI. The implementation delivered by the authors of [13] performs worse than (shows less speed up) our implementation of DOULION . So in this result we report the speed up and accuracy of our implementation of DOULION . The result is shown in Table VI. Clearly, APPROXEI (single-threaded) is better than DOULION both in terms of speedup and accuracy for most of the graphs.

E. PARAPPROXEI vs GraphPartition  In this experiment we compare the performance of PARAPPROXEI with that of GraphPartition(GP ) [23].

Since, GP is an exact counting method, We conduct the exact triangle counting using PARAPPROXEI with p = 1 (100% sampling), and tc = 32, and compare the execution time. The results are shown in Table VII. In this table, times under GP column are taken from corresponding paper [23], which is the time of running GP on a 1636-node Hadoop cluster. In all three cases for which we were able to collect graph from SNAP 1, our method significantly wins over GP using only 32 threads!



IV. CONCLUSIONS  In this paper we present an approximate triangle counting algorithm which is built on an edge iterator algorithm. Our  1http://snap.stanford.edu/     method is simple, yet it achieves a surprisingly high accuracy and speedup. We also present a multi-threaded version of our algorithm which is suitable for multi-core machines.

We show experimental results that validate that our approx- imate counting method is better compared to the state-of- the-art approximate triangle counting algorithm. Also, for exact triangle counting, our thread based implementation is significantly faster than an exact triangle counting method built on Hadoop cluster. Distributed computing paradigm is probably a better choice for graphs that are too large to fit in the main memory. However, we found that real-life graphs with as many as 6 millions nodes and 20 millions of edges easily fits in the memory of a typical desktop PC with 4GB of RAM, and for such graphs, our method is obviously a better alternative.

