A Lazy Approach to Pruning Classification Rules

Abstract  Associative classifcation is a promising technique for the generalion of highly precise classifers. Prpvious works propose several clever rechniques ro prune rhe huge ser of generared rules, with rhe rwofold aim of selecring a small ser of high quality rules. and reducing the chance of eve@  I n  rhis papez we argue rhar pruning should be reduced ro a minimum and rhar rhe ovailability of a large rule base may improve the precision of rke classifer: wirhour affecting its performance. In  L:? (Live and Let Live), a new algorirhm for associarive classifcarion, o lazy pruning technique irer- arively discards all rules rhar only yield wrong case classi- ficarions.

Classifcarion is performed in rwo steps. Inirially, rules which have already correcrly classified at leasr one train- ing case, sorred by confidence, are considered. Ifthe case is srill unclassijed, rhe remaining rules (unused during rhe rroining phasej are considered, again sorred by confidence.

Exrensive experimenrs on 26 darabases fmm rhe UCI machine learning darabase reposirory show thar L7 im- proves rhe classifcarion precision with respecr ro previous approaches.

ring.

1. Introduction  An important class of data mining problems is repre- sented by association rule discovery [Z]. Association rules describe the co-occurrence among data items in a large amount of collected data. Recently, association rules have been also considered a valuable tool for classification pur- poses.

Classification rule mining is the discovery of a small set of rules in the training database to form a model of the data.

the classifier. The classifier is then used to classify appro- priately new data for which the class label is unknown [9] .

Association rules are usually extracted without previous definition of the set of labels that belong to the domain of  0-7695-1754-4102 $17.00 0 2002 IEEE 35  Paolo Garza Politecnico di Torino  Dipartimento di Automatica e Informatica gaaa@polito.it  the head of the rules, while for classification rules this do- main corresponds to the set of class labels. Differently from decision trees, association rules consider the simultaneous correspondence of values of different attributes, hence a]- lowing to achieve better accuracy in general. as shown by a number of experiments [3,6,7, 1 I].

Recent approaches to associative classification (e.g., CAEP [3], CMAR [6], CBA [7] and ADT [ I l l )  extract a small set of high quality association rules, with specified thresholds for support and confidence. Since association mining may yield a huge number of classification rules, the rule base obtained by association mining is then pruned to reduce its size. Several different methods have been pro- posed for performing pruning, all achieving a good accu- racy. However, we argue that most pruning techniques may go too far, by discarding also useful knowledge to- gether with low quality rules. Hence, we propose a novel two steps classification technique, in which only a very re- duced amount of pruning is performed, by eliminating only ?harmful? rules, i.e., rules that only produce wrong classifi- cation results in the training data.

The contribution of this paper is as follows. First, we customized a good association rule extraction algorithm [61 to extract rules with variable suppon threshold, in order to generate a similar number of rules for all classes, including those that may have a small number of cases. Second, we designed a lazy pruning technique that only discards ?harm- f u r  rules from the rule base. Third, we developed a two steps classification approach, in which ?first? class rules (i.e. rules used in  the classification of training cases) are considered first, and second class rules (i.e., rules not used during the training phase) are used next for the same task of classification. Second class rules are considered only when a test case cannot be classified by means of first class rules.

In the following Section, we show by means of a moti- vating example how previous approaches fail the classifica- tion of a new test case because of excessive pruning. Sec- tion 2 discusses the probIem of associative classification.

In Section 3 we present the generation of the L3 classifier by means of multiple support thresholds and lazy pruning.

Housing Finance Social Health Class h:c f c  s:np he:r recommended h:c f c  s:sp he:r recommended h:lc f i c  s:np h e m  not recommended  Figure 1. Example training data set  Rule Conf Supp [ f c }  - rec. 100.00% 66.67% {h:c) - rec. 100.00% 66.67% (hex} i rec.. 100.M)% 66.67%  [ Attribute I Domain 11 Attribute I Domain I I Housine I convenient. 11 Social I nonorob. I  1ess.conv slightly-prob  inconvenient. recornmended notIec.

less-conv  Figure 2. Domains of attributes  Section 4 describes how L' classifies test data by means of its two levels. Section 5 provides an extensive suite of ex- periments to validate the L:' approach. Finally, in Section 6 we discuss the main differences between our approach and previous work on associative classification, and Section 7 draws conclusions.

1.1. Motivating Example  Current approaches to associative classification. albeit to a different extent. select association rules for the classifier by matching them with training set cases. Once one (e.g., CBA [7]) or more rules (e.g.. CMAR [ 6 ] )  cover a case. the case is discarded. Hence, all other rules that were covering (only) the same case will not he included in the classifier.

Consider the example training dataset in Fig. I .  which describes the characteristics of two classes of nurses: rec- ommended and not recommended. Values of attributes have been abbreviated. for the sake of rule representation. The actual domains of attributes are given in Fig. 2.

Given this training data. the extraction of all rules. with- out using suppon and confidence constraints, yields 38 rules. In Fig. 3 are reponed some of the extracted rules, sorted by descending confidence, descending suppon, and increasing length.

We now apply the pruning techniques proposed in pre- vious approaches to derive the classifier rules. A first prun- ing technique, applied by most previous approaches (e.g..

CMAR [6]. CBA [7] and ADT (111) uses general rules to prune more specific rules with lower confidence. The rules remaining after this pruning are 9.

CMAR next applies a pruning technique based on x' coefficient. Let all rules he positively correlated and not pruned by this technique. Finally. the last pruning technique adopted by CBA and CMAR is the dumbuse coverage tech- nique. In panicular, CBA extracts the minimal number of  {he:nr} - not rec. 100.00% 33.33% (he:r.s:np) - rec. 100.00% 33.33% {he:r.s:sp} + rec. 100.00% 33.33%  {s:np) + rec. 50.00% 33.33%  Figure 3. Some of the example generated rules  ,..... ...... ......

Rule Conf Supp ( fx )  - rec. 100.00% 66.67% {t ic}  + not rec. 100.00% 33.33%  Figure 4. Example CBA classifier  rules necessary to cover each case in the training data. Af- ter this pruning, the rules in Fig. 4 compose the final CBA classifier.

Differently from CBA. CMAR selects at least 6 rules be- fore discarding any training case. This allows the gener- ation of a "richer" classifier containing more rules. which will be able to cover a wider spectrum of test data. By using 6 = 2 (adequate for such a small training dataset), CMAR yields a classifier composed by the rules in Fig. 5 .

Now consider the following new (test) tuple: (h:lc, Elc.

s:sp, he:r). Both CBA and CMAR classifiers are not able to classify this tuple because of an excessive pruning. Indeed.

e.g.. rule (he  : 1': s : s p )  - mc.  (and several others which would cover the case) has been discarded. although with high confidence (IM)%)'  Excessive pruning affects (albeit to a different extent) all previous proposals of associative classifiers. The database coverage technique used by CMAR is a first step towards a reduction of the negative effect of excessive pruning. but still useful knowledge is lost. We argue that rules may he useful to cover new cases even if they have not been used to cover training data and should not he discarded. However,  'Suppan IS only 33.33%. low with resprcr CO other rules. so a simple threshold on suppon may have discarded chis ru le  as well.

Rule Conf Supp { f c )  - rec. I00.0OW 66.67% . , (h:c] - rec. 100.00% 66.67% {f ic )  - not rec. 100.00% 33.336 [he:nr] -not rec. 100.00% 33.33%  Figure 5. Example CMAR classifier     since the quality of these rules has not been verified dur- ing the training phase (they have not been used), these rules should be treated differently from the high quality rules se- lected during the training phase. To this end, we propose a two steps classification process, in which the second step considers rules usually pruned by the other algorithms.

2. Problem Statement  The database is represented as a relation R. whose schema is given by k distinct attributes A1 . . . A b  and a class attribute 6. The attributes may have either a cate- gorical or a continuous domain. For categorical attributes, all values in the domain are mapped to consecutive posi- tive integers. In the case of continuous attributes, the value range is discretized into intervals, and the intervals are also mapped into consecutive positive integers2 In this way, all attributes are treated uniformly.

Each tuple in R can be described as a collection of pairs (attribute, integer value), plus a class label (a value belong- ing to the domain of class attribute C). Each pair (attribute, integer value) will be called item in the reminder of the pa- per. A training case is a tuple in relation R. where the class label is known, while a test case is a tuple in R where the class label is unknown.

A classifier is a function from A, ,  . . . ,A ,  to C, that al- lows the assignment of a class label to a test case. Given a collection of training cases, the classification task is the generation of a classifier able to predict the class label for test cases with high accuracy.

Association rules extraction is a recently proposed ap- proach for the classification task. Association rules are rules in the form X + Y. When using them for classification purposes. X is a set of items, while Y is a class label. A cased  is said to match a collection of items X when X C d.

The quality of an association rule is measured by two pa- rameters, its support, given by the number of cases match- ing X U Y over the number of cases in the database, and its confidence given by the the number of cases matching X U Y over the number of cases matching X. Hence, the classification task can be reduced lo the generation of the most appropriate set of association rules for the classifier.

Our approach to such task is described in the next Section.

3. Mining classification rules  In our approach. abundance of classification rules is im- portant. to allow a wider selection of rules when classifica- tion is performed. Hence, our aim is to extract a large num- ber of rules, by settine rather low suuwrt and confidence " ..

'Thhe pmblcm of  di~~rrti~ation has k e n  widely dealt with in the ma- chine lcaming community (we. e.g.. 141) and will not k discussed funher in this p a p r  thresholds. Ideally, setting the support threshold close to zero could be considered, together with a limitation on the number of generated rules by screening rules on the con- fidence value. An attempt in this direction has been pro- posed in [IO], where classification rules are extracted only with a confidence threshold, but it is unclear how well the approach scales on training cases characterized by a large number of items.

Currently, the most efficient approach to the extraction of classification rules is proposed in CMAR [ 6 ] ,  which is based on the well-known FP-growth[S] algorithm. To per- form classification rule extraction. we use a variation of the rule extraction pan of the CMAR algorithm, with a variable support threshold, that allows us to treat uniformly classes characterized by an uneven number of training cases. In the remainder of this Section, we discuss the use of multiple support thresholds for rule extraction and the lazy pruning technique applied by L".

3.1. Multiple support thresholds  Most algorithms for the extraction of classification rules, set only one fixed threshold nrinrup for the minimum sup- port value. As already stated in [8], this is not the best choice when the training cases are unevenly distributed among classes. In [SI, the extraction algorithm uses adiffer- ent support threshold for each class ci given by minsup, = mansup* freq(c,), where f r e y ( c ; )  is the frequencyofclass e;. However. the number of extracted association rules de- pends also significantly on the characteristics of the data distribution in class ci. Hence, this definition of minsup, does not guarantee that a sufficient number of rules for classes having low frequency will be generated.

We use an iterative approach, in which the appropri- ate support threshold minsup, for each class is selected by analyzing the result of the previous extraction cycle.

As an initial value for support, analogously to 181, we set minsup!') = minsupr freq(c,) .  Afterrhis first step, since the availability for all classes of some rules with high conti- dence is important, we check the following two conditions for each class q: (a) an excessively low number of high confidence rules has been extracted for class ci with respect to the average number of rules (less than 15%). or (b) class c, is characterized by a significantly smaller number of rules with respect to the average number of rules in classes (less than 20%). If any of the above conditions is true. minsup, is further lowered as minsuppl") = 0.9 * minsupik-') where k is the iteration number. We have found after several experiments that the constant value 0.9 allowed us IO grad- ually decrease the support threshold. Furthermore, only a few iterations were necessary before both above conditions became false. The effect of variable support thresholds is further discussed in Section 5.

3.2. Lazy Pruning  Although the number of association rules generated by rule extraction can be huge, we argue that most rules can provide useful knowledge for the classification of test cases.

Hence, in L' lazy pruning is proposed, to limit the number of pruned rules to a minimum, as described below. This technique yields good precision results only if it is coupled with the novel classification technique described in Sec- tion 4, as shown by the experiments presented in Section 5.

Before performing lazy pruning, a global order is im- posed on the rule base (similar to [6, 71). Let rl and r2 be two rules. Then T I  precedes rz ,  denoted as T I  > T Z if ( I )  conf(rl) > conf(r2), or (2) conf(r1) = conf(r2) and sup(r1) > sup(r2). or (3) conf(r1) = conf(r2) and sup(rl) = sup(r2) and len(rl) > len(rz), or (4) if conf(rl) = conf(r2) and sup(r1) = sup(r2) and len(r,) = le.(.*) and lex(rl)  > lex(rz), where len(r) denotes the number of items in the body of r, and lex(r) denotes the position of T in the lexicographic order on items.

The only significant difference with respect to previous work is that in point (3) we son rules in decreasing length order, while previous approaches prefer short N k S  over long rules. The reason for this choice is to give a higher rank in the ordering to more specific rules (rules with a larger number of items in the body) over generic rules, which may lead to misclassification. Note that, since shorter rules are not pruned. they can be considered anyway. when specific rules are not applicable.

The idea behind lazy pruning is to discard from the clas- sifier only the rules that do not correctly classify any train- ing case, i.e., the rules that only negatively contribute to the classification of training cases. To this end, after rule sort- ing. we cover the training cases to detect "harmful" rules.

Lines 1-24 of the pseudocde in Figure 6 show OUT ap- proach. For each training case, we assign it to the first rule in the sort order that covers it (lines 2-17), and we check if the assigned class label was correct or wrong (lines 9- IO). The case is then discarded (line 16).) After all cases have been considered, the pruning step discards rules that only classified training cases wrongly (lines 18-23). Train- ing cases classified by discarded rules enter again the as- signment loop (lines 1-24), and the cycle is repeated until no training case is covered by discarded rules.

After having discarded "harmful" rules, the remaining rules are divided in two groups (lines 25-30):  used rules which have already correctly classified at least one training case,  spare rules which have not been used during the training phase, but may become useful later.

3N0~c that the case i o  discarded also if i t  is not covered by any mlr.

Procedure generateClassifier(ru1 es,duta) 1.  while datu not empty { 2.

3. covered=false; 4.

5 .

6.

7.

8.

9. if (d.class==r.class) r i gh t++ ; IO. else r.wrong++; I I .  covered=tNe;  13. NR--; 14. r=next rule from rules;  16. delete d from datu; 17. } 18. for each T in rules { 19.

20. delete r from rules; 21. data =data U r.dataClassified  23. ] 24. ] 25. for each T in rules [ 26. if r.right>O 27.

28. else 29.

30. ]  for each d in data {  NR = number of rules; r = first rule of rules; while (covered==false) and (NR>O) {  if r covers d { r.dataClassified = r.dataClassified U d;  12. }  15. ]  if r.wrong>O and r.right==O {  22. ]  usedRules = usedRules U r;  spareRules = spureRules U r ;  Figure 6. L7 classifier generation  Used rules are generated with a database coverage tech- nique similar to other approaches [6.7] and provide a high level model of each class. Spare rules. instead, allow us to increase the precision of the classifier by capturing "spe- cial" cases which are not covered by used rules (see Sec- tion 1.1). Both groups of rules are used to create the clas- sifier. In particular, used rules are assigned to the first level of the classifier, while spare rules yield the second level.

Rules in each level are ordered following the global order described above. Our pruning technique is very simple, compared with previous proposals (see, e.g.. 16.7, I I]), but it is very effective, as shown by the experiments in Sec- tion 5 .  It eliminates from the rule base only those rules which yield negative effects on the classification of train- ing cases. As discussed in Section 5, we observe that such a simple pruning is not adequate if the classifier is to be generated only with used rules. Hence, more tolerance in pruning is effective only if coupled with more tolerance in the generation of the classifier.

We finally note that pruning of more specific classifica- tion rules as suggested in [6, 71. even with a lower confi- dence, is not performed. Indeed, if the iterative rule deletion loop eliminates a general rule. the specific rule may remain and i f  it does not generate erroneous classifications, be as- signed to spare rules and become useful for test cases.

4. Classification  In this Section we describe the classification of test cases by means of the two levels L? classifier. When a test case is to be classified, rules of level I of the classifier are consid- ered. If no rule in this level matches the test case, then rules in level II are considered. If again no rule matches the test case, the case is assigned to the default class.

Differently from previously proposed associative classi- fiers, L:? usually contains a large number of rules. In par- ticular, level I of the classifier, containing rules used in the training phase to cover some training case, is characterized by a small number of rules. analogously to previous ap- proaches. As shown in Section 5 ,  this level performs the ?heavy duty? classification of most test cases and can pro- vide a general model of each class. By contrast, level 11 of the classifier may contain a large number of rules which are seldom used. These rules allow the classification of some more cases, which cannot be covered by rules in the first level. Hence, the few used rules in the second level allow a significant increase in classification precision, as shown in Section 5.

Since level I usually contained about 102-103 rules, it can easily fit in main memory. Thus, the main classifi- cation task can be performed efficiently. Level 11, in our experiments, included around lo5 rules. Rules were orga- nized in a compact list, sorted as described in Section 3.2.

Hence, level 11 of L? could be loaded in main memory as well, as discussed in more detail in Section 5. Of course, if the number of rules in the second level further increases (e.g., because the support threshold is further lowered to capture more rules with high confidence), efficient access may become difficult. We are currently considering a more compact representation of rules, based on the relationship between a generic rule and its specializations.

We finally note that. similarly to [7, I I], we only con- sider the first rule matching the test case to classify it, but a further increase in precision may be obtained by performing classification with multiple rules, as described in (61. In- deed, this technique is orthogonal to our approach and can be separately applied to each L3 level.

5. Experimental Results  In this Section. we describe the experiments to measure accuracy, classification efficiency, and memory consump-  tion for L?. We have performed a large set of experiments using 26 data sets downloaded from UCI Machine Learn- ing Repository [ I  ]. We compared L3 with the classification algorithms CBA [7], CMAR [6], and C4.5 [9]. The exper- iments show that L3 outperforms all previous approaches, by achieving a larger average accuracy (+0.63% over the best previous, i.e., CMAR), and improving accuracy on 14 data sets over 26.

The confidence constraint has not been enforced. i.e., minconf=O. The value of the lowest rninsup, (column 5).

and the average value of mznsupi (column 6) for each data set are shown in Table 1. We have adopted the same tech- nique used by CBA to discretize continuous attributes. A I O  fold cross validation test has been used to compute the accuracy of the classifier. All the experiments have been performed on a I m M h z  Pentium 111 PC with 1.SG main memory. running RedHat Linux 7.2.

Table I compares the accuracy of L3 with the accuracy of C4.5, CBA and CMAR. obtained using standard values for all the parameters. In parlicular, the columns of Table I are: ( I )  name of data set, (2) number of attributes, (3) num- ber of classes, (4) number of cases (records), (7) accuracy of C4.5, (8) accuracy of CBA, (9) accuracy of CMAR, and (10) accuracy of L?.

fi? has best average accuracy (+0.63% with respect to CMAR) and best accuracy on 14 of the 26 UCI data sets (over 50% of the data sets). The improvement in accuracy may be due to several factors: (a) the use of multiple support thresholds, (b) the lazy pruning technique, and (c) the use of rules included in the second level of the classifier.

It has not been necessary to adopt multiple support thresholds in most of the 14 cases in which L3 achieves better accuracy than previous approaches. Hence, multiple support thresholds only have a limited effect on the average accuracy value.

To observe the effect of rules in level II of L?, we com- pare the accuracy obtained by only using rules in level I of LJ with the accuracy obtained by using both levels.

The result of the experiment is reported in Table I .  The related columns of Table 1 are: (1 1) number of rules in the first level of L?, (12) number of rules in the second level, (13) accuracy of L3 using only rules in the first level, (14) difference between L3 with both levels (column (IO)) and L3 with only first level (column (13)).

By considering only rules in the first level, L3 achieves best accuracy on only 6 of the UCI data sets and an aver- age accuracy somewhat lower than CBA and CMAR. In particular. the average accuracy is about 1% less in the worst case (i.e., with respect to CMAR [6]). Hence, the first level already captures most fundamental characteristics of each class, thus providing a model of reasonable qual- ity. However, this experiment also shows the importance of rules stored in the second level. Indeed, lazy pruning     I Average I I I I 0.94 I 1.04 I 83.34 I 84.69 I 85.22 I 85.85 I I I 84.18 I +1.67 I Table 1. Comparison of L' accuracy with respect lo C4.5, CBA, CMAR  by itself (considering only level I rules) yields a "medium" quality classifier, while the joint effect with the presence of level 11 rules allows a significant gain in average accuracy (+1.67%). This result shows that the small number of rules used for covering training data (rules in level I). albeit very effective, is not sufficient. A significant improvement in ac- curacy is obtained by exploiting the knowledge contained in rules which have not been used for covering training cases (i.e., level II  rules). We observe that, usually, the number of test cases correctly classified by the second level is rather small, but sufficient to obtain a better accuracy for L?. For example, for data set labor level II  correctly classifies just 3 more test cases, but since the data set is composed by 57 cases, this allows a +5.26% in accuracy. In the case of data set glass the improvement obtained by the second level is more evident. Indeed. 23 more cases are correctly classified over a total of 214 cases (+10.75% in accuracy).

To verify if lazy pruning affected classification accuracy, we also ran tests (not reported here) using two level classi- fication without performing any pruning of rules. For these  experiments. each training case is considered only once for matching (lines 2-17 in Fig. 6) .  Rules that matched at least one training case (disregarding correctness of the match) are assigned to the first level. while the second level contains all remaining rules. These tests resulted in lower average pre- cision (-1.44%) with respect to the value reported for L:l in Table I .  showing that also two level classification by itself is not an effective classification technique.

Since the number of rules in the two levels of L' is sig- nificantly different (see Table I ) .  we analyzed the perfor- mance of L3 during the classification of test data. The re- sults are reported in column ( 5 )  of Table 2, which shows the CPU time required by the classification of the whole data set as a test set using L3. We have also make a test using only the first level, but we have not reported those times because they are very closed to the ones showed in Table 2. A larger time is necessary for the generation of the classifier with respect to previous approaches, owing to the lower (and variable) support threshold for assvciation rule extraction. However, we observe that the generation of the     classifier is a task that takes place very rarely. as opposed to the classification of new cases. Thus, a longer generation time may be acceptable, in order to increase significantly the precision of the classification step. Table 2 also shows the number ofcases classified by rules of the first level (col- umn (3) rightly classified + wrongly classified) and by rules of the second level + unclassified data (column (4)). The number of cases in each data set is reported in column (2).

We observe that a vast majority of cases is classified by rules of the  first level, while only a minimal number of accesses to the second level is required. Furthermore, rules of sec- ond level usually correctly classified each considered case.

Only in very few cases rules of the second level are not able to classify a case, or wrongly classify it. Hence, although the number of rules in the second level can be quite large, accessing it is a rare event that does not significantly affect performance. while it significantly improves accuracy.

To 0btain:efficent classification, it is important that rules in both levels can be stored in memory. Hence, we ana- lyzed main memory usage of L' for the storage of the rule base during the classification phase. Table 2 reports the re- sults. The columns of table 2 are: (6)  number of rules in the first level of L", (7) number of rules in the second level of LT3, (8) memory used to store the rules of the first level (in Kbyte), (9) memory used to store the rules of the sec- ond level (in Kbyte). As expected. the memory required for storing first level rules is negligible, while the second level rules need significantly more memory. especially for some data sets (up to SOMbyte). Hence, on currently available machines (512Mbyte main memory is a standard today) we are able to store second level rules in main memory. The ability of storing second level rules is closely related to the value of the support threshold. Since we believe that second level rules have to be stored in main memory, we selected support threshold values that did not generate an excessive number of rules, such that main memory storage becomes impossible. For the presence of the second level L3 required more memory of CBA and CMAR, and so L? memory pre- formance is:worse than the previous ones.

6. Previous Related Work  Associative classification has been first proposed in CBA [7]. CBA, based on the Apriori association rules mining al- gorithm [2], extracts a limited number of association rules ( m a  8oooO). A weakness of this approach is the reduced number of rules extracted by means of Apriori. because long and interesting rules are usually not generated. thus losing some relevant knowledge. Furthermore, as discussed in Section 1.1, after sorting on descending confidence, a pruning technique is applied, and only the minimal num- ber of rules necessary to cover training data is used to cre- ate the classifier. A new version of the algorithm has been  presented [SI, in which the use of multiple supports is pro- posed to increase accuracy in  presence of uneven class dis- tributions, together with a combination of C4.5 and Naive- Bayes classifiers, to be used when CBA rules wrongly clas- sify training cases. While using multiple supporl thresh- olds permits to overcome to some extent the limitations due to the use of the Apriori algorithm. i t  does not address the overpruning problem described in Section I . I .

ADT [ I l l  is a different classification algorithm based on association rules, combined with decision tree pruning techniques. All rules with a confidence greater or equal to a given threshold are extracted and more specific rules are pruned. A decision tree is created based on the remaining association rules. on which classical decision tree pruning techniques are applied. Analogously to other algorithms, the classifier is composed by a ma l l  number of rules and prone to the overpruning problem.

CMAR[6] is the latest classification algorithm based on association rules proposed i n  literature. CMAR proposes a suite of different pruning techniques: pruning ~Tspecialis- tic rules, use of the x z  coefficient. and database coverage.

As already noted, the database coverage technique is more tolerant than the coverage technique adopted by CBA, and allows more (but not all) rules to cover the same training case. Again, useful rules may be pruned, thus yielding the same overpruning problem. A further technique applied in CMAR to increase average accuracy is the classification of test cases by means of more than one rule. This technique is independent of the adopted pruning techniques, and is likely to be profitable in our setting as well. Hence, we are considering it as a further improvement of our classifier.

7. Conclusions  In this paper we have described L", a novel approach to classification by means of association rules. While pre- vious approaches suggested various techniques to generate a classifier containing a very limited number of rules, we believe that it is important to exploit all the knowledge that can be extracted from the training cases. Hence, we propose a lazy pruning technique, which only discards "harmful" rules, coupled with a two levels classification approach, in which rules normally discarded in previous approaches are included in the second level of the classifier and used only when first level rules are not able to classify a test case.

We observe that. even in presence of a large number of rules into the second level of L? classifier, the interpretabil- ity of techniques based on classification rules is not lost. In- deed, used rules (rules in the first level of the classifier) cap- ture recurring properties of the data, thus providing a high level model of each class, composed of just a few rules.

Furthermore, we note that overfitting is avoided by the second level of the classifier. Indeed, as shown in Section 5 ,     Table 2. Classification time and memory usage of L?

this level forces a priority ordering among rules that avoids considering a "medium" quality rule belonging to the sec- ond level, when a better rule is available for classification in the first level. Hence. considering second level rules can never cause a reduction in precision, since cases classified by these rules would he otherwise unclassified (or assigned to the default class).

The lazy pruning approach allowed us to significantly increase average classification precision over previous ap- proaches. In particular, the experiments show that the pres- ence of the second level provides a major enhancement in the classification quality, without reducing the efficiency of the classification activity.

