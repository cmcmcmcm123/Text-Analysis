Scalable Context-Aware Role Mining with MapReduce

Abstract?Cloud computing platforms facilitate efficiently pro- cessing complicated computing problems of which the time cost used to be unacceptable. Recent research has attempted to use role-based approaches for context-aware service recom- mendation, yet role mining problem has been proven to be difficult to compute. Currently proposed role-mining algorithms are inefficient and may not scale to cope with the huge amount of data in the real-world. This paper proposes a novel algorithm with much better runtime complexity, and in MapReduce style to take advantage of popular distributed computing platforms.

Experiments running on a medium-sized high performance com- puting cluster demonstrate that our proposed algorithm works well with both running time complexity and scalability.



I. INTRODUCTION  Recently, cloud computing platforms are widely applied for dealing with various of mining missions on big data. They involve the internet for providing on-demand access to shared resources, such as compute units, data, storage and applica- tions. MapReduce programming model [5] is one of the most popular distributed computing technologies for distributing big data processing across a large cluster of compute nodes in the Cloud. It is quite effective in simplifying the development of data-intensive applications, through the use of a simple API, and with a robust runtime system. These infrastructures truly facilitate efficiently processing complicated computing problems of which the time cost used to be unacceptable.

Role-based approaches are widely applied to data security domain [16] [9] [8]. Further, recent research has attempted to use role-based approaches to recommend mobile services to other members among the same context group [18] [21].

However, role mining problem has been proven to be difficult to compute, it is in fact NP-hard [15].

Two context-aware role mining algorithms based on matrix calculation have been recently proposed [21] - one is an ap- proximation algorithm and another one is a complete algorithm for verification. Both of them have their weaknesses and may not applicable under some circumstances; and therefore, this paper is a continuation work of [21]. Let u to be the number of users, b to be the number of behaviors, c to be the number of contexts, and r to be the total number of roles; although algorithm C2 (AC2) in [21] is a complete algorithm that can cover all roles, it has a complexity of O(u2 ? 2c) because of the combination procedure for listing the power set S = {?context, behavior? couples of the user}. As a result, if the number of contexts becomes large, AC2 will not be practical any more. Besides, the algorithm V2 (AV 2) has a complexity  of O(u2?c)), which is much faster than AC2. However, AV 2 is not complete, which means that its role coverage could be very low (< 10%) in certain situation, e.g., when the number of contexts and users are very large.

In this paper, similar to [18] and [21], we extend the behavior pattern recognition method [3] to identify context- aware roles from multi-user behavior patterns. Different from [18], [21] and all other related work to this paper, the proposed algorithm AD1 is complete as well as with applicable and practical running time complexity. To apply compute cluster for parallel computing, we have applied MapReduce [6] frame- work in this algorithm, which can be deployed to an Apache Hadoop environment. Leverage on high performance compute cluster, algorithm AD1 shows strong scalability. Experiments are also performed to demonstrate the capacity and scalability of this algorithm.

The organization of this paper is as follows. Section II summarizes the related work. Section III defines the context- aware role mining problem. Section IV presents our proposed new algorithm for context-aware role mining, and it is divided into five subsections. The first subsection talks about the main challenges for the algorithm design; the second subsection talks about main procedure and the third subsection explains the partition method; in the fourth subsection, methods for reducing the search space are discussed. Lastly, we present this map-reduce-style code in detail. After that, Section V presents experiment results to show the performance and scalability of the new algorithm. Finally, Section VI concludes the paper.



II. RELATED WORK  The concept of roles has been used in many areas such as linguistics, knowledge representation, relational database, and access control [2], [10], [14]; especially, roles have been widely used in data security domain, under RBAC, e.g., [16].

Numerous role mining approaches have been proposed, e.g., [8], [9], [17], [21].

Recently, context, which used to be mostly disregarded in RBAC, is considered as an important factor. To the best of our knowledge, [1], [12], [18] are the only papers addressing context-aware RBAC. However, both [1] and [12] did not describe how to discover or identify context-aware roles. Tra- ditional RBAC-based role mining algorithms (such as [9] and [17]) cannot be directly applied, since traditional RBAC mod- els only consider 2-D parameters, namely ?user, permission?, while context-aware role mining approach needs to consider      Fig. 1. User-context-behavior matrix (BiUCB)  Fig. 2. Role-context-behavior matrix (CBA)  at least 3-D dataset ?user, context, behavior? where context includes time and scene, etc.

Apache Hadoop is an open source system that allows for the processing of big data across cluster of workstation class computers [20]. Two of the key ideas of Apache Hadoop are originated from Google file Systems (GFS) [11] that allows for the distribution of data across thousands of computers in a cluster, and MapReduce [6] that handles the breakdown of a problem into subtasks so that they can be processed by the cluster. MapReduce [5] is a programming model from Google.

The model comprises of two main functions, Map and Reduce functions. The Map function takes in a key/value pair and outputs an intermediate list of key/value pairs. The Reduce functions will than take all values associated to the same key and produce the final output list of key/values. Users are tasked with providing the Map and Reduce implementation.

Apache Mahout is part of the ?Hadoop? family which provides scalable machine learning libraries to handle rec- ommendation mining problems under ?Hadoop? platform.

Apart from typical clustering and singular value decomposition (SVD) functionality, it also provides Naive Bayes, decision tree, etc., type of classifiers. Besides, frequent itemset mining is also supported. On the other hand, Chen et al. [4] and Wang et al. [19] are recent examples of MapReduce-based solutions to frequent itemset mining problem. To the best of our knowledge, this is the closest related mining problem considered in ?Hadoop?, and context-aware role mining is a new area due to be explored.

This paper is a significant extension of the work done in [18], [21], in which [21] proposed a simpler and more efficient algorithm to mine roles than [18]. However, the matrix-based algorithms presented in [21] does not guarantee completeness, i.e., the algorithm may fail to uncover some role patterns. This paper provides a scalable algorithm that is based on the ideas of divide and conquer and also Map-Reduce paradigm distributed computing as solutions to role mining problem.



III. PROBLEM DEFINITION  A service recommendation engine must know the services consumption behaviors in the user community before any recommendation can be made. We firstly study the knowledge acquisition by defining the role mining problem.

Fig. 3. User-role matrix (UA)  We follow the definition of the context-aware role mining problem from [18] and [21]. However, for the simplicity of presenting and computing, a traditional u ? c UCB matrix with b behaviors in cells is transformed to a u? (c? b) matrix by spreading each column in UCB to b columns and setting the respective values in the new column to be 1 according to the old cell values in UCB. We call this matrix BiUCB, of which each column represents a different context-behavior pair(cb-pair). As a result, a role can be presented as a set of users plus a set of column numbers of BiUCB.

Definition 1 - Context-aware role mining problem (CA- RMP): Given a user behavior pattern configuration bpc = ?U , C,B, BiUCB?, where U is a set of users, C is a set of contexts, B is a set of behaviors, BiUCB ? U ? ?C,B? is the user-context-behavior relation. CA-RMP is to find a state ?R,UA,CBA? that is consistent with bpc, such that for any ?R?, UA?, CBA?? that is consistent with bpc, |R| ? |R?|, where R, R? is a set of roles, UA, UA? ? U ? R is the user-role assignment matrix, CBA, and CBA? ? R ? ?C,B? is the role-context-behavior assignment matrix. The state is consistent with bpc, if every user in U has the same set of context-behavior assignment as in bpc. Each role must have at least two u ? U (minu ? 2) and two ?c, b? ? ?C,B? (minc ? 2) to ensure generality.

One can summarize the relationship between BiUCB, UA and CBA by the following formula:  BiUCB = UA? CBA where ? represents a boolean matrix multiplication between two boolean matrices in which X ? Y = ?  k  (Xik ? Ykj) [13].

Suppose there is a user set U = {u1, u2, u3, u4, u5}, a context set C = {c1, c2, c3, c4, c5}, and a behavior set B = {1, 2, 3}. A sample UCB can be constructed based on users? behavior patterns as shown in Figure 1 - where each column denotes a ?c, b? ? ?C,B?, each row denotes a u ? U , and a value of 1 in cell (i, j) represents that the user ui exhibit the ?c, b?j ; otherwise, a value of 0.

After mining roles from the behavior pattern configuration, we obtain a state ?R,UA,CBA?, where R = {r1, r2, r3, r4}, the resulting CBA and UA matrices are recorded in Figures 2 and 3, respectively. As shown in Figure 2, four mined roles r1, r2, r3 and r4 are listed in a CBA matrix, e.g., r2 has behavior 1 under context c1 and c3, and behavior 2 under context c2.

Figure 3 shows a UA matrix, where a 1 in cell (i, j) represents that user ui can play role rj . e.g., user u1 can play role r2.



IV. MAP-REDUCE MERGING-STYLE ALGORITHM (AD1) Generally, a modern high performance computing cluster  could have over ten thousands of computing units. By applying a powerful cluster and well designed distributing algorithms     with strong scalability, we expect to process huge input data sets with hundreds times of scale bigger than that for some standalone algorithms but with the same time cost.

A. Challenges in Distributing Role Mining Algorithm Design  The Divide and Conquer theory is often applied for obtain- ing strong scalability. By this approach, an essential issue is how to divide the data into parts and merge the intermediate results later. A naive divide policy is to separate the matrix by selecting parts of the columns and rows first, and then simply apply some standalone mining methods to find all the valid roles in the sub matrices, and finally merge all the roles generated from the sub matrices together. However, the roles related to the data across multiple sub matrices will be omitted.

As a result, this kind of partition method can not lead to a complete algorithm.

Generally, the original input data set of a role mining job is not very huge. A typical one should be no more than 1 million users, 100 contexts and 100 behaviours. However, the number of roles r is sensitive to all of b, c, u, minu and minc and r could be a combination of b ? c in the worst case. The huge computing load comes from the tremendous number of 1 on 1 comparing between roles.

minu and minc are the key parameters to control the output scale and the algorithm complexity. In most of the cases for getting a practical output, the role number should always not be very huge and could be well controlled by adjusting minu and minc. However, since the data is partitioned, it is hard to apply them to filter the data distributed in different machines.

B. The Main Procedure of Algorithm (AD1) To avoid the problem of the naive partition method above,  a bottom up merge-style algorithm AD1 is proposed to com- pletely cover all the roles. The main idea of AD1 is to firstly compute all the intermediate roles with fewer cb-pairs and then generate the more complex roles with more cb-pairs by merging them. For example, suppose we have two roles:  role1 : {u1, u2, u5, u6} ? {cbpair1} role2 : {u3, u4, u5, u6} ? {cbpair2}  after one round of merging, a new role is generated:  role3 : {u5, u6} ? {cbpair1, cbpair2}  The user set of role3 is generated by getting the intersec- tion set of the user sets of role1 and role2, while the cb-pair set of role3 is generated by getting the union set of the cb-pair sets of role1 and role2.

The essential process of AD1 is that, in each round of iterations, all the roles generated from last round are merged with each other. During this process, the logic is simple so that the tasks are easy to split and distribute to multiple computing units. The running time complexity of AD1 is:  O(log(b? c)? r2 ? u)  The factor log(b ? c) is the round number of the merge iteration, for after each round of this iteration, the maximum  Fig. 4. Partition Policy in Algorithm AD1  number of cb-pairs of the covered roles will become twice until it reaches b? c. Generally, for the strong requirement of minu and minc, there should always be no new qualified role generated after looping for only very few rounds far less than log(b?c). The factor r2 is for the number of all the role pairs.

The factor u is for merging the user set operations between the two roles.

When BiUCB is a random matrix, roles with more users and cb-pairs have less probability to be formed. Further, this probability reduces fast (exponential) with the increasing of user and cb-pair numbers in roles. Hence, r is close to the number of roles with very few users and cb-pairs, which means r is close to a polynomial function with u,c and b.

C. The Partition Policy of Algorithm (AD1) In each round of the merging process, we distribute the  huge number of merging tasks to multiple computing units.

The total number of merging operations for each round is O(r2). We do not simply generate the data of all the role pairs and split it by the build-in partition tools of Hadoop, because it will definitely generate huge intermediate data and bring unacceptable I/O cost. Instead, each mapper job loads all the roles to the memory and choose respective different role pairs according to their job IDs.

A partition example is shown in figure 4. All the role pairs are separated uniformly to each mapper, the numbers of role pairs assigned to each mapper are nearly even.

By the merging operations in the mapper, most of the role pairs will not generate any new role. As a result, the scale of the mapper output is very small, which brings extra benefits for reducing the cost of network transferring within the cluster.

D. Search Space Reduction  The primary search space for each round of the merge iteration is O(r2), which is still very large. Actually, only very small part of this search space is useful for generating new roles. It can be dramatically reduced by the operations in this subsection. For facilitating the expression, basic necessary formulations are introduced in table I.

Definition 2 - u-pairs(u1)={?u, |Scb(u) ? Scb(u1)| ? minc} u-pairs is the essential concept for applying minc to reduce  the search space. We also introduce the following lemmas that are useful for reducing the search space:     Lemma 1: ?r1, r2, n, r2 ? Succn(r1) ? n > R(r1) + 1? ?m, r3,m < n ? r3 ? Succm(r1), r2 ? Succn(r3) Lemma 2: ?r1, r2, |Su(r1) ? Su(r2)| < minu? ?n, r3 ? Succn(r1), r4 ? Succn(r2), |Su(r3) ? Su(r4)| < minu Lemma 3: ?u1, u2, r1, u1 ? r1 ? u2 ? r1 ? |Scb(r1)| ? minc? u1 ? u-pairs(u2) ? u2 ? u-pairs(u1) Lemma 4: ?u1, r1, u1 ? r1 ? |Scb(r1)| ? minc? Su(r1) ? u-pairs(u1) Lemma 5: ?u1, r1, |u-pairs(u1) ? Su(r1)| < minc? ?n, |u-pairs(u1) ? Su(Succn(r1))| < minc These lemmas are apparent. From them, we have the  following operations for reducing the search space:  ? From Lemma 1, in each round we only process the new roles generated in the last round.

? From Lemma 2, by applying a reverse index for finding roles related to a single user, we only process the role pairs have more than minu common users.

? From Lemma 3,4 and 5, if |u-pairs(u1) ? Su(r1)| < minc, we delete u1 from r1 and then if |Su(r1)| < minu, we delete role1. This step is very effective to reduce the number of useless intermediate roles.

To summarize, by the above approaches for search space reduction, the running time complexity of AD1 becomes:  O(log(b? c)? r ? (ave-user? ave-role))  The ave-user is the average user number of each role and ave-role is the average roles number related to each user.

ave-user ? ave-role is for finding all the related roles having more than minu common users with current role. If the BiUCB matrix is sparse, it will gain much performance benefit.

The total memory cost for loading the reverse index and u- pairs could be well controlled, for they are both static data so that multiple jobs can just load these data with general share memory methods without copying them once for each job. As a result, the total memory cost on a single cluster node will not increase with the number of CPU cores increasing.

TABLE I FORMULATION FOR SEARCH SPACE ANALYSIS  Expression Explanation Ncb(r1) the cb-pairs amount of role r1 Nu(r1) the user amount of role r1 Scb(u1) the cb-pairs set of user u1 Scb(r1) the cb-pairs set of role r1 Su(cb1) the user set of cb-pair cb1 Su(r1) the user set of role r1 R(r1) the round number that role r1 is generated  Succn(r1) the role set generated by merging role r1 and other roles in round n; we also have ?r ? Succn(r1), ?r2, r = r1 ? r2  Fig. 5. Main Procedure of Algorithm AD1  E. The Pseudocode of Algorithm (AD1) In the following pseudocodes in this paper, the parameter  roles is a list to store the known roles. Each item in this list is an object composed of 4 attributes: users, cb, nstatus, ostatus. The attribute users is the set for storing all the users acting this role and the attribute cb is the set for storing all the context-behavior pairs (cb-pairs) acted by this role. The attribute nstatus is for storing the status being processed in the current round of iteration while the attribute ostatus is for storing the status marked in the last round of iteration. The status of a role could be ?old?, ?new?, or ?sub?, of which the ?old? and ?new? status marks are used for avoiding the role comparing operations that have been executed before, and the ?sub? status is used for deleting the detected sub roles.

The pseudocode of the main procedure of AD1 is summa- rized in Figure 5 and a detailed explanation is given below:  ? line 3, we get the column number of BiUCB .

? line 4-5, the role set as initialized. For each role in this  set, its user set is the same as one column in BiUCB,     and it has only one respective column number in its cb-pairs. The status of each role is set to ?new? so that any role pair will be compared in the next round.

? In line 7-19 and line 22-34, some commands for submitting jobs to Apache Hadoop are applied.

? line 7-19, u-pairs restricted by minu and minc for all users are generated. In line 8 and line 9, all the rows and columns of BiUCB are loaded by each mapper job as the indexes with the name of ucbIndex and cbuIndex. In line 10, we set the number of cores used. In line 11, the input directory is assigned, which contains several files, each one of them contains the allocated part number for a single mapper to read. In line 12, the output data directory is assigned. In line 13-15, the source code files of mapper and reducer are assigned. In line 16-19, the partition and sort rules are assigned. In line 18, the reducer job number is set to 1, so that there is only one output file and could be load in line 24 of the next round of iteration.

? line 20 of the main procedure, there is a level-1 loop of maximum log(b? c) times.

? line 22-34, one round of role merge job is submitted.

In line 23, all the known roles will be loaded by each mapper job as a dictionary with the name of oldrole.

In line 24, u-pairs will be loaded by each mapper job as a dictionary with the name of upair.

? line 35, the roles generated in this round are counted.

If no role is generated, the iteration in line 20 stops.

? line 37, roles with less than minc cb-pairs are deleted.

The u-pairs mapper pseudocode of AD1 is summarized in  Figure 6 and a detailed explanation is given below:  ? line 3-4, two indexes for finding all cb-pairs of each user and all users of each cb-pair are loaded.

? line 5, each mapper read a different ID and the total part number from respective file in the input directory.

? line 6, there is the level-1 loop of u times.

? line 7, input users are separated by allcount and se-  lected by no. By the modulus and remainder, different mappers are allocated different users.

? line 8, we print the u-pair of the role itself.

? line 9, we get all the cb-pairs related to u1. line 10-13,  all the users which have some common cb-pairs with u1 are selected and the number of common cb-pairs are counted.

? line 14, there is the level-2 loop of maximum u times.

? line 15, the users with smaller id than u1 and less than  minc cb-pairs are skipped.

? line 16-17, the u-pairs are printed to standard out.

The output of u-pairs mapper are partitioned by the first  user id , and sorted by the whole line. The u-pairs reducer pseudocode of AD1 is summarized in Figure 7 and a detailed explanation is given below:  Fig. 6. u-pairs Mapper Code of Algorithm AD1  Fig. 7. u-pairs Reducer Code of Algorithm AD1  ? line 4-6, u-pair text lines are read and parsed from the standard I/O input one by one.

? line 7-8, add uid2 to the u-pair list of uid1 ? line 10 and 14, only the users with more than minu  u-pairs will be printed to standard I/O output.

? line 11 and 15, one single user and its u-pair list are printed to standard I/O output.

? line 12-13, start to build u-pair list for the next user.

The Merger mapper pseudocode of AD1 is summarized in  Figure 8 and a detailed explanation is given below:  ? line 3 and 4, the known roles and sub roles are loaded from the output of the Merger reducer of last round.

? line 5, from the data in roles, a index for finding all the roles of a user is build and it is loaded in the share memory.

? line 6, all u-pairs are loaded.

Fig. 8. Merger mapper code of algorithm AD1  ? line 8, each mapper reads a different ID and the total part number from respective file in the input directory.

? line 9 and 19, there are the level-2 and level-3 loops of maximum r times. But we ignore the role pairs that have been merged in the former rounds by line 22-23.

? line 10, input roles are separated by allcount and selected by no. By the modulus and remainder, each mapper is allocated a different part of role pairs.

? line 11-13, the sub roles are outputted to the stdout and skipped in the following processing.

? line 14, output the current role with status ?old?.

? line 15-18, all the related roles which have some same  user with the roles[i] are selected. And the number of common users are counted.

? line 20-21, only the roles with bigger position in the role list are merged with role[i].

Fig. 9. Merger reducer code of algorithm AD1  ? line 24-25, the user intersection and cb-pairs union of these two roles is computed.

? line 26 and 30, the roles with only 1 cb-pair will not be processed after the first round.

? line 27-29 and 31-33, a sub role is detected and added to the sub role list.

? line 34-36, if we can not find enough u-pairs for a single user in the user intersection of the two roles, this user is deleted from the intersection.

? line 37 and 38, a new role is created. Its user set is the pruned user intersection of these two roles and its cb-pair set is the union of that of these two roles.

In the Merger mapper output, one role is presented as one text line with three parts separated by tabs. The first part is its cb-pair list, the second part is its user list and the last part is its status. The input of the Merger reducer is sorted by the first 2 parts of a role text line, so that the same roles will be place in the neighbor lines, which allows further status merging.

The function of Merger reducer is mainly about removing duplicate roles and merging the status marks for the same roles. Its pseudocode is summarized in Figure 9 and a detailed explanation is given below:  ? line 4-6, role text lines are read and parsed from the standard I/O input one by one.

? line 8-11, the statuses of the same roles are merged.

For each role, the ?sub? mark replaces the ?old? and ?new? mark, while the ?old? mark only replaces the ?new? mark.

? line 14 and 17, the roles are printed to standard output.



V. EXPERIMENTS  The performance and scalability of AD1 are tested on the cluster at the Faculty of Engineering in University of New South Wales:     ? Leonardi Cluster; Torque 5.4.0; Python 2.4.3.

Leonardi is a medium-sized high performance Computing  cluster. It currently consists of 2,944 AMD Opteron 6174 2.20GHz processor cores, with a total of 5.8TB of physical memory (essentially 2GB of memory per core) and 100TB of usable disk storage. Leonardi runs the Rocks clustering platform on top of CentOS Linux. The maximum of 192 processor cores are used exclusively for our experiments.

Restricted by the platform of Torque in the Leonardi cluster, we do not use Hadoop to manage the mapper and reducer jobs. Instead, we develop a simple module to finish splitting, sorting and merging work to simulate the map- reduce computing model on this platform. Since most of the time cost of AD1 are generated by its mapper jobs, the implementation of our module will not affect the measurement of the experiments. We also believe that these experiments can be easily replicated using Hadoop.

A. Performance trend of algorithm AD1 In Figure 10(a), X-axis is set to user number from 2000  to 10000 and Y-axis is set to time cost. The input BiUCB matrices are all random sparse matrices with 50 contexts, 3 behaviors and the same density; minu = 8 and minc = 3 are applied to control the total time cost. In Figure 10(b), X-axis is set to context number from 50 to 100 and Y-axis is set to time cost. The input BiUCB matrices are all random sparse matrices with 1000 users, 3 behaviors and the same density; minu = 3 and minc = 2 are applied to control the total time cost. In both of them, CPU core number is 48. The time cost of AD1 increases according with increasing of context and user numbers. We also simulate a polynomial trend for them. These lines could approximately act as a O( n3 ) incremental trend.

B. Time Cost Reduction with minu and minc  In all the following experiments, the input BiUCB matrix is a random sparse one with 50000 users, 30 contexts, and 3 behaviors, the average number of cb-pairs for each user is 9;  In Figure 11, the number of CPU cores is 48. In Figure 11(a), X-axis is set to the minu from 4 to 32 and Y-axis is set to the time cost; minc = 5 is also applied to control the total time cost. In Figure 11(b), X-axis is set to the minc from 3 to 6 and Y-axis is set to the time cost; minu = 17 is also applied to control the total time cost. The time cost of AD1 decrease dramatically according with the increasing of minu and minc.

C. Time Cost Reduction with Reverse Index  In Figure 12, the number of CPU cores is 48, X-axis is three algorithms and Y-axis is set to the time cost. minu = 16 and minc = 5 are also applied to control the time cost. A?D1 is the algorithm without the revert index. AD1 obtains consider- able performance improvement with search space reductions.

D. Scalability test of algorithm AD1 In Figure 13, time costs (seconds) are recorded according  with the increasing of CPU cores from 3 to 192 and minu = 17 and minc = 4 are applied to control the total time cost.

The second row is the total time cost; the third row is the time  Fig. 12. Time Cost Reduction with Revert Index  Fig. 13. Execution time vs. CPU cores  cost of all the mappers while the fourth row is the time cost of all the reducers; the last row is the time cost of standalone steps in the process, such as generating the share revert index.

In Figure 14, X-axis is set to be the number of CPU cores and Y-axis is set to the time cost reduced multiples. The total time cost when CPU core number is 3 is set as the baseline, while the value for any other CPU core numbers are 26162 divided by their total time cost.

The time cost for task deploying, data transferring and the job management increases with the increasing of CPU core number and also there are some standalone steps in the algorithm. As a result, the total time cost is not exactly reduced linear related to the increasing of CPU cores, yet still reduced dramatically. The values in the last row in Figure 13 are almost consistent. For this test data set, after the the CPU core number become laster than 96, the total time cost will not reduce very much more, for the time cost portion of standalone steps becomes considerable.



VI. CONCLUSIONS  In this paper, we have proposed a novel algorithm for context-aware role mining. Role mining problem has been  Fig. 14. Scalability of algorithm AD1     (a) Execution time vs. number of contexts (b) Execution time vs. number of users  Fig. 10. Performance trends of algorithms AD1  (a) Execution time vs. minu (b) Execution time vs. minc  Fig. 11. Time Cost Reduction with minu and minc  proven to be difficult to compute, it is in fact NP-hard [15], and also shown to be hard to approximate [7]. The algorithm proposed in this paper offers complete role coverage, and yet provides efficient and scalable performance, in which the partition and search space reduction problems are also overcomed. AD1 is a distributed computing algorithm that has been deployed on a high performance cluster to test and confirm its performance on tens thousands of users and complex context-behaviors structures.

