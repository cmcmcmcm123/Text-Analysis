Distributed Confidence-Weighted Classification on MapReduce

Abstract?Explosive growth in data size, data complexity, and data rates, triggered by emergence of high-throughput technologies such as remote sensing, crowd-sourcing, social networks, or computational advertising, in recent years has led to an increasing availability of data sets of unprecedented scales, with billions of high-dimensional data examples stored on hundreds of terabytes of memory. In order to make use of this large-scale data and extract useful knowledge, researchers in machine learning and data mining communities are faced with numerous challenges, since the classification algorithms designed for standard desktop computers are not capable of addressing these problems due to memory and time constraints.

As a result, there exists an evident need for development of novel, more scalable algorithms that can handle large data sets. In this paper we propose such method, named AROW- MR, a linear SVM solver for efficient training of recently proposed confidence-weighted (CW) classifiers. Linear CW models maintain a Gaussian distribution over parameter vec- tors, thus allowing a user to estimate, in addition to separating hyperplane between two classes, parameter confidence as well.

The proposed method employs MapReduce framework to train CW classifier in a distributed way, obtaining significant improvements in both training time and accuracy. This is achieved through training of local CW classifiers on each mapper, followed by optimally combining local classifiers on the reducer to obtain aggregated, more accurate CW linear model. We validated the proposed algorithm on synthetic data, and further showed that AROW-MR algorithm outperforms the baseline classifiers on an industrial, large-scale task of Ad Latency prediction, with nearly one billion examples.

Keywords-confidence-weighted classification, MapReduce

I. INTRODUCTION  Recent advent of high-throughput applications which gen- erate data sets that can easily reach terabytes in size, such as remote sensing, crowd-sourcing, high-energy physics, social networks, or computational advertising, has brought forward a clear need for computational approaches that can efficiently learn from Big Data problems [1], [2], [3].

Emerging conferences that specifically address the Big Data issues, as well as the number of recent publications related to large-scale tasks, underline the significance of the Big Data field. Moreover, recently introduced ?Big Data Research and Development Initiative? by the United States government, aimed at providing support for these efforts, clearly indicates globally-recognized, strategic importance, as well as future potential and impact of Big Data-related research [4].

With the emergence of extremely large-scale data sets, re- searchers in machine learning and data mining communities are faced with numerous challenges related to the sheer size of the problems at hand, as many well-established classifi- cation and regression approaches were not designed and are not suitable for such memory- and time-intensive tasks. The inadequacy of standard machine learning tools in this new setting has led to investment of significant research efforts into the development of novel methods that can address such challenges. Classification tasks are of particular interest, as the problem of classifying input data examples into one of finite number of classes can be found in many areas of machine learning. However, state-of-the-art non-linear classification methods, such as Support Vector Machines (SVMs) [5], are not applicable to truly big data due to very high time and memory overhead, which are in general super- linear and linear in the data size N , respectively, significantly limiting their use when solving large-scale problems. Several methods have been proposed to make SVMs more scalable, ranging from algorithmic speed-ups [6], [7], [8], [9], [10], [11], to parallelization approaches [12], [13], [14]. However, scalability of SVM training is inherently limited as non- linear SVMs are characterized by linear growth of model size with training data size N [15]. This led to an increased interest in linear SVM models [16], [17], [18], [19], [20], which have constant memory and O(N) training time. These linear models provide a scalable alternative to non-linear SVMs, albeit with a certain drop in prediction accuracy.

Unfortunately, even linear time complexity may not be sufficiently efficient for modern data sets stored across petabytes of memory space, requiring researchers to develop and adopt new machine learning approaches in order to address extremely large-scale classification tasks. Signifi- cant research efforts culminated in several highly-influential frameworks for solving parallelizable problems that involve data sets which can not be loaded on a single machine. These frameworks for parallel computations include MapReduce [21], [22], AllReduce [23], GraphLab [24], [25], Pregel [26], and others. MapReduce in particular has become very popular framework in industry, with major companies such as Yahoo!, Google, and Facebook spearheading its use in large-scale commercial systems [27], [28].

Unlike other distributed frameworks that assume frequent communication and shared memory between the computa- tion nodes (e.g., [23], [24]), MapReduce framework, and its open-source implementation called Hadoop, allows limited communication overhead between the nodes, which results in very strong fault-tolerance and guaranteed consistency.

These favorable properties of MapReduce led to develop- ment of parallelizable variants of popular machine learning algorithms, such as k-means, perceptron, logistic regression, PCA, and others [29], [30], [31], [32]. However, the pro- posed classification methods mostly rely on iterative train- ing and two-way communication between the computation nodes [31], [32]. This may impose significant costs during training as it does not closely follow the computational paradigm of MapReduce, which derives its reliability from the high level of autonomy of computation nodes.

In this paper we describe an efficient linear SVM learner with sub-linear training time, capable of fully employing the MapReduce framework to significantly speed up the training.

The algorithm uses recently proposed Confidence-Weighted (CW) linear classifiers [33], [34] to train a number of SVM models on each of the mappers. Following completion of the map step, the local CW models are sent to the reducer that optimally combines local classifiers to obtain a single model, more accurate than any of the individual ones. Com- pared to the CW algorithms, the proposed method, named AROW-MapReduce (AROW-MR), allows significantly more efficient training of accurate SVMs on extremely large data sets due to the distributed training. We validate our approach on real-world, large-scale problem of Ad Latency prediction with nearly one billion data examples, where AROW-MR achieved higher accuracy and faster training than the baseline approaches.

The paper is organized as follows. In Section II we de- scribe the Confidence-Weighted classifiers, shown to achieve state-of-the-art performance on a number of real-world ap- plications. In Section III we overview popular frameworks for distributed learning, MapReduce and AllReduce. Then, in Section IV we describe the proposed algorithm, a dis- tributed variant of CW classifiers which can be used to efficiently train highly accurate models on large-scale prob- lems. In Section V we validate our approach on synthetic data set, and further show that the method outperforms the baseline approaches on real-world, industrial data set from the computational advertising domain. Lastly, we give the concluding remarks in Section VI.



II. CONFIDENCE-WEIGHTED CLASSIFICATION  In this section we review the recently proposed confidence-weighted classifiers. We first detail the CW algorithm, proposed in [33], followed by the description of Adaptive Regularization of Weights (AROW) algorithm from [34], an improved CW method shown to significantly outperform the original CW algorithm.

First described in [33], the CW algorithm is a linear classifier that, in addition to the prediction margin for the new data example, also outputs probability of the correct classification. This is achieved by maintaining a multivariate Gaussian distribution over the separating hyperplanes, and during the training procedure both mean ? and covariance matrix ? of the distribution are learned. In this way a more expressive and informative model is found, giving us information about noise in each of the individual features, as well as about the relationship between features.

Let us assume that a trained CW model, with known mean vector ? and covariance matrix ?, is given. Then, for an example (x, y) from data set D, described by feature vector x and binary label y ? {?1, 1}, this induces a Gaussian distribution over the prediction margin y? as follows,  y? ? N (y(?Tx),xT?x). (1)  Following (1), we can compute the probability of correct classification by using the equation for the normal cumula- tive distribution function, to obtain the expression  P(sign(?Tx) = y) =   ( 1 + erf(  y(?Tx)? 2xT?x  ) ) . (2)  The CW classifier is learned online, and the current model is updated each round after observing a training example.

During training, our belief about the classifier before the tth  training iteration, expressed through the current mean ?t and the current covariance matrix ?t, is updated so that the new example (xt, yt) is correctly classified with probability larger than some user-set parameter ?. In addition, we impose an additional constraint that our new belief before iteration t+1 is not too far from our belief at the previous iteration t.

More formally, the stated requirements yield the following optimization problem,  (?t+1,?t+1) = argmin ?,?  DKL (N (?,?)?N (?t,?t)  )  subject to P ( yt(?  Txt ? 0) ) ? ?,  (3)  where DKL is the Kullback-Leibler (KL) divergence. Since the problem (3) is non-convex, the authors of [33] solve an approximate convex problem, and derive closed-form updates for the parameters of the Gaussian distribution.

As formulated in (3), we seek such an update of the classification model so that the new training example is correctly classified with certain probability ?. In [34], the authors point out that this may be suboptimal for noisy data sets. More specifically, once the learning algorithm observes a noisy example, the update would modify the current model so that the noise is correctly classified, which could have an adverse effect on the generalization performance of the classifier. To address this issue, a new CW formulation is proposed in [34], called Adaptive Regularization of Weights     (AROW). In this approach, the following problem is solved,  (?t+1,?t+1) = argmin ?,?  DKL (N (?,?)?N (?t,?t)  ) +  ?1 ( max(0, 1? yt?Txt)  )2 + ?2(x  T t ?xt).

(4) We can see that at each training step the old and the new belief are still constrained to be close, as measured by the KL-divergence. However, unlike in the CW algorithm which aggresively updates the model in order to accomodate new examples, in AROW formulation the aggresiveness of maximization of margin and minimization of uncertainty for the new example are controlled by the regularization parameters ?1 and ?2, respectively. As shown in [34], after finding the derivative of the objective function with respect to the parameters, update equations for ? and ? in the case of misclassification of the tth example (i.e., when sign(?Tt xt) ?= yt), can be written in closed-form,  ?t+1 = ?t + ?tyt?txt,  ?t+1 = ?t ? ?t?txtxTt ?t, (5)  where ?t and ?t are computed as  ?t = ?t max(0, 1? yt?Txt), ?t = (x  T t ?xt + r)  ?1, (6)  and r = 1/(2?1), for ?1 = ?2. Online AROW training is initiated with a zero-vector ?0 and an identity matrix ?0, and it further proceeds to observe training examples and to update the parameters using equations (5) and (6).



III. MAPREDUCE FRAMEWORK  With the recent explosive growth of data set sizes, analysis and knowledge extraction from modern data sets using a single machine is becoming increasingly intractable. In par- ticular, training time of popular classification and regression methods (e.g., SVM, classification trees) is at best linear in training set size, which may be too expensive for problems with billions of examples. To address this pressing issue, a number of frameworks for distributed learning on clusters of computation nodes has been introduced, offering different levels of parallelization, node independence, and reliability [21], [22], [23], [24], [25], [26]. In this section, we describe one such framework which has become very popular in industry, called MapReduce. We also discuss AllReduce distributed framework, which is utilized in Vowpall Wabbit, a state-of-the-art distributed machine learning package.

MapReduce framework [21], [22], implemented as an open-source platform Hadoop1, consists of two distinct phases called map and reduce, which constitute one MapRe- duce job. In the map phase, mappers read parts of the dataset (possibly stored on multiple computers) and perform some action (e.g., filtering, grouping, counting, sorting) with  1http://hadoop.apache.org/, accessed June 2013  final results being sent to reducer in a form of ordered (key, value) pairs. In the reduce phase, reducer performs a summary operation on the data received from the mappers, where the received data is sorted by their key values. There may be multiple mappers and reducers, and the framework guarantees that all values associated with a specific key will appear in one and only one reducer. Note that the limited communication between computation nodes in MapReduce framework, which is allowed only from mappers to reducers, ensures high independence of mappers and significant fault- tolerance of the framework. Even in the case of mapper failure, the entire job is not significantly affected as remain- ing mappers are not aware of the failure, which can be fixed with a simple restart of the failed node.

We illustrate MapReduce abstraction on a simple example.

Given a dataset D with D features, we may want to find how many times each feature has a non-zero value, which can be achieved using several mappers and a single reducer.

Each mapper reads a part of the dataset, and for each example outputs (k, 1) if the kth feature was non-zero. When the mappers finish outputing (key, value) pairs, the reducer starts reading these pairs sorted by their key. Then, on the reducer side, we initialize the count variable to 0, and add all values associated with the same key as the ordered pairs are received. Once a key that is different from the one associated with the current count is read, reducer outputs the total count and resets the variable to compute the non-zero count for the following feature. In this way there is no need to store the individual counts, which lowers memory cost of the reducer.

There are several ways of utilizing MapReduce for dis- tributed learning: 1) read the data using multiple mappers, and learn the model on a reducer in an online learning manner; 2) maintain a global model that is used by all mappers to compute partial updates, which are aggregated on reducer to update the global model (requires running multiple MapReduce jobs for convergence); and 3) learn several local models on mappers, and combine them into a global one on a reducer. For the first option, distributed learning takes the same amount of time as learning on a single machine, with the benefit that there is no need to store the data on a single disk. The second option is typically used for batch learning [30], [31], [32], where each mapper computes partial gradient using the current model, while the reducer sums the partial gradients and updates the model. A new MapReduce job is then instantiated, with the updated model used by all mappers for the next round of gradient calculation. Thus, one job is analogous to one gradient descent step. Since learning may require several iterations to converge, multiple MapReduce jobs need to be ran one after another, which may be ineffective and time costly. In contrast, the third approach ensures more robust learning and small communication overhead as only a single job is run, and we utilize this approach to propose an efficient and accurate classifier in Section IV.

A. AllReduce framework  MapReduce abstraction allows very limited interaction be- tween the computation nodes to ensure high fault-tolerance.

In the following we introduce significantly less-constrained framework called AllReduce [23], which is utilized by the popular Vowpal Wabbit (VW) software package2 [35].

Unlike MapReduce, AllReduce framework assumes commu- nication between mappers as well, while the reducers are not used. In particular, when computing the update step for the current global model, partial update step computed on one mapper is communicated to all other mappers. Then, once every mapper receives a message from all other mappers, the aggregated update step is performed on all computation nodes simultaneously. A typical implementation of AllRe- duce is done by imposing a tree structure on the computation nodes, such that the partial messages are aggregated up the tree and then broadcasted down to all mappers.

Disadvantage of AllReduce framework is that the mappers need to run truly concurrently. However, it is common for large clusters to run many independent jobs requiring different amount of resources on the computation nodes, and for higher number of mappers there may be no guarantee that all of them will be available for concurrent execution.

Furthermore, as we have observed in practice, due to the fact that all nodes are required to send their updates before the next learning iteration starts, AllReduce learning will stall if any individual mapper fails once the job has started.



IV. CONFIDENCE-WEIGHTED CLASSIFICATION USING MAPREDUCE FRAMEWORK  In this section we present a distributed AROW algorithm, which can be used to efficiently train very accurate linear classifiers on large-scale data. Let us assume that we have M mappers, and on each mapper an AROW model is trained using only a subset of the whole data set D, by running the algorithm described in Section II. More specifically, on the mth mapper an AROW model is trained using a data set Dm ? D, such that  ? m=1,...,M Dm = D and  Di ? Dj = ?, i ?= j. We denote the trained AROW parameters for the mth mapper as ?m and ?m, which are sent to the reducer after the completion of the map stage. During the reduce stage, we learn the final, aggregated parameters ?? and ?? such that the multivariate Gaussian distribution N (??,??) is an optimal combination of M multivariate Gaussian distributions learned on mappers.

More formally, let us define objective function L to be minimized on the reducer,  L = EN (?,?)[DSKL (N (??,??)?N (?,?)  ) ], (7)  where the expectation is taken over the distributions over hyperplanes that separate the data set D, and DSKL is the  2https://github.com/JohnLangford/vowpal wabbit, accessed June 2013  symmetric KL-divergence, defined as  DSKL(A?B) =   ( DKL(A?B) +DKL(B?A)  ) . (8)  As can be seen from (7), the reducer computes aggregated parameters ?? and ?? such that the expected symmetric KL- divergence between the aggregated Gaussian distribution and hyperplane distributions, drawn from the probability distri- bution over separating hyperplane distributions for the data set D, is minimized. We note that the proposed method can be viewed as a generalization of the averaging CW model used for large-scale data sets, briefly discussed in [33]. Given the mapper-specific parameters ?m and ?m,m = 1, . . . ,M , empirical estimate of the objective function L can be ex- pressed as follows,  L = M?  m=1  P (N (?m,?m)  ) DSKL  (N (??,??)?N (?m,?m) ) ,  (9) where we define P  (N (?m,?m) ) , or probability of the mth  distribution over the separating hyperplanes, as the fraction of the training set used to train the mth AROW classifier. We refer to the final CW classification model as AROW-MR.

A. Reducer optimization of AROW-MR  The optimization function (9) is convex, thus there exists a unique set of (??,??) parameters that minimize L. In this section we derive update equations for AROW-MR param- eters, the mean and the covariance matrix of the aggregated Gaussian distribution over the separating hyperplanes.

In order to solve (9), we compute the first derivatives of the objecive function L with respect to the parameters of the aggregated Gaussian distribution. After finding the derivative of L with respect to ?? and equating the resulting expression with 0, we obtain the following update rule for mean ??,  ?? = ( M?  m=1  P (N (?m,?m)  ) (??1? +?  ?1 m )  )?1  ( M? m=1  P (N (?m,?m)  ) (??1? +?  ?1 m )?m  ) .

(10)  In order to compute the update rule for covariance matrix, we find the derivative of L with respect to ?? and equate the resulting equation with 0. After derivation, we obtain the following expression,  ?? ( M?  m=1  P (N (?m,?m)  ) ??1m  ) ?? =  M? m=1  P (N (?m,?m)  ) ( ?m + (?? ? ?m)(?? ? ?m)T  ) .

(11) Equation (11) is a Riccati equation of the form XAX = B, solved with respect to matrix X with matrices A and B given. After finding the decomposition of matrix A as     Algorithm 1 AROW-MapReduce (AROW-MR) Inputs: Data set D; number of mappers M Output: Parameters of AROW-MR ?? and ??  1. Map: Train the mth AROW classifier on subset Dm of the data set D using equations (5) and (6), one AROW classifier for each mapper, to obtain ?m and ?m, where m = 1, . . . ,M  2. Reduce: Combine the local AROW classifiers into an aggregated AROW classifier using equations (10), (11) and (12) 3. Output aggregated AROW parameters ?? and ??  A = UTU (e.g., using the Cholesky decomposition), we can compute X in a closed-form using the following steps,  XAX = B  XUTUX = B  UXUTUXUT = UBUT  (UXUT)2 = UBUT  UXUT = U0.5B0.5(UT)0.5  X = U?0.5B0.5(UT)?0.5.

(12)  By matching the elements of equation (12) with the elements of equation (11), we can find the closed-form solution for the covariance matrix ??. Then, in order to find the optimal parameters ?? and ??, equations (10) and (11) are solved iteratively until convergence (we empirically found that only a few iterations are sufficient for the optimization procedure to converge). The pseudocode given in Algorithm 1 summarizes the steps of the AROW-MR algorithm.

Let us discuss the time complexity of AROW and its distributed version AROW-MR. Given a D-dimensional data set D of size N , complexity of AROW training amounts to O(ND2). On the other hand, complexity of AROW-MR is O(ND2/M +MD2 +D3), where the first term is due to local AROW training on mappers, and the second and the third term are due to reducer optimization, which involves summation over M matrices of size D ? D and Cholesky decomposition of the result, respectively. For large-scale data sets, for which it holds N M , we can conclude that AROW-MR offers efficient training with significantly lower time overhead when compared to AROW algorithm.



V. EXPERIMENTS  In this section we present the results of empirical eval- uation of the AROW-MR algorithm. We first validated our method on a synthetic data set, then explored its performance on a real-world, large-scale task of Ad Latency prediction.

A. Validation on synthetic data  In order to better characterize the proposed distributed algorithm, in the first set of experiments we compared AROW and AROW-MR algorithms on synthetic data. We used the waveform data set generator, available from the UCI Repository, where we labeled the first and the second class as being positive and the third class as being negative. We  generated 50,000 training examples and 5,000 test examples, and set ?1 = ?2 = 0.1 through cross-validation. We split the training set into M disjoint subsets of equal sizes and used one subset to train a local AROW on one mapper, where we increased the number of mappers M from 2 to 100 to evaluate the effect of higher levels of parallelization.

We report the results of the original AROW which used the entire training data set (denoted by AROW-total, which was not affected by the number of mappers), the results of AROW-MR, as well as the results of a local AROW model trained on a single mapper, denoted by AROW-single, for which the number of training examples decreased as the number of mappers was increased (i.e., number of training examples for each local model was 50,000/M ). We included AROW-single results to illustrate the performance of local AROW models that are eventually combined on the reducer to obtain AROW-MR model. Experiments on synthetic data were run in Matlab, on MacBook Pro with 2GHz Intel Core i7 with 8GB of DDR3 memory.

Mean accuracy and training time after 10 repetitions are shown in Figures 1a and 1b, respectively, where the error bars in Figure 1a represent intervals of two standard deviations (we omitted error bars for AROW-single as the standard deviation was around 0.5 and error bars would clutter the figure). We can see in Figure 1a that the accuracy of AROW-MR initially increased as the number of mappers increased, statistically significantly outperforming AROW- total. The accuracy of local AROW models trained on each mapper (shown as AROW-single) dropped steadily with the increase of the number of mappers, which was expected as less training examples were used. Nevertheless, even though the accuracy of local models decreased, AROW- MR consistently outperformed AROW-total. Interestingly, as the number of mappers further increased, we can see that the accuracy of AROW-MR started decreasing when M surpassed 40, until it decreased to reach the accuracy of AROW for M = 100. This decrease is due to the fact that there are too few training examples on mappers for the local models to be close to convergence, which in turn affected accuracy of the aggregated model.

As illustrated in Figure 1b, distributed training also re- sulted in a significant speed-up in training time. We can see that AROW-MR training is order of magnitude faster than     20 40 60 80 100  88.1  88.2  88.3  88.4  88.5  88.6  88.7  88.8  88.9  Number of mappers  A c c u r a c y  [ % ]      AROW?total  AROW?single  AROW?MR  (a) Classification accuracy     0.2  0.4  0.6  0.8   Number of mappers  T i m e  [ s e c ]      AROW?total  AROW?single  AROW?MR  (b) Training time  Figure 1: Results on the synthetic waveform data set (with 50,000 training examples)  training of AROW-total, while at the same time achieving higher accuracy. Similarly to the accuracy results, the train- ing time did not decrease further as we increased the number of mappers beyond a certain point. Although the mapper time continued to drop (shown by the dashed line), this was countered by longer time spent to solve the optimization problem (9) in the reduce phase due to larger M . This validates the known result that for certain problems ?too much parallelization or distribution can have negative con- sequences? [36], and that the level of parallelization should be determined after deeper analysis of the problem being solved. Having said that, we can conclude that distributed training of AROW model resulted in significant accuracy and training time gains over the original AROW.

B. Ad Latency problem description  In the following section we compare the performance of AROW-MR and the baseline methods on large-scale, industrial task of Ad Latency prediction. However, before moving on to the discussion of empirical results, we first introduce this important problem in online advertising, as well as the large-scale data set used in the experiments.

Over the previous decade, income generated by internet companies through online advertising has been growing steadily at amazing rates, with the total revenue reaching a record $36.6 billion in the US in 2012 alone3. This burgeoning, highly competitive market consists of several key players: 1) advertisers, companies that want to advertise their products; 2) publishers, websites that host advertise- ments; and 3) intermediate players that connect advertisers to publishers. In some cases such clear segmentation is not possible, and certain companies can be found in more than  3http://news.yahoo.com/us-internet-ad-revenue-grows-15-percent-2012- 153752947.html, accessed June 2013  one role (e.g., Yahoo! or Google may provide both the products and the advertising space). Typically, advertiser designs an image of the advertisement, called a creative, specifying size and dimension requirements of the image to be shown on websites. This is then sent to the intermediate companies which have contracts with publishers, and which decide when and to whom the ads will be shown in order to maximize profits.

In order to retain existing and attract new users, publishers aim at improving user experience by minimizing page load times. In addition, equally important task for publishers is to ensure that the ads are delivered on time. Considering that ad latency time accounts for a significant percentage of the overall load time, improving ad load times would directly benefit both the user experience and the website revenue.

Thus, correctly predicting ad latency time, and using this information to decide which ad should be shown to a user, is an extremely important problem in online advertising where an additional latency of several milliseconds could result in a significant loss of revenue. In this paper, we considered Ad Latency dataset consisting of nearly 1.3 billon ad impressions, for which 21 features were measured at serve time, along with ad latency given in milliseconds.

Features can be divided into several groups: ? User-specific features include user?s device type, oper-  ating system, and browser. We also used user?s geo- graphic location (i.e., state and city), user?s physical distance from the colocation center serving the ad, user?s connection speed (e.g., broadband, dial-up), as well as internet service provider used by the user.

? Advertiser-specific features include the advertiser?s ac- count ID, size of the advertisement (2-D dimensions of the creative, and its size in kilobytes), as well as the creative ID of a specific image used by the advertiser.

? Publisher?s website can be partitioned into several re-     Table I: Features from the Ad Latency data set  Feature name Cardinality Device 15 Operating system 22 Browser 100 Connection speed 10 State 50 City 574 ISP ? Distance to colocation center continuous Account ID ? Creative ID ? Ad size (dimensions) 33 Ad size (size in KB) continuous Region ID ? Space ID (location on the page) ? Ad position 28 Hostname ? Ad network ? Serve type ? Colocation center ? Hour of the day 24 Day of the week 7  gions, where each region has several spaces on which the ad can be shown. Further, ad can be placed at several different positions in the space (e.g., top, bot- tom). Thus, publisher-specific features include region ID, space ID, position, ad network used to serve the ad, serve type, hostname, as well as colocation center used to serve the ad.

? Lastly, we use time-stamp of ad impression, using time of day and day of the week.

In Table I we give the data set features, as well as the cardinalities for discrete features (we omitted sensitive in- formation, which is marked with the ??? symbol).

We can represent the problem as a binary classification by thresholding the value of ad latency. An ad is considered late (i.e., y = 1) if the time period from the moment when the webpage loads to the moment when the ad renders is longer than k milliseconds, and not late otherwise (i.e., y = ?1).

Value of k can be selected depending on a product or ad campaign requirements, and we omit the specific value used in the experiments as it represents a sensitive information.

C. Validation on Ad Latency data  In order to evaluate performance of the classification algorithms, we randomly split the Ad Latency data set into training set, consisting of 997,055,154 examples, and non- overlapping testing set with 279,026,428 labeled examples.

For the Ad Latency prediction task, the publishers prefer low False Positive Rate (FPR), ensuring that very few ads are wrongly classified as late, thus minimally hurting revenue. At the same time, the publisher prefer high True Positive Rate (TPR), which improves user experience and increases profit. As these two goals are often conflicting, the optimal strategy is to maximize the area under the Receiver  Table II: Performance comparison of AROW and AROW- MapReduce on Ad Latency task in terms of the AUC  # mappers # reducers Avg. map time Reduce time AUC 1 0 408h n/a 0.8442  100 1 30.5h 1 min 0.8552 500 1 34 min 4 min 0.8577  1,000 1 17.5 min 7 min 0.8662 10,000 1 2 min 1h 0.8621  Operating Characteristic (ROC) curve, referred to as the AUC [37]. Thus, unlike in the experiments with the synthetic data, we report AUC as a measure of performance. Given a predicted margin for the nth example, y?n ? R, a binary classification prediction is found as sign(y?n ? ?), where different values of threshold ? result in different predictions and different TPR and FPR values. We can obtain an ROC curve by sliding the threshold ? from ?? to?, and plotting the resulting TPR and FPR in a 2-D plot.

The experiments were conducted using MapReduce on Apache Hadoop, with AROW-MR mapper and reducer im- plemented in Perl. Performance of AROW-MR was com- pared to AROW algorithm which was run on a single machine, as well as to the logistic regression implemented in highly-scalable Vowpal Wabbit package, run both on a single machine and in a distributed manner using AllReduce on Hadoop. We ensured that Hadoop scheduled exactly M mappers by storing the data in M gzip-compressed part files.

We first compared the performance of AROW learned on a single machine with AROW-MR learned in a distributed manner. Similarly to experiments presented in Section V-A, we increased the number of mappers to evaluate the effects of different levels of parallelization. The results are given in Table II. We can see that the running time of AROW- MR drastically improved over the non-distributed AROW (trained using a single mapper). While it took 17 days for AROW to train, we were able to train accurate AROW-MR models in less than an hour. Expectedly, average mapper time decreased and reducer time increased as the number of mappers was increased, as each mapper was trained on smaller partition of the data and reducer was required to combine higher number of local models. Interestingly, we can also see that the results in Table II validate the results obtained on synthetic data regarding performance gains with increasing levels of parallelization. In particular, both the accuracy and training time improved until we reached M = 1,000, and dropped slightly for higher number of mappers. The detailed results are presented in Figure 2, where we plotted ROC curves of the confidence-weighted models trained using different number of mappers. We can see that the curve for non-distributed AROW, denoted by ?1 mapper?, results in the smallest AUC, while the level of parallelization achieved with 1,000 mappers represents the best choice for the Ad Latency prediction task.

0 5 10 15 20 25 30 35 40            False Positive Rate [%]  T r u e  p o s i t i v e  r a t e  [ % ]      1 mapper  100 mappers  500 mappers  1,000 mappers  10,000 mappers  Figure 2: ROC curve for AROW-MR and AROW  Next, in Table III we show the performance of logis- tic regression (LR) model trained using VW package in both distributed mode (using AllReduce) and non-distributed manner (on a single machine). We can see that AROW-MR achieved higher accuracy than LR, which is a very popular approach using in large-scale classification. While AROW- MR obtained AUC of 0.8662, the best AUC achieved by LR was 0.8508; this increase in accuracy may result in significant increase of revenue in computational advertising domain. Interestingly, the results also indicate that more distributed training of LR actually hurt its generalization per- formance, which slightly dropped as the number of mappers was increased. Further, we can see that LR was trained in 8 minutes, while it took 25 minutes to train AROW-MR model.

However, although LR training is seemingly faster than training of AROW-MR, it is important to emphasize that VW package implements logistic regression in C language while, for technical reasons, AROW-MR was implemented in Perl. Considering that Perl is not an optimal choice for mathematical computations, we expect AROW-MR training time to improve significantly once implemented in C.

It is also worth noting that we were not able to run the LR experiment for more than one thousand mappers. The LR implementation in VW package uses AllReduce framework, which requires that all mappers run concurrently without any node failures, otherwise the training might fail. However, this is usually hard to guarantee in practice even for larger clusters, and it further exemplifies the advantage of the proposed algorithm over the competition. We can conclude that AROW-MR offers robust, highly efficient training of accurate classifiers, outperfoming the existing state-of-the- art for extremely large-scale, industrial-size problems.



VI. CONCLUSION  In this paper we presented AROW-MR, a distributed linear SVM solver capable of learning accurate models in time sub-  Table III: Performance of distributed logistic regression  # mappers # reducers Avg. map time Reduce time AUC 1 0 7h n/a 0.8506  100 0 1h n/a 0.8508 500 0 8 min n/a 0.8501  1,000 0 6 min n/a 0.8498  linear in the size of a training set. The proposed method uti- lizes the MapReduce paradigm, which provides distributed, fault-tolerant environment for large-scale machine learning.

Map phase of the method involves training a number of local AROW models on each mapper, followed by reduce phase which combines local classifiers to obtain a single, aggregated model more accurate than any of the individual ones. The experiments on synthetic data and real-world Ad Latency data set with nearly one billion examples indicate that AROW-MR allows for significant accuracy and training time improvements over the original AROW algorithm.

Moreover, on Ad Latency task the proposed method out- performed distributed logistic regression available in a pop- ular Vowpal Wabbit package, suggesting that the proposed distributed confidence-weighted model provides a scalable, accurate tool for large-scale classification problems.

