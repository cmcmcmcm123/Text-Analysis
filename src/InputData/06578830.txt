Improving Supply Chain Security Using Big Data David Zage, Kristin Glass, and Richard Colbaugh

Abstract?Previous attempts at supply chain risk management are often non-technical and rely heavily on policies/procedures to provide security assurances. This is particularity worrisome as there are vast volumes of data that must be analyzed and data continues to grow at unprecedented rates. In order to mitigate these issues and minimize the amount of manual inspection required, we propose the development of mathematically-based automated screening methods that can be incorporated into sup- ply chain risk management. In particular, we look at methods for identifying deception and deceptive practices that may be present in the supply chain. We examine two classes of constraints faced by deceivers, cognitive/computational limitations and strategic tradeoffs, which can be used to developed graph-based metrics to represent entity behavior. By using these metrics with novel machine learning algorithms, we can robustly detect deceptive behavior and identify potential supply chain issues.

Keywords-Supply Chain Risk Management, Deception Detection, Machine Learning

I. INTRODUCTION  In today?s world, the very nature of trade and commerce causes supply chains to be globally distributed, interconnected sets of people, organizations, and services. A single weakness or vulnerability in any portion of the supply chain anywhere across the globe has the ability to have adverse affect activity thousands of miles away. Unfortunately, business and federal agencies typically have neither a consistent nor comprehensive way of understanding the often opaque processes and practices used to create and deliver the hardware and software products and services that they procure [1]?[3]. This lack of protocols and understanding significantly increases the challenges and risks of maintaining a viable supply chain that remains free of exploits ranging from counterfeit materials, malicious soft- ware, or untrustworthy products [4].

Previous attempts at supply chain risk management (SCRM) are often non-technical and rely heavily on policies/procedures to provide security assurances [5]?[7]. This is particularity worrisome as there are vast volumes of data that must be analyzed and data continues to grow at unprecedented rates.

In order to mitigate these issues and minimize the amount of manual inspection required, we propose the development of mathematically-based automated screening methods that can be incorporated into SCRM. This way, we can put big data to use instead of drowning under the flood of data. In particular,  Sandia National Laboratories is a multi-program laboratory managed and operated by Sandia Corporation, a wholly owned subsidiary of Lockheed Martin Corporation, for the U.S. Department of Energy?s National Nuclear Security Administration under contract DE-AC04-94AL85000  we look at methods for identifying deception and deceptive practices that may be present in the supply chain.

Towards this end, there has been considerable interest in developing robust, reliable methods for detecting and defeating deception. The availability of such methods would represent a crucial advance in SCRM and a variety of other domains such as cybersecurity, counter-terrorism, border protection, and crime prevention [8], [9]. Recognizing that dependable, scientifically-rigorous detection of deception is a central aspect of this problem, many researchers and practitioners have ex- amined this identification task. Most of this work has focused on exploiting physiological and verbal/linguistic cues, often using concepts and tools taken from psychology, cognitive science, and neuroscience, and have relied upon human expert- intensive observations, evaluations, and decisions [10]?[13].

While recent research has begun to produce promising compu- tational methods for detecting deception [14]?[16], much work remains to be done. In particular, it is desirable to develop automated techniques which discover exploitable deception cues in multiple supply chain domains, combine analyst input with rigorous computational learning strategies, and have been validated against important real-world problems. We note that the goal of this work is not to create a ?perfect? SCRM solution, but to help supply chain analysts focus their efforts.

This paper proposes a computational approach to robust deception detection that addresses the above challenges. We begin by identifying constraints faced by deceivers which are both widespread and exploitable. These constraints can be organized into two broad classes:  1) Strategic tradeoffs, reflecting the fact that devoting ad- ditional resources to deception typically increases the likelihood that the deception will be successful but decreases the net payoff associated with this success  2) Cognitive or computational limitations, capturing the fact that ?the deceiver cannot think of everything?, and in particular that behavioral signatures which are indi- rectly related to the deceptive activity are both abundant and difficult to distort in a consistent manner  We examine a variety of graph-based metrics to capture these constraints which can used as behavioral signatures for potential deceiver activity. We then employ novel machine learning (ML) algorithms to exploit these constraints to ro- bustly detect deceptive behavior. Importantly, these algorithms require little labeled data (i.e., known examples of honest and deceptive activity) which is expensive to obtain [17]?[19]. The algorithms compensate for the limited availability of labels by     Fig. 1. Depiction of a bipartite graph data model, in which instances (red vertices) are connected to the features (blue vertices) they contain, and link weights (black edges) reflect the magnitudes taken by the features in the associated instances.

leveraging unlabeled data. We demonstrate the effectiveness of our solution through experiments on real-world datasets, showing that supply chain risks can be identified with surpris- ingly high (> 90%) accuracy.

The rest of the paper is organized as follows: In Section II we provide an overview of deceiver constraints and Section III we examine potential metrics that can be used as drivers behind our analytics. In Section IV we demonstrate the ef- fectiveness of using these constraints combined with ML and conclude in Section V.



II. OUR APPROACH  ?If falsehood, like truth, had only one face, we would be in a better shape. For we would take as certain the opposite of what the liar said. But the reverse of truth has a hundred thousand shapes and a limitless field.?- Michel de Montaigne, Essays, 16th century  As noted by Montaigne, identifying deception has always been a difficult, laborious task that has only become more difficult as time has progressed. A key element of our approach to robust deception detection is the recognition that deceivers face constraints, which can be leveraged to reveal deceptions.

Identifying these deceptions can then allow us to identify potential supply chain risks. Our analysis has identified two broad classes of constraints that are both widespread and exploitable: cognitive/computational limitations and strategic tradeoffs.

Cognitive/computational limitations capture the fact that ?the deceiver cannot think of everything?. These limitations often manifest themselves in behaviors and patterns that are indirectly related to the deceptive activity. For instance, it was demonstrated by Colbaugh et al. [20] that relational data, such as networks of collaborators, are difficult to distort so as to be consistent with the deception. It is even more challenging to manipulate the way such networks evolve over time to produce dynamical patterns that match those found in legitimate interactions [20]. Consider, for example, the Internet site of a supply chain vendor that purports to be something it is not. While it may be straightforward to construct content that is consistent with the deception, and even link to appropriate sites, it is quite another matter to manipulate the inlinks a site receives and the way these links change over time. Additionally, virtually all complex networks, from the Web to social interaction, can be efficiently and effectively represented as graphs. These observations are the basis for the graph-theoretic metrics we develop which can be  then be incorporated into behavior signatures and are in turn used to power our analytics.

The second class of deceiver constraints that are of interest are strategic tradeoffs. These typically reflect the fact that the deceiver must balance the requirements of the deception process with the underlying goals, which are to be achieved via deception. For instance, typically devoting additional resources to deception increases the likelihood that the deception will be successful but decreases the net payoff associated with this success. Adversarial tradeoffs are naturally represented and analyzed using game theory [21], [22], and as we will demonstrate in Section IV, these tradeoffs can be leveraged to nominate ?signatures? that may be associated with deceiver activity. These candidate signatures can then be employed as features in ML algorithms [23] that are designed to distinguish between deceptive and honest behavior.

We exploit the deceiver constraints associated with cogni- tive/computational limitations and strategic tradeoffs through their incorporation into analyst-informed semi-supervised ML algorithms [24]?[27]. Our approach formulates deception de- tection as a classification problem, in which deceptive and honest behavior are to be distinguished. As demonstrated in Fig. 1, the data is modeled as a bipartite graph of activity instances and the features that characterize them. It is assumed that only limited prior information is available regarding the class (deceptive or honest) of any of the instances or features.

This label knowledge is augmented with analyst guidance regarding which features might be expected to be useful and information present in unlabeled instances that are typically easy to acquire.



III. METRICS FOR AUTOMATED FRAUD DETECTION  While we focus on leveraging big data to increase supply chain security, one consistent challenge in developing solutions which function accurately in real-world settings is obtaining labeled instances of known ground truth. For example, verified instances of deceptive activity are typically in short supply. For these reasons, we examine domains such as e-commerce which hold similarities to supply chains (e.g., big data, complex interactions, etc) in order to develop our deceiver constraints.

E-commerce sites, such as online auctions, have become extremely popular by creating appealing environments that facilitate convenient interaction between consumers and ven- dors. As might be expected, these environments also attract dishonest individuals who seek to profit by deceiving hon- est users. We explore the development of mechanisms for detecting and constraining fraud activity in an unpublished proprietary e-commerce dataset consisting of multiple mil- lions of transactions between millions of users. Besides the transactions (links) and users (vertices), information on the (un)successful completion of each of the transactions was also provided, giving us ground truth.

Many e-commerce sites implement some form of reputation system, which computes and publishes reputation scores for users based on opinions and feedback from their transaction partners (and possibly other behavior measures). One way a fraudster can cheat honest users of a site is to manipulate the     site?s reputation system in order to appear to be a legitimate transaction partner, typically by engaging in legitimate activity for some time before committing a fraudulent act [28]. This creates a tradeoff: the fraudster must balance the benefit of legitimate behavior - enhanced reputation - with its costs - time and money. Fraudsters manage this tradeoff by inventing clever ways to deceive the reputation system (see the work by Hoffman et al. [28]), and the reputation systems adapt accordingly, leading to an arms race between fraudsters and defenses. One consequence of this co-evolutionary behavior is that the signatures of fraudulent activity become increasingly difficult for defenses to detect but also for the fraudsters themselves to manage.

One effective strategy employed by sophisticated fraudsters is to recruit accomplices (or create fake accomplice identities) with whom to interact [29]. The role of these accomplices is to build good reputations for themselves by conducting legitimate transactions with honest users, and then to ?propagate? their good reputations to fraudsters by engaging in (or pretend- ing to engage in) legitimate transactions with the fraudsters.

This fraudster-accomplice structure enables the design of the following innovative means of deceiving reputation systems.

A new fraudster (identity) joins the e-commerce site and inflates his reputation through interactions with accomplices.

Once his reputation score is sufficiently high, he is able to attract honest users and defraud them, at which time he is typically removed from the site. However the accomplices, and their good reputations, remain on the site because they have not engaged directly in the fraud. Thus the fraudster can simply rejoin the site with a new identity, quickly develop a good reputation by transacting with accomplices, and exploit this reputation to commit additional fraud, and of course the cycle can be repeated. It is easy to show through simple calculations that this strategy provides a cost-effective means of circumventing standard reputation systems.

The fraudster-accomplice strategy summarized above is an innovative way for fraudsters to build good reputations without expending excessive resources. However, it turns out that implementing this strategy creates structure in the transaction network of buyers and sellers, which may be easy to detect and difficult for the fraudster to mask [30]. To see this, observe that fraudsters are motivated to interact with accomplices, not with other fraudsters or honest users (at least until the fraud takes place), because their goal is to inflate their reputation.

Similarly, accomplices interact with honest users to boost their own reputation, and with fraudsters to propagate this reputa- tion, but there is no incentive for them to interact with other accomplices. As a result, this fraudster-accomplice behavior generates ?bipartite cores? within the overall transaction graph.

This behavior, in which the subgraphs in which fraudsters are not connected to other fraudsters and accomplices are not connected to other accomplices can be see in Fig. 2.

With this in mind, we look to answer three questions: Do these bipartite cores exist in our e-commerce transaction dataset?, Can they be efficiently detected in very large transac- tion networks?, and Do the cores contain fraudsters? If these bipartite cores actually contain fraudsters, then developing an automated fraudster discovery capability would be straightfor-  Fig. 2. Subgraph of an e-commerce transaction graph. The vertices are users, the edges denote transactions, and the colors denote members of graph communities. The presence of a ?bipartite core? of possible fraudsters (red vertices) and accomplices (magenta vertices) is clearly visible.

ward. Interestingly, one method we can use to detect these bipartite cores, even in very large graphs, is through the use of unsupervised ML. We employ a simple clustering approach based on spectral analysis and introduce a novel measure of ?similarity? to perform clustering on the transaction graph. We identify clusters of vertices that tend not to link to each other.

This similarity definition is based on the observation that a bi- partite core forms a kind of ?anti-community? graph structure, consisting of pairs of communities composed of vertices that are connected less densely than would be expected in a random graph. (The standard definition for graph communities as noted by Newman [31] is groups of vertices with interconnection densities that are greater than expected at random.) This unsupervised ML method can be implemented using standard community detection methods (see [31] for background on community detection), but rather than searching for groups of vertices that maximize intercommunity link density, we search for vertex groups that minimize this density. This search procedure can be conducted very efficiently, and so can be applied to large-scale data to detect the bipartite cores characteristic of fraudster-accomplice interaction.

We implemented the proposed approach to detecting bi- partite cores and the associated fraudster-accomplice groups on transaction graphs taken from the e-commerce dataset mentioned above. In particular, our tests showed that multiple bipartite cores exist in the dataset and the cores can be detected accurately and efficiently using the proposed unsupervised ML algorithm. More importantly, the discovered bipartite cores contain confirmed fraudsters as indicated by information pro- vided with the dataset. For example, we extracted a subgraph of the transaction network consisting of 55 individual vertices and 310 transactions, as seen in Fig. 2, the analysis resulted in the identification of 12 bipartite core fraudster candidates.

Of these 12, 3 were confirmed by outside evidence to be fraudsters and the other 9 were deemed highly suspicious     Fig. 3. Analysis of the triadic closure of an e-commerce transaction graph using the clustering coefficient. The fraudsters and accomplices have low clustering coefficients (below the red horizontal line) while benign users have higher clustering coefficient.

based on the ?interaction feedback? scores they received on the site (however, the suspicious nature of the latter 9 was not corroborated by evidence external to the site). Due to time and budgetary constraints, further analysis of the graph and bipartite cores is left for future work.

Not only can we leverage ML-based community detection, we can also use the triadic closure of the network to identify regions of the transaction graph, which are incongruent with the rest of the structure. Triadic closure captures the intuitive sense that if two vertices have a link in common, they are more likely to be linked themselves [32], [33] and is typically measured using the clustering coefficient [34]. As large-scale graphs of interest form, they tend to have small- world characteristics, in which most vertices are connected by a small number of links and have a higher than ran- dom clustering coefficient [35]. As the links created by the fraudster-accomplice do not form through natural interactions, fraudster and accomplice vertices will have markedly different clustering coefficients than normal users. We calculate the clustering coefficient Cn of a vertex v as follows:  Cv = ev  kv(kv ? 1)  where kv is the number of neighbors of v and ev is the number of links between these neighbors. As we can see from Fig. 3, the fraudsters and their accomplices (those points below the red line) have a lower clustering coefficient than the normal users. The benign users have clustering coefficient more in line with that found in small-world graphs while those of the fraudster and accomplice vertices tend more towards clustering coefficients found in random graphs.

In this section, we have developed complimentary mecha- nisms for constraining deceivers behavior patterns. As we will demonstrate in Section IV, we can incorporation these into semi-supervised ML algorithms designed to identify potential supply chain risks.



IV. CONDUCTING AUTOMATED SCREENING OF LARGE DATA SETS  As we noted in Section I, current SCRM screening practices have not kept pace with the evolving environment in which the supply chain exists today, especially when it comes to infor- mation technology suppliers. Recent surveys have found an alarming number of business executives and decision makers do not consider their SCRM strategies and techniques to be effective [2], [3]. In particular, current practices typically do not adequately leverage available information (e.g., Web data), are not sufficiently scalable to handle the vast volumes of data that characterize suppliers, and do not provide quantitative assessments to allow risk prioritization, information sharing, evaluation of assumptions, and analytics. In this section, we examine the feasibility of developing Web-based, scalable, and quantitative screening process for information technology suppliers in conjunction with the graph-based constraints that were developed in Section III.

As legitimate businesses are motivated to develop and maintain a commercially-beneficial Web presence, they devote considerable resources to this task. In contrast, vendors that represent increased supply chain risk (e.g., those attempting to make a profit by selling out of specification or counter- feit parts) are only motivated to cultivate a ?realistic? Web presence in order to appear legitimate. This effort does not contribute directly to their monetary goals. We posit that this discrepancy in incentives will be detectable via analysis of large-scale Web data. Moreover, it may be especially difficult for such vendors to distort the topological characteristics of their Web presence, such as the structure and composition of their links, as seen in Section III.

We approach the task of identifying supply chain risk as a ML classification problem of predicting the label of a given vendor vertex v in a given Web graph G. More specifically, for a given Web-based graph G = (V,E), where V and E are the vertex and edge sets, we consider the following vertex- label prediction problem: given an vertex v ? V that is of interest but unlabeled, infer the label of v using information contained in the remainder of the network. Logically, each vendor vertex v ? V is represented by a feature vector x ? R|P | which models the vendor?s Web presence. We wish to obtain a vector w ? R|P |, such that the classifier sign(wT v) returns +1 if vendor v is ?interesting? from a supply chain risk perspective and ?1 if v is ?not interesting?. The semi- supervised ML algorithm presented in [24], which we will briefly discuss below, is well-suited for this learning task, as it provides accurate classification even when limited labeled data is available for training by leveraging the information present in unlabeled instances, can be implemented efficiently at Web- scale, and permits analyst knowledge to be incorporated in a natural and effective manner.

We model the problem data as a bipartite graph Gb of vertex-sign instances and features as seen in Fig. 1. The adjacency matrix for the graph Gb is given by[  0 X XT 0  ] where the matrix X ? Rn?|P | is contains the features vectors     Fig. 4. Sample Web graph obtained in Web crawl for supply chain screening case study. The vertices are web domains, the edges denote hyperlinks, and colors delineate Web communities  of the n vendor vertices as rows. Let dest ? Rn be the vector of estimated signs for the vertices in the dataset and define the augmented classifier waug = [dTestw  T ]T ? Rn+|P | that estimates the polarity of both vertices edges and features.

We learn waug , and therefore w, by solving an optimization problem involving the labeled and unlabeled training data, and then use w to estimate the sign of any new vertex of interest with the simple classifier orient = sign(wTx). In order to learn the augmented classifier waug , we solve the following minimization problem:  min waug  wTaugL k nwaug + ?  |V1|? i=1  (west,i ? wi)2 (1)  where L = D?A is the graph Laplacian matrix for Gb, with D the diagonal degree matrix for A (i.e., Dii =  ? j Aij),  Ln = D ? 12LD?  2 is the normalized Laplacian, k is a  positive integer, and ? is a nonnegative constant. Minimizing Equation 1 solves for waug while enforcing that the learned values for the labeled instances correspond to the true values.

For further information, including performance comparisons to other ML algorithms, see [24]?[26].

Given the ML framework, we create sets of candidate quantitative features suitable for use with it. The set of features employed to characterize the Web presence of a vendor of interest are based on website content, hyperlink connectivity, and Web-graph community structure. For example, a candidate feature set might include the number of links on a vendor?s website, the centrality of a vendor?s website within its Web graph community, and the clustering coefficient of a vendor in relation to the entire graph or its Web graph community. Fig. 4 provides a visual example of the type of Web graph and its community structure that can be used in our analysis. It should  True Value Positive Negative  Predicted Value Positive 6 2Negative 0 70  TABLE I SEMI-SUPERVISED ML ALGORITHM USING (26,39) LABELS  True Value Positive Negative  Predicted Value Positive 6 3Negative 0 69  TABLE II SEMI-SUPERVISED ML ALGORITHM USING 13 LABELS  be noted that our technique works on graphs with millions of vertices and edges and a smaller network is displayed in Fig. 4 for visual clarity.

We tested the proposed vendor screening system with a range of datasets, and here we summarize one such test. The dataset used in this section is a representative collection of 78 suppliers of information technology products that were randomly selected from a larger corpus. They were evaluated by a team of SCRM analysts, which resulted in labeling 6 of the vendors as interesting and requiring further assessment and 72 of the vendors as uninteresting. To test the performance of the proposed ML-based vendor screening system, we first conducted a large domain-based Web crawl (using a crawling tool similar to Apache Nutch [36]), and collected a large-scale graph of 100,000 domains that contained all the data necessary to compute a feature vector x for each of the 78 vendors.

The accuracy of the proposed ML classifier was estimated using standard two-fold cross-validation [23]. Specifically, we randomly partition the data into two groups A and B, giving 39 labeled training instances in each validation round. We used set A to train the classifier and tested with set B, then used set B to train the classifier and tested it with set A, resulting in 78 classification instances. To evaluate the extent to which good performance could be achieved with limited labeled data, we varied the amount of labeled information made available to the algorithm during training. More specifically, we considered training procedures using 39 labeled vendors (100% of labels), 26 labeled vendors (67% of labels), and 13 labeled vendors (33% of labels).

The results of our experiments are presented in Table I and Table II. It can be seen in the confusion matrix in Table I that using the majority of labeled instances resulted in excellent performance, with the classifier having only two misclassifications and achieving better than 97% accuracy.

The classifiers were able to correctly identify the 6 interest- ing vendors that were manually identified by the analysts.

More interestingly, the results of classifier using only 13 labeled instances (Table II) are almost identical to those in the experiments, only having one more false positive and achieving 96% accuracy. This is particularly important in the present application, as it is a time-consuming (i.e., multi-day) undertaking to label even a single vendor. Note also that there are no false-negatives produced by the screening system; that     is, no interesting vendors that could increase the supply chain risk slipped through the screening.



V. CONCLUSION  In this paper, we have created automated screening methods based on robust deception detection that can be incorporated into supply chain risk management strategies. We identi- fied two classes of constraints faced by deceivers, cogni- tive/computational limitations and strategic tradeoffs, which are both widespread and exploitable. Based on these con- straints, we have developed graph-based metrics which can accurately and efficiently be used as behavioral signatures for potential deceiver activity. We then used machine learn- ing algorithms to exploit these constraints to robustly detect deceptive behavior. Importantly, these algorithms require little labeled data and can function in multiple domains. Finally, we demonstrated the effectiveness of our solution through experiments on real-world datasets, showing that minimal labeled data coupled with our constraints can lead to highly accurate detection of potential supply chain risk.



VI. ACKNOWLEDGMENTS  This work was supported by the Laboratory Directed Re- search and Development Program at Sandia National Labora- tories.

