Scalable Rule-Based Gene Expression Data Classification

Abstract-Current state-of-the-art association rule-based clas- sifiers for gene expression data operate in two phases: (i) Association rule mining from training data followed by (ii) Classification of query data using the mined rules. In the worst case, these methods require an exponential search over the subset space of the training data set's samples and/or genes during at least one of these two phases. Hence, existing association rule- based techniques are prohibitively computationally expensive on large gene expression datasets.

Our main result is the development of a heuristic rule-based  gene expression data classifier called Boolean Structure Table Classification (BSTC). BSTC is explicitly related to associa- tion rule-based methods, but is guaranteed to be polynomial space/time. Extensive cross validation studies on several real gene expression datasets demonstrate that BSTC retains the classification accuracy of current association rule-based methods while being orders of magnitude faster than the leading classifier RCBT on large datasets. As a result, BSTC is able to finish table generation and classification on large datasets for which current association rule-based methods become computationally infeasible.

BSTC also enjoys two other advantages over association rule-  based classifiers: (i) BSTC is easy to use (requires no parameter tuning), and (ii) BSTC can easily handle datasets with any number of class types. Furthermore, in the process of developing BSTC we introduce a novel class of boolean association rules which have potential applications to other data mining problems.



I. INTRODUCTION  Microarray technology allows biologists to simultaneously measure the expression of thousands of genes in a single experiment. This technology provides a unique tool to examine how a cell's gene expression pattern changes under various conditions. Microarray methods could also play a critical role in personalized medicine as they could be used to determine the unique genetic susceptibility of an individual to disease.

See Table I for a sample microarray dataset shown using the common discretized relational representation. In this table, each sample row consists of (i) a list of discretized genes and (iia) a class label A gene is present in a sample row if the sample expresses the gene. The absence of a gene in a row implies that the gene is not expressed in that sample Thus, the sample/gene expression relationships for relational microarray data are essentially boolean.

Leading associated rule-based methods such as Top-k [1], FARMER [2], CLOSET+ [3], and CHARM [4] which have been applied to microarray datasets aim to correlate gene expression patterns with the classification labels. For these algorithms the discovered correlations take the form of associ- ation rules [5] For an example association rule consider the  TABLE I RUNNINti EXAMPLE OF MIC'ROAtRAY DATA  Sample Expressed Genes Class Label S1 91 92 93 95 Cancer 82 91 93 96 Cancer 83 92 94 96 Cancer 84 92 93 95 Healthy 85 93 94 95 96 Healthy  data shown in Table L. Note that only the Cancer samples si and 82 express both genes gi and g3. Based on this observation we can create the following association rule: g1, 93 => Cancer.

This rule implies that if a query sample express both 91 and 93 (i.e., if g, and 93's associated genes are both expressed in their associated expression intervals), then the query sample is likely to be of type Cancer. Hence, we can use this rule to clavssif query samples of unknown type as Cancer if they express both g9 and 93. Note that there is nnothing special about the class label Cancer. After noticing that only Healthy sample S5 expresses both g5 and 96, we can also create the association rule 95, 96 => Healthy.

In this paper we focus on association rule-based classifiers (hereafter referred to simply as rule-based classifiers) for gene expression data. We focus on rule-based classifiers for two reasons: (i) rule-based classifiers have been demonstrated to be more accurate for gene expression analysis than other methods [1], [2], [6], [7] such as SVM [8] and tree-based C4.5 family algorithms [9], and (ii) as opposed to other classifiers such as SVM, rule-based classifiers can produce rules that can be used to understand the class characteristics. However, rule- based methods are not scalable due to their high association rule mining costs. Although these rule mining costs are "one- time costs" in the sense that rules must only be mined once per training set, larger training data sets are being generated at an ever increasing rate. It is very challenging for any exponential time method to keep up. Consequently, in this paper, we focus on extending accurate association rule-based classification methods to larger data sets.

This paper develops a scalable rule-based classifier called Boolean Structure Table Classification (BSTC) for microarray datasets. Given a labeled training set, such as the example in Table I, BSTC efficiently builds an accurate classifier. The emphasis on accuracy is easy to appreciate and comes from BSTC being related to association rule-based methods. Hence, BSTC supports its classifications with intuitive rules. The em- phasis on efficiency is also critical since large gene expression datasets are computationally taxing for existing association     rule-based algorithms and, as successful microarray techniques fuel the growth of gene expression datasets, these methods will quickly become infeasible. In contrast, BSTC s space and runtime costs are only polynomial. Hence, BSTC is scalable to large data sets on which current association rule-based methods are challenged computationally.

In an attempt to control runtime many current association rule methods [1], [2], [10] utilize support based rule pruning.

Using a large enough support cutoff does allow rule mining to finish more quickly, but doesn't completely resolve the issue.

If the user sets the support cutoff too small, then the rule mining step can take prohibitively long to complete. On the other hand, setting the support cutoff too high excludes the generation of important high-confidence lower-support rules [11]. In order to not miss too many important rules the user can't set the support cutoff too high. The end result is that in practice support cutoffs are difficult and time intensive to tune. In contrast, BSTC is fast and easy to use.

In addition, to the best of our knowledge all current associa- tion rule-based classifiers for gene expression data only handle datasets with two class labels. Although our example Table I data contains just two class labels, in practice microarray data can contain an arbitrary, though small, number of class types.

Unlike previous association rule-based classifiers, BSTC easily generalizes to datasets with more than two class types.

To develop an accurate, scalable, multi-class, and easy to use rule-based classifier we carefully considered the underlying primitives that power association rule-based methods. These methods use conjunctive association rules (CARs), where the rule antecedent is restricted to being a conjunction of terms.

In contrast, we approach this problem by relaxing the types of rules to an important and larger subset of the more general class of boolean association rules (BARs). We develop a novel method for compactly storing these BARs in a simple data structure called a Boolean Structure Table (BST). BSTs can then be used for BAR generation and classification. BST clas- sification (BSTC) collectively considers many simple BARs with 100% confidence in bulk. Because the rules are simple BSTC avoids extensive rule mining. Furthermore, considering rules in bulk keeps the computational cost low.

The main contributions of this paper are: 1) We propose a new polynomial time and space rule-  based classifier for gene expression data analysis that is accurate, scalable, easy to use, and generalizable to multi-class classification.

2) We extensively evaluate our method against the current leading association rule-based method (RCBT [1]), and show that our method is orders of magnitude faster on large datasets while maintaining high classification accuracy.

3) We introduce a subclass of more general boolean asso- ciation rules and relate them to existing CARs.

The remainder of this paper is organized as follows. First, in Section II we formalize the concept of BARs Then in Section III, we define a concept called Boolean Structure Tables (BSTs) which are related to an important class of  BARs. Section IV provides a polynomial time and parameter- free classifier based on BSTs. Section V presents an extensive empirical evaluation of our classifier. Finally, Section VI discusses related work, and Section VII briefly presents our conclusions and directions for future work.



II. PRELIMINARIES We work with the following type of data: We are given a  finite set G of genes and N collections of subsets from G.

These V collections are disjoint and represented as C1 = {Si,j ...* SI,mT }, *...* CN = {SNJ1, . SN,nmN}. Each Ci is called a class type or class label. Furthermore, we will refer to each set sij c G as a sample and every element g C G as a gene. We denote the total set of samples by S = UN iCi.

If g C si,j we will say that sample si,j expresses gene g.

Otherwise, if g CG and g si,j we will say that sample si,j doesn't express gene g. Similarly, we say that sample s is of class type Ci if and only if Ci contains s c G. Consider the Table I data. Here we have samples S = {sl, s2, 83, S4, S5} and genes G = {gI99293, 9g4,95,96} Furthermore, we have N = 2 classes: Cl = Cancer = {s1, S2, S3} and C2 Healthy = {S4, S5}.

Given such relational training data, a conjunctive associa- tion rule (CAR) is any element of 2GIx{1 ,.. N}. A CAR gjl, ..., gj, ==>- n can be interpreted as follows. If a query sample s contains all genes gjl -- gjr then it should be grouped with class type C '." Naturally, of the 21GI * N possible association rules some are more useful than others.

The following standard definitions were introduced in [5] to compare association rules: Support: The support of a CAR gjl, . . . gj,. X n, called S PPt> .Yj . ,jr n], is.

{SnSj st. {gjl, , gj,I C Sn?j I <- j < mn l.

Confidence: The confidence of a CAR g9 ...., gj,, sT9pp[gj,..,gj.=9> n]  I{sij s.t. {gjl  n is:  gJ'l)}IC Si, Ji }  Consider the CAR g1, g3 => Cancer for our running example in Table I. We can see that the example CAR has a support of 2 since only two Cancer samples, sl and s2, contain both g, and 93. Furthermore, we can see that the example CAR has confidence I (or 100%) since no healthy samples contain both g9 and g3.

A. Boolean Association Rules  For any sample s and gene g,1 < i < n CQ, let s [g C fO{I represent whether or not sample s expresses gene gi. Furthermore, define s5-gi to be -s gi Vgi C .

Now suppose that B(X' .. ') is a Boolean expression whose value depends on some subset of {Xl ....Xn,}. We can evaluate B to true or false given a sample s by com- puting B (s [gl,. , s ggn1 ). For example, consider the boolean expression-  B(xl,X2 ?3 ? X, ?6) = (xl A (?2V (X2 A X4)c     gi g2 g3 g4 g5 g6  sI s2 s3  0 0  (s4: gi) (s4: -g3,-g5) (s4: gl) & (s5: -g4, -g6) (s4: -g2, -g5) & (s5: -g4, -g5)  (s5: -g3, -g5)  (s4: gl) & (s5: -g4, -g6) (s5: -g4, -g5) (s5: -g3, -g5)  Fig. 1. Example BST for the Cancer Class  Using Table I we can evaluate  B(sg[91], 51 [92], 5i [93], 51 [94], s1 [95], 51[96])  to be (1 A 1) V (1 A 0) = 1. Note that B will only evaluate t Table I Cancer samples to True.

For a given class set Ci and boolean expression B i can create a Boolean association rule (BAR) of the foi B = Ci. The interpretation of any such BAR, B => Ci, "if B(s[gl]7. . . 7 S[gn]) evaluates to true for a given sample then s should belong to class Ci." From this point on we w work with the following generalized definitions of supp and confidence:  Support: The support of any BAR B => Ci, represented as supp(B = Cj) is:  {samples s e Ci s.t. B(s[g ]  Algorithm 1 Create-BST: The BST Creation Algorithm 1: Input: Finite set of Genes G, set of samples S, Class C 2: Output: The BST Table for Class Cj.

3: for all (c, h) C Ci x S -Ci do 4: initialize a pointer NULL 5: end for 6: for all (g, c)y G x C. s.t. g dc and g lUhEs-cih do 7: Set BSI'(g, c) Black Dot 8: end for 9: for all (g, c, h) C G x Ci x S -Ci s.t. g C c and g C h d  10: if pointer (c, h) r NULL then 11: push a copy of (c, h) - BSI'(g, c) 12: else 13: L {g C G s.t. g C h & g , c} 14: if L = 0 then 15 : (c h) L's address 16: else  he 17: L {g C G s.t. g , h & g c} 18 : (c, h) L's address 19: end if  we 20: end if irm 21: Push a copy of (c, h) -BSJ'(g, c).

22: end for , is 5S,  Vill aort  s [g ) evaluates to true}.

The corresponding numerical support value of B = C1 is denoted as supp(B = C7) .

Confidence: The confidence of a BAR B => Ci is  ~supp(B =-C1)~ {samples s s.t. B (s [g1 ], . . ., s [g,]) evaluates to true}  For CARs these definitions coincide with the CAR defi- nitions of support and confidence found in [5], [12]. Hence, they are natural generalizations of the previous definitions (see section III-C).

Consider our example boolean expression B in terms of Table I. We can see that the BAR B => Cancer (shown in Eq. 1) has support 3 and confidence 1.



III. BSTS AND BARs  The discussion in this section will focus on tables for each class Ci. These tables, called Boolean Structure Tables (BSTs), will form the basis for our classification method. In order to motivate the utility of BSTs for classification, we will present their close relationship to a special category of BARs which in turn, will be related back to CARs. Through this discussion we will demonstrate that BSTs contain all the information of the high confidence CARs already known to be valuable for microarray data classification.

lo  A. Boolean Structure Tables  A Boolean Structure Table (BST) T(i) is a two di- mensional table, T(i) = G x Ci, where each table entry refers to a maximum of S -Ci lists of up to CG genes each. For every Ci the associated BST, T(i), will require 0 (( S -Ci1) G Ci l) space and can be constructed with the same time complexity via Algorithm 1.

When the Algorithm 1 is run on the Table I example input  and for class Cancer, the Boolean Structure Table shown in Figure I is produced. In Figure 1 a black dot at location (g, s) indicates that no healthy samples express gene g but some cancerous sample does. A cell (g, s) is left blank only if sample s didn't express gene g. If (g, s) contains a list of the form (h :-gl, . . .,-p) it means that s may be distinguished from sample h by the non-expression of any one of genes g9 through g, Similarly, if (g, s) contains a list of the form (h : gl . ..,g) it means that s may be distinguished from sample h by the expression of any one of genes ga through gp. Such lists will hereafter be referred to as exclusion lists.

Note that there is no reason why the BST in Figure 1 was created for the Cancer class. We can just as easily build a BST for the Healthy class using the example shown in Table I.

In general if a relational gene expression dataset contains N classes, we can construct N different BSTs for the data set (one for each class).

1) Runtimc Complexity for BST Crcation. We can see that the total time to construct BSTs via Algorithm 1 for all of C1, ,CN is 0(>E(NS C)iCi G) Given that the class sets C1 are all disjoint we have 7 (S-C 1 I'Ia1< y:N 1 Q F rI_1Q2Ir 3 Q1C1 - Z71-i=SCS1 C- = S2. C* Hence,BSs can  be constructed for all Cis in time 0( Sl2 CQ).

Gene g0: (gi expressed) =# Cancer.

Gene g2: (g2 expressed AND [EITHER (g, expressed) OR (either g5 or g3 not expressed)] ) => Cancer.

Gene g3: (g3 expressed AND [EITHER {(g expressed) AND (either g4 or g6 not expressed)} OR { (either g2 or g5 not expressed) AND (either 4 or 95 not expressed)} ] ) =* Cancer.

Gene g: (g4 expressed AND [either g5 or g3 not expressed] Cancer.

Gene g: (g5 expressed AND [gp expressed AND (either g4 or g6 not expressed)] ) > Cancer.

Gene 96: ( 96 expressed AND [(either 94 or g5 not expressed) OR (either 93 or 95 not expressed)] ) > Cancer.

Fig. 2. Gene Row BARs with jfifi( Confidence Values.

B. BST Generable BARs  We view every BST cell, (g, c), as an atomic ]100% confident BAR. For example, Figure 1's (g3, sl)-cell corresponds to the BAR  g3 expressed AND g1 expressed AND (either g4 or g6 not expressed) => Cancer.

We refer to this rule as the Figure 1 BST's (g3, sl)- cell rule. Note that the cell rule is both (i) 100% confident, and (ii) supported by sample sl. Throughout the remainder of this section we will use such cell rules as atomic building blocks to construct more complicated BARs. Furthermore, in Section IV, we will directly employ BST cell rules to build a new classifier called BSTC.

1) Mining More Complicated BST BARs: Let T(i) be a BST for sample type Ci. We can view each row of T(i) as a 100% confident BAR by combining the row's cell rules. To see this, choose any gj CG and consider the CAR gj => Ci. This CAR can be augmented with exclusion list clauses from each of T(i)'s gj-row cells. The result will be a BAR with 100% confidence which is logically equivalent to a disjunction of T(i)'s gj-row cell rules. See Figure 2 for the gene row BARs which result from applying this idea to the BST in Figure 1.

For the remainder of this paper we will restrict our attention to BARs that may be generated by taking conjunctions of BST cell rule disjuncts. Henceforth we simply refer to these as BARs. It is very important to notice that all such BARs have a special form: Their antecedents consist of a CAR antecedent ANYDed with a disjunction of BST exclusion list clause conjunctions. Consider the BAR for gene 96 in Figure 2.

Gene 96 s rule antecedent consists of a CAR antecedent 96, conjoined to a disjunction of the Figure 1 exclusion list clauses (either p or 95 not expressed) and (either 93 or 95 not expressed). Progressive polynomial time algorithms for mining more complicated BARs via a BST can be found in an extended version of this paper [13].

C. BARs Relationships to CARs Let R O=> C, be any 100c confident BST created BAR  containing exclusion clauses for non-Ci samples hil ... hm.

Removing all exclusion list clauses related to {hi ... hp} C {hl,.h .I hm}, p < m, will create a new boolean association rule, R X Ci, with support - supp(R => Ci) and confidence > supp(RC )L. Let's consider the 93-row BAR from our running example:  (p3 expressed AND [EITHER f(gp expressed) AND (either p4 or 96 not expressed) OR { (either 92 or p5 not expressed) AND (either p4 or 95 not expressed)} ] ) => Cancer.

It has l00% confidence and support {sl s2}. Now, if we remove all exclusion list clauses related to sample row s5 we end up with the boolean association rule:  (93 expressed AND [EITHER (gp expressed) OR (either 92 or o5 not expressed) ] > Cancer.

This new rule has support {sl s2} and a confidence of X 2 . The preceding observation leads us to the  following theorem: Theorem 1: Let D be a relational data set containing s  samples no two of which are the same (i.e. no two sample rows express the exact same set of genes). Then there exists a pure conjunction B implying a class type C (i.e., a CAR) with confidence c and support supp for D there exists a 100% confident BST generated BAR B C for D that.

(i) has supp(B => C) = supp, and (ii) contains exclusion list clauses actively excluding (c-1) suppl non-C samples.

Proof: See the extended version of this paper [13].

U  Theorem I tells us how we can get CARs from BARs.

Furthermore, it says 100% confident BARs with large support and a small number of excluded samples are equivalent to high support/confidence CARs. Hence genes that show up in many high confidence, high support CARs will also be prevalent in many 100% confident BARS with high support and a low number excluded samples. Most importantly, we see that all high confidence CARs (which tend to be good classifiers) have closely related BAR counterparts.



IV. BST-BASED CLASSIFICATION In principal, 100% confident BST-generable BARs should  be sufficient for classification because they contain at least as much information as all generable CARs do (see section III- C). Indeed, beyond what CARs with similar support are capable of, 100% confident BARs supply us with "unpolluted" ground truth. Thus, it's not too surprising that the class of BST-generable BARs we've looked at so far will be enough to enable highly accurate classification.

Let Ci be a class set of interest and T(i) be the BST for class C? constructed from the given training data. From section III-B we can see that all BST generable BARs for class Ci are created by combining J(i) cell rules. Thus, we expect that by restricting our attention to the 0(lGC Ci i) atomic T(i) cell rules we will be, in some sense, still considering all T(t) generable BARs for C Our new scalable classifier, the     Algorithm 2 BST Cell rule quantized Evaluation (BSTCE) 1: Input: Class Ci, BST for the class JT(i), Samples S, Query  sample Q 2: Output: Classification value 3: for all non-empty exclusion lists e in T(i)'s cells do  Vvi Es.tQ[g }1j4: ee 5: end for 6: for all (g,s) C { C G s.t. Q[g] = 1} x Ci do 7: if T1(i)(g, s) contains a . then 8: T(i)[g][s] 1 9:  10: 11: 12: 13: 14: 15: 16:  else TIi)g[ Mm V st eisiniL(i)(g, s)}  end if end for for all non-blank sample columns s C T(i) do  V, Mean of non-blank T(i) [*] [s] values end for Return the Mean of step 16's V, values  Boolean Structure Table Classifier (BSTC) capitalizes on this line of thought by ignoring BAR generation and focusing exclusively on atomic BST cell rules.

A. BSTC Overview Let Q be a test/query gene expression data sample and  T(i) be a BST for class set Ci. BSTC is a heuristic rule- based classifier motivated by standard Boolean formula arith- metization techniques [14] such as those employed in fuzzy satisfiability [15]. By using these ideas we can avoid the highly costly process of support/confidence based association rule mining. Instead of explicitly generating rules, BSTC decides (heuristically) for all Ci, how well Q collectively satisfies T(i)'s atomic cell rules. BSTC then classifies Q as the sample class whose BST has the highest expected atomic rule satisfaction level from Q.

Intuitively, we expect BSTC to be accurate because it approximates the results of CAR-based classification: Suppose that a high support/confidence CAR exists which classifies our query sample Q as class Cj. This will only happen if all the CAR's antecedent genes, AG, appear in both (i) Q and, (ii) most of the training samples in the CAR s consequent class Cj. Let T(j) be the BST for class Cj. Because of (ii) most of T(j)'s sample columns must contain cell entries for all the AG genes. Furthermore, all T(j)'s AG cell entries will have few exclusion lists in common (by Theorem 1). Hence, T(j)'s expected atomic rule satisfaction level from Q (i.e. Q's classification value) should be heavily influenced (increased) by the AG rows and their few shared lists.

B. BST Cell Rale Satisfaction As above, let Q be a test/query gene expression data sample  and T(i) be a BST for class set Ci. Algorithm 2 BSTCE gives BSTC's method of calculating the level that 9 satisfies a given atomic T(i) cell rule We next explain the rational behind BSTCE.

We know that each T(i) (g, s)-cell exclusion list, L, cor-  responds to a disjunction in T(i)'s (g, s)-cell rule. Hence, if Q satisfies any one negation/inclusion in L, Q will satisfy L.

Algorithm 3 The BSTC Algorithm 1: Input: BSTs for all dataset classes 1Ml ... I1 (N), Query  sample Q 2: Output: Classification for query sample Q 3: for all i 1{ },...' do 4: CV(i) BSTCE(T1(i), Q) 5: end for 6: Return mno7{i{ CV(i) = max{CV(1),I.. , CV(N)}}  However, if Q expresses most of its genes in common with L's associated non-Ci sample we assume it's probably not of type Ci (i.e., Q is weakly excluded). Hence, we use BSTCE's line 4 ratio to approximate the probability that L correctly excludes Q from being of L's associated sample's class.

In order for the (g s)-cell rule to be satisfied all of (g,s)'s exclusiorn lists must be satisfied (i.e., logical AN\D).

If independence of each exclusion list's correct classification is assumed it's natural to multiply all of (g, s)'s list's proba- bilities. We don't assume independence and use a min instead (line 10). Finally recall that all black dots in T(i) correspond to genes expressed only in class Ci samples. If Q expresses a black dot gene it automatically satisfies all that gene's non empty T(i) cell rules. Hence, black dots are all assigned values of I in BSTCE's line 8.

Once we have used BSTCE lines 1-12 to calculate Q's  classification values (i.e. T(i)'s atomic rule satisfaction levels from Q) for each relevant simple (g, s)-cell rule, we are nearly finished. We have all the values required to judge Q's similarity to T(i) via an expectation calculation. For the sake of T(i)'s expectation calculation, all that is left to do is imagine choosing a relevant simple T(1) rule at random and then using it to classify Q. To randomly select a (g, s) rule we first imagine selecting a non-empty T(W) sample column uniformly at random and then picking a cell-rule from that column uniformly at random. The expected probability of correctly classifying Q with T(i) via this method (which heuristically is proportional to T(i)'s expected satisfaction level from Q) is then calculated by averaging the approximate cell rule satisfaction levels down each non-empty sample column (line 14) and then averaging the resulting non-empty sample averages (line 16).

C. BSTC Algorithm Suppose we are given relational training data D containing  sample rows S split up into disjoint class sets Cl ... CN.

BSTC uses D to construct N BSTs T(1) T (N) Next let G be the union of the elements contained in each sample row of D (i.e the gene set of D) and let Q be a query sample with expression information regarding G. BSTC will use the BSTCE algorithm to classify Q as being the C, with smallest i such that BSTCET(i)T 9) =max{BSTCE(T(j), Q) 0< j< NJ See Algorithm 3 for the BSTC algorithm.

Note that there is no reason why N must be 2. BSTC easily generalizes to datasets containing more than two class labels  1) BSTC Rantime: As noted in section III-A. 1 it takes time and space O(lS2SG&) to construct all the BSTs     gsI I2s g2 1 1  g4 0.5 g5 1,0.5  g6 -U-  sI s2 s3 (1 + 0.5)/2 = 0.75 1 1 0.5  CANCER classification value = (0.75 + 1 + 0.5)13 = 0.75  Fig. 3. BSTC cell rule Evaluation Example  T(1), ... . T(N). Thus, BSTC requires tirme and space O( S 2 GQ) to construct. Furthermore, during classification BSTC must calculate BSTCE(T(i), Q) for I < i < N. BSTCE (Algorithm 2) runs in O (( S -Ci ) &G Ci ) time per query sample. Therefore the BSTC worst case evaluation time is also f( S 2 * G) per query sample. See Sectiorn VII for more on BSTC's per-query classification time.

2) Biological Meaning ofBSTC Classificationrl Association rules mined from gene expression data provide an intuitive representation of biological knowledge (e.g., the expression of certain genes implies cancer). Hence, CAR-based clas- sifiers have the desirable ability to justify each non-default consequent class query classification with the biologically meaningful CAR(s) the query satisfied. BSTC, being rule- based and related to CAR-classifiers, also has this property.

BSTC can support it's query classifications with BARs of  any user specified complexity. Most simply, for any given query sample Q and c E (0,1], BSTC can justify it's classification of Q as class Ci by reporting all T(i) atomic cell rules with satisfaction levels > c. Note that returning this information requires no additional per-query classification time. Also note that section III-B. 1 methods can be used to mine more complex highly satisfied BARs if desired.

D. BSTC Example  Consider our running example from Table I. In order to construct BSTC we must construct both T(Healthy) and T(Cancer) (shown in Figure 1). Once both BSTs have been constructed we can begin to classify query sam- ples. Suppose, for example, we are given the query sam- ple =fgl expressed g2 not expressed g3 not expressed g4 expressed, g5 expressed, g6 not expressed}. To classify this query we must first calculate BSTCE(T(Cancer) 9) and BSTCE(T(Healthy), 9).

The evaluation of BSTCE(T(Cancer), Q) proceeds as fol-  lows: Since our query sample Q expresses gene g9 we can see that we must for example determine the fraction of both of the (95, s) -cell's exclusion lists satisfied by 9. The (95 si) cell's (S4 : gl) exclusion list is totally satisfied since 9 expresses ga. Hence, it gets a value of 1. However, the (85s -94g -96) exclusion list is only half satisfied since  TABLL 11 GENE ExIlRESSION DATAS1ETS  # Class I Class 0 # Class I # Class 0 Dataset Genes label label samples samples ALL/AMNL (ALL) 7129 ALL AML 47 25 Lung Cancer (LC) 12533 MPM ADCA 31 150 Prostate Cancer (PC) 12600 tumor normal 77 59 0varian Cancer (OC) 15154 tumor normal 162 91  although Q doesn't express 96, Q does expresses g4. Thus, in total, we only consider half of the simple (95,sl)-cell rule to be satisfied (i.e. the S5 exclusion list is the weakest link). Continuing to use BSTC's approximation scheme for the expected probability of Q's correct Cancer classification via the Figure 1 BST we obtain Figure 3. Note that only Figure 3 gene rows corresponding to genes expressed in Q are nrn- empty.

If we now evaluate BST-EXPECT(T(Healthy), Q) we ob- tain a final8 value of -. To finish, BSTC will compare Q's Cancer classification value of 4 to Q's Healthy classification value of 3 and conclude that Q is most probably Cancer.

Hence, Q will be classified as Cancer.



V. EXPERIMENTAL EVALUATION All experiments reported here were carried out on a 3.6 GHz  Xeon machine with 3GB of memory running Red Hat Linux Enterprise 4. For our empirical evaluation we use four standard real microarray datasets [16] Table II lists the dataset names, class labels, and the number of samples of each class. All discretization was done using the entropy-minimized partition [17] as in [1].

Executables for both RCBT and Top-k were provided by the authors of [1]. In all experiments, the Top-k rule generator was used to generate rule groups for RCBT. Unless otherwise noted we ran both Top-k and RCBT with the author suggested parameter values (i.e., support - 0.7, k - 10, nl - 20, 10 RCBT classifiers). Hence, while generating rules for RCBT we used Top-k with a minimum support value of 0.7 and found the 10 most confident covering rule groups (i.e. k = 10). Furthermore, during classification we used RCBT with the suggested 10 classifiers (1 primary and 9 standby). Finally, ni, the number of lower bound rules to use for classification per Top-k mined rule group, was set equal to 20. We coded BSTC in C++.

A. Preliminary Experiments Each of Table II's four gene expression datasets comes  with a clinically determined training set. The authors of [1] provided us with their discretizations of these four datasets.

We ran BSTC on their discretizations and BSTC matched RCBT's reported mean accuracy (about 96%) outperforming CBA (87%), IRG (8lX1), Weka 3.2 (C4.5 family single tree (74%), bagging (78%), boosting(74%)), and SVMligh' 5.0 (93C) in reported mean performance [1].

To compare BSTC and RCBT with the most recent R el 071 package SVM implementation [ 18] and randomForest version 4.5 [19] we rediscretized the four datasets and reran   sI s2 s3    TABLE III RES Ul TS USINC; GIVEN TRAINING DATA  # Class I # Class 0 Genes random.

Training Training After BSTC RCBT SVM Forest  Dataset Samnples Samnples Discr. Ac or.Accr. Ac o. Accuracy ALL 27 11 866 82.35% 91. 18% 91. 18% 85.29% LC 16 16 2173 100% 97.99% 93.29% 99.33% PC 52 50 1554 100% 97.06% 73.53% 73.53% OC 133 7 5769 l00% 97.67% l005% I00 Avg.

Accuracy ______ __95.59% 95.98% 89.5% 8954%  BSTC/RCBT To keep comparisons fair we ran SVM and randomForest on the same genes selected by our entropy discretization except with their original undiscretized gene expression values. SVM was run with its default radial kernel.

We ran randomForest 10 times with its default 500 trees for ALL, LC, and OC and its accuracy was constant. For PC we had to increase randomForest's number of trees to 1000 before its accuracy stabilized over the 10 runs.

Table III contains the number of class 0/1 samples in the clinically determined training set the number of genes selected by our entropy discretization, and our experimental results. As shown in this table the overall average accuracies of BSTC and RCBT are again best at about 96% each. When compared against RCBT, SVM, and randomForest on the individual tests we can see that BSTC is alone in having 100% accuracy on the majority of datasets.

However, BSTC's performance on the preliminary AML/ALL dataset test is relatively poor. This is likely due to over fitting. Every error BSTC made mistook a class 0 (AML) test sample for a class 1 (ALL) test sample (i.e. all errors were made in this same direction). And, the ALL training data has both (i) about 2.5 times as many class 1 samples as class 0 samples, and (ii) a small number of total samples/genes.

When the training set is more balanced and the number of samples/genes is larger we can expect that cancellation of errors will tend to neutralize/balance any over fitting effects in BSTC. And BSTC is a method meant primarily for large training sets where CAR-mining is prohibitively expensive.

As we will see below in Section V-B.1, BSTC s performance is much better for larger AML/ALL training set sizes.

B. Holdout Validation Studies  Holdout validation studies make comparisons less suscep- tible to the choice of a single training dataset and provide performance evaluations that are likely to better represent program behavior in practice. We next present results from a thorough holdout validation study completed using 100 different training/test sets from each of the ALL LC PC and OC data sets. For these holdout validation tests we benchmark BSTC against Top-k/RCBT because (i) BSTC/RCBT perform hest in our preliminary experiments, (ii) Top-k/RCBT is the fastest/most accurate CAR-based classifier for microarray data and (iii) we are interested in BSTC's CAR-related vs Top- k/RCBT s CAR based scalability.

For the holdout validation study we generated training sets of sizes 40% 60% and 80% of the total samples. Each training set was produced by randomly selecting samples from the original combined dataset. We then used the standard R dprep package's entropy minimized partition [17] to discretize the selected training samples. Finally, the remaining dataset samples were used for testing the two classifiers after rule/BST generation on the randomly selected training data. For each training set size we produced 25 independent tests. In addition to these training sets, we created an additional 25 1-x/0-y tests. To create these tests we chose training data by randomly selecting x class 1 samples and y class 0 samples to be used as training data. As before the remaining samples were then used to test both classifiers. For each dataset the x and y values are chosen so that the resulting 25 classification tests have the exact same training/test data proportions as the single related dataset test reported in section V-A. For each training set size we plot our results using a boxplot.

Boxplot Interpretation: Each boxplot that we show in this section can be interpreted as follows: The median of the measurements is shown as a diamond and a box with boundaries is drawn at the first and the third quartile. The range between these two quartiles is called the inter-quartile range (IQR). Vertical lines (a.k.a. "whiskers") are drawn from the box to indicate the minimum and the maximum value, unless outliers are present. If outliers are presents, the whiskers only extend to 1.5 x IRQ. The outliers that are near (i.e. within 3 x IRQ are drawn as an empty circle and further outliers are drawn using an asterisk.

1) ALIJAML (ALL) Experiment: Figure 4 shows the clas- sification accuracy for the ALL/AML dataset. As can be seen in this figure, BSTC and RCBT have similar accuracy across the ALL/AML tests as a whole. BSTC outperforms RCBT in terms of median and mean accuracy on the 40% and 80% training set sizes while RCBT has better median/mean accu- racy on the 1-27/0-11 training size tests. And, both classifiers have the same median on the 60% training set size. Over the 100 ALL/AML tests we see that BSTC has a mean accuracy of 92.13% while RCBT has a mean accuracy of 91.39% (they are very close).

It's noteworthy that BSTC is 100% accurate on the majority of 80% training size tests. However, BSTC appears to have slightly higher variance than RCBT on all but the 40% training tests. Considering all the results together both BSTC and RCBT have essentially equivalent classification accuracies on the ALL/AML dataset.

2) Lung Cancer (LC) Experiment: The results for the Lung Cancer dataset are reported in Figure 5. Here, again, both BSTC and RCBT have similar classification behavior. RCBT has higher mean and median accuracies on the 40% and 60% tests while BSTC outperforms RCBT on the 1-16/0-16 tests.

Meanwhile both classifier have the same median on the 80% training test. Over all 100 LC tests we find that BSTC has a mean accuracy of 96 32% while RCBT has a mean accuracy of 97.08% (again, they are very close).

As before, BSTC is alone in having 100% accuracy more     O Median * Mean ? Near outliers * Far outliers  40% Training 60% Training 80% Training 1 -27/0-11 Training 1.0 1.0 1 0]-  0.95  0.9 0.9 0.7 0.7  0.85-  0.8 -0.8- 0.8- 0.8-  0.75-  0.7 - 0.7- 0.7- 0.7  65-  0.6 -0.6- 0.6- 0.6-  BSTC RCBT  (a) Fig. 4.

BSTC RCBT BSTC RCBT  (b) (c) ALL Holdout Validation Results  BSTC RCBT (d)  then half the time for any training set size (see Figure 5 (d)).

However, RCBT has smaller variance for 3 of the 4 training set sizes. Therefore as for the ALL/AML data set both BSTC and RCBT have about the same classification accuracy on LC.

3) Prostate Cancer (PC) Experiment: RCBT begins to run ilnto a comiputational difficulties on PC's larger training set sizes. This is because before using a Top-k rule group for classification RCBT must first mine nt lower bound rules for the rule group. RCBT accomplishes rule group lower bound mining via a pruned breadth-first search on the subset space of the rule group's upper bound antecedent genes. This breadth- first search can be quite time consuming. In the case of the Prostate Cancer (PC) dataset all 100 classification tests (25 tests for each of the 4 training set sizes) generated at least one top-10 rule group upper bound with more than 400 antecedent genes. Due to the difficulties involved with a breadth-first search over the subset space of a several hundred element set, RCBT began suffering from long run times on many PC classification tests.

Table IV contains four average classification test run times (in seconds) for each PC training size. The 'BSTC' column run times reflect the average time required to build both class 0 and class I BSTs and then use them to classify all the test samples. Each Top-k' column run time is the average time required for Top-k to mine the top 10 covering rule groups (with minimum support 0.7) for each training set.

Table IV's 'RCBT' column gives average run times for RCBT using a time cutoff value of 2 hours for all the training sets. For each classification test if RCBT was unable to complete the test in less than the cutoff time, it was terminated and it s run time was reported as the cutoff time. Hence the  TABLE IV AVERACGE RUN TIMLES FOR THE PC TESTS (IN SEC-NI)S). t INDICATES  nl WAS LOWERED TO 2.

Training  40% 60% 80% 1.52/0.50  BSTC .3  4.93 5.78 5.57  Top- 0.09 5.06  120.63  21.32  RCBT  > 7110 > 7200 > 7200  # RCBT DNF  0, P  .00 24/25 t 25/25t T 25/25t  cJ  CZ C-)  O Median * Mean ? Near outliers * Far outliers  40% Training 60% Training 80% Training 1-16/0-16 Training  h0 1.0 T 1.0 1.0-  0.95 0.9 0.9 0.

0.8  0.8 0.8- 0.8- 0.8-  BSTC F  (a) RCBT BSTC RCBT BSTC RCBT  (b) (c) Fig. 5. LC Holdout Validation Results  BSTC RCBT (d)  RCBT column gives lower bounds on RCBT s average run time per training set test. Finally, the # RCBT DNF' column gives the number of tests RCBT was unable to finish in < the cutoff time, over the number of tests for which Top-K finished mining rule group upper bounds.

Explanation for varying nd values: Run time cutoffs were necessary to mitigate excessive holdout validation CAR- mining times. Even with a cutoff of 2 hours these 100 PC experiments required about 11 days of computation time, with most experiments not finishing. For the 80% and 1-52/0-50 training set sizes RCBT with nl = 20 failed to finish lower bound rule mining for all 50 tests within 2 hours. Thus RCBT's nl parameter was lowered from the default value of 20 to 2 in an attempt to improve its chances of completing tests. Not surprisingly, decreasing nl (i.e., mining fewer lower bound rules per Top-k rule group) decreases RCBT s runtime.

However, RCBT was still unable to finish lower bound rule mining for any tests.

Classification Accuracy: Figure 6 contains accuracy results for BSTC on all four Prostate Cancer test sets. Prostate Cancer boxplots for RCBT weren't constructed for training set sizes that RCBT was unable to complete all 25 tests within the time cutoffs. In contrast, BSTC was able to complete each of the 100 PC classification tests in less than 6 seconds.

Table V contains mean accuracies for the PC dataset with 40%, 60%, 80%, and 1-52/0-50 training. For each training set the average accuracies were taken over the tests RCBT was able to complete within the cutoff time. Hence, the 40% row means were taken over all 25 results. Since RCBT was unable to complete any 80% or 1-52/0-50 training size tests we report these BSTC means over all 25 tests. RCBT has slightly better accuracy then BSTC on 40% training. For 60% training  TABLE V MEAN AcCURACIES FOR THE PC TESTS THAT RCBT FINISHED.

Training BSTC RCBT  40U 75.08% 79.27% 60% 78.18% 85.45% 80% 84.98% 1-52/0 50 81.65%o   cJ  CZC-)    Median * Mean ? Near outliers * Far outliers  40% Training 60% Training  0.9-  0.8-  0.7-  0.6-  BSTC RCBT  (a)  80% Training 1-52/0-50 Training  0.9-  DNFI 0.8-  0.7-  0.6-  BSTC RCBT (b)  1 .u-  0.9-  DNFI 0.8-  0.7-  0.6-  BSTC RCBT (c)  i DNF cJ  CZC-)  40% Training 60% Training 80% Training 1-133/0-77 Training  0.95 DNF DNF DNF  0.9 0.9- 0.9- 0.9-  0.85 -  0.8 - BSTC RCBT  (d) Fig. 6. PC Holdout Validation Results  BSTC RCBT  (a) Fig.

0.8- 0.8 0.8- BSTC RCBT BSTC RCBT  (b) (c) 7. OC Holdout Validation Results  RCBT outperforms BSTC on the single test it could finish by more then 7%, although it should be kept in mind that RCBT's results for the 24 unfinished tests could vary widely.

Note that BSTC's (mean) accuracy increases monotonically with training set size as expected. At 60% training BSTC's accuracy behaves almost identically to RCBT's 40% training accuracy (see Figure 6).

4) Ovarian Cancer (OC) Experiment. For the Ovarian Can- cer dataset, which is the largest dataset in this collection, the Top-k mining method that is used by RCBT also runs into long computational times. Although Top-k is an exceptiounally fast CAR group upper bound miner, it still depends on performing a pruned exponential search over the training sample subset space. Thus, as the number of training samples increases Top-k quickly becomes computationally challenging to tune/use.

Table VI contains four average classification test run times (in seconds) for each Ovarian Cancer(OC) training size. As before, the second column run times each give the average time required to build both class 0/1 BSTs and then use them to classify all test's samples with BSTC. Note that BSTC was able to complete each OC classification test in about 1 minute.

In contrast, RCBT again failed to complete processing most classification tests within 2 hours.

Table VI's third column gives the average times required for Top-k to mine the top 10 covering rule groups upper bouhnds for each training set test (with the same 2 hour cutoff procedure as used for PC testing). The fourth column gives the average run times of RCBT on the tests for which Top- k finished mining rules (also with a 2 hour cutoff). Finally, the '# RCBT DNF' column gives the number of tests that RCBT was unable to finish classifying in < 2 hours each,  TABLE VI AVERAGE RUN TIMES FOR THE OC TESTS (IN SECOND)S). t INDICATES  nl WAS LOWERED TO 2.

Training BSTC Top-k RCBT # RCBT DNF 40% 30.89 0.6186 273.37 0/25 60% 61.28 41.21 > 5554.37 19/25 80% 71.84 > 1421.80 > 7205.43 t 21/22 1 133/0-77 70.38 > 1045.65 > 6362.86 t 20/23  over the number of tests for which Top-k finished. Because RCBT couldn't finish any 80% or 1-133/0-77 tests within 2 hours with nl = 20, we lowered nl to 2.

Classification Accuracy: Figure 7 contains boxplots for BSTC on all four OC classification test sets. Boxplots were not generated for RCBT with 60%, 80%, or 1-133/0-77 training since it was unable to finish all 25 tests for all these training set sizes in < 2 hours each. Table VII lists the mean accuracies of BSTC and RCBT over the tests on which RCBT was able to produce results. Hence, Table VII's 40% row consists of averages over 25 results. Meanwhile Table VII's 60% row results are from 6 tests, 80% contains a single test's result, and 1-133/0-77 results from 3 tests. RCBT has better mean accuracy on the 40% training size, but the results are closer on the remaining sizes ( < 4% difference over RCBT's completed tests). Again, RCBT's accuracy could vary widely on its uncompleted tests.

CAR Mining Parameter Tuning and Scalability: We  attempted to run Top-k to completion on the 3 OC 80% training and 2 OC 1-133/0-77 training tests. However it could not finish mining rules within the 2 hour cutoff. Top-k finished two of the three 80% training tests in 775 min 43.6 sec and 185 min 3.3 sec. However, the third test ran for over 16,000 mnm (> 11 days) without finishing. Likewise, Top-k finished one of the two 1-133/0-77 tests in 126 min 45.2 sec but couldn't finish the other in 16,000 min (> 11 days). After increasing Top-k's support cutoff from 0.7 to 0.9 it was able to finish the two unfinished 80% and 1-133/0-77 training tests in 5 min 13.8 sec and 35 min 36.9 sec, respectively. However, RCBT (with nl= 2) then wasn't able to finish lower bound rule mining for either of these two tests within 1,500 min. Clearly, CAR-mining and parameter tuning on large training sets is  TABLE VII  MEAN AcCU1ACIES FOR THE OC TESTS THAT RCBT FINISHED).

Training BSTC RCBT  40% 92.05% 97.66% 60% 95.75% 96.73% 80% 94 12% 98.04% 1-133/077 9380 96.12%   cJ  CZC-)  0.95 -  0.9 -  0.85 -  0.8 -  0.75 -  0.7 -  0.65 -  0.6 -  0.55 -  0.5 - BSTC RCBT  (d)  Median 9 Mean 0 Near outliers * Far outliers  1.01 1- 1 .01 1.0 -    computationally challenging. As training set sizes increase, it is likely that these difficulties will also increase.



VI. RELATED WORK While operating on a microarray dataset, current CAR [1],  [2], [3], [4] and other pattern/rule [20], [21] mining algorithms perform a pruned and/or compacted exponential search over either the space of gene subsets or the space of sample subsets.

Hence, they are generally quite computationally expensive for datasets containing many training samples (or genes as the case may be). BSTC is explicitly related to CAR-based classifiers, but requires no expensive CAR mining.

BSTC is also related to decision tree-based classifiers such  as random forest [19] and C4.5 family [9] methods. It is possible to represent any consistent set of boolean association rules as a decision tree, and vice versa. However, it is gen- erally unclear how the trees generated by current tree-based classifiers are related to high confidence/support CARs which are known to be particularly useful for microarray data[ 1], [2], [6], [7], [11]. BSTC is explicitly related to, and motivated by, CAR-based methods.

To the best of our knowledge there is no previous work on mining/classifying with BARs of the form we consider here.

Perhaps the work closest to utilizing 100% BARs is the TOP- RULES [22] miner. TOP-RULES utilizes a data partitioning technique to compactly report itemlgene subsets which are unique to each class set Ci. Hence, TOP-RULES discovers all 100% confident CARs in a dataset. However, the method must utilize an emerging pattern mining algorithm such as MBD-LLBORDER [23], and so generally isn't polynomial time. Also related to our BAR-based techniques are recent methods which mine gene expression training data for sets of fuzzy rules [24], [25]. Once obtained, fuzzy rules can be used for classification in a manner analogous to CARs. However, the resulting fuzzy classifiers don't appear to be as accurate as standard classification methods such as SVM [25].



VII. CONCLUSIONS AND FUTURE WORK To address the computational difficulties involved with  preclassification CAR mining (see Tables IV and VI), we developed a novel method which considers a larger subset of CAR-related boolean association rules (BARs). These rules can be compactly captured in a Boolean Structure Table (BST), which can then be used to produce a BST classifier called BSTC. Comparison to the current leading CAR classifier, RCBT, on several benchmark microarray datasets shows that BSTC is competitive with RCBT's accuracy while avoiding the exponential costs incurred by CAR mining (see Section V- B). Hence, BSTC extends generalized CAR-based methods to larger datasets then previously practical. Furthermore, unlike other association rule-based classifiers, BSTC easily general- izes to multi-class gene expression datasets BSTC's worst case per-query classification time is worse  then CAR-based methods, after all exponential time CAR mining is completed (O( SlS CGl) versus O( Si CGi)). As future work we plan on investigating techniques to decrease this cost by carefully culling BST exclusion lists.

ACKNOWLEDGM[ENTS We thank Anthony K.H. Tung and Xin Xu for sending  us their discretized microarray data files and Top-k/RCBT executables. This research was supported in part by NSF grant DMS-0510203, NIH grant I-U54-DA021519-OlAf, and by the Michigan Technology Tri-Corridor grant GR687. Any opinions, findings, and conclusions or recommendations ex- pressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies.

