GPU Accelerated Item-Based Collaborative Filtering for Big-Data Applications

Abstract?Recommendation systems are a popular marketing strategy for online service providers. These systems predict a customer?s future preferences from the past behaviors of that customer and the other customers.  Most of the popular online stores process millions of transactions per day; therefore, providing quick and quality recommendations using the large amount of data collected from past transactions can be challenging. Parallel processing power of GPUs can be used to accelerate the recommendation process. However, the amount of memory available on a GPU card is limited; thus, a number of passes may be required to completely process a large-scale dataset. This paper proposes two parallel, item-based recommendation algorithms implemented using the CUDA platform. Considering the high sparsity of the user-item data, we utilize two compression techniques to reduce the required number of passes and increase the speedup. The experimental results on synthetic and real-world datasets show that our algorithms outperform the respective CPU implementations and also the na?ve GPU implementation which does not use compression.

Keywords - recommendation systems; GPU; CUDA; collaborative filtering; big-data

I. INTRODUCTION Internet-based service providers such as Amazon and  Netflix widely use Collaborative Filtering (CF) to provide personalized recommendations based on past behaviors of the users. CF-based recommender systems can be divided into two categories; the user-based CF and the item-based CF [1], [2]. Due to the large volume of transactions generated by the online stores, providing quick and quality recommendations is a challenging task.

Graphics Processing Units (GPUs) are designed to handle highly parallel workloads and can execute thousands of concurrent threads. With the introduction of CUDA (Compute Unified Device Architecture)1  platform, a general purpose parallel computing architecture, GPU computing has become more and more popular in general-purpose, large-scale, data mining applications [3-5]. Parallel processing power of GPUs can be used to speed up the CF process [6-8]. However, the amount of memory available on a GPU card is limited;   1 http://www.nvidia.com/object/cuda_home_new.html  therefore, a number of runs may be required to completely process a large-scale dataset. This limits the performance gain that can be achieved by using GPUs. In most scenarios, the user-item data is very sparse because there are large numbers of users and large variety of items but each customer only purchases a relatively small number of items. Thus, representing the user-item data in a compressed format can significantly reduce the memory requirements and the processing time. The existing CUDA implementations [6-8] of user-based CF systems do not consider the sparsity of the user-item data.

Various studies have shown that the item-based CF methods outperform the user-based CF methods in many scenarios [1], [2], [9], [10]. Due to its ability to scale to large numbers of users and its predictive accuracy, item-based CF is used by many large-scale online service providers such as Amazon, Google, YouTube, and Netflix. Some recommendation algorithms compute recommendations based on the user ratings of the items. However, most of the times, user ratings are not available but only the purchase information. In such situations, Deshpande and Karypis [1] have shown that the item-based CF approach combined with the conditional probability-based similarity measure produces higher-quality recommendations than the user-based CF approach.

The conditional probability-based similarity of two items depends on the co-occurrence of those two items in the transaction history. Thus, it is only required to know whether the user has purchased an item or not (i.e. the user rating of the item is not necessary). This property of conditional probability-based similarity measure enables us to compress the user-item data and significantly reduces the memory requirements, thereby making it a good candidate for GPU implementation. In this paper we propose two GPU accelerated item-based CF algorithms, using conditional probability as the similarity measure. Considering the high sparsity of the user-item data, we utilize compression techniques to reduce the required number of passes to completely process large-scale datasets. Data compression also allows us to reduce the number of computations required in populating the item-item similarity matrix. The experiments      show that the proposed GPU implementations are very efficient in processing large-scale datasets in a timely manner and achieved up to 30X and 125X speedups compared to the respective CPU implementations and up to 5X and 18X speedups compared to the na?ve GPU implementation which does not use data compression.



II. RELATED WORK There is very limited work on using GPUs to accelerate  recommendation system applications. Kato and Hosino [6] proposed a CUDA implementation of user-user k-nearest neighbor search for user-based CF. Li et al. [7] proposed a social network-aware top-N recommender system using GPUs. This approach also uses user-based CF and requires social network information to produce recommendations.

Both above work do not consider the sparsity of the user-item matrix, and thus, require a number of passes to process large- scale datasets using GPUs. Zhanchun and Yuying [8] also proposed a GPU accelerated user-user similarity calculation method, emphasizing on addressing the accuracy limitation of user-based CF techniques by using implied similarity and default values to fill the missing values. This might increase the accuracy of the recommendation process, but still require a number of passes to completely process large-scale datasets as the amount of memory available in GPU is limited.

Our proposed methods consider the sparsity of user-item data and use compression techniques to reduce the number of runs required to process large-scale datasets in GPU, thereby increasing the possible speedup. Furthermore, our methods are based on item-item CF which is proven to be more accurate and scalable than user-user CF methods [1], [2], [10].



III. PRELIMINARIES  A. Item-based Collaborative Filtering Let n denote the number of distinct users and m denote the  number of distinct items in the dataset. Then, this dataset can be represented by an n?m binary matrix R that will be referred to as the user-item matrix, such that Ri,j is one if the ith user has purchased the jth item, and zero otherwise. Let N denote the number of recommendations that need to be computed for a particular user. The top-N recommendation problem is formally defined as follows:  Given the user-item matrix R and the set of items Xui purchased by a given user ui (i = 1 to n), identify a set of items Xri (Xui Xri = {} and |Xri |  N) which is most likely to be purchased by the user ui.

Item-based CF process consists of two stages. The first stage is to compute the similarities between the different items; this produces an m?m similarity matrix S. Each column j in S is then sorted to get the top-k similar items for a particular item j. The second stage is to derive the top-N recommendations for a particular user ui, using the sorted similarity matrix S and the items already purchased by the user Xui. The focus of this paper is on accelerating the first stage (i.e. item-item similarity calculation) using GPUs. Since the similarity of each pair of items <Ii, Ij> can be individually calculated without depending on the other items, this stage can be highly parallelized using GPUs.

B. Conditional Probability-Based Similarity Calculation The similarity between a pair of items i and j can be  calculated based on the conditional probability of purchasing one of the items given that the other has already been purchased. The conditional probability of purchasing item j given that item i has already been purchased [P(j|i)] is defined as the number of users that purchase both items i and j divided by the total number of users that purchased i. If P(j|i) is directly used as the similarity measure, each item i will tend to have high similarities with the frequently purchased items (such as milk or bread), thus those items will appear on the top of the recommendation list. According to Kitts et al. [10], this problem can be corrected by penalizing the frequently purchased items by dividing P(j|i) with a quantity that depends on the frequency of item j, thus avoiding them dominating the recommendations.

In the original form, the rows of the user-item matrix R correspond to the binary purchase information, in which case, the similarity calculation gives equal weight to all users. It is shown in [1] that the co-purchasing information derived from the users that have bought fewer items is a more reliable indicator for the similarity of two co-purchased items than the information derived from the users that tend to buy a large number of items. By scaling each row of R to be of unit length, we can make users that have purchased fewer items contribute higher weight to the similarity measure than the users that have purchased more items. Based on these facts, Deshpande and Karypis [1] proposed the following formula to compute the similarity between two items using the normalized user- item matrix R [1],  ( , )  =  ,:{ ,    && , } [ ( ) ? ( (  )) ]  (1) Here,  and 1. The numerator in (1) is the sum of the corresponding entries of the jth column in the matrix R, for the set of the users who have purchased both items i and j. In the remainder of the paper we refer this as ?weighted co-occurrence frequency?.



IV. PROPOSED APPROACH  A. CUDA Implementation From a programmer?s point of view, the CUDA  programming model is a collection of threads running in parallel. A CUDA program consists of a host program running on the host CPU, and one or more parallel kernel functions executed on a GPU. Each kernel is executed by multiple blocks, and each block contains multiple threads. We split the similarity matrix computation into three steps and use a separate GPU kernel function for each step. First, Kernel 1 computes the weighted item-item co-occurrence frequencies (numerator in (1)) for possible pairs of items. Then, using that information Kernel 2 computes the item-item similarity according to (1). Finally, Kernel 3 sorts each column of the similarity matrix to find top-k similar items for each item.

Before starting the GPU computation, input data (user- item matrix) needs to be transferred from the host memory to the GPU?s memory which is called the global memory. If the     GPU does not have enough global memory space to hold the entire input dataset, multiple iterations are required to complete processing the entire dataset. When the user-item matrix is very large, this will have an effect on the runtime of the algorithm and reduce the performance gain. To address this issue, we propose two memory efficient implementations for step 1 (weighted co-occurrence frequency calculation).

The first method, GPU_BP_CF (GPU accelerated Bit- Packed Collaborative Filtering), uses bit packing to compress the data to reduce the memory requirement and increase the speedup. The second method, GPU_CMP_CF (GPU accelerated Compact Collaborative Filtering), uses a compact format of the user-item matrix which only stores non-zero elements. When user-item matrix is sparse, this method also reduces the memory requirement and gives significant speedup.

B. GPU_BP_CF Algorithm In this implementation we do not normalize the rows of R  beforehand. Each column of the user-item matrix is packed into n/32 (n is the number of users)2 32-bit integers; such bit- packing reduces the size of input data by 32 times, making it possible to store more transactions in the GPU memory at a time; thus reducing the number of iterations require to process the entire dataset.  We also compute the weight of each user wu (which is the reciprocal of the number of items purchased by the user) and transfer this information separately to the GPU to be used for row normalization.

To facilitate coalesced memory access, the user-item matrix is stored in the transposed (item-user) format in GPU memory. Each column of the user-item matrix is packed into 32-bit integers and then stored as a row in the item-user matrix. Fig. 1 depicts the bit packing process and computing the co-occurrence frequency of the pair of items Ii and Ij.

When the data sparsity is high bit packing allows to efficiently rule-out the users who have not purchased the interested pair of items without checking the individual bits. This is done using bitwise AND operation and significantly improves the processing time. After performing the bitwise AND operation, we only need to unpack the entries with non-zero values.

Then we multiply the non-zero bits with weight of the corresponding user (wu) and take the summation to get the weighted co-occurrence frequency.

Figure 1.  GPU_BP_CF algorithm - compressing the user-item matrix  using bit-packing.

2 If n is not a multiple of 32, add padding bits of zeros to the  end.

1) Kernel 1 ? Weighted co-occurrence frequency calculation  As opposed to the serial CPU implementation, in GPU we can compute the co-occurrence frequency of multiple pairs of items in parallel. We divide the item-item similarity matrix into m x By two dimensional blocks, and each block is divided into Tx x Ty threads. The value of Tx needs to be a multiple of 32 to utilize full power of coalesced memory access. Then value of By is given by m/Ty. Each row in each thread block computes the co-occurrence frequency of the pair of items <Ix, Iy> where Ix = bx and Iy = by*Ty + ty; here (bx, by) is the block index and ty is the row index within the block. Each row of threads reads the corresponding Tx elements (or columns) from the bit-packed item-user matrix and computes the weighted sum in parallel.  This is repeated (n/32)/Tx times until all the elements of the corresponding row are processed.

Fig. 2 illustrates the block and thread layout for Kernel 1 implementation of GPU_BP_CF algorithm.

Figure 2.  GPU_BP_CF Algorithm - thread layout for Kernel1 - computing weighted co-occurrence frequency of items in parallel.

2) Kernel 2 ? Compute the similarity values Once Kernel 1 is executed for all grids, we have the  weighted co-occurrence frequencies for all pairs of items.

Using this information Kernel 2 computes the similarity values for each pair of items in parallel. The similarity matrix is divided into BxB blocks of TxT threads. The value of T needs to be a multiple of 32 to achieve coalesced global memory access. Then value of B is given by m/T. Each thread computes the similarity of the pair of items <Ix, Iy> according to (3). Item indices are given by Ix = bx*T + tx and Iy = by * T + ty, where (bx, by) is the block index and (tx, ty) is the thread index within the respective block.

3) Kernel 3 ? Sort the similarity matrix Once the item-item similarities are computed, we need to  find the top-k similar items for each item. Kernel 2 sorts columns of the similarity matrix in parallel to get top-k similar items for each item. Each column is only required to be sorted partially until we get the top-k items. The value of k is relatively small compared to the number of items. Therefore, in Kernel 3 we use the partial insertion sort algorithm implemented by Garcia et al. [4].

C. GPU_CMP_CF Algorithm In this algorithm we compress the user-item matrix by  storing only the non-zero entries. As shown in Fig. 3 we use two arrays to store the compressed data. In the data array, for each user, we store the corresponding item numbers of the items purchased by that user. Row boundary for each user is maintained in a separate indices array of size (n+1) where n is the number of users. For each user we record the corresponding transaction starting index of the data array. For the last user we need to store the transaction ending index as well. Using this compact format significantly reduces the data volume; thus, reducing the number of iterations required to process the entire dataset. Moreover, if the data is uncompressed, it is required to compute the co-occurrence frequency of all possible pairs of items. When we use the compact format, it is only required to count the item pairs that actually occurred in the transactions. Here also we do not normalize the rows of R beforehand. The number of items purchased by a particular user can be found using the indices array, and will be used for normalization at the time of weighted co-occurrence frequency calculation.

Figure 3.  Compressing the user-item matrix into compact data and indices  arrays.

Kernel 1 ? Weighted co-occurrence frequency calculation Fig. 4 depicts the block and thread layout for Kernel 1 in  GPU_CMP_CF algorithm. The data array is divided into G grids. The number of grids depends on the available GPU memory. The GPU loads and processes one grid at a time.

Using the compact format reduces the number of iterations required. Each grid is divided into 1 x B one dimensional blocks which work in parallel. Each block is divided into Tx x Ty threads. The value of Tx needs to be a multiple of 32 to utilize the full power of coalesced memory access. Then the value of B is given by n/Ty. Each row of threads in each thread block processes a corresponding user of the data array given by the index u = by * Ty + ty, where by is the block index and ty is the thread index within the respective block. For a particular user index u, row length (i.e. the number of items purchased by the user (Cu)) is found using the indices array. Each thread tx finds the co-occurring items Iy for the item Ix from the set of items purchased by the user u and atomically increases the weighted co-occurrence frequency count of the respective index <Ix, Iy> in the item-item similarity matrix by 1/Cu.

Kernel 1 is executed on all G grids, one grid at a time, before proceeding to the other kernels. Implementations of   3 http://www.flixster.com/  Kernel 2 and Kernel 3 are similar to what is described in the previous section.

Figure 4.   GPU_CMP_CF Algorithm - thread layout for Kernel1 - computing weighted co-occurrence frequency of items in parallel.



V. RESULTS AND OBSERVATIONS We compared the performance of our parallel algorithms  with the respective serial implementations on the CPU and with a na?ve GPU implementation which does not use data compression. The CPU version of the bit-packed algorithm is named as CPU_BP_CF and the CPU version of the compact matrix algorithm is named as CPU_CMP_CF. The na?ve GPU implementation is named as GPU_UNCMP_CF.

The GPU versions of the algorithms are implemented using C++ and CUDA, and CPU versions are implemented in C++. All the experiments were carried out on a server running 64bit Fedora 17 with an Intel Xeon 3.3GHz CPU and 64GB memory, and an NVIDIA GeForce GTX680 GPU card with 2GB memory, CUDA runtime version 5.0, and compute capability 3.0. We evaluated the performance and scalability of the proposed algorithms using synthetic datasets of different volumes and sparsity. We also tested our algorithms on real-world datasets derived from the movie rental website Flixster3. The runtimes reported here are the total runtimes for the respective versions, including the time to read input data from the database and the data transfer time between the CPU and the GPU.

A. Runtime comparison with data volume To test the runtime variation of the algorithms according  to the increasing number of items, we fixed the number of users to 100K and increased the number of items from 5000 to 10000. The sparsity of the datasets is set to 90%, k is set to  First we compared the runtime of the proposed GPU implementations with the respective serial CPU implementations. Fig. 5 presents the runtime comparison and Fig. 6 presents the respective CPU/GPU speedups. It is visible that the runtime of the CPU versions increases rapidly with the increasing number of items, whereas the runtime of the GPU versions scales well with the increasing number of items. The GPU_CMP_CF algorithm has above 20X speedup compared to its CPU counterpart, and the speedup slightly increases with the increasing number of items. The GPU_BP_CF algorithm has above 60X speedup compared to     its CPU counterpart, and there is more noticeable increase in the speedup (up to 80X) with the increasing number of items.

Figure 5.  Runtime vs number of items [sparsity 90%, #users 100K].

Figure 6.  CPU/GPU speedup vs number of items [sparsity 90%, #users 100K].

Next we compared the runtime of the proposed GPU implementations with the runtime of the GPU_UNCMP_CF implementation which does not use any data compression.

According to Fig. 7, it is visible that the runtimes of the proposed GPU implementations scale well with the number of items in the dataset and outperform the GPU_UNCMP_CF implementation which does not use any data compression.

Figure 7. Comparison of GPU implementations - runtime vs number of  items [sparsity 90%, #users 100K].

We also compared the runtime variation of the algorithms according to the increasing number of users by fixing the number of items to 10K and increasing the number of users from 50K to 100K. The sparsity of the datasets is set to 90%, k  In these tests also, the GPU_CMP_CF algorithm achieved up to 20X speedup compared to its CPU counterpart, and the GPU_BP_CF  algorithm achieved up to 80X speedup compared to its CPU counterpart. We have omitted the graphs due to the space limitations.

B. Runtime comparison with data sparsity This section presents the runtime variation of the  algorithms according to the sparsity of the user-item matrix.

The number of users in the dataset is set to 100K and the number of items is set to 10K. The sparsity of the dataset is varied from 97% to 80%, k is set to 50, Fig.

8 presents the runtime comparison of the proposed GPU implementations with the respective serial CPU implementations and Fig. 9 presents the respective CPU/GPU speedups. It is visible that runtime of the CPU versions increases rapidly with the increasing density of the dataset, whereas runtime of the proposed GPU versions scales well with the increasing density of the dataset. The speedup achieved by the GPU_CMP_CF algorithm increased from 20X to 30X whereas speedup of the GPU_BP_CF algorithm increased from 40X-125X.

Figure 8.  Runtime vs sparsity of the dataset [#items 10K, #users 100K].

Figure 9.  CPU/GPU speedup vs sparsity of the dataset [#items 10K,  #users 100K].

We also compared the runtime of the proposed GPU implementations with the runtime of the GPU_UNCMP_CF implementation which does not use data compression, at different sparsity levels of the dataset. Fig. 10 depicts the runtimes of the three GPU implementations against the sparsity of the dataset. When the sparsity of the dataset is over 80%, it is visible that the proposed GPU implementations outperformed the GPU_UNCMP_CF implementation and achieved 2X-18X speed increase.

5000 6000 7000 8000 9000 10000  Ru nt  im e  (s )  Number of itemsGPU_CMP_CF GPU_BP_CF CPU_CMP_CF CPU_BP_CF   5000 6000 7000 8000 9000  Sp ee  du p  Number of itemsCPU_CMP_CF/GPU_CMP_CF CPU_BP_CF/GPU_BP_CF         5000 6000 7000 8000 9000 10000  Ru nt  im e  (s )  Number of itemsGPU_CMP_CF GPU_BP_CF GPU_UNCMP_CF            97% 95% 93% 90% 87% 85% 83% 80%  Ru nt  im e  (s )  Sparsity of the datasetGPU_CMP_CF GPU_BP_CF CPU_CMP_CF CPU_BP_CF          97% 95% 93% 90% 87% 85% 83% 80%  Sp ee  du p  Sparsity of the datasetCPU_CMP_CF/GPU_CMP_CF CPU_BP_CF/GPU_BP_CF     It is also noticeable that the GPU_CMP_CF algorithm is faster than the GPU_BP_CF algorithm when data sparsity is above 90% and vice versa when data sparsity below 90%. The reason behind this is, in the compact algorithm we only compute the similarity for the item pairs that actually co-occur in the transactions.  When the data is very sparse, the average number of items purchased by a user is low; thus, the required number of co-occurrence frequency computations is low. In the bit-packed implementation, although we rule out most of the irrelevant users using bitwise AND operations, it is still required to consider all possible pairs of items. When the data get denser, the number of co-occurring pairs of items increases; thus, the performance gain realized by the bit-pack implementation increases compared to the performance gain realized by the compact implementation.

Figure 10.  Comparison of GPU implementations - runtime vs data sparsity  [#items 10K, #users 100K]  To evaluate the performance of the proposed algorithms on real-world data, we used the version of the Flixster dataset4 prepared by Mohsen Jamali [11]. We only considered the users that have watched at least two movies. The original dataset consists of around 109K such users and for these users we extracted five different datasets with the number of items varying from 10K to 20K. The sparsity of these datasets is around 99%. The results obtained on Flixter datasets confirmed our observations on the synthetic datasets.

Displaying a similar behavior to the experiments on the synthetic datasets, here also the speedup achieved by the proposed GPU implementations increased with the number of items in the dataset.  The GPU_CMP_CF algorithm achieved up to 28X speedup and the GPU_BP_CF algorithm achieved up to 36X speedup compared to their respective CPU counterparts. Both algorithms also outperformed the GPU_UNCMP_CF algorithm which does not use data compression. We have omitted the graphs due to the space limitations.



VI. CONCLUSION In this paper we proposed two GPU accelerated similarity  calculation algorithms for item-based collaborative filtering systems. Considering the sparsity of large-scale user-item data, and limited memory available on GPUs, we use compression techniques to reduce the data volume. By doing   4 http://www.cs.ubc.ca/~jamalim/datasets/flixster.zip  so, we reduce the number of iterations required to completely process large-scale datasets and also reduce the number of computations required for item-item similarity calculation; thereby increasing the speedups achieve by the GPU implementations. The experimental results show that our proposed parallel GPU implementations outperform not only the respective serial CPU implementations but also the na?ve GPU implementation which does not use data compression.

The proposed algorithms scale well with the increasing volume of the user-item data and produce results in a timely manner, making them applicable for large-scale recommendation systems.

