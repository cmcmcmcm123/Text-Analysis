The Application of Association Rules Algorithm on Web Search Engine

Abstract   Aiming at the prevalently concerned mining problem about constructing concept search in current Web search engine area, especially applying the Vector Space Model VSM to web search mining based on the association rules, this paper provides a highly efficient mining algorithm EARS. The EARS algorithm implements the association rules pruning based on VSM via constructing association library and computing similarity. The EARS stores the frequent itemsets via multi-dimensional linked lists and utilizes the recurrence relation among the frequent itemsets in order to effectively obtain the association rules. We verify the extended function of retrieving and querying on the web and the results of these experiments indicate that our method is effective and feasible.

1. Introduction   With the rapid development of Internet, Web has been a major information source. Due to the huge and mass information involved, it becomes more and more difficult that find out useful information fast and accurately. Although the search engine based on key words is widely used, its precision ratio and recall-precision are hot issues of all time. For this reason, the concept-based retrieval [1] is proposed, which is an outcome combining data mining technology with search engine mechanism and also is one of realizing intelligent information retrieval methods.

Concept-based retrieval expand the users? query requests in form of concepts via the transformation of association between concepts, namely the  transformation associated library, and then submits them to retrieval systems, outputting the query results eventually. Since concept-based retrieval understood and dealt with the users? requests from the level of a word?s conceptual meaning, breaking through the defect that keyword matching is restricted to appearance form, it can effectively eliminate the difference between query requests expressed by the users and the query results by the system executing, thus increasing precision ratio and recall-precision of the system [2]. The two key technologies are the transformation of semantic library and query expansion.

The transformation of associated library making use of association rules mining algorithm, automatically mines the association between words form the text to structure, in favor of query expansion and adjustment. Currently, there are two major methods to structure associated library:  using manual method to structure associated library, demanding domain experts to accomplish.

It is a rather time-consuming project and the associated library can not reflect the change between concepts in time.

automatically extracting concepts and words from the text set and then discovering the correlation between the concepts and the words, structuring the associated library finally.

Query expansion is based on the association  library and conducts the correlation query expansion according to correlation and degree of correlation.

Therefore, association rules mining technology has been an important step of realizing intelligence search for query expansion [3, 4]. Based on the associated library, the query requests of the users will be correspondingly expanded, to make retrieval  Copyright ? 2009 by the Institute of Electrical and Electronics Engineers, Inc. All rights reserved.

intelligent and increasing the precision ratio and recall-precision.

Due to the knowledge discovery in mass data, the quality and efficient of mining algorithm are always the bottleneck in the application and extension of association rules. A lot of researchers conduct a great of research about the topic of association rules mining, expecting to improve and optimize the original algorithm for better efficiency of algorithm.

The representative algorithm are Apriori [6] DHP[7] Partition[8] Sampling[9] and so on.

This paper proposes an improved association rules mining technology, which can automatically export the correlation between concept and word from the text. This method applies the vector space model (VSM) to the association rules mining algorithm, in order to discover the correlation between concept and word in the text., according to which structuring the associated library. The associated library can effectively solve the problem of query expansion to achieve the purpose of increasing precision ratio and recall-precision.

2. Web vector space and association rules   The major task of concept searching is building feature extraction and Vector Space Model formed by the term frequency count and applying the association rules and correlation process to establish the feature words association library.

2.1 Vector Space Model VSM   Since the VSM [5] has strong computability and operability, it has been widely applied to many applications in information retrieval areas as text retrieval automatic abstraction keyword automatic extraction text classification and search engine, achieving good effects. In VSM, a document space is regard as a vector space consisting of a orthogonal word vector set, in which each document d represents a standardizing eigenvector:  V(d)=(i1,w1(d); ? ;ij,wj(d); ? ;im,wm(d))  ij is word item, wj(d) is the weight value of ij in d,  which is generally defined as a function of tfj(d), the  frequency of ij ?s occurrences in d, namely  wj(d)= (tfj(d)). tfj(d) represents the frequency of  characteristic item ij ?s occurrences in d. There are  many patterns of , in which the classic and widely  used computable function is TF?IDF. Its form is as  follows:  = tfj (d) log[N/nj]  In which, N is the amount of all the documents and  nj is the amount of documents including word item ij.

2.2 The calculation of similarity degree   We compute the similarity degree based on the cosine distance which can be formally defined as:  (1)  2.3 The transformation of associated library  The construction of the association library is a crucial step in realizing concept searching, aiming at automatically extracting concepts and words from document sets and finding out the correlations among them.

2.3.1 The concept of association rules  In association rules, itemset I={i1,i2, ?, in} represents a set consisting of n different data items.

For a given transaction database D, each transaction T is a subset of itemset I. The implication of association rules is X Y, in which X I,Y I and X Y= , and its support and confidence are respectively defined as:  support(X Y) = P(X Y)   (2) confidence(X Y) = P(Y|X)      (3)  The task of association rules mining is find out all strong association rules, making support larger than the given support?s threshold and confidence larger than the given confidence?s threshold. From (2), we can see that support reflects the frequency of the implication X Y?s occurrences in transaction T and the itemset whose frequency is larger than the given threshold is defined as frequent itemset. The      generation of frequent itemset indicates that there is a certain similarity between corresponding objects. In VSM, a document is regard as an object and a word is regard as an object item. If some object items always in some documents, we think that these document items have a certain similarity. Through (3), we can compute the confidence between document items and then make sue the similarity between document items.

2.3.2 The pruning of association rules  If the association rules mined by current association rules algorithm are directly added into associated library for query expansion, it will be found out that there are a lot of useless rules in the mining results. Once these rules are added into the associated library, some errors will occur in the process of query expansion and it also decrease the precision ratio and recall-precision. In order to make the rules in associated library more meaningful and convenient for query expansion, we make some limits in the process of mining as follows:  (1) Support threshold value limit. Support reflects the itemsets in the database are whether universal laws or not. In order to make sure the frequent itemsets with more universal laws, we demand the frequency of frequent itemsets? occurrences must be larger than 20 in the database with 1000 items, namely the least support threshold value is 0.02. You can certainly increase the support threshold value to make the frequent itemset more meaningful.

(2) Confidence threshold value limit. If the confidence threshold value is set too small, the rules with too small confidence will also be mining out.

And if these rules are added into associated library, it will make the query expansion set too large and decreasing the precision ratio, simultaneously increasing the retrieval cost. In this paper, the least confidence threshold value is set as 0.7.

(3) Constraints limit. In general, the number of left and right in the association rules is not limited. When retrieving, query string is generally 1-2 words, and as in query expansion, only 1-2 will be expanded. So,  we needn?t generate the rules with the number of left and right lager than 2. Adding number constraint condition in mining algorithm not only increases the efficiency of algorithm but also eliminates the invalid rules.

(4) Correlation interest threshold value limit.

Though we can eliminate invalid rules via above limits, we must make out whether the rules with high support and high confidence is really valid or not and reflect the real correlation between words or not. To solve the problem, we introduce the interest threshold of association rules to judge the association rules. For a given transaction set D, the interest of association rule X Y in D is defined as:  (4)  According to (4), if interest is small than 1, it indicates more interest in this rule, namely its practical value lager in use. If interest is equal to 1, it indicates that X and Y are mutually independent and this rule should be eliminated. Through above steps? process, the association rules mined out are basically in accord with the reality and they could be added into the associated library.

2.3.3 The structure of associated library  The associated library includes two structure tables, hierarchy relationship table and correlation relationship table. Hierarchy relationship describes hierarchical relation between words and its basic element is word node. The attribute of word node is described as follows:  struct Concept {  char itemid[]; //hierarchy number which a node in  char itemname[]; //name of a word node  double psweight; //the weight value from father  node to the node itself  double spweight; //the weight value from the node  itself to its father node  struct concept *son; //a point to a array of child  nodes  }      The correlation relationship table describes the correlation between words, specific representation as following:  struct Relationship {  char itemstring1[]; // word string  char itemstring2[]; //word string  double weight[]; //weight value of confidence  }  In the expression, itemstring1 and itemstring2 could be a string consisting of multiple words.

3. The architecture of web search engine based on association rules  The structure of web search engine based on association rules usually includes two parts: query service in client and background processing in server, which are shown as in the figure 1.The query service in client includes three parts consisting of the user?s query request, concept associative spread and pattern match. The background processing includes four parts consisting of document collection, feature extraction, transformation of associated library and adjustment of associated library.

Figure1. The Architecture of Web Search Engine  Based On Concept Retrieval   3.1 The background processing in server   The major process is building document library, VSM [5] and feature word associated library. The document library could be constructed using the information acquisition tool to collect document files including original documents and their URLs. VSM storing each document?s major feature words and word frequency is a word frequency matrix formed by feature word extracting and the word frequency counting for the documents in the document library.

The feature word associated library is build up after the process of association rules mining algorithm, mining the correlation between feature words in VSM.

3.2 The foreground query request in client  When a user submits a query request, the system will find out the corresponding feature words from the feature words associated library and process the concept expansion to form a string of related concepts. They are regarded as query request submitted to the system for pattern matching and the documents meeting conditions will be submitted to the user. If system doesn?t find out the feature words corresponding with the query request, system will re-mining the corresponding feature words from the VSM, do the expansion process and adjust the associated library.

4. The improved mining algorithm EARS  The key points of increasing association mining efficiency are usually including the followings: decreasing the amount of scanning database and reducing the I/O load  generating small candidate frequency set  making rational use of relationship between back and forth items solving the sub-problems in the process of generating association rules optimizing implementation code to reduce the space complexity as far as possible. We provide an improved mining algorithm called EARS to do these.

The main idea is that we use a multi-dimensional list structure to store the Tid set of each frequency set in database and recursion relations between levels of each frequency set and effectively obtain association rules via list operation and set operation at last.

4.1 Correlation theory  For any two itemsets X, Y, if X Y, X is the father itemset of Y and Y is the child itemset of X. If X Y and |X|+1=|Y|, Y is the proper sub-itemset. The proper sub-itemset is a special child itemset. If Y, the proper sub-itemset of X, is a frequency set, Y is the proper sub frequency set of X. Let D be the number of records in database, Tid be the identifier of a record in database; Lk be the set of  frequency set in level k,      usually called large k-itemset, Lk,m be the m-th of the set consisting of large k-itemsets, Lk,m[i] be the i-th item of the m-th element of large k-itemset, |X| be the number of elements that X includes, X.Set be the set of all items composing the itemset X, X.Set[i] be the i-th item of itemset X, in which i 0, X.SupTidSet be the i-th Tid number of itemset X?s support Tid set,

X.sup be the supports of itemset X, namely the number of elements belonging to X.SupSet, X.Tid be the Tid number which records X in database, X.Level be the level number, namely |X|, X.NearOffspring be the proper sub frequency set nodes queue of X.

Theory 1: X is any a k-itemset and Y, Z are two different itemsets. If Y,Z X Y Z there are Y Z=X, X.SupTidSet =Y.SupTidSet  Z.SupTidSet.

Theory 2: If the frequency sets in k-1 level are in ascending order, all the items included by the every frequency set will also be in ascending order. When the frequency itemsets in k level are found, we use dual cycle to matching mine the two itemsets in k-1 level from small to large. The generated items included by the frequency sets sort ascending and then the large itemset Lk in k level will sort ascending.

Theory 3: For nonempty itemset, X and Y, the number of both X.set and Y.set?s items is m, and X Y , X.Set[1]<X.Set[2]<?<X.Set[m] Y.Set[1] <Y.Set[2]<?<Y.Set[m]. If the same item, , is deleted from (or inserted into) X and Y simultaneously, obtaining X? and Y?, the size relationship of X? and Y? is the same as the size relationship of X and Y.

4.2 Data structure  The core data structure of the algorithm is a multi dimensional list. The vertical list stores the frequency set nodes. Each node includes X.Set X.SupTidSet

X.Sup X.Level X.NearOffspring. The horizontal list consists of queues pointed by each frequency set node?s NearOffspring, each node of which store a pointer pointing to a proper sub frequency set node of

X.Set. These pointers re-point to frequency set in the vertical list and the node store three fields: Rules-Route Off-Set and Con-Set. Rules-Route is a path pointer, generating rules. When mining rules, we  depend on breadth-first search to expand the queue to deduce all the rules. Off-Set stores extra items that proper sub frequency set Y is bigger than current frequency set X. Con-Set stores rule?s consequent of association rules, namely the conclusion part. The major structure of the list is shown in Figure 2.

Where, L1,1 L1,2, ? ,L1,m L2,1 L2,2, ? ,L2,n L3,1, ? ,L3, represents respectively frequency set node from each level, composing vertical queue. (X) expresses the pointer pointing to node X.

Figure 2. The Structure of Multi Dimensional List  The feature of data structure is that each frequency set node is adjacent to its proper sub frequency itemset sequence, and through the widely used list operation, breadth-first search, we can conveniently get all the child frequency set of any a frequency set.

As breadth-first search is used, the obtained child frequency set sequence has been arranged as from low to high level, of which we can make use to increase the efficiency of algorithm in the course of algorithm design.

4.3 Algorithm description  Let Tid number of a record in database be unique in the whole database and the records in database sort ascending as Tid. In this condition, the algorithm EARS can be parted into three steps:  Step1: By scanning database once, it generates all the order frequency set L1 and X.SupTidSet and X.Sup are recorded. If we set minsup to 0.5, the process of L1?s generation is shown as table 1 and table 2.

Step 2: By multiple cycles based on L1 , it generates all the large itemset Lk in level k(k 2).

Table 1.  Database Table  Tid Items  001 a c d  002 b c e  003  a b c e  004 b e  Table 2. The List Structure After Big Itemset of Level L1 Is Obtained X X?L1 X.Sup X.SupTidSet  {a} 2 001 003  {b} 3 002 003 004  {c} 3 001 002 003  {e} 3 002 003 004  In this process, it matching mine two large itemsets in k-1 level from small to large via dual cycles. For any two frequency sets Lk-1,m and Lk-1,n(m<n) in level k-1,if the conditions are satisfied that Lk-1,m[1]=Lk-1,n[1] Lk-1,m[2]=Lk-1,n[2],?,Lk-1,m[k-2]=Lk-1,n[k-2] Lk-1,m[k-1]<Lk-1,n[k-1],  X.Set=Lk-1 m.Set?Lk-1 n.Set is respectively a k-itemset.

According to theory 1, there is X.SupTidSet= Lk-1,m.SupTidSet Lk-1,n.SupTidSet.IfX.Sup minsup*D in which X.Sup belongs to k-itemset X, X is frequency set in level k, otherwise, X is a weak itemset.

We can use theory 2 to accelerate dual cycle scanning for this process. For any a frequency in level k, it can be obtained by matching the least two frequency sets in level k-1 consisting of k-1 elements from Lk,x, namely Lk-1,m={Lk-1,1 Lk-1,2 ? Lk-1,k-3 Lk-1,k-2} and Lk-1,n={Lk-1,1 Lk-1,2 ? Lk-1,k-3 Lk-1,k-1}.

When deducing frequency set in level k, if the first k-2 items of Lk-1, m and Lk-1,n are different, the smaller frequency needn?t to match the following other frequency sets, in other words, the inner cycle can be stopped. After deducing a frequency set Lk,x by using Lk-1,m and Lk-1,n(m<n), Lk,x is added into the vertical list and the support set and support number of Lk,x are written into corresponding domain. The pointer node pointing to Lk,x is added into the proper sub nodes sequences pointed by respective NearOffspring of frequency Lk-1,m and Lk-1,n and the item obtained from Lk,x-Lk-1,m is recorded into the off-set domain of proper sub node newly added into Lk-1,m meanwhile, the item obtained from Lk,x-Lk-1,n is recorded into the off-set domain of proper sub node newly added into Lk-1,n. It searches the frequency sets which also regard Lk,x as  its proper sub frequency set in the frequency sets sequence at level k-1 after Lk-1,n. Table 3 describes the storage structure of multiple list generated by example1 through step2.

Table 3. The Storage Structure of Ultiple List Generated By Example  L X. Sup X. SupTidSet X. NearOffspring  {a} 2 001 003 [ ({a c}) ,c]  {b} 3 002 003 004 [ ({b c}) ,c]  [ ({b e}) ,e ]  {c} 3 001 002 003  [ ({a c}) ,a ]  [ ({b c}) ,b]  [ ({c e}) ,e ]  {e} 3 002 003 004 [ ({b e}) ,b]  [ ({c e}) ,c]  {a c} 2 001 003  {b c} 2 002 003 [ ({b c e}) ,e ]  {b e} 3 002 003 004 [ ({b c e}) ,c]  {c e} 2 002 003 [ ({b c e}) ,b]  {b c e} 2 002 003  Step 3: By using the information of current data structure, it gets the association rules meeting the minconf. In other words, it regards any a frequency set in vertical list as the antecedent of association rules, and then mines out the conclusion meeting minconf, namely the rule?s consequent. When frequency set X is a rule?s antecedent, first, the next pointer of each node in NearOffspring is copied into Rules-Route pointer of current node and the value of Off-Set is copied into Con-Set, forming an initial Rules-Route which generates rules. Through scanning this list to get a node, the Con-Set value Y of this node is the rule?s consequent and the support of corresponding frequency set Z can be obtained from the pointer of this node, in which according to the generation process, we know that Z=X Y. If confidence (X Y)= (Z.Sup/X.Sup) minconf, it is a strong rule meeting the requirement, otherwise, it?s a weak rule. If confidence (X Y) is a weak rule, apparently the child nodes of Z don?t combine with X to generate a strong rule. Otherwise, the NearOffspring list of Z is scanned. If it is not null, each node in the list is fetched one by one and Y is copied into the Con-Set domain of the node in the list.

Simultaneously, the item of Off-Set domain is inserted into Con-Set, making the item of Con-Set sort ascending to form rule?s consequent M.

Comparing M with rule?s consequent N of the last node in Rules-Route, if |N|<|M| or |N|=|M| and N<M(Theory 3), this node will be linked to the end of Rules-Route. Scan the next node of Rules-Route and repeat the above process until to the last node. When example 1 uses frequency set {c} as rule?s antecedent, Table 4 lists the Rules-Route queue and its corresponding association rules generated by the derivations. As long as the minconf is given, it could judge which are the strong rules meeting the requirement.

Table 4. The Rules-Route Queue And Its Association Rules  The node point to freq-set  Off- Set  Con- Set  Sup.of freq-set  Asso.

Rules Conf.

({a c}) A a 2 c a 0. 667  ({b c}) B b 2 c b 0. 667  ({c e}) E e 2 c e 0. 667  ({b c e}) E b e 2 c b e 0. 667   5. The results and analysis of experiment   To verify the efficiency of EARS?s expansion function in searching and retrieval, we test on a computer with CPU 2.4G, Memory 2GB and Windows XP operating system. The original documents are from the TREC-7 document set downloaded from the Internet, including 23251 documents. Choosing ten thousand words as feature word, the documents are carried on the feature extraction and word frequency counting, forming word frequency matrix stored in the Oracle9i database. When minsup=0.04, minconf=0.4, the number of records in the database increases from 2,000 to 10,000 and the execute efficiency of EARS and Apriori is shown as Figure 3.From Figure 3, we can see that as records increasing, both of the runtime are prone to increase, while, from the perspective of data, the increment speed of EARS? runtime is slower than Apriori?s. In addition, the runtime of EARS is less than Apriori?s at any time.

Theoretically speaking,  EARS scans the database once only when discovering the large 1-itemset in the whole process of mining, which solves the problem about the heavy load of I/O.

With the help of Tid sets recorded in the memory, the algorithm doesn?t generate candidate large itemset Ck.

The advantage of using the data structure, multiple dimensional pointers, is that when generating association rules, the expenditure of matching time is decreased as much as possible by making the best use of previous work.

The design of the algorithm makes use of the ordered data structure and set operations.

Therefore, the execute efficiency of EARS is improved effectively.

6. Conclusion  In the current developing process of network search engine technology, how to using web data mining technology is a key point. In the research process of the web search engine technology, this paper analyses in depth on the query expansion technology based on the association rules. We propose an association rules mining algorithm EARS based on multiple dimensional list. The results of experiment indicate that the efficiency of EARS is higher than Apriori?s and the runtime of EARS varies within a small range, namely having a good flexibility. It thus is concluded that EARS is an effective and scalable association rules algorithm, which can be widely used to query expansion in web information retrieval.

7. Reference  [1] Fetzer C,Hagstedt, K,Felber P. ?Automatic Deteciton and Masking of Non-Atomic Exception Handling? Networks, (DSN2003), 110-116.

[2] Yen S J, Lee Y S. ?Mining Interesting Association Rules and Sequential Patterns?. International Journal of Fuzzy Systems, 2004-6 (4), 78-84.

[3] Alasffar A H, Deogun J S. ?Concept-based Retrieval with Minimal Term Sets?. Foundations of Intelligent Systems: 11th Int?l Symposium, Springer, Poland, 2004: 114- 122.

[4] Qiu Yonggang,Frei H P. Concept Based Query Expansion[C].SIGIR?03,2003:160-169.

[5] Saltom G, Wong A, Yang C S. ?A Vector Space Model for Automation Indexing?. Communications of the ACM, 2005, 18(5): 613-620.

[6] Agrawal R, Srikant R. ?Fast Algorithm for Mining Association Rules in Large Databases.? Proceedings of the Santiago , Chile , 2004  [7] Park J S. ?Using A Hash-Based Method with Transaction Trimming forMining Association Rules.? IEEE Transactions on Knowledge and Data Engineering, 2007.

[8] Savasere A, Omiecinski E, Navathe S. ?An Efficient Algorithm for Mining Association Rules in Large Databases.? Proceedings of the 21st International Conference on Very large Database, Switzerland, 2002.

