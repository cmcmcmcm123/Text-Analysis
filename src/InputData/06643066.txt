Distributed File Management System Based on SSH2 and HDFS

Abstract?As time goes on, the current running  information system produces massive data. Obviously the previous hardware could not fit for managing the much bigger data any more. In order to upgrade the system, there are three main problems need to consider: enough storage space, high reliability, high performance and relatively low cost. As an open-source J2EE framework, SSH2 is widely used by developers to develop all kinds of management information systems. Hadoop, another open- source project running in an integrated cluster, is good at dealing with the big data. In order to solve the three problems, this paper puts forward the way to integrate the SSH2 and Hadoop.

Keywords: HDFS; SSH2; distribution; big data.



I.  INTRODUCTION With the rapid development of information  technology, the scale of the accumulated data is becoming huger. Age of Big Data, as a new buzzword word, is coming. It is a great challenge to deal with the big data that we have to consider at least four main problems:  1.High Storage Space: There is enough storage space to store the big data. Even it could grow with the scale of the data.

2.High Reliability: The stored data should be safe.

3.High Performance: Much more computing power  is required to process the big data.

4.Economic: Expense is always very important.

Though some high-performance mainframes can be  qualified for the task, the high cost is difficult to be accepted by most enterprises or institutions. Clearly, what we need is to process the big data effectively but with relatively low cost.

The research on the storage system based on the distributed network has become a hot topic. Now, there are many kinds of distributed network storage system, such as OceanStore, CFS, PAST, Tangler, Pangaea, Granary and so on [1]. Generally, distributed file system is multi-user, and to a certain degree it is hard to manage.

This paper will introduce how to design and develop a distributed file management information system to provide a easy way to manage the distributed  files. On the other hand, this paper shows a way to change a single computer system into a distributed one quickly.

The reminder of this paper is arranged as follows: The core technology is introduced in Section 2. In Section 3, we describe the details of system construction and conclude the paper in Section 4.



II.  CORE TECHNOLOGY  A. HDFS HDFS[2-5](Hadoop Distributed File System)is a  subproject of Hadoop. It is a distributed file system running on commodity hardware (cheap, easy to get).

HDFS lives in a cluster containing many nodes and integrates all nodes? storage resources. So that it has large storage capacity. Generally, in a huge cluster, the rate of node failure is very high. It is thus clear that HDFS must have high tolerance of faults [6]. Compared with the traditional way of file storage, HDFS?s outstanding character lies in that it can save and back up a file on several nodes by splitting it into blocks [7].

Figure 1.  Hadoop Distribution File System Model  HDFS uses the master-slave structure as in Figure1.

There are two kinds of nodes in a HDFS cluster: NameNode and DataNode. NameNode is responsible for the management and maintenance of the cluster?s internal file directory information such as the information of the location of the file blocks. DataNode which is managed by the NameNode is responsible for the storage of the file blocks, and sends back information list of the stored file blocks regularly. But   DOI 10.1109/ICCIS.2013.152    DOI 10.1109/ICCIS.2013.152    DOI 10.1109/ICCIS.2013.152    DOI 10.1109/ICCIS.2013.152    DOI 10.1109/ICCIS.2013.152    DOI 10.1109/ICCIS.2013.152    DOI 10.1109/ICCIS.2013.152    DOI 10.1109/ICCIS.2013.152    DOI 10.1109/ICCIS.2013.152    DOI 10.1109/ICCIS.2013.152         HDFS has its disadvantage. Hadoop is more suitable for big data set consisted by large files instead of the small files.

The client gets the file from the cluster as following: First it should send a request to the NameNode, and then the NameNode will return an address list of the file blocks which belong to the required file. Finally the client reads the file blocks from the ?nearest? (Ordered by transmission time) DataNodes. Similarily, when the client wants to write a file into the cluster. First it needs to send a request to the NameNode, then the NameNode will check user?s authority and the existence of the file. And then the NameNode will add a new record for the file. Finally client can write file blocks into the appointed DataNodes.

B. SSH2 SSH2[6-7], a framework used widely in the  development of Web system, consists of three parts: Struts2, Spring and Hibernate  Struts2 is the next-generation of Struts1, and it is a brand new architecture combining Struts1 with WebWork technology. Struts2 provides a series of actions to execute and response the users? requests.

Spring is famous for its great concept: IOC(Inversion Of Control). The relationships among the entities are clearly defined in a series of configuration files. As a result, every entity or component could be seen as liberated that they are low coupled. Spring is responsible for the business layer and ensures the accuracy of business operation.

Figure 2.  System Architecture  Hibernate is responsible for interacting with the database and almost all kinds of popular databases are supported. The core of Hibernate is Object/Relation Mapping (ORM) which creates ?a virtual object database? that can be used within the programming language. Hibernate separates the data operation from business logic effectively during the system development process. It makes developers focus on the  business logic and reduces the difficulty and cost for system migration.



III.  SYSTEM CONSTRUCTION As showed in Figure 2, system implementation is  similar to a common Web system based on SSH2 architecture. The most important difference is that this system relies on a storage cluster based on HDFS. From the user?s aspect, storage cluster is transparent, and they don?t know and needn?t care where their files are stored.

For the developers, the files uploaded by the users won?t be stored on one certain machine directly. Instead, they may have been split into blocks and distributed on several machines, but the developers needn?t care how to allocate every block, it will be finished by HDFS.In addition, the information associated with the files, such as the information of the users, is still stored in the relational database. When the user wants to download or upload his or her own files, system will first check the user?s ID according to the information from the database, then the user will be authorized. Finally files could be uploaded or downloaded through the interaction between the user and the cluster.

Figure 3.  User information management  The system is designed with two major functions as following.

a) User information management: As the Figure 3  shows, the system administrator has the right to add a new user or remove one. Also a certain user?s detail information can be searched though some key fields like user name.

b) File information management: As the Figure 4, the user could upload or download the file from the system. Of course, if necessary, the chosen files can be deleted.

Figure 4.  File information management  During the system development process, we set Hadoop pseudo distribution pattern to imitate Hadoop cluster through one machine, another machine running         MySql works as a database server. In this way, it becomes much easier to develop and debug .

The most important function of the system is to make sure that user could upload files to the cluster and download them. Before uploading the files, system will calculate the files? MD5 code to avoid the duplicate files .The key code is shown as the following:  Public String getFileMD5Str(File file) { FileInputStream in = new FileInputStream(file); FileChannel ch = in.getChannel(); MappedByteBuffer Bb = ch.map(FileChannel.MapMode.READ_ONLY, 0, file.length()); messagedigest.update(byteBuffer); return bufferToHex(messagedigest.digest()); }   Public void Upload() {  ?? FileSystem fs = FileSystem.get(URI.create(dest), conf); OutputStream out = fs.create(new Path(dest), new  Progressable() {  publicvoid progress() { System.out.println("finish uploading "); } });  ?? }   Public void DownLoad() {  ?? FileSystem fs = FileSystem.get(URI.create(dest), conf); FSDataInputStream  hdfsIS = fs.open(new Path(dest)); OutputStream out = new FileOutputStream(mySrc); IOUtils.copyBytes(hdfsIS, out, 4096,true);  ?? }

IV.  CONCLUSIONS This paper puts forward the design and shows some  construction details of a web system to interact with users, and manages the distributed files based on SSH2 and HDFS. As a result, to a certain degree, we could solve the four problems we just mentioned in the first section. With the help of Hadoop, flexible storage space, high reliability and performance are all available. SSH2 provides a stable and visual plat for the users who could share or back up their own files through the Web browser. Most important, both SSH2 and Hadoop are open-source and for free.

