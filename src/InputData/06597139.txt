A Throughput Driven Task Scheduler for Improving MapReduce Performance in Job-intensive Environments

Abstract?MapReduce has been proven to be a highly desirable platform for scalable parallel data analysis. The task scheduling in MapReduce is very crucial for the job execution and has a marked impact on the system performance. To the best of our knowledge, the previous scheduling algorithms rarely consider the job-intensive environments and are not able to provide high system throughput. Hence this paper proposes a novel technique for job-intensive scheduling to improve the system throughput. Firstly, by making an in-depth analysis of job-intensive environments, we sum up 4 major factors which affect the system throughput. Secondly, based on the factors, an efficient technique, called throughput driven task scheduler is proposed, in which, we adopt a series of effective measures to improve the throughput of a MapReduce cluster system. Finally, plenty of simulation experiments are made and the experimental results show that the scheduler can provide higher throughput than the previous systems and is able to meet the requirements of practical job-intensive applications.

Keywords-MapReduce; scheduling; throughput;

I. INTRODUCTION  Nowadays, daily internet access, business computing and scientific research etc. generate a mass of data increasingly.

The advanced analysis [1], [2] on Big-Data has become a very hot topic in many fields. MapReduce [3], a distributed data processing framework, has been proven to be a powerful technique to process Big-Data analysis. Specifically, the MapReduce system uses a master node to manage the cluster and some slave nodes to provide computing capacity. While the system accepts a job, the task scheduler on the master splits the job into some tasks and assigns them to slaves automatically. Generally, users need only provide the simple map and reduce functions to process terabytes of data.

In the parallel data analysis, it is indubitably that task assignment (or task scheduling) is crucial for the system performance. The unbefitting task scheduler usually causes a lot of network transmission, and reduces the system processing capacity. For the MapReduce platform, there have been plenty of scheduling algorithms, which could provide suitable approaches of task assignment in many applications.

However, none of the existing schedulers have considered the job-intensive environment which means in a period of time, the system receives a batch of jobs, and all the jobs must be completed within the stipulated time. Actually,  the job-intensive environment is quite familiar in many IT companies, for example, search engines (e.g. Google) would usually choose a period of midnight (e.g. 3:00 am- 5:00 am) to process some analysis on users? web logs, and shopping sites (e.g. Amazon) prefer tidying the transaction information while network access is infrequent (also in midnight). Then the submitted jobs must be accomplished within the stipulated time, otherwise the daily work will be influenced. If the workload gets heavy or the scale of cluster is limited, this objective may be difficult to achieve.

master  task slot split of f1 split of f2  s1  s2 s3  s4  (a) Cluster Status  assign task  request task  data transfer  s2 s3 s4s1  master  (b) Task Assignment for Fairness  Figure 1. Example of Scheduling  Whereas the existing schedulers have not considered the job-intensive environment, and cannot guarantee the system throughput capacity. Figure 1 illustrates a simple example to explain the problem. As Figure 1(a) shows, there is a distributed system (e.g. Hadoop [4]) which has 4 slave nodes and each slave provides 1 task slot for processing, and the system adopts the popular Fair task scheduler [5], [6]. In the underlying file system there are 2 files f1, f2, which are split into several splits. Then the system receives 2 jobs j1 (related file is f1), j2 (related file is f2), and both the weights of j1, j2 are set to 1, which means either of j1, j2 should obtain 2 slots for processing. For the situation described in Figure 1(b), the scheduler would allocate a split of f2 to the idle slot on s3 to ensure the fair principle and lead to more network overhead, obviously it is not benefi- cial for the system throughput. Therefore, to provide high system throughput, we propose a novel technique, called Throughput Driven task (TD) scheduler, for the job-intensive MapReduce environment. The scheduler mostly focuses on the map phase, which dominates the computational cost of the most MapReduce applications. In particular, we make  2013 IEEE International Congress on Big Data  DOI 10.1109/BigData.Congress.2013.36     the following contributions.

1) We describe the scheduling model for job-intensive MapReduce environment, and summarize several fac- tors which can impact the system throughput.

2) We propose a novel scheduling approach, TD sched- uler, which can guarantee high ratio of local task assignments, avoid hotspots and take full advantage of the system resources effectively.

3) We evaluate TD scheduler through experiments with real data set. The improvement of system throughput is testified by the experimental results.

The rest of this paper is organized as follows. We state the overview of the MapReduce framework and several previous approaches of task scheduling as background knowledge in Section II. We define the problem model and state the design requirements in Section III. We describe the TD scheduler in detail in Section IV. We evaluate the performance of TD scheduler in Section V. Finally, the conclusion of this paper is stated in Section VI.



II. BACKGROUND KNOWLEDGE  In this section, we overview the MapReduce framework, and present the related work about pre-existing scheduling strategies in the MapReduce environment.

A. MapReduce Overview  As a successful open-source implement of MapReduce, the Hadoop system [4], has arisen for several years. It contains a single master node and several slave nodes.

The master takes charge of the load balancing and task scheduling, the slaves are response for the actual data storage and each slave has a certain number of task slots for data processing. Functionally, Hadoop is constituted by 2 major subsystems. a) The underlying file system HDFS [7] stores the data in the form of file for further analysis, and due to the huge size, files would be split into a number of isometrical splits for distributed storage, the master records the location of each split in block map. b) The data processing system is used to process the MapRedcue jobs. In general, a MapReduce job uses a specific file stored on HDFS as its input. While a job is submitted, the scheduler will split the job into a number of map and reduce tasks and each map task processes a specific split of the input file. The tasks would be assigned to the task slots on slaves to be executed in parallel.

As Figure 2 shows, while processing the MapReduce jobs, each slave periodically sends a heartbeat to the master to report the statuses of task slots on it. If a task slot is idle, the scheduler would assign a suitable task to the slot.

B. Related Work  As the kernel of parallel data analysis, the task scheduling technique has been studied for years. The default scheduling approach of Hadoop, Job Queue Task scheduler can afford  task slots splits  block map  task scheduler  master  slaves  he ar  tb ea  t respon setask slots  splits  task slots  splits  Figure 2. Hadoop Framework  a fundamental and effective way to assign tasks. Actually, it is a First In First Out (FIFO) strategy, which means all the jobs are sorted into a queue according to the submission time, and the later jobs will not be scheduled until the former jobs are finished.

The Capacity scheduler [8], which is designed for multi users to share one underlying cluster, can allocate resources to different users according to the configuration and avoid individual user monopolizes the system. It supports multiple job queues and each queue has a certain proportion of resources. In each queue, jobs are scheduled with the FIFO strategy, and the unoccupied resources would be shared with the other queues. Fair scheduler [5], [6] is also motivated by the multi-user situation. It has several job pools constituted by some jobs and each job is assigned a specific weight.

The difference is that in Fair scheduler each job expects to obtain the computing resources according to the weight, and the idle resources would be assigned to the job with the most vacancies. Thomas Sandholm et al. [9] presented a Scheduling strategy which can adjust the allocated capacity of users. The scheduler allows uses to apply for more computing resources if their submitted jobs are important.

Besides, there are several further studies on task schedul- ing. Jorda Polo et al. [10] presented a method applying to the deadline scheduling. Each job is submitted with a specific deadline, according to the accomplished tasks, the scheduler calculates the end time of the job. If there is a job which cannot finish before the deadline with its allocated resources, the idle resources will be allotted to it. Kamal Kc et al. [11] presented a job execution cost model to predict the finish time of a new job. If the expectant time can not meet the deadline, the job will be rejected.

In practice, the performance of each slave node is not identical, some older machines may be much slower than the others. Therefore, Matei Zaharia et al. [12] presented the LATE Scheduler for the heterogeneous environment.

According to the time cost per task, the scheduler estimates the processing ability of each slave and the finish time of the tasks in processing, the slower task will be restarted on the slave with good processing capacity. YongChul Kwon et al.

[13] discussed the skew problem in MapReduce applications.

The technique detects the skew through calculating the finish time of tasks, the rest of the slower tasks will be split and     assigned to the other slaves.

In brief, the previous approaches rarely pay attention to  the job-intensive environment. In contrast, our technique adequately considers the pressure of batch jobs and can provide higher throughput.



III. PRELIMINARIES  Based on the MapReduce scheduling framework, we will define the problem model in job-intensive environment and illustrate the design requirements for our TD scheduler.

A. Problem Definition  In the job-intensive environment, the job queue Q = {j1, j2, j3 . . .} is a set of MapReduce jobs which are sorted in ascending order of submission time. Each job in Q is related to a specific input file stored on HDFS. For the most general situation of MapReduce, we can make the following 2 assumptions.

1) Because in a single job all the tasks in the map phase execute the same computing, the execution time of the map tasks in a single job is almost the same if we do not consider the nonlocal processing.

2) For each job in Q, the map tasks would only read the splits of the input file from HDFS and process their own computings. Hence we assume that even if job j1 uses the same input file that the preceding job j2 uses, j1 would not get speedup by j2.

For each job j in Q, we denote the size of j as |j|, which is equal to the split number of the input file of j. We also denote the number of task slots which are occupied by j as j.occupied, the number of remaining untreated tasks of j as j.remain. Each job j is attached with a parameter j.demand, which means the slot number that j expects to occupy. It can reflect both the size and expected completion time of the related job. Based on j.demand, we introduce a new conception relaxed restriction, which means in job- intensive environment, the scheduler can prior consider the system throughput and does not guarantee each scheduled job j in Q can occupy j.demand slots, but according to the uses? requirements, the scheduler guarantees j.occupied will not much smaller than j.demand so that j will not be finished much later than expected.

Given a MapReduce cluster with T task slots and a job queue Q with a certain number of jobs, throughput is inverse proportional to the completion time of Q. Hence the goal of TD scheduler is to finish all the jobs in Q in the shortest possible time under the premise that all the jobs meet relaxed restriction.

B. Design Requirements  In the job-intensive environment, we summarize the fol- lowing 4 major factors which can impact the system through- put. We design the TD scheduler to satisfy the requirements of the 4 factors.

1. High ratio of the local processing. Nonlocal tasks would lead more network transmission and more execution delay. In the distributed environment, it is necessary to maintain high ratio of the local processing.

2. Choosing a befitting nonlocal task. In practice, for the high parallelism, nonlocal execution cannot be avoided, and choosing a better nonlocal task would be beneficial for the throughput. For instance, there are jobs j1 (input file is f1) and j2 (input file is f2), the split sizes of both f1 and f2 are 128MB. The map phase of j1 processes some computing-intensive tasks (e.g. skyline query), the average execution time per task is 40s. In contrast, j2 processes IO-intensive tasks (e.g. word count), and the time is 5s.

Therefore, ideally if we assign a nonlocal task of j1, there would be few performance reduction if the data transmission speed over the network can reach to 128MB/40s= 3.2MB/s, and 25.6MB/s for nonlocal task of j2. However we cannot insure the data transmission speed can meet the requirement because the reduce phase or the other applications could occupy the network resources. As a result, it is better to non-locally assign computing-intensive tasks for throughput.

3. Avoiding hotspots. While processing in parallel, data stored on each node may be unbalanced in a certain period of time, there will be some nodes from which many nonlocal tasks read data, and we call the nodes as hotspots. the hotspot can distinctly impact system thoughput because a number of nodes read data from it so that the transmission speeds become lower, and the performance of hotspot itself decreases.

4. Full use of system resources. Some task schedulers [14], [15] allow a small job to occupy the entire cluster, and shut down the idle nodes for energy saving. However, allowing nodes to be idle is undoubtedly inapposite for system throughput. Hence it is fundamental to make full use of system resources.



IV. TD SCHEDULER  To make TD scheduler more practical, we adopt the popular multi-job co-scheduling model which has been used for many scheduling strategies (e.g. Fair scheduler), instead of scheduling the jobs one by one. We denote the set of the co-scheduled jobs as Ssch. Then in order to facilitate the description, in Section IV-A, we introduce the scheduling approach in the simple case that ssch is stable (correspond- ing to Factors 1-3 in Section III-B), which means each job in Ssch has abundant tasks so that no job ends and no new job comes in. The update of Ssch will be described in Section IV-B (corresponding to Factor 4 in Section III-B). Finally, we describe the process comprehensively in Section IV-C.

A. Task Assignment for Stable Co-scheduled Jobs  For the MapReduce cluster with T task slots and a job queue Q = {j1, j2, j3 . . .}, it is very straightforward that     the first n jobs in Q can join Ssch (or be co-scheduled) if ?n  i=1 j.demand ? T . We denote ?  j?Ssch j.demand as  D. Then, if a slot s on node n becomes idle and requires task, what the scheduler actually do is a) to select a suitable job j from Ssch and then b) to select a task of j on the suitable node to assign. The detailed methods are described as follows.

1) Select the Suitable Job: For each job j in Ssch, we denote the average execution  time per map task as function amt(j), which can be given a default value according to the job type in the beginning, and estimated through the average completion time per map task while j is in processing. Therefore, we sort all the jobs in Ssch in ascending order of amt(j), j ? Ssch to generate a queue S?sch. Obviously in S  ? sch, the former jobs are IO-  intensive and the latter ones are computing-intensive. Hence the basic method for job selection is  1) Considering Factor 1 in Section III-B, while a task slot is idle, if there is at least one task belonging to the jobs of S?sch on node n, we make a local assignment.

Otherwise, a nonlocal assignment is unavoidable.

2) Based to Factor 2 in Section III-B, for the local assign- ment in S?sch, the former jobs which have untreated local tasks will be prior selected. For nonlocal assign- ment, the latter jobs in S?sch will be prior selected.

Figure 3 shows an example of basic method. As Figure 3(a) shows, the first 4 jobs in Q can be co-scheduled and the corresponding S?sch = {j2, j4, j3, j1} can be calculated (Line 4). The job selection is showed in Figure 3(b), where a slot on node n1 and a slot on n3 are idle. There are tasks of j1 and j2 on n3, hence the former job j2 in S?sch will be selected for local assignment. In contrast, there is no task belonging to S?sch on node n1, so the later job j1 in S?sch will be selected for nonlocal assignment. However if we only consider the system throughput, in the early stages of processing, most of the nodes could store a few tasks of the former jobs (e.g. j2, j4) in S?sch, then the former IO- intensive jobs may occupy most of the slots. Whereas the later computing-intensive jobs can only get a small number of slots for processing (Line 5 in Figure 3(a)), and the completion time is not guaranteed, which is against to the relaxed restriction. To avoid the problem, we improve the basic method by using the threshold technique.

For the queue S?sch, lower bound L ? (0, 1) means that for each job j in S?sch, the scheduler ensures j.occupied ? L ? j.demand, so that all the jobs could be accomplished in satisfactory time.

If only using the lower bound, the scheduling strategy is still not perfect enough because there may be sticky slots, which means some slots may forcibly execute tasks of the same job for a period of time. For example as Figure 4 shows, the IO-intensive jobs occupy quite a lot of slots, whereas the computing-intensive job j only occupies j.demand?L = 70 slots for guarantee. When slot s finishes  Q j1 j2 j3 j4 j5 ...

j.demand 200 300 100 350 350 ...

amt(j) 23 4 10 5 - ...

sequence number in S?sch 4 1 3 2 - ...

possible occupied slots 50 480 70 400 - ...

(a) Jobs in Q (T = 1000)  m  n1 idle  n2 busy  n3 idle  . . .

14 tasks of j1 17 tasks of j2  ...

27 tasks of j5 ...

(b) Job Selection  Figure 3. Example of Basic Method  a task of j and requests task for processing, a task of j will have to be assigned to s because j.occupied = 69 < 70, even if there is no local task of j. The sticky slots are unsuitable for parallel scheduling and can increase nonlocal assignment potentially. Hence, we introduce upper bound to reduce the sticky slots.

master done  task of j demand=100 L=0.7occupied=69  slot s  Figure 4. Sticky Slot  For the queue S?sch, upper bound H (H > 1) means that for each job j in S?sch, the scheduler ensures j.occupied ? H ? j.demand. Through limiting the maximum number of slots which IO-intensive jobs can occupy, the upper bound enables the computing-intensive jobs occupy more slots and reduces the probability that sticky slots emerge. Note that the upper bound should not be set too high otherwise it will be meaningless. Specifically, ?j ? S?sch, T ? j.demand?H > (D ? j.demand) ? L, which means if a job j occupies j.demand ? H slots, the other jobs can obtain more than (D ? j.demand)? L slots. In other words,  H < (T ? L?D)/dmax + L (1)  where dmax = max{j.demand|j ? S?sch}. Given a user defined L, the scheduler can compute H according to S?sch.

As Figure 3(a) shows, if L = 0.7, H < (1000 ? 0.7 ? 950)/350+ 0.7 = 1.66.

Consequently by utilizing the combined action of lower and upper bounds, the basic scheduling can select suitable job for high system throughput. Then we describe the method for task selection.

2) Select the Task on Suitable Node: In the previous step, we have get the suitable job j and  determined the assignment is local or not. The local assign- ment is simple so we would not give unnecessary details.

Whereas the nonlocal assignment could face the hotspot     problem. Hence we will mainly illustrate the method for the nonlocal task selection in the following (corresponding to Factor 3 in Section III-B).

In general, a hotspot is the node which occurs data transmission with a number of other nodes simultaneously, and it will distinctly affect the system performance. To further analyze the problem, we firstly divide the hotspots into two categories.

The node n is an actual hotspot if the total number of the nodes which occur data transmission with n is larger than the threshold ?. In other word, given the network IO bandwidth ?, if there is a nonlocal task which reads data from n, the performance will not be affected if the data transmission speed can reach to ?/?. Besides, node n is a potential hotspot if the remaining untreated workload of n is larger than the others, in the foreseeable future, a potential hotspot may become an actual hotspot.

To avoid actual hotspot, a data structure hot queue is introduced. If there is a new nonlocal task assignment that a slot requires data from node n at time t, the hot queue appends a tuple (n, t) to the tail. We denote ? as the size of each split of input file, and consider the network I/O speed at ?/?. Then for current time tcur and the tuple (n, t), the tuple is overdue if t+ ?? ?/? ? tcur. Periodically, the hot queue inspects the tuples from the head and eliminate the overdue ones. In addition, there is a corresponding statistical table. For each node n appears in the hot queue, the table records the total number countn of the tuples which contain n. if countn = ?, we forbid more connections to n.

To avoid potential hotspot, We firstly calculate the remaining untreated workload of node n, Rn =?  j?S? sch  amt(j)? rsjn, where rs j n is the number of the  remaining untreated tasks of job j on node n. Then the potential hotspot with the largest remaining workload has the highest probability to become an actual hotspot, we tend to select it and reduce its remaining workload.

In summarize, while the scheduler has selected j to make a nonlocal assignment for the idle slot s, we firstly get the node set Nj which still have untreated tasks of j, and eliminate the nodes which already have ? connections according to the statistical table. Finally, we choose the node nj which has the most remaining workload in Nj and assign a task of j on nj to s.

B. Update of Co-scheduled Jobs  As the processing continues, some jobs in Ssch will be finished and the following ones join in. When Ssch updates, some idle nodes may obtain no tasks for processing, which is not conducive to system throughput. Hence in this section, we introduce the strategy for full utilization of system resources (corresponding to Factor 4 in Section III-B). Firstly the scene which can produce idle nodes will be described through an example.

As Figure 5 shows, job j1, j2, j3, j4 are co-scheduled.

However in last phase of j1, the remaining untreated tasks j1.remain = 0 and 150 tasks of j1 are stil- l in processing, so j1 can only occupy 150 slots al- though it demands 400 slots. For the other jobs, maximum (j2.demand+ j3.demand+ j4.demand)?H = 780 slots can be occupied. Hence 70 slots would acquire no tasks.

Figure 5. Example of Idle Nodes  To solve the idle-resource problem, we introduce a con- ception job status for the jobs in processing. Specifically, the job js is senile if it has been scheduled for some time and will be finished soon, js.remain = 0 and js.occupied ?= 0. We denote the jobs in status senile as set Ssen, and Osen =  ? js?Ssen  js.occupied. The job ja is adult if it is scheduled steadily and ja.remain > 0 & ja.demand ? L ? ja.occupied ? ja.demand ? H . We denote the jobs in status adult as set Sadu. The job jt is teenaged if it has just been allowed to become adult and jt.occupied < jt.demand? L, then the idle nodes will be prior assigned to jt to ensure it will become adult in a short time. We denote the jobs in status teenaged as set Stee, and redefine D =  ? j?Sadu?Stee  j.demand (mentioned in Section IV-A). The job ji is infantile if it is scheduled only for avoiding idle resources, it is not restricted by L and has lower priority than the jobs in Sadu. We denote the jobs in status infantile as set Sinf , and note that the size of Sinf is no more than 1.

waiting processing finished  infantile teenaged adult senile  PL  1 PL  3 PL  2  priority level   3>2>1  Figure 6. Temporal Relations between Statuses  In addition, job not in processing is either in status waiting (the set is Swait) or finished (the set is Sfin). The 6 statuses have strict temporal relations. As Figure 6 shows, each submitted job is in one of the 6 statuses. The teenaged jobs have the highest priority to be selected and the priority of infantile jobs is the lowest. The status can be transformed only in the order from left to right. Then the first job jw in Swait can be transformed to status infantile if Sinf = ? and  Osen +D < T (2)     The infantile job ji can disaffiliate from Sinf if  Osen +D + ji.demand ? T (3)  Then ji will become a teenaged job if jt.occupied < jt.demand? L, else it can become an adult job directly.

With the formulas above, we ensure there are no idle resources in the data processing. Thus far, our proposed TD Scheduler has been introduced. In the next section we will describe the process of TD in more detail.

Algorithm 1: TD Scheduler input : an empty slot s on node n; the last task of s belongs to job j output: a new task assigned to s  j.occupied ??;1 if j ? Sadu ? Stee && j.occupied < j.demand ? L then2  jass ? j;3 AssignTask SpecificJob(jass, n, s);4  else5 if j ? Ssen then6  if j.occupied == 0 then7 add j to Sfin;8  if Sinf ?= ? then9 jinf ? get the infantile job from Sinf ;10 if Osen + D + jinf .demand ? T then11  remove Jinf from Sinf ;12 if Jinf .occupied < jinf .demand? L then13  add jinf to Stee ;14  else15 add jinf to Sadu ;16  else if Osen + D < T then17 add the first job of Swait to Sinf ;18  if Stee ?= ? then19 jass ? get the first job in Stee ;20 AssignTask SpecificJob(jass, n, s);21  else22 Jass ? AssignTask(n, s) ;23  jass.occupied + +, jass.remain?? ;24 if jass ? Stee && jass.occupied ? jass.demand? L then25  remove jass from Stee to Sadu ;26  if jass.remain == 0 then27 add jass to Ssen;28  end;29  C. Algorithm Description  In practice, the primary function of TD scheduler is assigning a untreated task to a task slot while the slot becomes idle until all the submitted jobs are finished. We firstly introduce the frame of TD scheduler in Algorithm 1.

While a slot on node n completes a task of job j and the scheduler receives the request, j.occupied ? 1 (line 1). If j ? Sadu?Stee and j.occupied < j.demand? L, j becomes the selected job jass, and a task of j will be assigned to n (lines 2-4) (The function AssignTask SpecificJob(jass, n, s) is described in Algorithm 3). Else we will update the job status if j is senile (lines 6-18). Specifically, j becomes finished if j.occupied == 0 (lines 7, 8). If Sinf ?= ?, we remove the infantile job from Sinf if it conforms to Formula 3 (lines 10-12) and add it to Stee or Sadu according to the  slot number it occupies (lines 13-16), if Sinf == ?, we consider whether the first job in Swait can become infantile according to Formula 2 (lines 17, 18). Then we assign a task to s (lines 19-23). If Stee ?= ?, we assign a task of the first job in Stee to s (lines 19-21). Else we choose a task of a suitable job to assign (lines 22, 23) (Function AssignTask(n, s) is described in Algorithm 2). Finally we update the variables of j (line 24). The selected job jass can be added into Sadu or Ssen due to the related restrictions (lines 25-28). Then the algorithm ends.

Algorithm 2: AssignTask(node n, slot s) input : node n, slot s output: the job to which the assigned task belongs  sort Sadu in ascending order of amt() ;1 for each job j in Sadu do2  if n has untreated tasks of j && j.occupied < j.demand?H then3 assign a task of j on n to s ;4 return j ;5  if n has untreated tasks of jinf in Sinf &&6 jinf .occupied < jinf .demand?H then  assign a task of jinf on n to s ;7 return jinf ;8  for each job j in Sadu by reverse order do9 if j.occupied < j.demand?H then10  nass ? GetNonHotspotNode(j);11 assign a task of j on nass to s ;12 return j ;13  nass ? GetNonHotspotNode(jinf );14 assign a task of jinf on nass to s ;15 return jinf ;16  Algorithm 2 describes the details of function Assign- Task(node n, slot s). Firstly we sort the jobs in Sadu in ascending order of amt() (line 1). we scan the Sadu until we get a job j of which n has untreated tasks and j.occupied < j.demand?H , then assign a task of j on n to s and return (lines 2-5). If we do not get a suitable job, we select the job jinf in Sinf if n has untreated tasks of jinf and jinf .occupied < jinf .demand?H (lines 6-8). If still no suitable job, we scan the Sadu by reverse order until we get a job j that j.occupied < j.demand ?H , then we use the method described in Section IV-A to get a node nass (nass has the most untreated remaining workload and it will not become an actual hotspot), and we assign a task of j on nass to s and return (lines 9-13). If we still do not get the suitable job, we select jinf and get the related non-hotspot node nass, then assgin the task and return (lines 14-16).

Algorithm 3: AssignTask SpecificJob(job j, node n, slot s))  input : job j, node n, slot s  if n has untreated tasks of j then1 assign a task of j on n to s;2  else3 nass ? GetNonHotspotNode(j);4 assign a task of j on nass to s;5     Algorithm 3 describes the function Assign- Task SpecificJob(job j, node n, slot n). If n has untreated tasks of j, assign a task of j on n to s (lines 1, 2). Else get a non-hotspot node nass, and assign a task of j on nass to s (lines 4, 5).



V. EXPERIMENTAL EVALUATION  In this section, we will evaluate the performance of TD scheduler through comparing to the Hadoop default FIFO scheduler and the popular Fair scheduler. In particular, we employ the following 4 kinds of applications: word count, inverted index, grep and distributed sort to compose the job queues for testing. And we use the real data sets (web pages extracted from Wikipedia, ) to build our experimental data sets. To imitate different real applications, we also generate 3 types of job queues. Small type means each job size (or the split number of input file) in the queue is small (? 100).

Large type means each job size is large (? 250). Mix type means the job size can be varied [50?300]. Each type of the job queue is composed by the 4 kinds of applications and all the split sizes are 64MB. The input file sizes of the job queues span [100 - 300] GB (default 200GB). The default value of the lower bound L=0.7, upper bound H=1.3.

We adopt a cluster constituted by 1 master and 50 slaves.

Each node has Core i7 870 CPU, 500G disk, 8G memory.

Each slave node has 2 slots for map tasks and 2 for reduce tasks, so there are total 100 map task slots. We use Hadoop 0.20.1 as our software environment. In Section V-A, we evaluate the performance of TD through the following 3 aspects: completion time of a job queue, the ratio of local task assignment, the occurrence number of hotspots. We evaluate the impact of the threshold in Section V-B.

A. TD-scheduler Performance  In Figure 7, we compare the completion time of the 3 schedulers using different types of job queues. To accurately reflect the performance differences, the initialization time of the job queue has not been considered. For each type of job queue, the completion time of our TD scheduler is significantly shorter than the others, in other word, TD scheduler can provide better throughput. In particular, by comparing the 3 experimental results showed in Figure 7, higher ratio of the small jobs in a job queue will markedly lead to more completion time of Fair and FIFO. In contrast, TD shows better performance.

We also evaluate the network overhead through calculat- ing the ratio of local task assignment, and the results are described in Figure 8. For the large type, Fair and FIFO can maintain high ratio of local assignment(70%), for small type, the ratio is reduced to a very low level(35%). Whereas TD scheduler can hold a high ratio of local assignment to reduce the network overhead.

In Table I, we divide the process into 5 stages on average and record the the occurrence number of hotspots in each  stage, then we repeat the same process 3 times and calculate the average values as the experimental results. As Table I shows, the TD scheduler can avoid majority of the hotspots.

Note that there are still some hotspots, which can be caused by 2 reasons: a) the nonlocal assignment is not allowed to be off-rack considering the network overhead. b) In the final stage, the distribution of the remaining data may be quite unbalanced, hence the hotspots are unavoidable.

Table I OCCURRENCE NUMBER OF HOTSPOTS  Scheduler Job Type Stage in Processing (percent)  Total 1-20 21-40 41-60 61-80 81-100  Fair small 25.3 19.7 23 24.3 29 121.3 mix 17.7 15.3 18.3 15 20 86.3 large 3 6.3 4 4.7 9 27  FIFO small 23 18.3 19.7 22.7 25 108.7 mix 14 13.3 15.7 16.3 19.7 79 large 2 5.7 3.7 4 6.3 21.7  TD small 0 0 0.7 0 2 2.7 mix 0 1 0 0 1.7 2.7 large 0 0 0 0 0.7 0.7  B. Threshold Analysis  In this section, we firstly introduce the impact of the lower bound in TD. As Figure 9(a) shows, high lower bound leads to lower throughput, specially if L > 0.7, the throughput would reduce signally. Whereas it would has little decrease if L < 0.7, and this is because when L < 0.7, the restraint of the lower bound is flabby enough, most of the task assignments will not be forced. Therefore, we would better to set L = 0.7 if there is no strict demand on deadline.

The upper bound is designed to avoid sticky slots, which can impact the system performance. We generate a job queue (size = 200GB and H = 1.3), and ?j, j.demand ? 35. Then for each co-scheduled job set, H < 1.56. Then we evaluate the impact of upper bound on throughput in Figure 9(b), where H ? [1.1? 1.5]. It is straightforward that high upper bound give rise to high throughput, however while H > 1.3, the throughput becomes lower, because the exorbitant upper bound will lead to numerous sticky slots. Hence the upper bound H = 1.3 is a suitable choice according to the experimental results.

5.5   6.5   7.5   8.5  0.5 0.6 0.7 0.8 0.9  C om  pl et  io n  T im  e (?   s)  Lower Bound  small jobs mix jobs  large jobs  (a) Completion Time vs. Lower Bound   5.5   6.5   7.5   8.5  1.1 1.2 1.3 1.4 1.5  C om  pl et  io n  T im  e (?   s)  Upper Bound  small jobs mix jobs  large jobs  (b) Completion Time vs. Upper Bound  Figure 9. The Effect of Bound  Summary: The experiments above show that the TD scheduler could provide higher throughput than the previous             1 1.5 2 2.5 3  C om  pl et  io n  T im  e (?   s) Data Size (? 100GB)  TD FIFO  Fair  (a) Small Jobs        1 1.5 2 2.5 3  C om  pl et  io n  T im  e (?   s)  Data Size (? 100GB)  TD FIFO  Fair  (b) Mix Jobs       1 1.5 2 2.5 3  C om  pl et  io n  T im  e (?   s)  Data Size (? 100GB)  TD FIFO  Fair  (c) Large Jobs  Figure 7. Completion Time of TD-scheduler   1 1.5 2 2.5 3  L oc  al A  ss ig  nm en  t R at  io (  % )  Data Size (? 100GB)  TD FIFO  Fair  (a) Small Jobs   1 1.5 2 2.5 3 L  oc al  A ss  ig nm  en t R  at io  ( %  ) Data Size (? 100GB)  TD FIFO  Fair  (b) Mix Jobs   1 1.5 2 2.5 3  L oc  al A  ss ig  nm en  t R at  io (  % )  Data Size (? 100GB)  TD FIFO  Fair  (c) Large Jobs  Figure 8. Ratio of Local Task Assignment  ones. Besides, we evaluate the impact of the threshold on the throughput and give the best threshold setting.



VI. CONCLUSIONS  Distributed data analysis on Big Data has become a very hot topic and MapReduce is a suitable technique used for this field. Indubitably, in the distributed data processing, task scheduling is a crucial component and can signally affect the system performance. However, the previous scheduling al- gorithms have not considered the job-intensive environment and can not provide high throughput. To solve this problem, we firstly study on the job-intensive problem and summarize 4 major factors which can impact system throughput. Then, we propose a novel approach, TD scheduler to improve the system throughput for MapReduce processing. Finally, a large number of experiments verify the performance of TD scheduler. The results show our method can provide higher throughput than others and shows excellent performance if the job queue contains a high percentage of small jobs, it is very appropriate for practical applications.

Acknowledgement. This research was supported by the National Basic Research 973 Program of China under Grant No. 2012CB316201, the National Natural Science Foun- dation of China under Grant Nos. 61033007, 61003060, the Fundamental Research Funds for the Central Univer- sities under Grant No. N100704001, the National Research Foundation for the Doctoral Program of Higher Education of China Grant No. 20120042110028 and the MOE-Intel Special Fund of Information Technology under Grant No.

MOE-INTEL-2012-06.

