On Access Control Schemes for Hadoop Data Storage

Abstract?Hadoop is a distributed Big Data storage and processing framework hugely adopted in different sectors from online media, education, government and social media to handle the enormous growth of information in their respective domains. However, the core architecture of the solution is based on a trusted cluster. It lacks native methods for protecting sensitive data that cross over enterprises and are exposed or accessed illegally. In this paper, propose an access control scheme for data stored in Hadoop, based on concepts from BitTorrent and the secure sharing storage over the cloud.

The proposed schemes can impose access control for data owners, and prevent unauthorized access and illegal authorization.

Keywords- access control; Hadoop; authorization; BitTorrent; secure sharing storage.



I. INTRODUCTION Research in newer ways of storing and processing data  has led to the development of various No-SQL based solutions [1]. These newer technologies enable shift in paradigm from traditional relational solutions. Unlike traditional DBMS/RDBMS solutions, they offer capabilities of storing and processing enormous amounts of data. They enable aggregation of different data sources from structured, semi-structured and unstructured. With distributed & parallel architectures, they offer competencies of processing large historical datasets (often in TeraBytes), as well as real time continuously gather data. Among others, the Hadoop platform has emerged as a forerunner for big data processing and storage [1, 2]. Major organizations such as Facebook, Yahoo, IBM and Microsoft as well as other government and educational institutions have widely accepted it as key part of their data architecture. Amazon & Oracle wrapped Hadoop as one of their key businesses by elevating it to the ?cloud? and offering ?Platform as a Service? [3]. However, the core architecture of the Hadoop framework was conceived for a trusted cluster environment, with a small elite set of advanced programmers as its user base. Until recently, security aspects of the Hadoop framework remained widely unaddressed [4]. However with its rapid acceptance in different sectors and its use for handling critical data, the need for proper data protection and access mechanisms have become quite prevalent.

In the paper, we propose two novel access control schemes for the Hadoop data storage, addressing  unauthorized access and illegal authorization attacks on data stored in the different nodes throughout the Hadoop cluster.

The first scheme is based on virtual piece concepts from BitTorrent [5]. The other is designed with the secure sharing storage over the cloud [6] methodology.

The remainder of this paper is organized as follows: Section 2 provides a background on the Hadoop framework, its architecture, process flow and security risks. We also introduce concepts from the BitTorrent and secure sharing storage over the cloud. Next we introduce our Access Control Schemes for Hadoop data storage in Section 3.

Section 4 concludes the paper with discussion of future work.



II. BACKGROUND The following subsection sections, provides an  overview of the Hadoop architecture, its process flows, associated security risks, overview of the BitTorrent protocol for peer-to-peer file sharing and the secure sharing storage over the cloud methodology.

A.  Hadoop Architecture Hadoop is an open source framework for distributed  storage and data-intensive processing, first developed by Yahoo. It offers a file system called Hadoop Distributed File System (HDFS) [7, 8] and computing paradigm MapReduce [9, 10]. HDFS is a distributed file system that splits and stores data on nodes throughout a cluster, with a number of replicas. It provides an extremely reliable, fault tolerant, consistent, efficient and cost-effective way to store large amounts data. The MapReduce paradigm provides a model for computation on huge amounts of data spread across a cluster. It consists of two key functions: Mapper and Reducer. The Mapper processes input data splits in parallel through different map tasks and sends sorted, shuffled outputs to the Reducers that in turn groups and processes them using reduce tasks for each group.

The HDFS stores the data splits into multiple nodes called DataNodes. The namespace & meta-data of each file and its blocks are stored in a master node called NameNode.

The MapReduce model enables distributed processing of data stored throughout the HDFS. A JobTracker node schedules & coordinates a number of map & reduce tasks on multiple data processing nodes called TaskTrackers. The   DOI 10.1109/CLOUDCOM-ASIA.2013.82    DOI 10.1109/CLOUDCOM-ASIA.2013.82    DOI 10.1109/CLOUDCOM-ASIA.2013.82     architecture and process flow is given in Figure 1. The TaskTrackers are always deployed in the DataNodes.

Figure 1. Hadoop Architecture and Process Flow  B. Process flows There are two types of process flow for accessing &  processing data in the Hadoop framework: one being direct operations on the HDFS and other being processing data using the MapReduce paradigm.

? HDFS Operations Process flows of HDFS include the read and write  operations. HDFS read operations on a data file, from Hadoop clients need locations of its data blocks stored across different nodes. An RPC request to the NameNode returns the addresses (metadata) of DataNodes that has a block of the requested file. The returned addresses of the DataNodes are used directly by Hadoop clients to read the data blocks. Write operations from Hadoop clients, sends a request to the NameNode for writing a new file. The NameNode performs checks to ensure that the file does not already exist and that the client has the right permissions. It next makes a record of the new file and returns a list of suitable DataNodes to store the file blocks. The Hadoop client then, splits and writes the data blocks to the respective DataNodes.

? MapReduce Jobs MapReduce jobs are scheduled and coordinated by the  JobTracker. Hadoop clients submit jobs to the JobTracker. It in-turn consults the NameNode to learn the addresses of DataNodes that have the blocks of the input files for a job.

The JobTracker initiates the TaskTrackers running near or on those DataNodes that contains the blocks of the input files. The TaskTrackers start a Map task and monitor its progress and report its status to the JobTracker. Each node stores the intermediate data on local disk after completion of a Map task. Simultaneously, the JobTracker also starts Reduce tasks on TaskTrackers. All intermediate data from Map tasks that have same output keys are sent to be respective Reduce tasks for final computation. Outputs are written back into the HDFS.

C. Security Risks The Hadoop framework is designed for a trusted  environment. Its operations are based on assumptions that clusters would consist of cooperating, trusted machines used by trusted users in a trusted environment. There aren?t any native Hadoop mechanisms to provide authentication, encryption, authorization and privacy [11, 13, 14]. This creates serious security vulnerabilities as malicious users can read, corrupt or even delete data by bypassing access control?s rules. All users have direct access to all data stored in DataNodes without any fine-grain authorized access control mechanisms. A job to the JobTracker could be executed without proper validation or authentication.

Without encryption, sensitive data in the DataNodes may be leaked. Yahoo proposed addressing these concerns using Kerberos [12]. However, it lacks requirements of role-based access control, attribute-based access control and active directory [11, 13, 14] and is unable to provide a comprehensive solution. Bellow some of the key security risks and challenges are summarized.

? Authentication A typical Hadoop cluster is based on a trusted  environment. Such a setting does not require any authentication of users or services.  Hadoop integrated with Kerberos can provide some form of authentication mechanism. The NameNode and JobTracker can ensure that any HDFS or MapReduce requests are being executed with the required authentication. This configuration, realizes service-to-service and user-to-service authentications.

However, Kerberos fails to control operations and permission of user submitting jobs. Further it does not provide any security while directly accessing the data blocks store in the DataNodes.

? Authorization The core authorization mechanism in Hadoop is through  its file-system. The HDFS enables authorization mechanisms based upon access control lists (ACL) similar to that used in POSIX file-systems. However, the authorization mechanisms in Hadoop are still in their infancy. Increase in the number of users and data files, makes it difficult to control and manage the access control lists and permissions [13, 14]. Hadoop clients may directly read or write a data block bypassing the NameNode as long as it can supply an appropriate block id [13, 14]. Moreover, the Hadoop clients may register themselves as another TaskTracker or DataNodes leading to data corruption, task interruptions and other malicious activities.

? Confidentiality Hadoop natively doesn?t implement any cryptographic  framework to support encryption & decryption of files in HDFS. This enables users with administrative rights read data that are stored in the DataNodes. Moreover, RPC communication protocols between the Hadoop services     transports data over an unprotected network vulnerable to leaks and eavesdropping.

? Audit The audit mechanisms of Hadoop are the similar to that  in Unix, where in it is difficult to keep track of audit events.

Data owners do not have a mechanism to track access's histories to their data.

D. BitTorrent BitTorrent is a popular peer-to-peer transport protocol  commonly used for sharing large files [5]. BitTorrent splits the data files into virtual pieces that are shared between peers (nodes participating in sharing of a file). It maintains a torrent seed file with indexes and hashes for each virtual piece of a file. The torrent file also includes other meta- information such as addresses of the Trackers (servers that assists in the communication between peers), communication ports and block sizes. The tracker maintains and provides a list of peers involved in sharing a file. Peer acts as a client as well as a server. Once it receives a complete data block, it acts as a server for that block. Each peer discovers other peers via one or more trackers from the torrent file. When the multiple peers download the same file concurrently, they upload to each other making it possible to support very a large number of downloaders with only a modest increase in load. The advantage of the BitTorrent is fast distribution and transportation of large files. There have been multiple-use cases for sharing data using the BitTorrent protocol as surveyed in [15].

E. Secure Sharing Storage over the Cloud A secure sharing storage scheme based on elliptic curve  cryptography over the cloud was proposed in [6]. The scheme can imperatively impose the access control policies of data owners, preventing the cloud storage providers from unauthorized access and making illegal authorization to  access the data. The advantage of this scheme is that only the data owner has control of who has access to his data.

In this scheme, assuming that Alice has the private key ka and the public key kaG, Bob has the private key kb and the public key kbG, and the Cloud Storage Provider has the private key kc and the public key kcG. Alice stores a message m encrypted as me in the cloud. The encrypted message me = m + r?kcG + t?G, where r and t are random numbers. G is a point that satisfies some conditions on the elliptic curve. In order for Bob to access the message, he sends a request to Alice with his public key kbG. Alice computes tcG = -rb?kbG - rc?kcG - r?kcG - t?G, where rc and rb are also some random numbers. Alice then sends (rc?G, tcG) to the cloud storage provider and rb?G to Bob. The cloud storage provider re-encrypts the data me via the formula mc = me + kc?rc?G + tcG. Finally, Bob receives mc from the cloud storage provider and decrypts mc with the formula mb = mc + kb?rb?G to gain the message m (mb = m).



III. NOVEL ACCESS CONTROL SCHEMES In the section, we propose two novel access control  schemes for Hadoop data storage. One is based upon concepts from BitTorrent protocol and data encryption. The other is derived using the secure sharing storage over the cloud methodology.

A. Access Control Scheme with BitTorrent and encryption BitTorrent uses virtual pieces to distribute a data file.

Users download data files with their torrent seeds. The notion of virtual pieces, allows creation of access tokens for the metadata information such as addresses of blocks stored on DataNodes. Only users with right access tokens can access block addresses, and the respective blocks stored in the DataNodes. This scheme includes four phases: creating and distributing access token, gain access token and access blocks. Figure 2 shows the process of creating and distributing the access token over a Web server.

Figure 2. Creating and Distributing Access Token over a Web Server     ? Creating and Distributing Access Token A Hadoop client that is owner of a data file can create  and distribute the access token for the file. The steps involved are as follows.

Step 1: Request the NameNode: When a Hadoop client uploads or accesses its data file in HDFS, it sends a request to the NameNode.

Step 2: Return the metadata: After receiving a request, the NameNode returns the metadata of the requested data file. The metadata includes block ids and the address of DataNodes that store the blocks. The Hadoop client creates a record of metadata for each block.

Step 3: Generate the metadata file: The Hadoop client generates a metadata file with the records of metadata returned by the NameNode.

Step 4: Create the access token data: The Hadoop client encrypts each record of metadata. If a block is authorized to a user, this block?s record in the metadata file is encrypted with a shared key with the Hadoop client and the user. If a block is authorized to a group, this block?s the record is encrypted with a shared key with the Hadoop client and the group.

Step 5: Create the access token file: The Hadoop client uses the access token data and other file information, such as file head, timestamp, size and so on, to create an access token file of the data file.

Step 6: Create the torrent file: The Hadoop client uses a BitTorrent tool, to split the virtual pieces of the access token file, create indexes and hash for each virtual piece to generate a torrent file.

Step 7: Distribute the access token file and torrent file: The Hadoop client uploads the torrent file to a Web server  to distribute the access token seed. The access token file is also uploaded to the server of announce list in the .torrent file.

? Gain Access Token When a user wants to access the blocks of a data file, it  downloads its respective torrent file from the Web server. It also uses a BitTorrent tool, to download the access token file. Then, it decrypts the access token data with its shared key to gain the authorized access token of blocks. Without the shared key, a user cannot decrypt the access token data and is not able to gain the authorized access token, thus unable to identify addresses of blocks in the DataNodes.

? Access Blocks The authorized access token of blocks, allows access to  the metadata of the blocks and their addresses on the DataNodes.

B. Access Control Scheme with Secure Sharing Storage In the earlier scheme, a Hadoop client authorizes users  access to the blocks stored in DataNodes via an access token and shared key. Although only users with a shared key may decrypt the access token data to gain the access token, data owners do not gain knowledge about who downloads the access token file and accesses the blocks. A secure sharing storage over the cloud addresses this concern. This scheme also includes four phases: creating and distributing access token, gain access token and access blocks. Figure 3 shows the steps of the creating and distributing access token over the cloud.

Figure 3. Creating and Distributing Access Token over the Cloud  ? Creating and Distributing Access Token The steps 1 to 5 are same as discussed in ?Access Control Scheme with BitTorrent and Encryption?. The additional steps involved are given bellow.

Step 6: Encrypt the access token file: The Hadoop client uses the secure sharing scheme over the cloud to encrypt the access token file.

Step 7: Distribute the encrypted the access token file: The Hadoop client distributes the encrypted access token file to the cloud storage provider.

? Gain Access Token A user needing to access to blocks of a data file,  requests the Hadoop client about the owner of the data file.

The Hadoop client sends a response to the user as well as the cloud storage provider according to concepts from the secure sharing storage scheme over the cloud. The user downloads the re-encrypted token file from the cloud storage provider and decrypts it. Finally, the user decrypts the access token data to gain the authorized access token.

? Access Blocks When a user gains the authorized access token, they  know the metadata of the blocks authorized by the data owner, including addresses of the blocks stored in the DataNodes. The user may access the data blocks stored in the DataNodes via their metadata information.



IV. CONCLUSION In our work, we identified some of the security risks of  Hadoop system and proposed two novel access control schemes for storing data. It aimed at allowing the data owners to control and audit access to their data. However, there could be a burden on data owners to update the access token and torrent files when the metadata of their file blocks changes. A flexible key sharing key management solution for the access control would be investigated in the future.

We also aim to implement the complete solution, evaluate its performance and application for access control.

