Implementation of Projected Clustering based on SQL queries and UDFs in Relational Databases

Abstract?Projected clustering is one of the clustering ap- proaches that determine the clusters in the subspaces of high dimensional data. Although it is possible to efficiently cluster a very large data set outside a relational database, the time and effort to export and import it can be significant. In commercial RDBMSs, there is no SQL query available for any type of subspace clustering, which is more suitable for large databases with high dimensions and large number of records. Integrating clustering with a relational DBMS using SQL is an important and challenging problem in todays world of Big Data. Projected clustering has the ability to find the closely correlated dimensions and find clusters in the corresponding subspaces. We have designed an SQL version of projected clustering which helps to get the clusters of the records in the database using a single SQL statement which in itself calls other SQL functions defined by us.

We have used PostgreSQL DBMS to validate our implementation and have done experimentation with synthetic as well as real data.



I. INTRODUCTION  Big data is described by a large number of attributes and huge volume of records which pose challenges such as standardizing the data inorder to have all the attributes to a common scale, transforming the data inorder to reduce the dimensionality, constructing a distance measure for knowing the similarity between two objects, etc. Comprehensible analysis of data is required to help data scientists and industry people so that the data being loaded into the database can make sense. Clustering is one such way to analyze the data and has many applications in various domains such as Bank, Supermarket, Bioinformatics etc. Recent DBMS like Oracle 11g and SQL Server 2008, facilitate clustering using SQL. Clustering algorithm [11] or any such algorithm with a relational DBMS [2] is an important and challenging problem. Although it is possible to efficiently cluster a very large data set outside a relational database, the time to export it can be significant. Moreover dynamic updating of the databases will decelerate the clustering process.

When we implement clustering in some other languages like C, Java etc we need to frequently export and import the files for maintaining the ACID properties of database. Therefore, being able to cluster data set stored inside a relational database, has significant advantage.

There are many algorithms for clustering data, such as K- means [1], [2], [3], [11], EM [5] and projected clustering [6], [7]. One of the main characteristics of projected clustering is that the records are projected along the most relevant attributes  which give us a better understanding of the data. In case of big data, the volume of data is too huge and the structure is quite unordered. However there is an implicit structured format for this type of data too. Nowadays large volume of data are generated in every domain and the databases are quite big with too many attributes. Not all attributes are relevant for all the formed clusters. By knowing the relevant attributes, we can remove insignificant attributes thereby reducing the storage requirement of each cluster. So we need to apply a clustering technique to find the relevant clusters in various subspaces with appropriate attributes relevant for that subspace.

The subspace clustering [8] is the task of detecting all clusters in all subspaces of a given dataset. This means that a data point might be a member of multiple clusters, each existing in a different subspace. Projected clustering is a type of subspace clustering which seeks to assign each point to a unique cluster, but clusters may exist in different subspaces. This technique can be more suitable for high dimensional data, since projection of relevant dimensions for a cluster is intended to reduce the dimensionality of data specific to that cluster. Thus, it is a way of finding the most relevant dimensions for a cluster thereby finding the projection of data with high dimensions onto these relevant dimensions. Implementation of projected clustering using SQL, allows clustering of large data sets stored inside a relational DBMS eliminating the need to export data.

Apart from this we can also have clustered data in the form of a table which can help us to build the visualization in a comparatively easy manner. The clustered data of a table have certain meaning and some order. So when a query comes, the probability of accessing the data from the same cluster is more.

Hence clustering can be a preprocessing step to build some sort of organizing structure of the table which can pave the way for efficient query processing. This clustered table can also be used for the fragmentation of the data in a meaningful manner in distributed environment. There are attempts to use SQL queries and User Defined Functions(UDFs) for implementing Data mining algorithms in [12]. The major challenge in SQL version of any algorithm is that apart from executing the required tasks, one has to work around the storage of data and manipulating the data such that the time complexity of the algorithm is not accelerated.SQL version of projected clustering in general, has not been addressed in the literature to the best of our knowledge.

The major contributions in this paper are  2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS)     1) Design and Implementation of SQL procedures for projected clustering  2) Optimized version of SQL projected clustering for effective and fast computation of the clusters

II. CLUSTERING RELATIONAL DATABASES USING SQL  A. Motivating concepts  In K-means clustering, a data set is partitioned into K clusters in such a way that the sum of the total clustering errors for all clusters is reduced as much as possible while inter distances between clusters are maintained to be as large as possible1.

Traditional K-means clustering forms clusters with dataset consisting of all the attributes. In high dimensional spaces not all dimensions may be relevant to a given cluster and different sets of points may cluster better for different subsets of dimensions. So there may be a possibility of clusters within clusters or different clusters within one application based on different attributes2.

A hybridized K-means clustering approach for high dimensional dataset is for reducing the dimensions of the database [3]. Due to the growth of high dimensional dataset, conventional data base querying methods are inadequate to extract useful information. Principal Component Analysis ( L2 PCA) [9] is used as a first phase for K-means clustering for dimensionality reduction. But the relevant attribute selection for a cluster is a question here and in case of a high dimensional data it is good if we select some relevant attributes for a particular cluster and can remove all remaining attributes.

Fast Algorithms for Projected Clustering (PROCLUS) is based on some relevant attribute selection for clustering as described in [6]. PROCLUS returns clusters along with the relevant dimensions in each cluster that signify the correlation among the dimensions in that particular cluster. The performance is enhanced by computation of Manhattan distance for finding potential data points for each cluster as well as finding the relevant dimensions of each cluster. Hence it can be adopted to cluster high dimensional data with relational DBMS.

The problem of projected clustering for categorical datasets is addressed in [7]. This method has a probability for more number of outliers which will not help us in an advantageous manner to cluster all the required set of records in the database relation. In the paper, mean is used to represent a cluster, which implies that center of the cluster may not exist physically. Thus a non-existent record may form the center of the cluster which may create ambiquity in understanding the records within the cluster.

B. Clustering using SQL  Programming K-means clustering algorithm in SQL [1] shows that it is feasible to get an SQL implementation of the well- known K-means clustering algorithm in DBMS, that can work on the records of a table with numerical attributes. The paper introduced two implementations of K-means in SQL.

Carlos extended the idea of programming K-means in SQL by Integrating K-means clustering with a relational DBMS using  1http://en.wikipedia.org/wiki/Cluster analysis 2http://cs.gmu.edu/cne/modules/dau/stat/clustgalgs-/clust5 bdy.html  SQL [2]. Three SQL implementations of the popular K-means clustering algorithm have been presented by him in the paper.

SQLEM is fast clustering [5] based on the efficient SQL implementation of the EM algorithm. It provides a cluster membership probability per record. There are three strategies to implement EM in SQL: horizontal, vertical and a hybrid one.

In general EM is based on probability computation which may become computationally complex with increase in size and dimensionality of RDBMSs. But we do not have a standard algorithm that addresses this problem of complexity in EM3.

A fast K-means prototype to cluster transaction data sets using disk-based matrices is an efficient disk-based K-means cluster- ing for relational databases [4]. The disk-based implementation and the SQL-based implementation represent complementary solutions to implement K-means in a relational DBMS, but the SQL-based solution will become more valuable as disk density and CPU speed increase and hardware costs decrease.

The problem of integrating projected clustering in relational databases using SQL has to take care of many aspects such as storage of data, accessing data in an optimized manner, manipulation of data inorder to perform the actions as well as creation or redesigning a relational schema for storing the output of the algorithm. In general, these problems are not addressed in the literature. Hence we are focussing on the integration of projected clustering using SQL.

TABLE I. LIST OF SYMBOLS  Symbol Definition N Total number of records in a relation Nl Number of records in locality l Nk Number of records in cluster k K Total number of clusters k A cluster t Total number of attributes l Average number of dimensions  Bcnt Number of badmedoids rij Value of j  th attribute of ith  record in a relation Rk Set of records in cluster k A The set of attributes in a relation Ak The set of attributes of kth cluster in a relation Yk Mean of kth cluster Ykj The average distance between records in cluster k  to it?s centroid along dimension j Xkj The average distance between records in a  locality to the medoid k along dimension j ?k The standard deviation of kth cluster Zkj Value indicating the relationship between mean,  average distance and standard deviation wk Weight of cluster k

III. DEFINITIONS AND NOTATIONS  To understand the algorithm a few definitions and notations are defined here. The symbols we used are shown in Table

I. Let D be the database, R = (r1, r2, ...., rN ) be the set of records with each ri being described by a set of t attributes as A = (a1, a2...., at) . In the context of database, each point (feature vector) of a cluster is in fact a record ri and each dimension (feature) is an attribute ai. The centroid or mean of a cluster of records Rk ? R with attributes A is the algebraic average of all records along each attribute in a cluster k. In most of the clustering algorithms the distance between two records is calculated with the help of norms [10]. For a record  3http://en.wikipedia.org/wiki/Expectation-maximization algorithm     Fig. 1. Locality of medoid A with respect to medoid B and medoid C on the basis of ? value.

ri and a record rj , with t attributes, Lp ( p-norm distance ) is defined in (1).

Lp = (?t  k=1 |rik ? rjk |  p ) 1  p  (1)  When p = 1, it is L1 norm, when p = 2 it is L2 norm and so on. The L1 norm is the absolute difference between two records as shown in (1). This is same as Manhattan distance [6] or the City block distance4.

Manhattan segmental distance: This is defined rela- tive to some subset of dimensions Ai. Specifically, for any two records ri and rj with Ai dimensions, the Manhattan segmental distance between ri and rj relative to Ai, is defined in (2).

dAi (ri, rj) =  ? k?Ai |rik ? rjk | |Ai|  (2)  Employing the Manhattan segmental distance as opposed to the traditional Manhattan distance is useful when com- paring records in two different clusters that have varying number of attributes.

Locality : Assume we have a dataset with many clusters and each cluster is represented by a medoid mk. If we need to find the locality of medoid, Lk then  1) Calculate the Manhattan distance dk to it?s nearest medoid.

2) Find all the records Rk ? R which fall within distance dk.

Rk calculated above is the locality of medoid mk. In Figure 1, A, B and C are three medoids and the nearest medoid of A is C with distance ?. We have taken ? instead of ?/2 inorder to include the possibility of overlapping localities around medoids. The average distance, Xkj between records in a locality Lk to it?s medoid mk along each attribute j with Nl records in it?s locality is shown in (3).

Xkj =  ?Nl i=1  ??mk,j ? rij ?? Nl  (3)  Based on the average distance we can calculate the mean of a cluster Yk with t attributes by using (4).

Yk =  t? j=1  Xkj  t (4)  4Books.google.co.in/books?isbn=080582281X  Fig. 2. SQLPROCLUS takes as input SQL query for clustering a table and gives three tables as output signifying the clusters, relevant dimensions for each cluster and best medoids.

The computed mean and average distance of a cluster k can be used for finding the standard deviation which is explained in (5).

?k =  ?? j (Xkj ? Yk)   t? 1 (5)  We can compute Zkj , that indicates how the j-dimensional average distance, Xkj associated with the medoid mk is related to the average Manhattan segmental distance associated with the same cluster with mean Yk, by using (6).

A smaller value of Zkj indicates that along dimension j the records Rk are more closely correlated to the medoid mk.

Zkj = Xkj ? Yk  ?k (6)  The weight, wk of a cluster k calculated by using Ykj with |Ak| attributes ( projected cluster ) is defined in (7).

wk = ?  j  Ykj |Ak|  (7)

IV. SQL VERSION OF PROCLUS  We have implemented the projected clustering algorithm in PostgreSQL5 using the following two methods.

1) A naive translation of projected clustering in Post- greSQL.

2) An optimized version using efficient indexing, rewrit- ten queries, and reduced intermediate database tables.

SQL PROjected CLUStering has mainly three phases. Initialization phase, Iterative phase and Refinement phase. The purpose of the Initialization phase is to find a potential set of medoids by a greedy approach. Thus if K clusters are required to be formed, select B ?K medoids from the set of original records, where B is a small integer constant. This greedy approach has the advantage of picking some representatives from each cluster using L1 and the reduction to the sample set significantly reduces the running time of the Initialization phase. The Iterative phase is the core part of SQLPROCLUS for selecting a good set of medoids. This is presented as an optimization problem. The final Refinement phase is for improving the quality of clustering. Refinement phase uses the distribution of records in clusters for finding the dimensions and assigning records to each cluster.

We implement each part of PROCLUS as separate SQL functions and finally implement PROCLUS in SQL, named NAIVE SQLPROCLUS. The advantage of each function is that we can use it separately for finding medoids, finding dimensions, assigning records to each clusters, detection and replacement of bad medoids etc.

5http://www.PostgreSQL.org/docs/8.0/static/tutorial.html     A. Basic Framework  The overview of the framework is shown in Figure 2.

1) Setup: Create and populate tables with the required records  2) PROCLUS  a) Initialization phase b) Iterative phase c) Refinement phase  3) Output  a) Assign table (Clustered table) b) Dimension table c) BestMedoid table  Apart from the main tables specified above, certain temporary tables like Medoid, Submedoid, Locality, etc. are also used.

The main tables as well as the temporary tables used in the implementation are shown in Table II and Table III respectively.

TABLE II. MAIN TABLES IN SQLPROCLUS  Table name Size Contents Data N x t Original records  Dimension (l*K) x 2 Medoids and their relevant dimensions  Assign N x (t+1) Original records with their assigned medoid  BestMedoid K x t Best medoid set  TABLE III. TEMPORARY TABLES IN SQLPROCLUS  Table name Size Contents Medoid (B*K) x t Potential medoids obtained  using greedy approach SubMedoid K x t Required medoids  selected during iterative phase  Locality Nl x (t+1) Medoids with records in its locality which is  subject to change during iterative phase  BadMedoid Bcnt x 1 Detected bad medoids  B. NAIVE SQLPROCLUS  We implement SQL procedures inorder to facilitate the projected clustering within relational databases. The input to this algorithm is a Data table which we want to cluster, number of clusters, K, the average number of dimensions for each cluster, l as explained in Algorithm 1. NAIVE SQLPROCLUS is a direct translation of the projected clustering algorithm into SQL. We are making use of several intermediate tables, various disk accessing statements such as Select, Update, Delete, Insert for storing and retrieving the result of each function.

The main algorithm for SQLPROCLUS is as follows  Algorithm 1 SQLPROCLUS for clustering procedure SQLPROCLUS(Datatable,K, l) {1.Initialization Phase} Perform Greedy( K ) {2. Iterative Phase} BestObjective = ? Perform Subset ( K ) repeat {Approximate the optimal set of dimensions} Perform locality ( ) Perform FindDimensions ( K, l ) { Form the clusters } Perform Assignpoints ( K, l ) Perform Evaluateclusters ( K, l ) Store the result of Evaluateclusters in objectiveV ar.

if (ObjectiveV ar < BestObjective) then  BestObjective = ObjectiveV ar Insert records of SubMedoid table into BestMedoid table.

Perform Detectbadmedoids ( K )  end if Insert count of records of Medoid table in mednt.

Insert count of records of Badmedoid table in badmedcnt.

if (medcnt and badmedcnt > 0) then  Perform Replacebadmedoids ( ) else  exit end if if (BestMedoid remains same) then  Terminate the iterative phase.

Insert records of BestMedoid table into SubMedoid table.

Perform Assignpoints ( K, l )  end if until termination condition is met {3.Refinement Phase} Insert the records of Assign table into Locality table.

Perform FindDimensions ( K, l ) Perform AssignPoints ( K, l ) return Assign table.

end procedure  Each function takes as input, the number of clusters to be formed, K and the average number of dimensions in each cluster, l. The output of the functions is in the form of tables.

At the beginning of each function we delete the records from the output table using Truncate statement to make the table empty so as to avoid redundancy of data before insertion.

The excerpts from the implementation of one of the user defined functions FindDimensions in SQL is as follows.

CREATE OR REPLACE FUNCTION FindDimensions(K int, l int) returns table(medoidid int,.

.,dimsnvalue numeric) as declare begin     Fig. 3. Main table flow in SQLPROCLUS  Fig. 4. Performance comparison of OPTIMIZED and NAIVE SQLPROCLUS by varying N  x := array[[NULL,NULL,...,NULL]]; ......................................

x[q][k] := x[q][k]::float/clstrcnt[q]; .....................................

FOR k IN 1 .. dim LOOP mul:=0; mul := (x[q][k]-y[q]); mul :=mul*mul; squaresigma[q] := squaresigma[q]+mul; END LOOP; .....................................

sigma[q] := squaresigma[q]::float/(dim-1); z[k] := z[k]::float/sigma[q]; .....................................

update Dimset set dimsn=999999 where dimsn=minval; .....................................

Return query select * from Dimension; end; language plpgsql;  C. OPTIMIZED SQLPROCLUS  The naive translation of PROCLUS into SQL has certain limitations in performance. These limitations are due to the large number of scans required for convergence in clustering algorithm. Iterative phase of PROCLUS require multiple scans of the same data in order to compute the segmental distances with respect to the medoids selected. Several optimizations  Fig. 5. Performance comparison of OPTIMIZED and NAIVE SQLPROCLUS by varying l  Fig. 6. Performance comparison of OPTIMIZED and NAIVE SQLPROCLUS by varying k  were done to improve the performance without changing the PROCLUS behavior. In this paper, the optimizations in- clude index creation, cursor usage, rewrite SQL functions for reducing the overhead of disk access, replacing certain intermediate tables consisting of few records, by arrays. The excerpts from the implementation of optimized SQL version of FindDimensions is as follows.

CREATE OR REPLACE FUNCTION FindDimensions(K int, l int) returns void as declare begin insert into avg_dim_dist select meid,avg(dist_a),...

......

Fig. 7. Performance comparison of OPTIMIZED and NAIVE SQLPROCLUS by varying t     from locality group by meid;  insert into avg_dim SELECT meid,COALESCE(am,0) + COALESCE(bm,0) + ...

FROM avg_dim_dist ;  .....

update avg_dim_dist t set am = am - s.dist,  bm = bm - s.dist, ....

from avg_dim s where t.meid = s.meid;  update avg_dim s set dist = sqrt((COALESCE(t.am*t.am,0) + ...

....

FROM avg_dim_dist t where t.meid = s.meid;  update avg_dim_dist t set am = am/s.dist,  bm = bm / s.dist, ...

....

from avg_dim s where t.meid = s.meid;  ......

end; language plpgsql;

V. EXPERIMENTAL EVALUATION  We evaluated the performance of NAIVE SQLPROCLUS and OPTIMIZED SQLPROCLUS on synthetic data and on real data by varying the size of database, average dimensionality of clusters, number of clusters and dimensions of original table.

In varying the number of records we fixed the attribute size to be 10, average number of dimensions to be 5 and number of clusters as 5. This analysis is shown in Figure 4. We can see a sudden rise in time from 20000 to 25000. This is due to the time taken for page access of the database records from the hard disk. In NAIVE SQLPROCLUS the number of disk accesses is quite high since the records were fetched as and when required and lots of redundant computations were done without looking into the optimizations such as batch update, local caching and temporary table creation. But in OPTIMIZED SQLPROCLUS, the time has reduced drastically showing the optimal performance. In varying the average number of dimensions we fixed the size of the records to be 10000 and number of clusters to be 3. The analysis based on average dimensionality of clusters shows that the graph remains almost constant for both the cases but the time is very large for NAIVE as shown in Figure 5. In varying the number of clusters we fixed the size of the records to be 10000 and average number of dimensions to be 5. This analysis is shown in Figure 6. In analysis based on varying the number of dimensions we fixed the number of records to be 10000, average dimensions as 4 and number of clusters as 3. This analysis is shown in Figure 7. Figure 4, Figure 5, Figure 6 and Figure 7 clearly exhibit that OPTIMIZED version has significant improvement with respect to all parameters over the NAIVE version.



VI. CONCLUSION  Integrating clustering algorithm with a relational DBMS using SQL is an important and challenging problem. There are many algorithms for clustering data such as K-means and EM.

Among them, projected clustering is the one which is more suitable for high dimensional data since it projects the relevant dimensions for each cluster. Hence each cluster can be stored with limited storage requirement. We incorporated the opera- tions needed for projected clustering in SQL. Our experiments show that optimized version of SQL for projected clustering has good performance. This clustering can be further utilized for developing a prototype for efficient query processing. This will have significant relevance in query processing applications in mobile, tablet and other such portable devices which has limited storage.

