Classifying using Specific Rules with High Confidence

Abstract?In this paper, we introduce a new strategy for mining the set of Class Association Rules (CARs), that allows building specific rules with high confidence. Moreover, we introduce two propositions that support the use of a confidence threshold value equal to 0.5. We also propose a new way for ordering the set of CARs based on rule size and confidence values. Our results show a better average classification accu- racy than those obtained by the best classifiers based on CARs reported in the literature.

Keywords-Data Mining; Supervised Classification; Class As- sociation Rules; Association Rule Mining;

I. INTRODUCTION  Classification Association Rule Mining (CARM) or as- sociative classification, introduced in [1], is a well-known Data Mining technique for the extraction of classification rules. CARM integrates Classification Rule Mining (CRM) [2] and Association Rule Mining (ARM) [3] for mining a special subset of association rules called Class Association Rules (CARs). A classifier based on this approach usually consists of an ordered CAR list l, and a mechanism for classifying unseen transactions using l [1], [7], [11].

Associative classification has been used in several tasks  including text segmentation [18], determination of DNA splice junction types [5], mammalian mesenchymal stem cell differentiation [16] and prediction of protein-protein interaction types [17], among others.

In CARM, similar to ARM, we have a set of items  I = {i1, i2, . . . , in}, a set of classes C and a set of labeled transactions D, where each transaction in D comprises a set of items X ? I and one class c ? C. The support of an itemset X ? I (denoted as Sup(X)) is the fraction of transactions in D containing X (see Eq. 1). The idea of CARM aims at extracting a set of CARs, where a CAR is an implication of the form X ? c, being X ? I and c ? C.

In CARM, as well as in ARM, two threshold values are commonly used to determine the interest of a CAR: 1) Support: The support of a CAR X ? c (see Eq. 2) is the fraction of transactions in D that contains X?{c}.

2) Confidence: The confidence of a CAR X ? c (see Eq. 3) is the probability of finding c in transactions  that also contain X , which represents how ?strongly? the rule antecedent X implies the rule consequent c.

Sup(X) = |DX | |D| (1)  where DX is the set of transactions in D containing X .

Sup(X ? c) = Sup(X ? {c}) (2)  Conf(X ? c) = Sup(X ? c) Sup(X)  (3)  Many studies [3], [12] have pointed out the combinatorial number of association rules that could be obtained when a small support threshold is used. To address this problem, some works [4], [9], [10] propose to prune the search space each time a CAR satisfies predefined support and confidence thresholds, this strategy allows us to obtain general (small) rules. However, this strategy has two main drawbacks: 1) It does not allow to generate specific (large) rules, some of which could be more interesting (i.e. ?with higher confidence?).

2) It extends a candidate CAR satisfying the support threshold until this CAR also satisfies the confidence threshold, which implies that many branches of the CARs search space could be explored in vain.

In this paper, we introduce a new pruning strategy to generate the set of CARs. The rules computed by this strategy are specific (large) rules with high confidence. For mining the set of CARs, an appropriate confidence threshold value supported by two propositions, is used. Besides, we propose a new way for ordering the set of CARs based on the size of the CARs and their confidence value. The ordered CARs together with the ?Best K rules? mechanism are integrated in a CAR based classifier (called CAR-IC) which is more accurate than CBA, CMAR, CPAR, TFPC and HARMONY classifiers.

This paper is organized as follows: The next section  describes the related work. The third section introduces our classifier. In the fourth section the experimental results are   DOI 10.1109/MICAI.2010.24     shown. Finally, the conclusions as well as the future work are given in section five.



II. RELATED WORK Several classifiers based on CARs have been developed  [1], [4], [5], [7], [11], [19]. In general, these classifiers can be divided in two groups according to the strategy used for computing the set of CARs: 1) Two Stage classifiers. In a first stage all the CARs satisfying the support and confidence thresholds are mined and later, in a second stage, a classifier is built by selecting a small subset of CARs that fully covers the training set. The classifiers CBA [1], CMAR [7] and MCAR [8] follow this strategy.

2) Integrated classifiers. In these classifiers the subset of CARs is generated directly. The classifiers CPAR, [11], TFPC [4] and HARMONY [19] follow this strategy.

In the literature, regardless of the strategy used for com- puting the set of CARs, there are five main schemes for ordering CARs: a) CSA (Confidence - Support - Antecedent size): The CSA ordering scheme sorts the rules in a descending order according to their confidence. Those CARs that share a common confidence value are sorted in a descending order according to their support, and in case of tie, CSA sorts the rules in ascending order according to the size of their rule antecedent. This scheme has been used by the CBA classifier [1].

b) ACS (Antecedent size - Confidence - Support): The ACS ordering scheme is a variation of CSA. But it takes into account the size of the rule antecedent as first ordering criterion followed by confidence and support.

The classifier TFPC [4] follows this scheme.

c) WRA (Weighted Relative Accuracy): The WRA order- ing scheme, proposed in [13], assigns to each CAR a weight and then sorts the set of CARs in a descending order according the assigned weights. The WRA has been used to order CARs in two versions of the TFPC classifier [9], [10]. Given a rule A ? B the WRA is computed as follows:  WRA(A ? B) = Sup(A)(Conf(A ? B)?Sup(B)) d) LAP (Laplace Expected Error Estimate): The LAP ordering scheme was introduced by Clark and Boswell [14] and it has been used to order CARs in CPAR classifier [11]. Given a rule A ? B, in [11] the LAP is defined as follows:  LAP (A ? B) = Sup(A ? B) + 1 Sup(A)+ | C |  where C is the set of predefined classes.

e) ?2 (Chi-Square): The ?2 ordering scheme is a well known technique in statistics, which can be used to  determine whether two variables are independent or related. After computing an additive ?2 value for each CAR, this value is used to sort the set of CARs in a descending order in the CMAR classifier [7].

There are three main satisfaction mechanisms reported in the literature [1], [7], [10].

1) Best rule: This mechanism selects the first (?best?) rule in the order that satisfies the transaction to be classified (unseen data), and then the class associated to the selected rule is assigned to this transaction [1].

2) Best K rules: This mechanism selects the best K rules (for each class) that satisfy the transaction to be classified and then the class is determined using these K rules, according to different criteria [10].

3) All rules: This mechanism selects all rules that satisfy the transaction to be classified and then these rules are used to determine the class of the new transaction [7].

Algorithms following the ?Best rule? mechanism could suf- fer biased classification or overfitting since the classification is based on only one rule. On the other hand, the ?All rules? mechanism includes rules with low ranking for classification and this could affect the accuracy of the classifier. Since the ?Best K rules? mechanism has been the most used satisfaction mechanism for CAR based classifiers, and it reports the best results, we will use it in our work.



III. PROPOSED CLASSIFIER  First of all, in subsection III-A, we propose two propo- sitions that support the use of a confidence threshold value equal to 0.5 for mining the CARs. An important part of our classifier is the pruning strategy used during the mining of the set of CARs, thus in subsection III-B we introduce this strategy. In section III-C, we propose a new way for ordering the set of CARs and describe how the pruning strategy and the new ordering are used to define a new CAR based classifier.

A. Determining the confidence threshold  All the CAR based classifiers use different support and confidence thresholds for mining the set of CARs. The threshold values used in those works must be carefully defined by the user, there is not a guideline that helps to the user to choose these values. In this paper, we propose to use a confidence threshold that allows to obtain CARs with different antecedent, avoiding ambiguity at the classification stage.

In order to determine an appropriate confidence threshold  we introduce two propositions. The proposition 1 guarantees that the sum of the confidence values of all CARs having identical antecedent is 1. Later, in proposition 2 we show that only one CAR can have a confidence value greater than 0.5.

Proposition 1: Let X be an itemset and C = {c1, c2, . . . , cm} be the set of predefined classes, the fol- lowing equation is fulfilled:  m?  i=1  Conf(X ? ci) = 1 (4)  The demonstration is immediate from the definitions of itemset support (Eq. 1), CAR support (Eq. 2) and CAR confidence (Eq. 3).

Proposition 2: Let X be an itemset and C =  {c1, c2, . . . , cm} be the set of predefined classes, only one CAR X ? ck (ck ? C) can have a confidence value greater than 0.5.

Since Conf(X ? c) takes values between 0 and 1, the  demonstration is immediate from the proposition 1.

Based on proposition 2, if we set the confidence threshold  to 0.5, for each itemset X we can obtain at most one CAR having X as antecedent, and in this way, ambiguity at the classification stage is avoided. For our experiments, we will use this value as confidence threshold.

B. Mining the set of CARs In order to generate the set of CARs, we follow the  ideas of CA [6], a frequent itemset mining algorithm, which according to the experiments shown in [6], outperforms other efficient algorithms for mining frequent itemsets, like Apriori (used in CBA), Fp-growth (used in CMAR) and TFP (used in TFPC).

In [12], for mining ARs the authors propose partitioning  the itemset space into equivalence classes grouping itemsets of the same size k which have a common (k ? 1)-length prefix. An equivalence class grouping k-itemsets will be denoted as ECk. In our proposal, unlike the algorithm proposed in [12] we consider each predefined class c ? C as another item and we propose to divide the CAR space into equivalence classes defined by the following equivalence relation: ?The CARs of size k that share the consequent (the same class) and the first k ? 2 items of the antecedent (which has k ? 1 items) belong to the same equivalence class?.

Similar to CA [6], in order to compute support values,  we take advantage of bit-to-bit operations by representing the dataset as an m x n binary matrix, being m the number of transactions and n the number of items including the class items.

In the literature, regardless of the used strategy for mining  the CARs, each CAR satisfying the support and confidence thresholds is extended by adding a new item. The extended CAR, called candidate CAR, is searched in the transaction dataset to obtain their support and confidence values. Recent algorithms for mining the set of CARs [4], [9], [10] prune the CAR search space each time a CAR satisfying the support and confidence thresholds is found, it means that CARs satisfying both thresholds are not extended anymore.

This strategy produces more general rules. Besides, these algorithms do not prune the CAR search space when a CAR only satisfies the support threshold; instead of it they continue extending the CAR until it satisfies the confidence threshold. With this pruning strategy, at most one CAR is obtained for each branch of the CARs search space but many branches could be explored in vain.

In order to allow generating more specific rules with high  confidence, we introduce the following pruning strategy.

Let X be an itemset, c be a class and i be an item,  if the candidate CAR X ? c does not satisfy either support threshold or confidence threshold we do not ex- tended the CAR anymore, i.e., we prune the CARs space avoiding to generate candidate CARs from CARs that not satisfy the support and confidence thresholds. Otherwise, if the candidate CAR X ? c satisfies the support and confidence thresholds we follow extending the CAR while Conf(X ? {i} ? c) >= Conf(X ? c), thus allowing to obtain, for each branch, many CARs with high confidence.

C. Ordering and Classifying Once the set of CARs has been generated, the CAR list  is sorted. As it was mentioned earlier, for classifying we use more specific rules with high confidence; therefore, we propose a new way for sorting the set of CARs. First we sort the set of CARs in a descending order according to the size of the CARs and in case of tie, we sort the tied CARs in a descending order according to their confidence.

Algorithm 1 CAR-IC (training phase) Input: training dataset db Output: the classifier 1: Answer ? ? 2: CARs ? Generating CARs(db) 3: Answer ? Ordering CARs(CARs) 4: return Answer  Algorithms 1 and 2 show respectively the pseudo code of the training phase and classification phase of CAR-IC. In the training phase (Alg. 1), theGenerating CARs function computes the set of CARs from the training dataset, and after that, the Ordering CARs function sorts the obtained set of CARs, in the way described above.

Algorithm 2 CAR-IC (classification phase) Input: set of sorted CARs, unseen transaction t Output: the assigned class 1: Answer ? ? 2: BestK ? Select BestK(t) 3: Answer ? Classify(BestK) 4: return Answer  For classifying unseen transactions, we decided to use the     ?Best K rules? satisfaction mechanism, since it has reported the best results [4], [7], [10].

In the classification phase (Alg. 2), to classify an unseen  transaction t, for each class, the ?Best K rules? (we used K=5 as in the other evaluated classifiers) covering t are selected using the Select BestK function. Later, the class having the greatest average of the confidence values of their selected K rules, is assigned to t (Classify function). If there is a tie, one of the tied classes is randomly assigned.

If no rule covers t, the default class is assigned (in our algorithm, we use the majority class as default class).



IV. EXPERIMENTAL RESULTS In this section, we report some experimental results com-  paring our classifier (CAR-IC) against the best classifiers based on CARs reported in the literature: CBA [1], CMAR [7], CPAR [11], TFPC [4] and HARMONY [19]. Other good classifiers like RCBT [22] and DDPMine [23] were not included in the experiments because the authors of these works did not provide their programs, and from the description of the algorithms given in their papers it is impossible to implement them. Moreover, the first one was evaluated using only four gene expression datasets (which were not provided by the authors either) and the second one was evaluated using only 8 unusual datasets from the UCI repository.

Table I DATASET CHARACTERISTICS.

Dataset # instances # items # classes adult 48842 97 2 anneal 898 73 6 ecoli 336 34 8 flare 1389 39 9 glass 214 48 7 heart 303 52 5 hepatitis 155 56 2 horseColic 368 85 2 iris 150 19 3 led7 3200 24 10  letRecog 20000 106 26 mushroom 8124 90 2 pageBlocks 5473 46 5 pima 768 38 2  waveform 5000 101 3  The first four classifiers were downloaded from the Frans Coenen?s homepage (http://www.csc.liv.ac.uk/?frans); and for HARMONY, we used the accuracy values reported in [19] since the program was not provided by the authors and it is impossible to implement the classifier from the paper.

Experiments were done using ten-fold cross-validation, re- porting the average accuracy of the ten folds. Our tests were performed on a PC with an Intel Core 2 Duo at 1.86 GHz CPU with 1 GB DDR2 RAM, running Windows XP SP2.

As in other works [1], [4], [7], [11], experiments were  conducted using several datasets, 15 in our case (see char- acteristics in Table I). The chosen datasets were originally  Table II CLASSIFICATION ACCURACY.

Dataset CBA CMAR CPAR TFPC HARMONY CAR-IC adult 84.21 79.72 77.24 80.79 81.90 82.11 anneal 94.65 89.09 94.99 88.28 91.51 91.80 ecoli 83.17 77.01 80.59 58.53 63.60 82.06 flare 84.23 83.30 64.75 84.30 75.02 85.98 glass 68.30 74.37 64.10 64.09 49.80 68.12 heart 57.33 55.36 55.03 51.42 56.46 53.21 hepatitis 57.83 81.16 74.34 81.16 83.16 84.56 horseColic 79.24 80.06 81.57 79.06 82.53 82.47 iris 94.00 92.33 94.70 95.33 93.32 96.06 led7 66.56 72.31 71.38 68.71 74.56 72.71  letRecog 28.64 26.25 28.13 27.57 76.81 73.14 mushroom 46.73 100.00 98.52 99.03 99.94 98.54 pageBlocks 90.94 87.98 92.54 89.98 91.60 91.82 pima 75.03 72.85 74.82 74.36 72.34 75.23  waveform 77.58 72.22 70.66 66.74 80.46 73.06 Average 72.56 76.27 74.89 73.96 78.20 80.72  Table III RANKING POSITION BASED ON ACCURACY.

Dataset CBA CMAR CPAR TFPC HARMONY CAR-IC adult 1 5 6 4 3 2 anneal 2 5 1 6 4 3 ecoli 1 4 3 6 5 2 flare 3 4 6 2 5 1 glass 2 1 4 5 6 3 heart 1 3 4 6 2 5 hepatitis 5 3 4 3 2 1 horseColic 5 4 3 6 2 1 iris 4 6 3 2 5 1 led7 6 3 4 5 1 2  letRecog 3 6 4 5 1 2 mushroom 6 1 5 3 2 4 pageBlocks 4 6 1 5 3 2 pima 2 5 3 4 6 1  waveform 2 4 5 6 1 3 Average 3.13 4.00 3.73 4.53 3.13 2.27  taken from the UCI Machine Learning Repository [15], and their numerical attributes were discretized by the author of [20] using the LUCS-KDD discretized/normalized ARM and CARM Data Library. The discretization technique used in LUCS-KDD is different from those used in [1], [7], [11]; thus, the performance reported in tables II and III may be different from those reported in previous studies, even for the same classifier and the same dataset [21].

For CBA, CMAR, CPAR and TFPC classifiers we used  the support threshold set to 0.01 and the confidence thresh- old set to 0.5, as their authors suggest. In [19], the authors of HARMONY obtained the best results using a support threshold of 0.5. In our classifier CAR-IC we used the confidence threshold also set to 0.5, based on our previous analysis (see section III-A).

Table II shows the accuracy of all tested classifiers on the  15 datasets and Table III shows the ranking position obtained by each classifier according to its accuracy value. Analyzing these tables, we can see that CBA has the worst performance in average accuracy while it has a good performance in     average ranking; this is because, although CBA has low accuracy values for some datasets, it reaches the first place in 3 of the tested datasets and the second place in other 4 datasets. On the contrary, CMAR has a good average accuracy but a poor average ranking w.r.t. the other evaluated classifier.

Our proposed classifier, CAR-IC, has the best average  classification accuracy outperforming all other classifiers and having in average a difference in accuracy of more than 2.5% w.r.t. the classifier in the second place. Additionally, CAR-IC has the best average ranking position w.r.t. the other evaluated classifier. The classifier with the second best performance was HARMONY, which was the second best in average accuracy as well as in average ranking.

Although the original implementations of CBA, CMAR  and CPAR use different discretization/normalization tech- niques, we consider interesting to show, in Table IV, a comparison of the accuracies obtained by our classifier, CAR-IC, against the best reported accuracies of all the evaluated classifiers. In the case of HARMONY, the authors did not report which technique was used for discretiza- tion/normalization.

Despite the discretization/normalization technique is not  the same, CAR-IC obtains the best average accuracy being 1.09% better than the second best.



V. CONCLUSION In this paper, we have proposed an accurate classifier  based on CARs. This classifier, called CAR-IC, introduces a new pruning strategy for obtaining specific rules with high confidence; moreover, we propose two propositions that support the use of a confidence threshold equal to 0.5 for mining the rules. Besides, we propose a new way for ordering the set of CARs using the size of the CARs and their confidence value. The experimental results show that CAR-IC has better performance than CBA, CMAR, CPAR, TFPC and HARMONY classifiers. In general, CAR-IC has the best average classification accuracy as well as the best average ranking.

As future work, we are going to study the problem of  producing rules with multiple labels, it means rules with multiple classes in the consequent. For this, we will study the use of confidence thresholds smaller than 0.5 allowing to obtain more than one rule with the same antecedent.

