An Algorithm of Complementation Mining Frequent  Neighboring Class Set

Abstract?This paper addresses character that present frequent neighboring class set mining algorithms is inefficient to extract long frequent neighboring class set, and proposes an algorithm of complementation mining frequent neighboring class set. This algorithm is suitable for mining any frequent neighboring class set in large spatial data through using top-down search and complementation mining strategy, and it builds digital database of neighboring class set via neighboring class weight sets. The algorithm generates candidate frequent neighboring class set via top-down and complementation search strategy, namely, it gains candidate frequent item set not only by computing k-subset of (k+1)-non frequent neighboring class set but also by computing their complementary sets. The mining algorithm computes support of candidate frequent neighboring class set by digit logical ?and? operation. The algorithm improves mining efficiency through these methods. The result of experiment indicates that the algorithm is faster and more efficient than present algorithms when mining frequent neighboring class set in large spatial data.

Keywords- spatial data mining; neighboring class set; top-down strategy; weight sets; complementation mining

I. INTRODUCTION As we all know, geographic Information Databases is an  important style of spatial database in spatial data mining, mining spatial association rules from Geographic Information Databases is one important part of spatial data mining and knowledge discovery (SDMKD), which is also known as spatial co-location pattern as in [1]. Spatial co-location pattern are some implicit rules expressing construct and association of spatial objects in Geographic Information Databases, and also expressing hierarchy and correlation of different subsets of spatial association or spatial data in Geographic Information Databases as in [2]. Nowadays, in research of spatial data mining, there are mainly three kinds approaches of mining spatial association rules as in [3], such as, layer covered based on clustering as in [3], mining approach based on spatial transaction as in [2, 4, 5, and 6] and mining approach based on non-spatial transaction as in [3]. The first two methods may be also used to mining frequent neighboring class set, The spatial association as in [4, 5, and 6] is quite single, because they only express spatial association among these objects which are all close to objective. However, neighboring class set expresses another spatial association among these objects which are close to each other. MFNCS as in [2] uses the similar method of  Apriori to search frequent neighboring class set, which gains some right instance of (k+1)-neighboring class set only through connecting right instance of k-neighboring class set according to down-top strategy, and so this algorithm is only suitable for mining short frequent neighboring class set according to character of Apriori. Hence, this paper proposes an algorithm of complementation mining frequent neighboring class set, denoted by CMFNCS, which is suitable for mining any frequent neighboring class set according to top-down search and complementation mining strategy.



II. PRELIMINARY KNOWLEDGE According to these representations as in [2], every object in  spatial domain forms a spatial data set, which is expressed as this data structure, denoted by <Object Identify, Class Identify and Spatial Location>. Here, identify of different class in spatial data set is denoted by Class Identify, identify of different object instance in the same class is denoted by Object Identify, location coordinate of object is denoted by Spatial Location. We regard an object as an instance of corresponding class, and so spatial data set is made up of these instances of spatial Class Identify. Sets of Class Identify are thought as spatial class set, denoted by C = {C1, C2?Cm}, means there are m different classes.

A. Definition and Property Definition 1 Neighboring Class Set, it is a subset of spatial  class set in spatial data set, which is expressed as {Ct1, Ct2?Ctk} (tk  m), denoted by NCS.

Let I = {it1, it2?itk} be an instance of neighboring class set denoted by NCS = {Ct1, Ct2?Ctk}, here, itj is an instance of Ctj (j?1, 2?k).

Example, let {V, X, Y, Z} be a NCS, and I = {V5, X9, Y7, Z1} is an instance of NCS.

Definition 2 Neighboring Class Universal Set, it includes all these spatial classes in spatial data set, namely, it is a spatial class set, which is expressed as {C1, C2?Cm}, denoted by NCUS.

Definition 3 Neighboring Class Set Length, its value is equal to the sum of class set contained in neighboring class set.

If the length of NCS is equal to k, it is denoted by k-NCS.

Example, let {V, X, Y, Z} be a NCS, and its length is 4.

This work was fully supported by science and technology research  projects of Chongqing Education Commission (Project No. KJ091108), and it was also fully supported by science and technology research projects of Chongqing Three Gorges University (Project No. 10QN-22, 24 and 30).

Definition 4 Right Instance of Neighboring Class Set, let I = {it1, it2?itk} be an instance of NCS, if ?  ip and iq (ip, iq ?I), and distance (ip, iq)  d, and then we think I be an right instance of NCS. Here, d is the minimal distance used by deciding two spatial objects are close to each other, Euclidean distance is expressed as distance (ip, iq).

Definition 5 Neighboring Class Set Support, it is equal to the sum of right instance of neighboring class set, which is denoted by support (NCS).

Definition 6 Frequent Neighboring Class Set, its support is not less than the minimal support given by user.

Property 1 Let k-NCS is not frequent neighboring class sets, and (k+1)-NCS is not also frequent neighboring class set.

Here, k-NCS ?  (k+1)-NCS.

Property 2 Let (k+1)-NCS is frequent neighboring class set, and k-NCS is also frequent neighboring class set. Here, k- NCS ? (k+1)-NCS.

B. Problem Description According to above definition, property and these  representations as in [2], we describe frequent neighboring class set mining as follows:  Input:  (1) Class set is denoted by C = {C1, C2?Cm}, instance set is denoted by I = {i1 i2 ?, in}, each ik (ik? I) is expressed as above defined data structure.

(2) Minimal distance is denoted by d.

(3) Minimal support is denoted by s.

Output:  Frequent neighboring class set.



III. THE ALGORITHM OF COMPLEMENTATION MINING FREQUENT NEIGHBORING CLASS SET  A. Building  Digital NCS Database In order to build digital database of neighboring class set,  we need find corresponding NCS of every right instance, and turn NCS into a digit. The course of turning used by the algorithm is expressed as follows:  Step1, we define the order of class as C = {C1, C2? Cj?Cm}, and so each class existing in right instance has order denoted by Noj.

Step2, and then, we use weight denote every class, if the order of this class is denoted by Noj, and then  12 ?jNo  is defined as weight of class.

Step3, we let { 112 ?No , 122 ?No ... 12 ?LNo } be defined as weight sets, here L is equal to Neighboring Class Set Length, and namely, it is the sum of class existing in this neighboring  class set. We compute this expression =  ? L  j  Noj   12  to gain a digit.

We use this method to gain a digit by scanning every right instance and this digit also expresses this corresponding NCS of right instance.

In order to save these digits, we use this data structure expressed as follows:  Structure neighboring class set {  Int Digit; // saving digit expressed as NCS of right instance  Int Count; // saving the sum of same digit expressed as NCS} NCS  Example, here class set is expressed as C = {V, W, X, Y, Z}, the first three right instances are express as I1 = {W1, X3, Y4, Z2}, I2 = {V3, W4, Y5}, I3 = {W2, X4, Y2, Z1}.

Now we use the above introductive method to build digital NCS database, this course is expressed as follows:  NCS of I1 is expressed as {W, X, Y, Z}, and these orders are expressed as {2, 3, 4, 5}, weight sets is expressed as {2(2-1), 2(3-1), 2(4-1), 2(5-1)}, and then, we will gain this digit 30, NCS [0].

Digit =30, NCS [0].Count=1.

NCS of I2 is expressed as {V, W, Y}, and these orders are expressed as {1, 2, 4}, weight sets is expressed as {2(1-1), 2(2-1), 2(4-1)}, and then, we will gain this digit 11, NCS [1]. Digit =11, NCS [1].Count=1.

NCS of I3 is expressed as {W, X, Y, Z}, weight sets is expressed as {2(2-1), 2(3-1), 2(4-1), 2(5-1)}, and then, we will gain this digit 30, because NCS [0] has already saved this information, and only NCS [0].Count= NCS [0].Count +1=2.

...

B. The method of generating candidate frequent NCS The algorithm generates candidate frequent neighboring  class set according to top-down search and complementation mining strategy. Hence, this course includes two parts as follows:  Firstly, it uses digit logical operation to generate k- candidate frequent item sets of digit formed by (k+1)-digit non- frequent item sets. The algorithm adopts search strategy which is similar to B_UDMA as in [7] and ITDASN as in [8].

Secondly, it uses computing complementary set of above k- candidate frequent item sets to generate candidate frequent item sets of digit. The strategy is similar to ATCM as in [4].

The example of generating is expressed as follows:  Suppose spatial class set is expressed as C = {V, W, X, Y, Z}, and it is Neighboring Class Universal Set according to definition 2, here it is not frequent neighboring class set.

Step1, the algorithm generates 4-candidate frequent item sets according to top-down search strategy, and then computes their support.

There are three non-frequent neighboring class sets as follows:  (1)NFNCS1= 2(1-1) + 2(2-1) + 2(3-1) + 2(5-1) = 23;    (2)NFNCS2= 2(1-1) + 2(2-1) + 2(3-1) +2(4-1) = 15;  (3)NFNCS3= 2(1-1) + 2(3-1) + 2(4-1) + 2(5-1) = 29;  There are two frequent neighboring class sets as follows:  (1)FNCS1= 2(2-1) + 2(3-1) + 2(4-1) + 2(5-1) = 30;  (2)FNCS2= 2(1-1) + 2(2-1) +2(4-1) + 2(5-1) = 27;  Step2, the algorithm computes these complementary sets of 4-candidate frequent item sets according to complementation mining strategy as follows:  Complementary set of NFNCS1 is 2(4-1) = 8;  Complementary set of NFNCS2 is 2(5-1) = 16;  Complementary set of NFNCS3 is 2(2-1) = 2;  Complementary set of FNCS1 is 2(1-1) = 1;  Complementary set of FNCS2 is 2(3-1) = 4;  These complementary sets are all subsets of frequent neighboring class sets FNCS1 or FNCS2, and so the algorithm need not compute their support.

Step3, the algorithm generates 3-candidate frequent neighboring class set, and gains frequent neighboring class set as step1.

Step4, the algorithm computes these complementary sets of 3-candidate frequent item sets as step2.

?  C. The method of computing support Accordingly to chapter A of III and digit logical operation,  we may gain this property as follows:  Property Let digit p and q express two right instances, let Cp be a NCS denoted by p, let Cq be a NCS denoted by q, then Cp ? Cq ? p & q=p.

This algorithm uses logical operation to compute support according to this property. The process is expressed as follows:  Suppose class set is expressed as C = {V, W, X, Y, Z}, there are 5 instances of neighboring class sets as follows:  No.1 right instance of NCS = {V1, X2, Y3, Z4};  No.2 right instance of NCS = {V2, X3, Z5};  No.3 right instance of NCS = {W1, X5, Y4, Z3};  No.4 right instance of NCS = {V5, Y2, Z2};  No.5 right instance of NCS = {X4, Y3, Z1};  Their digits denoting neighboring class set of right instance are expressed as {29, 21, 30, 25 and 28}. Suppose a candidate is 24 which is denoted by C-NCS = {Y, Z}, and then 29&24 =24, 21&24 ? 24, 30&24=24, 25&24=24, 28&24=24. Because of this, we write 4 to support (C-NCS).

D. The process of mining frequent neighboring class set Input:  (1) Spatial class set is denoted by C = {C1 C2 ? Cm}.

(2) Instance set is denoted by I = {i1 i2 ? in}.

(3) The minimal distance is denoted by d.

(4) The minimal support is denoted by s.

Output: Frequent neighboring class set.

Step1, firstly, the algorithm computes the entire right instances denoted by I? from instance set in spatial database as I by the minimal distance as d.

Step2, and then, building neighboring class set database as NCS after scanning once right instance set as I? via the introductory method in chapter A of III.

Step3, using top-down search and complementation mining strategy to generate candidate frequent NCS according to this chapter B of III. Namely, there are two parts as follows:  Firstly, if a k-candidate is not subset of FNCS which saves frequent neighboring class set, and it is not superset of NFNCS which saves non-frequent neighboring class set, the algorithm will compute its support. If it is frequent neighboring class set, and it is written to FNCS, otherwise, the algorithm will compute (k-1)-subset of k-candidate.

Secondly, the algorithm computes complementary set of k- candidate, if the complementary set is not subset of FNCS, and it is not superset of NFNCS, the algorithm will compute its support. If it is frequent neighboring class set, and it is written to FNCS, otherwise, it will be written to NFNCS.

Step4, repeating to execute step3 until search all frequent neighboring class set.

Step5, the algorithm outputs FNCS according to chapter A of III.



IV. THE ANALYSIS AND COMPARING OF CAPABILITY At present, there are very little documents of research  frequent neighboring class set. Here we compare MFNCS as [2] with CMFNCS which is introduced by this paper as follows:  A. The Analysis of Capability MFNCS: this algorithm uses idea of Apriori to find  frequent neighboring class set, which is made of three stages, firstly, computing all the frequent 1-NCS, secondly, generating all the 2-NCS by range query, and generating all the k-NCS (k>2) by iteration. The algorithm has some repeated computing and superfluous candidate frequent neighboring class set, which gains some right instance of (k+1)-neighboring class set only through connecting right instance of k-neighboring class set. As this algorithm adopts down-top strategy to generate candidate frequent neighboring class set, it is only suitable for mining short frequent neighboring class set.

CMFNCS: this algorithm uses top-down search and complementation mining strategy, which is suitable for mining not only short frequent neighboring class set, but also long frequent neighboring class set. The algorithm gains candidate frequent item sets not only by computing k-subset of (k+1)-non    frequent neighboring class set, but also by computing their complementary set. The mining algorithm computes support of candidate frequent neighboring class set by digit logical ?and? operation. The algorithm improves mining efficiency through these methods.

B. The comparing of experimental result Now we use experiment to testify above analyses and  comparison. Two mining algorithms are used to generate frequent neighboring class set from 12267 right instances, whose class sets are expressed as digit from 3 to 8191, neighboring class set does not include any simple class, namely, it has two classes at least, the number of spatial class set is denoted by m=13, the number of right instance included by these neighboring class set observe the discipline which is expressed as follows:  NCS of digit denoted by 8191 has one right instance.

NCS of digit denoted by 8190 has two right instances.

NCS of digit denoted by 8189 has one right instance.

NCS of digit denoted by 8188 has two right instances.

?  Our experimental circumstances are expressed as follow: Intel(R) Celeron(R) M CPU 420  1.60 GHz, 1GB, language of the procedure is Visual C# 2005.NET, OS is Windows XP Professional.

The experimental result of two algorithms is expressed as Fig.1, where support is absolute. The runtime of two algorithms is expressed as Fig.2 as support and length of neighboring class set change.

Figure 1. The experimental result  Figure 2. The comparing of runtime

V. CONCLUSION This paper proposes an algorithm of complementation  mining frequent neighboring class set, which is suitable for mining any frequent neighboring class set according to top- down search and complementation mining strategy. The result of experiment indicates that the algorithm is faster and more efficient than present algorithms when mining frequent neighboring class set in large spatial data.

