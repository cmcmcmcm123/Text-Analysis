Zero-Knowledge Private Graph Summarization Maryam Shoaran

Abstract?Graphs have become increasingly popular for mod- eling data in a wide variety of applications, and graph sum- marization is a useful technique to analyze information from large graphs. Privacy preserving mechanisms are vital to pro- tect the privacy of individuals or institutions when releasing aggregate numbers, such as those in graph summarization. We propose privacy-aware release of graph summarization using zero-knowledge privacy (ZKP), a recently proposed privacy framework that is more effective than differential privacy (DP) for graph and social network databases. We first define group- based graph summaries. Next, we present techniques to compute the parameters required to design ZKP methods for each type of aggregate data. Then, we present an approach to achieve ZKP for probabilistic graphs.



I. INTRODUCTION  Nowadays, the graphs of many real world datasets are very large. For example, Facebook, the most well-known social net- work, contains data for over 900 million users and their rela- tionships. Therefore, effective summarization methods need to be employed in order to make the analysis of such large graphs possible. We focus on graph summarization based on attribute groups. For instance, the nodes of a social graph can be grouped by attributes age and profession, and statistics about the number of cross-group edges can be recorded. Statistics can reveal interesting facts about a graph. For instance, they could show surprising strong connections between groups of people in different age and profession groups. As such, group- based graph summarization (GGS) is a ubiquitous operation in virtually all the graph/social network software products (cf.

[4], [3], [1], [2], etc) The result of GGS is a smaller summary graph, where each node summarizes a group of nodes in the original graph.

The problem is that, as with other aggregations, the sum- mary graphs are often released to other parties for further research purposes, and this brings up the matter of privacy.

Aggregate data included in a summary graph can reveal sensitive and private information about the nodes (participants) of the underlying graph (network).

Privacy-preserving data release has become one of the most important problems today. ?-Differential Privacy [13], [11], [12] (DP for short) has been one of the leading privacy mech- anisms in recent years. DP provides privacy for an individual of interest (IOI) by adding random noise to numerical outputs.

Some recent studies (cf. [14], [20]), however, have high- lighted situations in which DP might not provide sufficient privacy protection. This is especially pronounced in social net- works where different types of auxiliary information, including  the structure of network or the groups the individuals belong in, are often readily available to the public (cf. [14]).

More specifically, whereas the goal of DP is to protect the participation of an individual (or relationship) in a dataset, in social networks we also need to protect the evidence of participation (cf. [20]). To see this we present the following example. Suppose there are two groups g1 and g2 and we want to publish the number of edges between them. Bob, a member of g1, has an edge to Alice, a member of g2. As a consequence of this connection, some friends of Bob introduce edges to Alice. What we want to protect is Bob?s edge to Alice. DP works in this case by ensuring that for any true answer, c or c ? 1, the sanitized answer would be pretty much the same.

However, this is not strong enough; the existence of Bob?s edge changes the true answer not just by 1, but by a bigger number as it causes more edges to be created between the two groups.

Going beyond differential privacy, Gehrke, Lui, and Pass proposed ?zero-knowledge privacy? (ZKP) in [14], which provides stronger privacy, especially for social graphs. The definition of ZKP is based on classes of aggregate functions.

ZKP guarantees that an attacker cannot discover any personal information more than what can be inferred from some ag- gregate on a sample of a database with IOI removed. The sample complexity defines the level of privacy tolerance in ZKP. For instance, suppose in the Bob?s example above the network size is 10000 and the sample size is  ? 10000 = 100.

With such a sampling rate of 0.01 the evidence provided by say 10 more edges caused by Bob?s edge will essentially be protected; with a high probability, none of these 10 edges will be in the sample.

In this paper, we use ZKP to provide individual privacy in graph summarization. We address connection measures for groups in social graphs and present ZKP mechanisms for private release of such aggregate outputs. To the best of our knowledge, we are the first to use ZKP for group-based graph summarizations, which are ubiquitous in analyzing social graphs.

As ZKP inherently depends on the precise characterization of sample complexity, we propose methods to compute the sample complexity of our aggregate functions. In order to achieve this, we present techniques to express the aggregate functions as averages of specially designed, synthetic attributes on the nodes of the graph. Then we derive precise prescriptions on how to construct ZKP mechanisms for the aggregate functions we consider.

More specifically, our contributions in this paper are as follows.

? We define group connection measures for graph sum- marization and consider different scenarios for zero- knowledge private release of summary graphs based on the type of personal information that is to be protected.

We introduce synthetic attributes that simplify the con- struction and analysis of ZKP-mechanisms for graph summarization.

? We present detailed examples and numeric evaluations of our ZKP mechanisms in terms of the parameters involved.

These evaluations are valid for any case and illustrate the trade offs involved when building ZKP mechanisms for graph summarization.

? We also present summarization measures for probabilistic graphs. This is especially important in social networks having edges of different influence captured by probabil- ities assigned on the edges.



II. RELATED WORK  Graph summarization is a ubiquitous method for analyzing large graphs. Virtually all the graph/social network products (cf. [4], [3], [1], [2], etc) create summaries in the form of smaller graphs by grouping the nodes based on attributes.

The common goal of privacy preserving methods is to learn from data while protecting sensitive information of the individuals. k-anonymity for social graphs (cf. [23], [8], [9]) provides privacy by ensuring that combinations of identifying attributes appear at least k times in the dataset. The prob- lem with k-anonymity and other related approaches, e.g. l- diversity [24], is that they assume the adversary has limited auxiliary knowledge. Narayanan and Shmatikov [26] present a de-anonymization algorithm and claim that k-anonymity can defeated by their method using auxiliary data accessible by the adversary.

Among a multitude of different techniques, differential pri- vacy (DP) [6], [10], [13], [11] has become one of the leading methods to provide individual privacy. Various differentially private algorithms have since been developed for different domains, including social networks [17], [27]. However as already shown, DP can suffer in social networks where specific auxiliary information, such as graph structures and friendship data, is easily available to the adversary. Important works showing the shortcomings of DP are [20], [21].

Gehrke, Lui, and Pass in [14] present the notion of zero- knowledge privacy which is appealing for achieving privacy in social networks. Zero-knowledge privacy (ZKP) guarantees that what can be learned from a dataset including an indi- vidual is not more than what is learned from sampling-based aggregates computed on the dataset without that individual.



III. GRAPH SUMMARIZATION  We denote a graph as G = (V,E), where V is the set of nodes and E ? V ?V is the set of edges connecting the nodes.

We consider S ? 2V to be a set of disjoint node groups of  g? g? ?  (a)  g? g? ? ( .75,.33,1)  (b)  .4 .6  Fig. 1. A graph and its summarization.

size r or more that a social network wants to release statistics about.

Definition 1: The S-graph of G is GG,S = (S, ES), where  ES = {(g?, g??) : g?, g?? ? S and ?v? ? g? and ?v?? ? g?? such that (v?, v??) ? E}.

This definition says that two groups g? and g?? in S are connected through an edge in GG,S if there exists at least one edge in G that connects a node in g? to a node in g??.

Definition 2: The S-graph summarization (S-GS) is a func- tion  w1 : S ?? [0, 1] w2 : ES ?? [0, 1]? [0, 1]? [0, 1]  w1(g) = |g| |V |  w2(g ?, g??) = (x, y, z), where  x = |{v? ? g? : ?v?? ? g??, s.t. (v?, v??) ? E}|  |g?| z =  |{v?? ? g?? : ?v? ? g?, s.t. (v?, v??) ? E}| |g??|  y = |{(v?, v??) : v? ? g?, v?? ? g??, (v?, v??) ? E}|  |g?| ? |g??| .

Throughout the paper, we will refer to the elements of w2 as w2(g?, g??)[x], w2(g?, g??)[y], and w2(g?, g??)[z], or w2[x], w2[y], w2[z], whenever g? and g?? are clear from the context.

We will also use w2[.] to refer to any of three elements x, y, or z.

Example 1: Fig.1 (a) shows a simple graph G, and S consisting of two groups g? and g??. Group g? has four nodes and group g?? has six nodes. There are several edges (eight of them) connecting members of g? to members of g??.

Fig.1 (b) shows how g? and g?? are represented by a node each in GG,S . The nodes and the edge connecting them in GG,S are labeled by w1 and w2 measures, respectively, as described above. Specifically, we have w1(g?) = 44+6 = .4 and w1(g  ??) = 64+6 = .6. Since three out of four nodes in g ? and  all the six nodes in g?? are connected with some nodes in the other group, we have w2(g?, g??) = ( 34 ,  4?6 ,  6 ) = (.75, .33, 1).



IV. BACKGROUND ON ?-ZERO-KNOWLEDGE PRIVACY  Zero-Knowledge Privacy (ZKP) introduced by [14] is a privacy framework that is stronger than Differential Privacy (DP). ZKP is especially desirable in social networks where we need to protect not only the participation of a connection, but also easy to find evidence of the participation, as for example, the evidence given by other connections that were influenced by the connection.

ZKP is defined in relation with classes of sampling-based aggregate information. The class of sampling-based aggre- gation represents our tolerance for information release. For example, we can say that we are only comfortable to release the average age of a population computed on a  ? n random  sample. ZKP uses the notion of a simulator from zero- knowledge, and says that a simulator with the acceptable aggregate information can essentially compute whatever an adversary can compute by accessing the result of the mecha- nism ([14]). We describe ZKP in the following using a setting of graphs.

Let G be a graph. We denote by G?? a graph obtained from G by removing a piece of information (for example an edge).

G and G?? are called neighboring graphs.

Let San be a mechanism that operates on a graph G (the complete database), and computes a sanitized answer to a query. The adversary?s goal in a privacy scenario is to gain information about private matters of individuals (nodes) or connections (edges) in G using this released sanitized answer.

Let Adv(San(G), z) denote the output of the algorithm that an adversary employs to breach privacy. The adversary can interact with mechanism San and may have access to some auxiliary information z. The information in z is considered to be general, and easily accessible, e.g. information about the structure of the network (graph) or the groups that individuals belong in.

Let agg be a class of randomized algorithms that first select k = k(n) random samples (nodes) without replacement from G??, and then compute some aggregate information. Such algorithms output an approximate answer to the query.

Let Sim, ?the simulator,? be an algorithm. We denote by Sim(T (G??), z) the information that the simulator can com- pute given the aggregate information computed by a T ? aggk.

In plain language, imagine Sim to be a person who can be ?extremely smart and capable (ESC)? and who has access to aggregates computed by the algorithms of class agg on the database where the sensitive information has been removed.

Also, assume that the simulator also has access to background information z.

On the other hand, imagine the adversary to be a person who is also ESC, and has access to San(G) as well as background information z. ZKP assures an individual that the participation in the network does not jeopardize her/his privacy. ZKP provides this guarantee by sanitizing the query answers such that the information that the adversary could extract from the output (sanitized answer) is computationally indistinguishable  from the information that could be computed using sampling- based aggregates calculated on the network data that misses the individual of interests?s sensitive information. That is, the adversary is not better off than some simulator even though he has access to the output of mechanism San computed on the whole database.

Definition 3: (Zero-Knowledge Privacy [14]) The mecha- nism San is ?-zero-knowledge private with respect to agg if there exists a T ? agg such that for every adversary Adv, there exists a simulator Sim such that for every G, every z ? {0, 1}?, and every W ? {0, 1}?, the following hold: Pr[Adv(San(G), z) ?W ] ? e? ? Pr[Sim(T (G??), z) ?W ] Pr[Sim(T (G??), z) ?W ] ? e? ? Pr[Adv(San(G), z) ?W ] where probabilities are taken over the randomness of San and Adv, and T and Sim.

By this definition, ZKP guarantees that any additional information that an adversary can obtain about an individual by having access to the output of the mechanism is virtually not more than what can be computed by a simulator using some sampling-based (approximate) aggregates even without access to the mechanism and the sensitive data.

Note that the selection of k ? the number of random samples ? in agg algorithms is very important and it should be chosen so that with high probability very few of the nodes connected with the node whose information has to be private will be chosen. We will often index agg by k as aggk to stress the importance of k. To satisfy the ZKP definition, a mechanism should use k = o(n), say k =  ? n or k = 3  ? n2, where n,  the number of nodes in the database, is sufficiently large (see [14]). DP is a special case of ZKP where k = n.

As it will be illustrated in the upcoming sections, the specifications of algorithm T ? agg, e.g. sample size, are only used to compute the level of privacy needed in the ZKP mechanism. We stress that the ZKP mechanism is the only algorithm applied on the data. The simulator is only an abstract notion.

Achieving ZKP. Let f : G ? Rm be a function that produces a vector of length m from a graph database. For example, given GG,S , f produces the results of the S-GS functions, i.e. w on edges.

We consider the L1-Sensitivity to be defined as follows.

Definition 4: (L1-Sensitivity) For f : G ? Rm, the L1-  sensitivity of f is  ?(f) = max G?,G??  ||f(G?)? f(G??)||1  for all neighboring graphs G? and G??.

Another essential definition is that of ?sample complexity?.

Definition 5: (Sample Complexity [14]) A function f :  Dom ? Rm is said to have (?, ?)-sample complexity with respect to agg if there exists an algorithm T ? agg such that for every D ? Dom we have  Pr[||T (D)? f(D)||1 ? ?] ? 1? ?.

T is said to be a (?, ?)-sampler for f with respect to agg.

This definition bounds the probability of error between the randomized computation (approximation) of function f and the expected output of f . Basically, functions with low sample complexity (smaller ? and ?) can be computed more accurately using random samples from the input data.

When the released information, as typical, is real numbers, the ZKP mechanism San achieves the privacy by adding noise to each of the numbers independently.

Let Lap(?) be the zero-mean Laplace distribution with scale ?, and variance 2?2. The scale of Laplace noise in ZKP is properly calibrated to the sample complexity of the function that is to be privately computed. The following proposition expresses the relationship between the sample complexity of a function and the level of zero knowledge privacy achieved by adding Laplace noise to the outputs of the function.

Proposition 1: ([14]) Suppose f : G ? [a, b]m has (?, ?)- sample complexity with respect to agg. Then, mechanism  San(G) = f(G) + (X1, . . . , Xm),  where G ? G, and Xj ? Lap(?) for j = 1, ? ? ? ,m independently, is  ln ( (1? ?)e?(f)+?? + ?e (b?a)m?  ) ?ZKP with respect to agg.



V. ZKP MECHANISM FOR GRAPH SUMMARIZATION  In this section we design a ZKP mechanism to release a graph summarization. Let GG,S = (S, ES) be the S-graph for a graph G. Let f be a function that takes a graph GG,S as input and produces c = |S|+3 ? |ES | numbers, or differently said, a c-dimensional vector corresponding to w1 and w2 aggregates for the groups and the connecting edges.

Let f = [f1, . . . , ft] be the vector (or subvector) that is to be privately released. We apply a separate Sani (ZKP) mechanism, for i ? [1, t], to each of the elements of f . Let us assume that each Sani provides ?i-ZKP for fi with respect to aggki , where ki = k(n)/t and n = |V |. Then, based on the following proposition, f will be (  ?t i=1 ?i)-ZKP with respect  to aggk(n), where k(n) = ?t  i=1 ki.

Proposition 2: (Sequential Composition [14]) Suppose  Sani, for i ? [1, n], is an ?i-ZKP mechanism with respect to aggki . Then, the mechanism resulting from composing   Sani?s is ( ?n  i=1 ?i)-ZKP with respect to agg( ?  ki).

In this paper, we consider edge (connection) privacy. We  note that node privacy will not be considered in this work, since, as it is widely considered (cf. [17], [20]), it results in too noisy output with practically no utility.

A. Edge (Connection) Privacy  Consider GG,S and GG?e,S , where GG?e,S is a neighboring graph of G obtained from G by removing edge e. In the edge privacy scenario, the total number of nodes (groups) and the size of each group are identical in GG,S and GG?e,S . Therefore,  1A set of computations that are separately applied on one database and each provides ZKP in isolation, also provides ZKP for the set.

the sensitivity of any w1 function, including the ones in f , is zero, that is, ?(w1) = 0.

On the other hand, removing an edge in G can change by at most 1 the numerator of each element x, y, and z in w2 measures of GG?e,S . Note that this change affects only one w2 measure in the whole graph GG?e,S . Therefore, the sensitivities of the elements of any w2 function, including the ones in f , are  ?(w2[x]) = r ?(w2[y]) =  r2 ?(w2[z]) =  r  where r is the minimum group size in S.

In the following sections, the ZKP mechanisms are sepa-  rately designed for w1 and w2 functions in f .

1) ZKP Mechanism for w1: Suppose w1(g) is an element  of f , where g is a group in GG,S . Let San = w1(g)+Lap(?) be a ZKP mechanism which adds random noise selected from Lap(?) distribution to the output of w1(g) in order to achieve ZKP. Our goal here is to come up with the right ? to achieve a predefined level of ZKP.

Based on the definition of ZKP, one should first know the sample complexity of the w1 function. For this, without change in semantics, we will express w1 so that it computes an average rather than a fraction of two counts. Then, using the Hoeffding inequality (cf. [25]) we compute the sample complexity of w1.

Expressing w1. We assume that in addition to the regular node attributes (if any), we have |S| new boolean attributes, one for each possible group. We denote these new attributes by upper- case B?s indexed by the group id. A node v in graph G will have Bg(v) = 1 if v belongs to group g, and Bg(v) = 0, otherwise. We have that,  Proposition 3:  w1(g) =  ? v?V Bg(v) |V | .

Therefore, w1(g) can be viewed as the average value of attribute Bg over all nodes in G.

ZKP Mechanism. Let G = (V,E) be a graph enriched with boolean attributes as explained above. We would like to determine the value of ? > 0 for Lap(?) distribution which is to be used to add random noise to a w1(g) measure included in f . For this, first we compute the sample complexity of w1 to be able to use Proposition 1 and establish an appropriate value for ?.

Let T be a randomized algorithm in aggk, the class of randomized algorithms that operates on an input graph G. To randomly sample a graph G, algorithm T uniformly selects k = k(n)/t random nodes from V , reads their attributes, and retrieves all the edges2 incident to these k sample nodes.3 From the sampled nodes and their incident edges with other sampled nodes we consider GG?,S? = (S ?, E ?S). Then, T approximates the value of w1(g) using GG?,S? . Since we have expressed  2Clearly, only non-dangling incident edges, whose both end nodes have been sampled, will be retrieved.

3For other possible methods of graph sampling see for example [14].

w1(g) for a group g as an average, based on the Hoeffding inequality we have  Pr[|T (g)? w1(g)| ? ?] ? 1? 2e?2k?2 .

From this and Definition 5, we have that w1 has( ?, 2e?2k?  )  -sample complexity with respect to aggk.

Now we make the following substitutions in the formula of  Proposition 1: ? = 2e?2k?  , ?(w1(g)) = 0, b ? a = 1, and m = 1 and obtain that mechanism San is  ln ( e  ? ? + 2e  ??2k?2  ) -ZKP  with respect to aggk.

Similarly to DP, one can set ?, the Laplace noise scale, to  be proportional to ?the error? as measured by the sum of the sensitivity and sampling error, and inversely proportional to the ZKP privacy level  ? = ?(w1) + ?  ? =   ? ? 1  ? k .

Regarding ?, we can consider for instance a sample size k =  ? n2, and have ? = 13?  k .

From all the above, the privacy level obtained will be  ln ( e  ? ? + 2e  ??2k?2  ) = ln  ( e? + 2e?  3? k?2 3?k  ) ? ln  ( e? + 2e?  3? k )  ? ?+ 2e? 3 ? k.

Thus, we have that by adding Lap (   ?? 3?k  ) noise, mechanism  San will be ( ?+ 2e?  3? k )  -ZKP with respect to aggk. Of course, the privacy achieved is in fact better than this because of the above inequalities. We address finding of the exact ? given a ZKP privacy level and sample complexity in Section VII.

Example 2: Let graph G be a social graph with one hundred million participants/nodes (|V | = n = 100, 000, 000), and g?, g?? be two groups. Suppose the requested output vector is  f = ?w1(g?), w2(g?, g??)[x], w2(g?, g??)[y], w2(g?, g??)[z], w1(g??)?.

and suppose that the minimum group size in S is r = 5000.

Assume we would like to have for f a ZKP mechanism expressed with respect to an acceptable aggk, where  k(n) = 3 ? 100, 000, 0002 = 215, 443.

To privately release the first output in f , a randomized algorithm T can uniformly select  k1 = 215, 443/5 = 43, 089.

nodes and approximate the value of w1(g?) using these sam- ples. Let (?1, ?1) be the sample complexity of w1(g?) where  ?1 =  ? k1  =  ? 43, 089  = 0.0285  ?1 = 2e ?2k1?21 = 2e?2?43089?(0.0285)   = 7.97 ? 10?31.

The sensitivity of f is  ?(f) =  r +   r2 +   r = 0.0004.

Now, if we would like to use a mechanism which is 0.1-ZKP, we add random noise selected from a Laplace distribution with scale  ?1 = ?(f) + ?1  ? =  0.0004 + 0.0285  0.1 = 0.289  to the actual value of w1(g?). With this noise scale, the ZKP privacy level of the mecahnism is precisely  ?1 ? ( ?+ 2e?  3?k1 ) =  ( 0.1 + 2e?35.06  ) ? 0.1 with respect to aggk.

2) ZKP Mechanism for w2: Suppose the function w2(g, g  ?)[.] is an element of f , where g and g? are groups in GG,S . Let San = w2(g, g?)[.] +Lap(?) be a ZKP mechanism that adds random noise selected from Lap(?) distribution to w2(g, g  ?)[.]. To come up with the right ? first we compute the sample complexity of the w2 function.

Expressing w2[x] and w2[z]. To express the x or z elements of the w2 function, we introduce |S| new boolean node attributes, each corresponding to a group. We denote these new attributes by B? indexed by the group id. A node v will have B?g(v) = 1 if v has an edge with some node in group g, and B?g(v) = 0, otherwise. Now for each pair of groups g and g? we can show the following proposition.

Proposition 4:  w2(g, g ?)[x] =  ? v?g B  ? g?(v)  |g| w2(g, g  ?)[z] =  ? v?g? B  ? g(v)  |g?| Hence, the x (or z) elements of w2(g, g?) can be viewed as  the average value of attribute B?g? (or B ? g) over the subset of  nodes in G that are in g? (or g).

Expressing w2[y]. To express y in w2, we introduce |S| new node attributes, each corresponding to a group. We denote these new attributes by B?? indexed by the group id. Each attribute B??g is a boolean vector of dimension |g|, where each dimension corresponds to a node in g. A node v will have B??g (v)[u] = 1, where u ? g, if (v, u) is an edge in graph G, and B??g (v)[u] = 0, otherwise. For each pair of groups g and g? we can show that  Proposition 5:  w2(g, g ?)[y] =  ? v?g,u?g? B  ?? g?(v)[u]  |g| ? |g?| =  ? v?g?,u?g B  ?? g (v)[u]  |g| ? |g?| Therefore, the y measure in w2(g, g?) can be viewed as the  average of B??g?(v)[u]?s or B ?? g (v)[u]?s.

ZKP Mechanism. Let G = (V,E) be a graph enriched with boolean attributes as explained above. We would like     to determine the value of ? > 0 for the Lap(?) distribution which will add random noise to w2(g, g?)[.].

Let T be a randomized algorithm in aggk. Algorithm T randomly samples graph G by uniformly selecting k = k(n)/t random nodes from V and retrieving all the incident edges.

With this sampling, the nodes in the groups of GG,S and the edges between them are randomly sampled as well. We call this sampled S-graph G?G,S = (S ?, E ?S). Let us assume that we have a sample of each group and edges between groups and the size of a sample group g is kg . Then, algorithm T approximates w2 using the data from group samples. For the sample complexity of the elements of w2, since we expressed them as averages, we can use the Hoeffding inequality as follows.

Pr[|T (g, g?)[x]? w2(g, g?)[x]| ? ?] ? 1? 2e?2kg?2  Pr[|T (g, g?)[z]? w2(g, g?)[z]| ? ?] ? 1? 2e?2kg??2  Pr[|T (g, g?)[y]? w2(g, g?)[y]| ? ?] ? 1? 2e?2(kg?kg? )?2 .

Let us focus first on w2[x] (w2[z] is similar). Now we make  the following substitutions in the formula of Proposition 1: ? = 2e?2kg?   , ?(w2(g, g?)[x]) = 1/r, b? a = 1, and m = 1.

From this, we have that mechanism San is  ln ( e  1/r+? ? + 2e  ??2kg?2  ) -ZKP  with respect to aggk.

Again, one can set ?, the Laplace noise scale, to be  proportional to ?the error? as measured by the sum of the sensitivity and sampling error, and inversely proportional to ?  ? = ?(w2)[x] + ?  ? =   ?  (  r +  ? kg  )  Regarding ?, we can consider for instance a sample size k =  ? n2, and have ? = 1  ?  kg .

From all the above, the privacy level obtained will be  ln ( e  1/r+? ? + 2e  ??2kg?2  ) = ln  ( e? + 2e  ?  1/r+1/ 3 ?  kg ?2 3 ?  kg )  ? ln ( e? + 2e?  ?  kg )  ? ?+ 2e? 3 ?  kg .

Thus, we have that by adding noise randomly selected from  the Lap  ( ?  ( r +  ?  kg  )) distribution to w2[x], San will be(  ?+ 2e? ?  kg )  -ZKP with respect to aggk.

By substituting for the proper sensitivity and sample com-  plexity, similar computations can be carried out for a San mechanism for w2[y].

Example 3: Let us consider Example 2 again with the output vector  f = ?w1(g?), w2(g?, g??)[x], w2(g?, g??)[y], w2(g?, g??)[z], w1(g??)?.

To privately release the second output in f , a randomized algorithm T can again uniformly select  k2 = k(n)/5 = ? 100, 000, 0002/5 = 43, 089  nodes and approximate the value of w2(g?, g??)[x]. The actual value of function w2(g?, g??)[x] is computed on G. Suppose that the minimum group size in S is r = 5000 and the size of the sample group corresponding to g? in G?G,S is kg? = 50000.

Let (?2, ?2) be the sample complexity of w2(g?, g??)[x] where  ?2 =  ?  kg? =  ?  = 0.0271.

?2 = 2e ?2kg??22 = 2e?2?50000?(0.0271)   = 2.54 ? 10?32.

The sensitivity of f is  ?(f) =  r +   r2 +   r = 0.0004.

Now, if we would like to use a mechanism which is 0.1- ZKP, we can add random noise selected from a Laplace distribution with scale  ?2 = ?(f) + ?2  ? =  0.0004 + 0.0271  0.1 = 0.275  to the actual value of w2(g?, g??)[x].

With this noise scale, the ZKP privacy level of the mecah-  nism is precisely  ?2 ? ( ?+ 2e?  ?  kg? ) =  ( 0.1 + 2e?36.84  ) ? 0.1 with respect to aggk.



VI. EVALUATION  We focus on a single output w1(g) to evaluate our approach (the evaluation based on w2 is similar). In our methods, the amount of noise added to the output is independent of the database, and it only depends on the aggregates we compute and their sensitivities. Therefore, the following analysis is valid for any database.

A. Parameters Affecting Noise Scale  Considering the formula of noise scale ? = ?(f)+?? , the sampling error ? is an important factor specifying ?. The error in turn has reverse connection with the sample size and the size of the database graph. Recall that throughout the paper we considered the error to be ? = 13?  k , where k is the sample  size with values for example k = 3 ? n2.

Fig. 2 illustrates the relationship between the noise scale ? and the sample size k and the database size n. In this figure we assumed that the output vector f has five elements and the ZKP-level ? is 0.1. Each curve in the figure corresponds to a sample size, namely, k = 3  ? n2 and k = 4  ? n3. The figure  shows that as the graph size decreases from one billion to one million the noise scale increases non-linearly to the amounts that are not practical in our setting. Therefore, we conclude that ZKP mechanisms are more practical in very big databases with sufficiently large sample size.

0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  1 200 400 600 800 1000  N oi  se S  ca le  ( ?)  n (*106)  k=n2/3  k=n3/4  Fig. 2. Relationship between noise scale and database size.

0.5   1.5   2.5   10% 30% 50%  N oi  se  Probability  ?=0.02 ?=0.06  ?=0.1  Fig. 3. Probability vs noise.

B. The Noise  The analysis in this section provides a better understanding of the amount of noise which is added to the output. We consider w1 function.

We first compute the cumulative distribution function of Laplace distribution in an interval [?z, z] as follows,  Pr(?z ? x ? z) = ? z ?z  2?e  ?|x| ? dx = 1? e?z? .

Therefore, Pr(|x| ? z) = e?z? .

Let pr = Pr(|x| ? z). Value z for a specified cumulative  probability pr can be calculated using the above equation as  z = ?? ? ln(pr) = ?? ? ? ln(pr).

Figure 3 illustrates the minimum absolute noise z as a function of cumulative probability pr for three different values of ? when ? = 0.1. Each point (pr, z) on the curve for a given ? means that  pr percent of the time the random noise has an absolute value of at least z.

For example, for ? = 0.02, we have that 50% of the time the absolute value of noise is at least 0.14, and 30% of the time it is at least 0.24. These values of ? are practical as our outputs are fractions.



VII. FROM PRIVACY LEVEL TO NOISE SCALE  In this section we address the problem of computing the noise scale based on the required privacy. For a given privacy level ?, the right value for ? can be computed using Propo- sition 1, and the sample complexity of the function. For this, we need to solve the following equation with respect to ?.

ln ( (1? ?) ? e?(f)+?? + ? ? e (b?a)m?  ) = ?.

The sample complexity (?, ?) of the function and the sensi- tivity ?(f) are computed as described in Section V. Suppose b ? a and m are both equal to 1 (as in Section V). Thus, by assigning ? a value (which depends on k), ? is the only variable in this equation. By setting x = e  ? , we have the  polynomial equation  (1? ?)x?(f)+? + ?x? e? = 0 which can be solved for x using various methods (cf. [19], [15]), and finally, we have ? = 1ln x .

Example 4: Let us consider Example 2 again with the output vector  f = ?w1(g?), w2(g?, g??)[x], w2(g?, g??)[y], w2(g?, g??)[z], w1(g??)?.

Suppose that we would like to design a (0.1)-ZKP mechanism for w1(g?).

To compute the corresponding noise scale ?1, we use the above polynomial equation. We assume that the min- imum group size in S is r = 5000. The sensitivity is ?(f) = 1r +  r2 +  r = 0.0004, and we consider k1 =  k(n)/5 = 3 ? 100, 000, 0002/5 = 43, 089 (as in Example 2),  i.e. ? = 13?k1 =  3?43089 = 0.0285. We have that w1(g ?) has a  sample complexity of  (?, ?) = (?, 2e?2k1?  )  = (0.0285, 7.08 ? 10?31).

Now if we plug all the values in the equation  (1? ?)x?(f)+? + ?x? e? = 0 we have  (1? 7.08 ? 10?31)x0.0004+0.0285 + (7.08?31)x? e0.1 = 0  which has root x = 31.731745. This results in a noise scale ?1 that is very close to (albeit slightly lower than) what we computed in Example 2. Therefore, setting the noise scale to be proportional to ?(f)+ ? and inversely proportional to ? is a good enough approximation for achieving ?? ZKP .



VIII. PRIVATE PROBABILISTIC A-GS  In this section we consider graphs with probabilistic edges.

Such graphs are very common in modeling influences in social networks (cf. [18], [7], [22], [5], [16]).

A. Probabilistic Graphs  We will denote a probabilistic graph by G = (V,E), where V is the set of nodes, E ? V ? V is the set of edges, and additionally, assigned to each edge e ? E, there is an existence probability p(e) ? [0, 1]. A probabilistic graph defines a probability distribution over a set of deterministic (regular) graphs called possible instances (PIs). Let PI(G) (or simply PI when G is clear from the context) be the set of all PIs of a probabilistic graph G and PIi(G) (or simply PIi) denote one single PI. The existence probability of each PI is computed as  p(PI) = ?  e?E(PI) p(e) ?  ? e/?E(PI)  (1? p(e)). (1)  B. Probabilistic Graph Summarization  We define the summarization of a probabilistic graph G in a similar way as for deterministic graphs. We have a set of disjoint groups of nodes, and any two groups g and g? are connected in the summary graph if at least one edge connects a node from g to some node in g?. We denote the probabilistic summary graph corresponding to G as S-GS.

Computing the w1 measure does not change in the prob- abilistic case as it is based on the existence of nodes and their attribute values which none is probabilistic. However, due to probabilistic edges the numerators of x, y, and z of the w2 measure are computed differently. That is, instead of computing the exact value, their expected values over the set of possible instances will need to be computed (cf. [16]).

For this, let X , Y , and Z be random variables representing the x, y, and z measures, respectively. To compute E[X] or E[Z] we have the following theorem.

Theorem 1: Let g and g? be two groups in a probabilistic summary graph, and let Evj = {e1, . . . , enj} be the set of edges connecting a node vj ? g to the nodes of g?. We have that  E[X(g, g?)] = E[X] =  ? vj?g  ( 1??e?Evj (1? p(e))  ) |g| .

For E[Y ], it can be verified that, Theorem 2: Let Vg be the set of nodes in a group g and  Egg? = Vg ? Vg? be the set of all possible edges between two groups g and g? in a probabilistic summary graph. We have that  E[Y (g, g?)] = E[Y ] =  ? ei?Egg? p(ei)  |g| ? |g?| .

C. Zero-Knowledge Private Probabilistic A-GS  We focus on edge (connection) privacy in this section.

Let GG,S = (S, ES) be the probabilistic summary graph corresponding to a graph G. Let f be a subvector that is to be privately released.

As stated before, the w1 elements in f are computed and privatized as illustrated in Section V-A1. For the elements of the E[w2] functions in f , we need to view them as averages to be able to use the Hoeffding inequality in the process of privatization. We do this by defining new synthetic attributes.

Expressing E[Y ]. For each node v ? V , we assume to have |S| new attributes called P ??, each indexed by a group id.

Each attribute P ??g is a vector of dimension |g|, where each dimension corresponds to a node in g. For a node v we have P ??g (v)[u] = p(evu), where u is a node in g, and p(evu) is the probability of the edge between v and u. Clearly, P ??g (v)[u] = 0 if there is no edge between v and u in G.

For each pair of groups g and g? we have the following proposition.

Proposition 6:  E[Y (g, g?)] =  ? v?g,u?g? P  ?? g?(v)[u]  |g| ? |g?| =  ? v?g?,u?g P  ?? g (v)[u]  |g| ? |g?| Note that, with this expression, E[Y ] is the average of the  elements of attribute P ??g? over the nodes of g, or vice versa.

Expressing E[X] or E[Z]. To be able to view E[X] or E[z] functions in f as averages, for each node v ? V , we consider S new synthetic attributes called P ?, each indexed by a group id. For each attribute P ?g, we compute the attribute value as  P ?g(v) = 1? ? u?g  (1? P ??g (v)[u]) = 1? ? u?g  (1? p(evu)).

Now for each pair of groups g and g? we have the following proposition.

Proposition 7:  E[X(g, g?)] =  ? v?g P  ? g?(v)  |g| E[Z(g, g?)] =  ? v?g? P  ? g(v)  |g?| Clearly, E[X(g, g?)] (E[Z(g, g?)]) is now the average of P ?g?  (P ?g) attribute over the nodes of g (g ?).

ZKP Mechanism. Let G = (V,E) be a probabilistic graph augmented with synthetic attributes P ?s and P ??s. To compute the sample complexity, a randomized algorithm T , in aggk, samples graph G by uniformly selecting k = k(n)/t random nodes from V and all their incident edges. Then, T approxi- mates E[w2[.]] using the data from sample groups. Since we redefined the elements of E[w2[.]] as averages, we have the following inequalities for their sample complexities using the Hoeffding inequality.

Pr[|T (g, g?)[x]? E[w2(g, g?)[X]]| ? ?] ? 1? 2e?2kg?2  Pr[|T (g, g?)[z]? E[w2(g, g?)[Z]]| ? ?] ? 1? 2e?2kg??2  Pr[|T (g, g?)[y]? E[w2(g, g?)[Y ]]| ? ?] ? 1? 2e?2(kg?kg? )?2  It can be verified that the sensitivities of E[w2[.]] functions are similar to the regular case. Thus, by plugging the above parameters in Proposition 1, we have the following for the San mechanism of E[X], where r is the minimum group size in S, and kg is the size of a sample group g.

Proposition 8: By adding noise randomly selected from the  Lap  ( ?  ( r +  ?  kg  )) distribution to the output of E[X],  San will be ( ?+ 2e?  ?  kg )  -ZKP with respect to aggk.

A similar San mechanism can be proposed for E[Y ] by substituting for the sensitivity and sample complexity.



IX. CONCLUSIONS  We addressed zero-knowledge privacy for graph summa- rization. We focused on group connection measures that are supported by virtually all the social-graph software products.

Our techniques are crucial to be applied on summary graphs before public release of the information. To the best of our knowledge, this is the first work to use the ZKP framework for graph summarization. We focused on ZKP mechanisms for edge privacy and introduced methods to compute the ZKP parameters. Furthemore, we presented an approach to achieve ZKP for the release of graph-summarization for probabilistic data. The upshot is that ZKP is quite useful for protecting not only the participation of a connection, but also the evidence of its participation. However, from a utility point of view, ZKP can only be applied meaningfully on big social graphs.

