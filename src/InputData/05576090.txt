Decision Rule Acquisition Algorithm based on Association-Characteristic  Information Granular Computing

Abstract  This is a article of research which bases on the classical granular computing and the Association Rules, focus on the association rules and decision-making rules of the information system. Firstly, defines a association- rule characteristic Information Granule, which can be treated as a sub-definition of the Information Granule, and a association-rule characteristic Information Granularity matirx is defined as well. Secondly, we present a new approach of computing on support degree and confidence degree, which are commonly used in problems of association-rule and decision-rule in information systems. Finally we build a whole knowledge-obtaining algorithm for the association rules and decision-making, and test its availability by numerous experiments.

1. Introduction  Granular Computing is a fast-developing subject,  and it is a fusion of the rouge set, fuzzy mathematic and Artificial Intelligence.[1,2,3,4] Base on the philosophical thinking, it builds the granular structure of the real- world problem, and finds the solution of that problem by setting the computing model on the granular structure. It is a combination of Philosophical Thinking, Methodology and Computing Model. Studying the granular computing methods and building the granular computing models, under the guide of the philosophical thinking, is the main research direction of Granular Computing in the future.

In this article, we present the definitions of the association-rule Information Granularity matrix, the association-rule Information Granul, the association-rule Information Granularity matrix, and the Algorithm that based on the theories above.

2. Primary Definitions   All printed material, including text, illustrations, and charts, must be kept within a print area of 6-1/2 inches (16.51 cm) wide by 8-7/8 inches (22.51 cm) high. Do not write or print anything outside the print area. All text must be in a two-column format. Columns are to be 3- 1/16 inches (7.85 cm) wide, with a 3/8 inch (0.81 cm) space between them. Text must be fully justified.

A format sheet with the margins and placement guides is available as both Word and PDF files as <format.doc> and <format.pdf>. It contains lines and boxes showing the margins and print areas. If you hold it and your printed page up to the light, you can easily check your margins to see if your print area fits within the space allowed.

Definition 1: Information system S =?U?AT?,the U is the dataset of training samples , AT is attribute set.

One of the records of the training sample in S is event T,  UT ? ? Definition 2: granulating the information system: information system S?set B?AT, according to whether B has a value, actual range VB={ 1, 0}.

Then B can decompose into binary granularity set according to the quotient set U/IND{B}[5]. And the combination of all the Information Granul is GM?granularity matrix?.

For example: in different events, the property B, which is in the information system S, is set to 1? ? ? ? ? ? ? ? ? ? ?0 0 1 0 0 1 0 0 1 1 0, then B can be decomposed into a binary granulation:  b1=100100100110? For example:three atributes a,b and c, which are in  the information system S, are set value of 101000011001? ? ?100101001101 110101000101 then these atributes can be converted into 3 binary granulation:  a={101000011001} b={100101001101} c={110101000101}  And they can be convert into the matrix GM?   DOI 10.1109/GrC.2010.38         ? ?  ? ? ?  ? =   GM  Definition 3: Granular degree of binary information granule. Assuming the amount of digit 1 in a binary information granule of attribute P is the granular degree of this information granule, |P|, then, the support number of an itemset of this attribute P is equal to the granular degree |P|. For example, the granular degree |a| of binary information granule a: 101000011001, is 5.

Definition 4:  Ik (K itemset) and its granulating. In the information system S, the set which contains the associating K attributes is called K itemset. Its granulating is the conjunctive of those k binary granule of attributes, And the outcome is a binary Information Granule whose granular degree is much smaller.

For example? information system S?C1, C2 ?Ck, are  the binary information granules of those K atributes, Then: C1 ?  C2 ?  ? ? Ck, are K itemset? its granular degree is |C1 ?  C2 ?  ? ? Ck, |.

Definition 5? support degree? The support degree of the k itemset, is the probability of the k itemset in U, equal to P(Ik)=|Ik|/|U|?The granular degree of the itemset is its support number.

Definition 6: K Item association-rule. The support degree of Ik  which is bigger than an assigned value? Definition 7? Confidence degree. Assume C is the condition attribute of the association-rule, D is the decision-making attribute of the association-rule, then confidence degree of C?D is P(D|C )?|D ?  C |/|C |.

Definition 8: decision-making rules. The association- rule which satisfies the minimum-support-degree and minimum-confidence-degree at the same time, the rule C?D is called decision-making rule.

3. The Basic Theory of Association-rule Characteristic Information Granule  Definition 9: the granulating of association- characteristic of Ik:.is the binary bunch whose length is |AT|, which is associated with the attributes subscript.

The binary bunch value will be set to '1' ,whose position relative with the attribute appears in Ik t, otherwise it will be set to '0'.

All of the association-characteristic information granule in Ik could be combined into a matrix RMk.? We will use association-characteristic information granule Pki to represent associated IKi.. Pki is a line of RMk? For example, in the information system S, AT={a,b,c}. 2 item-set? a,b? ?, b,c? ,{a,c} means P21?? ? ?110 P22=? ? ?011 P23=? ?101 ;its 2 itemset's association-rules martix.

? ?  ? ? ?  ? =   2RM  Theorem 1: the amount of atributes in the information system S is N?? |AT|? N? , its 1 itemset association- rule matrix is, N steps matrix.For example: in the information system S ,AT={a,b,c}. 1 itemset association-rule matrix is:  ? ?  ? ? ?  ? =   1RM  Definition 10: the granulating of the K itemset's association rules.:same with definition 9.

Theorem 2: the granule degree of Pk iS Pki?GM=||Pki ? GMj|| If |Pki ? GMj|?Pki ? then||Pki ? GMj|| ?1, otherwise ||Pki ? GMj|| ?0 Prove: ? if |Pki ? GMj|?|Pki| means the jth event satisfy the association rules Pki, ? the Information Granule which Pki stands for should be set to 1, otherwise should be 0.

? if |Pki ? GMj|?|Pki | stand for the number jth event  do not satisfy the itemset association Pki , ? so Pki so the Information Granule which Pki stands for should be set to 1.

? the granule degree of Pk::  |Pk|?Pki?GM=? =  ? U  j jki GMP     Theorem 3: Assume RMki,RMkj as two item association-characteristic information granule,and |RMki ? RMkj|?K+1,?RMki ? RMkj. are k+1 item association-characteristic information granule.

Definion 4 K items association-rules information granule Pk,  is a combined with condition attribute set C and decision-making attribute set D,  DkCkk PPP ?? ?= , the support degree of decision- making rules C?D ?|Pk|/|Pk-D|.

4. Decision Rule Acquisition Algorithm based on Association-Characteristic  Assume that there are n attributes and m events in the information system S.

Step 1: Granulating the information system S, create the GM of the information system.

Step 2: Build RM1 based on Theorem 1.

Step 3:According to Definition 5 and Theorem 2, calculate the support degree of the GM1.

Step 4:Delete the association-characteristic information granule form the matrix whose support degree is below the specified minimum-support-degree.

Step 5: FOR(i?1,i<?n? i++? {  According to Theorem 3, can obtain the i+1 association- characteristic information granule matrix (RMi+1), form RMi.

Delete the association-characteristic information granule whose support degree is below specified minimum?support?degree.

If the matrix is empty, then break  }  step6: According to Theorem 5 computing the decision- making rules.

5. Simulation Experiment Information system S as the Table 1.

Table1. list of the experiment information of the system  U AT T1 a,b,d T2 a,b,c,e T3 a,c T4 a,b,c T5 a,c,d T6 a,b,d,e T7 a,b T8 a,c,f T9 a T10 b,c T11 c,g   STEP1: according to definition 7, granulating the existence of each attribute in database,  obtain the binary granule of all the attribute sets. The result is shown below?  a? (11111111100), b? (11010110010), c? (00111001011), d? (11001100000), e? (01000100000), f? (00000000100), g? (00000000001).

Combine information granule of the attributes above to the following information granule matrix:  ? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ?  =   g f e d c b a  GM     STEP2: build RM1 based on Theorem 1 as follows.

? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ?  =  ? ? ? ? ? ?  ?  ?  ? ? ? ? ? ?  ?  ?  =   7,1 6,1 5,1 4,1 3,1 2,1 1,1   P P P P P P P  RM  STEP3: according to Definition 5 and Theorem 2, calculate the support degree of the GM1.

P1?1?GM?[1000000]?GM=? =  ?  6,1  j jGMP =9  P1?2?GM?[0100000]?GM=? =  ?  6,1  j jGMP =6  P1?3?GM?[0010000]?GM=? =  ?  6,1  j jGMP =7  P1?4?GM?[0001000]?GM=? =  ?  6,1  j jGMP =3  P1?5?GM?[0000100]?GM=? =  ?  6,1  j jGMP =2  P1?6?GM?[0000010]?GM=? =  ?  6,1  j jGMP =1  P1?7?GM?[0000001]?GM=? =  ?  6,1  j jGMP =1  STEP4: If the association-characteristic granule whose support number is below 2(e.g. P1?6 and P1?7), it will be deleted from matrix. And then obtains a new matrix RX1' as the follows:  ? ? ?  ?  ?  ? ? ?  ?  ? =   ' 1RX  STEP5: According to Theorem 3, obtain Two item association-characteristic information granule matrix RM2 and two item association-characteristic information granule matrix RM2' of association-rule(as shown in Figure 1 and Figure 2). Then obtain the RM3, RM3' ,RM4 and RM4' as the same way(as shown in Figure 3,4,5,6).It         is shown that the RM4' is empty, which means, RM3' is the target.

? ? ? ? ? ? ?  ?  ?  ? ? ? ? ? ? ?  ?  ?  =   2RX   Figure 1:RM2   ? ? ? ? ? ?  ?  ?  ? ? ? ? ? ?  ?  ?  =   ' 2RX  Figure 2: RM2'   ? ? ? ? ? ? ? ?  ?  ?  ? ? ? ? ? ? ? ?  ?  ?  =   3RX  Figure 3: RM3   ? ?  ? ? ?  ? =  1110000'  3RX  Figure 4: RM3'    [ ]11110004 =RX Figure 5: RM4    [ ]?='4RX Figure 6: RM4'  Since |RX4'|?0, RX3 is the target , and from RX3 we can get 3 association rules: {a,b,e:2}?{a,b,d:2}?{a,b,c:2} STEP6: assume the value of minimum-confidence is 30??according to Theorem 5, calculates the decision-  making rules. Obtains the decision-making rule as the following process: {a,b,e:2}? p(be|a)=2/9  , p(ae|b)=2/6  , p(ab|e)=2/2  , p(a|be)= 2 /2  p(b|ae)= 2/2, p(e|ab)= 2/5.

{a,b,d:2}? p(bd|a)= 2/9 , p(ab|d)=2/3  , p(ad|b)=2/6  , p(a|bd)= 2/2   p(d|ab)= 2/5 p(b|ad)=2/3.

{a,b,c:2}? p(bc|a)= 2/9 , p(ab|c)=2/7  , p(ac|b)=2/6  , p(a|bc)= 2/3   p(b|ac)=2/5  p(c|ab)=2/5.

Then delete the one whose support degree is below 30% and get the final result: (ae ? ? ? ??b) , (ab?e) , (a?be) (b?ae) (e?ab) (ab?d) (ad ??b) (a ? ? ? ? ??bd) (d?ab) (b?ad) (ac?b) (a?bc) (b?ac) ? ?(c?ab) 6.  Algorithm Analysis? To show the advancements of this algorithm, experiment is taken under the environment of Intel core 2.0 GHz, 3 GB memory, Windows7 operation system, and the programming language is C++. The screenshot of the experiment is Figure 7.

Figure 7 The screenshot of the experiment   The system use dataset which is collected from a financial company. There are 5000 records and 90 attributes in database.

Compares the algorithm in our artical with Apriori algorithm, figure 8 offers distinguish of execution time between the two algorithms when their support degrees are different. Fgure 9 shows that, the less support degree is, the shorter execution time of our algorithm will be, it figures out that our algorithm will be much faster when the support degree is small, and it will be more effective when the records in database are huge.

20% 30% 40% 50%  Classical Approach  Article Approach   Figure 8, comparison of time cost while the support  degrees are different                  20% 45% 70% 100%  Classical Approach  Article Approach   Figure 9, comparison of time cost while the records  are different  The algorithm that we presented in the article does  not need to store all the candidate itemsets, it just needs to save boolean type ?0? and ?1? as bit mode. As a result, this will save more memory, and with better spatial features.

7.  Conclusion Under the background of granule computing, our research team present a new algorithm to solve the problems of association-rule and decision-making rule in data mining. The results of the experiments prove that our algorithm is feasible and efficient. There are still a lot of work to do to improve the efficiency of our algorithm, such as the reduction of the granule matrix in step 5. Overall, the work of research show the promising future of granule computing.

