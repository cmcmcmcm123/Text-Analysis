Distributed Storage Evaluation on a Three-Wide Inter-Data Center Deployment

Abstract?The demand for cloud storage is exploding as an  ever increasing number of enterprises and consumers are storing and processing their data in the cloud. Hence, distributed object storage solutions (e.g., QFS, Swift, HDFS) are becoming very critical components of any cloud infrastructure. These systems are able to offer good reliability by distributing redundant information across a large number of commodity servers, making it possible to achieve 10 nines and beyond with relative ease.

One drawback of these systems is that they are usually designed for deployment within a single data center, where node-to-node latencies are small. Geo-replication (i.e., distributing redundant information across data centers) for most open-source storage systems is, to the best of our knowledge, accomplished by asynchronously mirroring a given deployment. Given that geo- replication is critical for ensuring very high degrees of reliability (e.g., for achieving 16 nines), in this work we evaluate how these storage systems perform when they are directly deployed in a WAN setting. To this end, three popular distributed object stores, namely Quantcast-QFS, Swift and Tahoe-LAFS, are considered and tested in a three-wide data center environment and our findings are reported.



I. INTRODUCTION  Modern distributed object storage solutions, like HDFS[1], Swift[2], Quantcast-QFS[3], Tahoe-LAFS[4], Riak[5], Azure[6], Colossus[7], and Amazon Dynamo[8], offer very good availability and reliability at a low price point by distributing data across a very large set of inexpensive commodity servers that consist of unreliable components.

Despite their success, many of these systems have been designed to distribute data across large clusters of servers redundantly (either by using replication or erasure coding) primarily within a single data center. Geo-replication, i.e., distribution of redundant information across data centers, is typically handled by asynchronously mirroring a given deployment.

There are several limiting factors that affect the perfor- mance of distributed object store deployments in a WAN set- ting (in the presence of TCP slow start and shared bandwidth), as will become clear after an in-depth description of these sys- tems in Section II. In this work, we evaluate three distributed object stores, namely Quantcast-QFS, Swift and Tahoe-LAFS in a three-wide data center environment, in order to assess the impact of WAN latencies on these systems. We focus on the read/write performance of these systems, and ignore other features such as repair, ease of use, maintainability, recovery performance, and compatibility to other system components.

These considerations, though important, are more subjective and application dependent. Moreover, our eventual goal is to understand the weaknesses of these distributed storage systems  IL NJ  GA  32.6 MiB/s  60.2 MiB/s 25ms  PD Fill  PD F E  dito r w  ith Fre  e W rite  r an d T  ool s  Fig. 1. Multi-site Data Center Deployment with Network Bandwidths.

in a WAN setting, and thus we make the conscious choice on these restricted but most fundamental issues.



II. QUANTCAST-QFS, TAHOE-LAFS AND SWIFT  In this section, we provide some background information for the three open-source storage systems that we chose to evaluate, and briefly describe the characteristics that are relevant to this work.

A. Quantcast-QFS  Quantcast-QFS is a high-performance, fault-tolerant, dis- tributed file system developed by Quantcast corporation, im- plemented in C++, that underlies its MapReduce [9] infras- tructure1. File storage in QFS utilizes the (9, 6) Reed-Solomon (RS) codes [10], or simple replication codes. The system con- sists of a large number of chunk servers and a single meta-data repository residing on a dedicated meta-server. Each chunk server is responsible for storing erasure-coded chunks of files.

The meta-server is responsible for balancing the placement of chunks across the set of chunk servers, maintaining the chunk placement information, and issuing repair requests.

In order to understand the processes underlying a read/write operation in QFS we first need to introduce some basic terminology. QFS uses the concepts of chunk blocks, chunks, stripes, and strides for storing data in an erasure-coded form across a set of chunk servers. Figure 2 shows how this terminology translates into the physical layout of the data stored on the chunk servers. Assume that we are given a 1GiB  1Note that Quantcast-QFS is not related to SAN-QFS developed by Sun corporation. In what follows we will refer to Quantcast-QFS as QFS.

Server 1 D1 D7 D13 ?  Server 2 D2 D8 D14 ?  ?   Server 6 D6 D12 D18 ?  Server 7 P1 P4 P7 ?  Server 8 P2 P5 P8 ?  Server 9 P3 P6 P9 ?  D1  D2  D6  P1  P2  P3  Stride  Da ta   Pa  rit y  Chunk block Chunk = 64MiB  Stripe = 64KiB  D6139  ?  Fig. 2. QFS physical data layout.

file, we store at each chunk server up to 64MiB chunks, we use the default (9, 6) coding, and we set the stripe size (i.e., the size of each individual cell in Figure 2) to be 64KiB. The file will consist of several chunk blocks, where each chunk block is treated independently. In our example, a chunk block consists of 9 chunks (i.e., rows), 6 chunks containing data and 3 chunks containing parity information. If possible, each chunk will be disseminated to a distinct failure domain (failure domains can be defined to be individual nodes, racks, zones, etc.), such that loss of up to 3 data/parity chunks will not result in any data loss.

A write happens in a streaming fashion. When a client needs to store a file it first contacts the meta-server and requests 9 chunk servers that will be responsible for storing the first set of 9 64MiB chunks. Then, once 6 stripes of data, i.e., a data stride (which is equal to 6 ? 64 = 384KiBs in our example), become available for writing, the client erasure codes the data stride, resulting in a full stride (data and parity). Finally, the client appends each data/parity stripe within the stride to the corresponding chunk on each of the 9 chunk servers (so chunks fill up from left to right in the figure, one column at a time).

Once a chunk is full, i.e., once 64MiB/64KiB=1024 strides have been appended, the chunk block becomes full and the chunk servers notify the meta-server that the block has been written (when and how often the meta-server is contacted by the chunk servers is an interesting design issue in itself). Then, the process repeats from the beginning, using a new set of 9 servers, until the whole file is written. Notice that depending on application characteristics, one can tune the chunk size and stripe size parameters accordingly in order to minimize the main memory needed for erasure coding strides, the total number of roundtrips between client, chunk servers and the meta-server, etc. Notice that each write request in QFS results in possibly multiple round-trips between the client and the meta-server, as well as the meta-server and the chunk servers.

Data reads are accomplished by choosing 6 out of 9 chunks (based on load, availability, etc.), retrieving those chunks and reconstructing all data strides. Notice also that a client can request to read a random offset from a given data object, in  which only the corresponding partial chunks from each chunk block will be returned to the client.

The meta-server in QFS is a single-point-of-failure (SPOF); however, there exists a checkpointing mechanism such that the whole system can recover fairly quickly, without losing files that had been successfully stored before the last checkpoint.

In addition, to achieve high reliability, QFS allows for certain placement policies to be specified such that chunks are placed onto different failure domains (e.g., across nodes, racks, and zones). Finally, given that chunk servers issue frequent keep- alive heartbeats to the meta-server, the meta-server has a very accurate view of the status of the cluster, and is hence responsible for issuing repair requests for lost chunks.

B. OpenStack Swift  As part of the OpenStack architecture, Swift was originally designed to provide a reliable, distributed storage facility for maintaining virtual machine images. Despite its roots, it can be used as a generic object store, that uses replication for increased reliability. It is implemented in Python.

Swift comprises of proxy servers, object servers, container servers, and account servers. Swift also depends on three rings, one for accounts, one for containers, and one for objects. The rings are based on consistent hashing (cf., Amazon Dynamo [8]) and are used for identifying the servers that are responsible for storing information about an account, a container, and an object, respectively. Each ring conceptually divides the entity namespace into a pre-specified number of partitions, and each partition is assigned to multiple object servers (hence entities falling within a given partition are replicated multiple times; the replication factor is configurable). Each partition replica is guaranteed to reside in a different failure domain (or zones in Swift terminology), where the zones are statically defined during system configuration. Swift rings are static, and they only change manually. In other words, when a server fails, the corresponding ring data structure is not updated, but the proxy server handling a request that maps to the failed server will detect the failure and use the existing ring to identify a handoff server, which will temporarily replace the failed server.

When the failed server rejoins the cluster, a repair process will be initiated, involving the previously failed server and the handoff server, to bring the system back to steady state.

If the failed server is permanently removed from the cluster, a system administrator will have to update the correspond- ing ring and another repair process will guarantee that all objects that were stored on the failed server are replicated three times. Notice that the ring architecture implies that each account/container/object server is responsible for only a fraction of objects. Hence, objects are uniformly spread across all servers without the need for a load balancing mechanism.

The proxy servers are responsible for handling user re- quests and also act as load-balancers, firewalls and a caching layer for the Swift cluster (for caching accounts, containers and objects). Writing an object involves locally storing the object at the proxy server, consulting the appropriate ring to find the locations where that object needs to be stored, and transmitting the object to that location. If the object is an account, it is transmitted to the appropriate account servers. Account servers are responsible for storing a list of container names associated     with each account. This information is maintained in a SQLite database and replicated in three account servers. Similarly, if an object is a container, it is transmitted to the appropriate container server. Container servers are responsible for storing a list of object names associated with each container. Once again, this information is maintained in a SQLite database on each container server. Finally, for first class objects, the object is copied to three object servers as a binary file on the local file system. Delete requests are symmetric to writes.

Notice that in Swift each write/delete request needs to update the appropriate container database and store/delete the actual object in the appropriate object server, which results in two round-trips per request. Object read requests do not involve the container servers.

In terms of repair, Swift takes a pro-active approach.

Each object server periodically checks whether each partition the server is responsible for needs to be repaired. For each partition, the server polls all other object servers that should also be replicating this partition, and if any objects within the partition are missing (or if an object server has failed), ?rsync? is run to repair the partition (or replicate the partition to a temporary handoff server).

C. Tahoe-LAFS  Tahoe-LAFS (Least-Authority File System) is an open- source, distributed file system, implemented in Python, that can tolerate multiple data server failures or attacks, while preserving the privacy and security of the data. The underlying idea is that users can store data on the Tahoe-LAFS cluster in an encrypted form, using standard cryptographic techniques.

Clients are responsible for maintaining the necessary cryp- tographic keys needed to access the data, and without those keys, no entity is able to learn any information about the data, including its placement across the cluster. In addition, data is erasure-coded for increased reliability, and erasure coding parameters can be modified in the client configuration file on a per-client basis.

A Tahoe-LAFS cluster consists of a set of storage peers and a single coordinator node, called the Introducer, whose primary purpose is to announce the addition of new storage peers to the existing pool of peers by following a publish/subscribe paradigm, and relay relevant node information to clients upon read/write requests. The storage servers are responsible for storing file shares. Shares are encoded pieces of files. Each file has a configurable number of shares. When a client issues a write request, it first encrypts the file locally and breaks it up into segments. It then erasure codes each segment, creating blocks, and sends one block from each segment to a particular server. The set of blocks on a given server is one share. Notice that so far the only difference between QFS and Tahoe-LAFS is that Tahoe-LAFS does not split the files into multiple chunk blocks. The client also computes a set of cryptographic hashes on the encrypted files and each file segment and creates Merkle trees that are stored along with each share. The client can read a file by reversing this process.

Tahoe-LAFS chooses the servers that will store the shares of the file using a hashing algorithm. The hash of the encryp- tion key of the file is used to generate a list of servers. First, the introducer removes all servers that do not have enough  capacity to store a given share. Then, the introducer returns a list of servers to the client. The client generates a hash value for each server, using a concatenation of the hashed encryption key and the server name, and sorts the resulting hash values to create a random server permutation. Subsequently, the client asks each server in that order whether it can hold a share of the file or not, and repeats until all shares have been assigned to a server. A single server can be responsible for multiple shares. When a client needs to read a file it has to ask all known servers whether they are holding any shares of the file or not.

So far, Tahoe-LAFS is a pure key/value store that offloads the responsibility of managing keys to the clients. In other words, it does not maintain directories and file names. In order to make Tahoe-LAFS behave more like a file system, it implements a file system layer, which is essentially meta-data information about directories and files organized into a data structure that is then stored as a native object within Tahoe- LAFS itself. On the outset, this seems similar to the SQLite databases in Swift. At closer inspection though, this is not the case. Notice that in Tahoe-LAFS all files are encrypted and erasure coded. Hence, writing a file involves first updating the file system meta-data object which in turn implies retrieval of the whole object, decryption and updating at the client side, and writing the updated version back to Tahoe-LAFS, and then writing the actual file. Hence, each write/delete request involves first accessing and updating the file system meta- data object and then updating the shares on the corresponding storage servers. This results in two round-trips per server in the whole cluster. Reading a file also involves accessing the file system meta-data object, since in order to locate the file shares the client needs to find the hashed encryption key used to store the file, which is stored in the file system meta-data object. Hence reads, also require two round-trips between the client and all servers in the cluster.

The introducer in Tahoe-LAFS is a single-point-of-failure for new clients or new storage peers, since they need to rely on it to be able to join the storage network. However, it is not a SPOF in the traditional sense, because in Tahoe-LAFS the file placement information is decided by hashing the keys held by the clients, and the introducer does not have to maintain any file (or chunk) placement information; therefore, losing the introducer does not jeopardize the normal operation of existing clients and storage peers.

The downside of this introducer-storage-nodes architecture is that due to the lack of a centralized meta-server, Tahoe- LAFS is not able to provide complete statistics and details for file storage and placement, though from a security point of view, this is a choice by design. Moreover only a lazy repair policy can be implemented; in other words, the clients are responsible for frequently iterating through their file space and verifying that all files are complete; however, when a storage peer fails, the system is not able to initiate an efficient repair mechanism specifically to replace the given peer.



III. THE TEST ENVIRONMENT  For our tests, two physical configurations are used. The main configuration consists of three geographically distant data centers. The baseline configuration consists of a single data center.

The three-site layout is depicted in Figure 1. The sites selected for the test were roughly arranged in a geographically equilateral triangle with several hundred miles separating each.

Site 1 is in IL, site 2 is in NJ, and site 3 is in GA. The connectivity between sites varies significantly from site to site, and we observe that network characteristics are not symmetric.

Maximum throughput between sites varied depending on direc- tion. Furthermore, in one case throughput varied significantly from one measurement to the next, even when measurements were taken within very short time spans. While the network was far from ideal, and prevented us from determining the best performance that could be obtained from each storage system, direct comparison of the three systems is still possible and meaningful.

For the three-site configuration, within each data center we used three hosts as storage servers and one host, referred to as meta host in what follows, for supporting tasks (i.e., the QFS meta-server, the Swift proxy, and the Tahoe-LAFS introducer).

The meta host in GA was also used to drive the tests. Each host has an Intel Xeon E5-2690 8-Core processor (2.9 GHz), 64GiB of main memory, and nine 7200RPM SATA drives of 2TB each in a JBOD configuration running XFS. The operating system is Ubuntu 12.04 LTS with Linux kernel 3.2.0. For the single site configuration we used a total of ten hosts, nine as storage servers and one meta host for everything else. All hosts within a data center were connected to the same switch using 10-gigabit Ethernet.

All the tests were driven using Cosbench (Cloud Object Store benchmarking) [11], an open-source tool developed by Intel. Cosbench implements a driver that simulates user-defined workload configurations. Users can choose the characteristics of the workload in terms of the size of files, the type and per- centages of create/write/read/delete operations, and the number of concurrent threads of execution. We extended Cosbench by adding plug-ins to provide interfaces to QFS and Tahoe-LAFS; the interface to Swift is provided as part of the Cosbench package. The tests were organized such that data written to the object store was randomly generated, and data read from the object store was discarded; no disk I/O on the test host impacted the throughput measurements.

Results presented in this paper with regard to performance are measured in MB/s (powers of 10) and are referred to by Cosbench as bandwidth. In Cosbench, bandwidth is not computed similarly to the traditional throughput measurement (i.e., total bytes over elapsed time), but it is a summation over the throughput of each individual thread of execution. This method of calculation can yield larger values than would be observed by the traditional computation, especially for tests involving files of variable sizes, because it does not capture the idle time for threads in-between job scheduling. Clearly, bandwidth is a good measure in practice, because it does not reflect any design choices related to the Cosbench job scheduler itself.



IV. RESULTS  Measurements were collected for each system either read- ing or writing fixed-sized objects. Tests with concurrent read- ing and writing were not conducted. In our tests we used work- loads consisting of 100MB and 1GB objects. For brevity, we  Read Bandwidth      M B  /s  Lo g  S ca  le   10 20 30 40 50 Threads  QFS Swift Tahoe  Fig. 3. QFS/Swift/Tahoe-LAFS Multi-Site Read Performance  Write Bandwidth      M B  /s  Lo g  S ca  le   10 20 30 40 50 Threads  QFS Swift Tahoe  Fig. 4. QFS/Swift/Tahoe-LAFS Multi-Site Write Performance  present averages over all workloads. The tests were executed with a varying number of concurrent threads. In most cases, the addition of threads, up to a point, increased overall bandwidth, but also resulted in the degradation of the throughput of each individual thread, as expected, due to contention on resources.

A. Multi-site Test Results  The results are shown in Figures 3 and 4. There are a few observations that should be noted.

? Writing of larger objects (1 GB vs 100 MB; not shown in the figures) was observed to be slightly faster for all three storage systems for the same total volume of data written. This is likely due to the reduced number of interactions per MB with the meta/proxy server.

The difference, though, was not significant.

? Read performance from multiple sites was slightly better than write performance for all three storage systems. This is expected in erasure-coded systems, since reads in fact transfer less data than writes.

? Performance increased as more threads, to a point, were allowed to access the objects.

Overall, we can see that QFS exhibits the best performance and scalability with respect to concurrent requests. Note that the y-axis is in log-scale. Also notice that as more threads are used, the read/write performance flattens to a point where the available bandwidth dictates what is the maximum throughput that can be obtained, as expected.

B. Single-Site Test Results  In order to understand the impact that the network imposes on a multi-site environment we established a single site with     Read Bandwidth      M B  /s  Lo g  S ca  le   10 20 30 40 50 Threads  QFS Swift Tahoe  Fig. 5. QFS/Swift/Tahoe-LAFS Single-Site Read Performance  nine storage hosts and one meta host. Two sets of tests were conducted: one is when the reads/writes are initiated from a node located in the same site, which is referred to as local read/write; the other is when they are initiated from a node located in a different geo-location, which is referred to as remote read/write. For the former, QFS, Tahoe-LAFS and Swift are all tested, while for the latter we focus on QFS.

Figures 5 and 6 show the single-site test results, while Figures 7 and 8 show the remote read/write performance for QFS. A few observations are noted below:  ? In terms of local read/write performance, QFS is the clear winner among the three systems. Notice the logarithmic scale on the y-axis, once again.

? Accessing the single site object stores from a local host results in nearly one order of magnitude increase in performance, as shown in Figures 7 and 8. Even taking into account the bandwidth limitation between data centers, this significant difference is remarkable.

We suspect that this is due to the system optimiza- tion done in QFS for single-site deployment (further discussion is included in Section V).

? When accessing the single site object store from a remote location, the performance drops slightly be- low the performance observed with the object store distributed across multiple sites. This small difference might be caused by the fact that when testing on the multi-site configuration, one set of storage hosts is local to the host running the Cosbench driver and, thus, one third of the data transfers are local. Notice that in any application scenario where we expect the majority of client requests to originate from remote locations, this implies that in fact deploying these systems in a multi-site configuration is preferable than the single-site configuration (both in terms of reliability and performance). Of course, this is not the case, for example, for MapReduce deployments.



V. DISCUSSION  There are certain limiting factors in terms of performance when trying to deploy distributed storage systems in a WAN setting, mainly due to the latency introduced by the physical connection, TCP slow start, and of course due to shared bandwidth.

Systems that rely on a meta-server, like QFS (and, for ex- ample, HDFS), introduce large latencies when reading/writing  Write Bandwidth      M B  /s  Lo g  S ca  le   10 20 30 40 50 Threads  QFS Swift Tahoe  Fig. 6. QFS/Swift/Tahoe-LAFS Single-Site Write Performance  Read Bandwidth      M B  /s  Lo g  S ca  le   10 20 30 40 50 Threads  Multi-site Local Remote  Fig. 7. QFS Single-Site Read Performance  objects across the WAN, because every read/write request for each chunk of the file incurs one round-trip delay to the meta- server for the client and each storage server involved (although storage servers will typically aggregate multiple chunk replies into a single message digest for the meta-server). If the meta- server is located in a remote data center from the client, we expect this architecture to add a significant overhead for read/write requests. In addition, given that QFS splits files into 64MiB chunks, and chunks are uniformly distributed across all storage servers, we expect QFS to suffer significantly from TCP slow start.

Systems that use consistent hashing to determine object placement (e.g., Riak and Amazon Dynamo), need to use a distributed consensus protocol (e.g., Paxos [12]) in order to keep the state of the cluster up-to-date, which requires at least one round-trip delay per server for each cluster update. But read/write requests happen independently of the cluster management protocol. Hence read/write requests go directly to the relevant storage servers, without any additional round-trip delays. From that respect, even though Swift uses consistent hashing in order to determine object placement, the ring is statically allocated during system configuration (and can change only manually). Hence, Swift does not have the overhead of running a distributed consensus protocol to keep the consistent hashing ring up-to-date. On the other hand, Swift does have to maintain meta-data information (accounts and containers) as native objects within the object store itself, hence, incurring at least one round-trip delay for every update of each replica of the meta-data object. Although, in Swift this latency can be hidden completely due to the caching layer at the Proxy servers. We also observe that under-provisioning the proxy setup in Swift can have a detrimental effect in terms of scalability. Swift does not scale well as the number of concurrent threads increases, resulting in a large number     Write Bandwidth      M B  /s  Lo g  S ca  le   10 20 30 40 50 Threads  Multi-site Local Remote  Fig. 8. QFS Single-Site Write Performance  of dropped operations (not shown in our figures since we plot averages across all workloads for successful operations only). This is because all data transfers have to go through the proxy server, which eventually becomes oversubscribed and starts to shed requests. Clearly this is an indication that they proxy server is not designed to scale gracefully as the number of clients increases, given that, ostensibly, for all storage systems eventually all data has to go through the sole Cosbench driver running on the meta host, in which case we would expect Cosbench to become oversubscribed and we should be observing the same behavior for QFS and Tahoe- LAFS, which is not the case. Nevertheless, a more robust Swift configuration would include several proxy servers that would load-balance requests, but this is something that we did not test in our configuration, and we plan to do as future work, since it would require a multi-driver configuration of Cosbench.

Tahoe-LAFS is similar to Swift, in that the meta-data objects are stored within Tahoe-LAFS itself, necessitating at least one round-trip delay for each erasure-coded share of the meta-data object, for every write/delete request, and an additional round-trip to all relevant servers to execute the request. On the other hand, reads are accomplished by submitting requests to all known storage peers (given by the introducer) simultaneously, hence the relevant peers are found with one round-trip to every server. In Tahoe-LAFS currently, a second round-trip is incurred, after choosing the peers to read the file from (the intention here is to be able to select which peers to read from, after the initial negotiation phase, based on various heuristics). But the poor performance of Tahoe-LAFS, on the multi-site and single-site environment, can be attributed to several factors, already pointed out by the developers themselves. First, the default stripe size is optimized for reading/writing small objects (the stripe size determines the granularity at which data is being encrypted and erasure coded). Second, there are several implementation issues such as inefficient message passing, and the expensive and frequent hash read/write seeks that are needed in order to reconstruct shares on the storage peers. Third, Tahoe-LAFS has to deal with the overhead of reading/writing the file system meta-data objects (i.e., the mutable directory objects), every time an object is accessed. Fourth, when creating new objects, Tahoe-LAFS has to generate a new public/private key, which is an expensive operation. Surprisingly, reads exhibit the same performance as writes, even though reads ideally have to transfer less data than writes. This is probably because both reads and writes of shares happen simultaneously to all relevant storage peers, hence the extra data transfers are hidden by  parallelism. Moreover, this is an indication that pinging all available peers and requesting shares within two round-trips, as well as the fact that every read request has to read the mutable directory object, dominate the overall cost.



VI. CONCLUSION  We conducted extensive experiments with QFS, Swift and Tahoe-LAFS, which are three very popular distributed storage systems. Our focus was to deploy these systems in a multi-site environment and measure the impact of WAN characteristics on these systems. In addition, as a baseline, we also measured performance on a single-site configuration. Overall, we observe that WAN characteristics have an even larger than expected impact on the performance of these systems, mainly due to several design choices of these systems. Ideally, across the WAN, we would like to reduce the amount of round-trips to a minimum, which is something not particularly important on a LAN. In addition, we notice that good system design and extensive optimizations can have a significant effect on performance, as is seen by the relative difference between QFS, Swift and Tahoe-LAFS. It is important to point out here that QFS is implemented in C++, Swift and Tahoe-LAFS in Python. In addition, QFS is heavily optimized for MapReduce style processing. For future work we are planning to also test Riak and HDFS, as well as our own proprietary solution that is designed, from the ground up, for WAN deployment.

