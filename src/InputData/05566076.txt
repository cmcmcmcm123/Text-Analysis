Sampling Based N-Hash Algorithm for Searching  Frequent Itemset

Abstract?Searching frequent itemsets is the critical problem in generating association rules in data mining, classic Hash-based technique, put forward by J. S. Park, for searching frequent itemsets has two shortcomings: one is that it is difficult to choose an appropriate hash function; the other is that it is liable to cause hash colliding. In order to solve the two problems, Chen Y.M.

proposed N-Hash algorithm which needn?t to choose hash function and avoided hash colliding. In this paper, the sampling technique is employed to improve the efficiency of N-Hash algorithm.

Keywords- data mining; association rule; frequent itemset; N- Hash algorithm;  sampling.



I.  INTRODUCTION Association rule[1] is widely used in business management,  for example, it can help a supermarket manager to plan marketing or advertising strategies, so association rule mining has become an important data mining task and a focused theme in data mining research. In general, association rule mining can be viewed as a two-step process [2]:  ? Step 1 Find all frequent itemsets: By definition, each of these itemsets will occur at least as frequently as a predetermined minimum support count, min_sup.

? Step 2 Generate strong association rules from frequent itemsets: By definition, these rules must satisfy minimum support and minimum confidence.

Searching frequent itemsets is the critical problem in generating association rules, there are several algorithms for the task, such as the Apriori algorithm [3], Hash-based technique [4]. In 2007, Chen Y.M.[5] pointed out that the classic Hash- based technique has two shortcomings: one is that it is difficult to choose an appropriate hash function; the other is that it is liable to cause hash colliding. In order to solve the two problems, Chen Y.M. (2007)[5] proposed N-Hash algorithm which needn?t to choose hash function and avoided hash colliding. But the efficiency of N-Hash method still has room to be improved, we will employ sampling technique to make N-Hash algorithm more efficient.



II. PRELIMINARIES For convenience, we firstly list some basic concepts for  later use, and then we give the Hash-based technique and N- Hash algorithm a brief introduction.

A. Basic Concepts[2]  Let },...,,{ 21 miiiI = be a set of items, Let D , the task- relevant data, be a set of database transactions where each transaction T  is a set of items such that IT ? . Each transaction is associated with an identifier, called TID. Let A be a set of items. A transaction T is said to contain A  if and only if TA ? . An association rule is an implication of the form BA ?  , where IA ? , IB ? , and ?=? BA . The rule BA ?  holds in the transaction set D  with support s (sometimes is referred to as relative support), where s  is the percentage of transactions in D  that contain BA ? , i.e.

( ) ( )support A B P A B? = ? . The rule BA ?  has confidence c in the transactions set D , where c is the percentage of transactions in D  containing A  that also contain B , i.e. ( ) ( | )confidence A B P B A? = . Rules that satisfy both a minimum support threshold (min_sup) and a minimum confidence threshold (min_conf) are called strong. A set of items is referred to as an itemset. An itemset that contains k items is a k -itemset. The occurrence frequency (or support count) of an itemset is the number of transactions that contain the itemset. If the relative support of an itemset I satisfies a prespecified minimum support threshold, then I  is a frequent itemset. The set of frequent k -itemsets is commonly denoted by kL .

B. Hash-based Technique In 1995, by experiments Park discovered that the  computation of searching the frequent itemsets is mainly dominated by searching the 2-itemsets 2L , they proposed Hash-based technique[4], which means hashing itemsets into corresponding bucket. The Hash-based technique can be used to reduce the size of the candidate k -itemsets, kC , for 1k > .

It performs as follows: When scanning each transaction in the database, we can generate all of the 2-itemsets for each transaction, by a hash function, hash (i.e. map) those 2-  Supported by the Scientific Research Foundation of CUIT under Grant KYTZ201001 and by the Folk Culture Research Centre of Philosophy and Social Science Key Research Institute in Sichuan Province (MJ09-03).

itemsets into the different buckets of a hash table structure, and increase the corresponding bucket counts. The following example shows its usage.

Table 1 gives out the transactional data we considered.

TABLE I.  TRANSACTIONAL DATA  TID List of item_IDs 1 i1, i2, i5 2 i2, i4 3 i2, i3 4 i1, i2, i4 5 i1, i3 6 i2, i3 7 i1, i3 8 i1, i2, i3, i5 9 i1, i2, i3  Here, the function ( , ) (10 ) mod 7h x y x y= +  is used as the Hash function, as for the transaction 1, we can generate 2- itemsets { i1, i2}, { i1, i5} and { i2, i5}. As for the 2-itemset { i1, i2}, calculate (1, 2) (10 1 2) mod 7 4h = ? + = , then 4 is the bucket address of the 2-itemset {i1, i2}, the 2-itemset {i1, i2} is sometimes called bucket content of bucket address 4. After the other transactions and 2-itemsets have been dealt with, we can get a Hash table (see table 2).

TABLE II.      HASH TABLE GOT BY CLASSIC HASH TECHNIQUE  Address 0 1 2 3 4 5 6  Count 2 2 4 2 2 4 4  Content {i1, i4} {i1, i5} {i2, i3} {i2, i4} {i2, i5} {i1, i2} {i1, i3}  {i3, i5} {i1, i5} {i2, i3} {i2, i4} {i2, i5} {i1, i2} {i1, i3}  {i2, i3}   {i1, i2} {i1, i3}  {i2, i3}   {i1, i2} {i1, i3}  From table 2, we see that in address 0, there are two different bucket contents, content 1 4{ , }i i and content 3 5{ , }i i , this phenomenon is Hash colliding.

To avoid Hash colliding, Chen Y.M.[5] put forward N-Hash Algorithm to generate Hash table.

C. N-Hash Algorithm  1) The main idea of N-Hash Algorithm The k-dimensional vector is employed to stand for k-itemset, as for the bucket content which need to be put into a bucket address, if the bucket address to contain it had been existed, this bucket is called old bucket, we put the content into corresponding old bucket address, at the same time, the bucket count increased by 1; if the bucket address to contain it had not been existed, a new bucket address is added to contain it, at the same time, the bucket count of the new bucket address is 1. The idea comes from our daily life: a technique to check whether a playing card is lost or not. So the method is called  Hash algorithm based on naturally assigning bucket, for brevity, it is called N-Hash Algorithm.

2) Process  of N-Hash Algorithm  Step 1 Denote the set of existed bucket address as A , firstly let A = ? ;  Step 2 Scan the transaction database, take out the transaction one by one, when got the 2-itemset{ , }s ti i from the transaction that is being dealt with, use 2-dimensional vector ( , )s t  as the bucket address of { , }s ti i , let stn be the count of { , }s ti i , check A , if ( , )s t A? , then let ( , )s t A?  and 1stn = ; if ( , )s t A? , then let  1st stn n= + .  Repeat step 2 until all transaction have been dealt with, then we obtain a Hash table  Steps 3 According to prespecified minimum support, frequent itemsets are generated.

3) Example of N-Hash Algorithm Take the data in Table 1 as an example, we will show how N-Hash Algorithm works.

Step 1 Set A = ? ; Step 2 Scan the transaction database, take out the transaction one by one. As for transaction 1, generate the following 2-itemsets: 1 2{ , }i i , 1 5{ , }i i  and 2 5{ , }i i . As for  1 2{ , }i i ,  since (1, 2) A? , then let {(1, 2)}A =  and  12 1n = .  As for 1 5{ , }i i , since (1,5) {(1,2)}A? = , then let {(1,2), (1,5)}A =  and 15 1n = . As for 2 5{ , }i i , since  (2,5) {(1, 2), (1,5)}A? = , then let 25 1n =  and {(1, 2), (1,5), (2,5)}A = .

As for transaction 2, generate the 2-itemset 2 4{ , }i i , as for  2 4{ , }i i , since (2, 4) {(1, 2), (1,5), (2,5)}A? = , then let {(1,2), (1,5), (2,5), (2, 4)}A =  and 24 1n = .

As for transaction 3, generate the 2-itemset 2 3{ , }i i , as for  2 3{ , }i i , since (2,3) {(1,2), (1,5), (2,5), (2,4)}A? = , then let {(1, 2), (1,5), (2,5), (2,4), (2,3)}A =  and 23 1n = .

As for transaction 4, generate the following 2-itemsets:  1 2{ , }i i , 1 4{ , }i i  and 2 4{ , }i i . As for 1 2{ , }i i , since (1, 2) A? , then let 12 1 1 2n = + = . As for 1 4{ , }i i , since (1, 4) {(1, 2), (1,5), (2,5), (2, 4), (2,3)}A? = , then let  {(1,2), (1,5), (2,5), (2, 4), (2,3), (1, 4)}A =  and 14 1n = .

As for 2 4{ , }i i , since  )}4,1(),3,2(),4,2(),5,2(),5,1(),2,1{()4,2( =? A , then let 24 1 1 2n = + = .

As for the other transactions and 2-itemsets, we can deal    with them similarly. Finally, we obtain a Hash table shown in Table 3  TABLE III.   HASH TABLE GOT BY N-HASH ALGORITHM  Address (1, 2) (1, 5) (2, 5) (2, 4) (2, 3) (1, 4) (1, 3) (3, 5) Count 4 2 2 2 4 1 4 1  Content {i1, i2} {i1, i5} {i2, i5} {i2, i4} {i2, i3} {i1, i4} {i1, i3} {i3, i5} {i1, i2} {i1, i5} {i2, i5} {i2, i4} {i2, i3}  {i1, i3} {i1, i2}    {i2, i3}  {i1, i3} {i1, i2}    {i2, i3}  {i1, i3}

III. IMPROVEMENT ON N-HASH ALGORITHM  A. Aadvantages and disadvantages of N-Hash Algorithm N-Hash algorithm has some advantages. Its rationale is  simple and clear, it can be carried out without any Hash function and will not encounter Hash colliding problem, it absorbed the merits of 2-dimensional Hash algorithm [6] which employ 2-dimensional vector to represent bucket address. N- Hash algorithm generate the appropriate bucket address just for the bucket content, the number of address is the most but necessary and the least but sufficient number needed to avoid Hash colliding, that is it is the best value.

However, N-Hash algorithm also has disadvantages, there is still room for it to be improved. When we put some bucket content, i.e. a itemset, into its corresponding bucket address, we need to check those existed addresses to see whether one of them matches or not, the process to do this is fairly time- consuming. Take Table 3 as example, suppose that we have a 2-itemset on hand to be handled, say 1 3{ , }i i , we have to check the previous 6 addresses: (1, 2) , (1,5) , (2,5) , (2,4) , (2,3) and (1, 4) , the 7th address (1,3)  is the one we want, to find the address (1,3)  for 1 3{ , }i i  cost us 7 times of matching, so much time is wasted. In the next, we use sampling methods to solve the problem and to further improve the efficiency of N-Hash algorithm.

B. Sampling method Sampling method is a popular method in computational  statistics; two important terminologies related to it are population and sample.  The population is defined in keeping with the objectives of the study, a sample is a subset of population. Usually, when the population is large, if the sample is scientifically chosen, it can be used to represent the population, because the sample reflects the characteristics of the population from which it is drawn.

C. Main idea of  sampling based N-Hash algorithm Here, sampling method is employed to improve N-Hash  algorithm. Usually, in data mining, the population is large, so the sampling method is appropriate. Take the above database as an example, suppose that the transaction data in Table 1 is a carefully chosen sample of some population P (Certainly the sample here is too small, in practice, a sample will often be larger, we just use a small sample to demonstrate our idea),  after the sample has been dealt with, we got the Hash table (see Table 3). From table 3, we discovered that the occurrence frequency of 1 3{ , }i i  is high, but there are so many addresses before 2-itemset 1 3{ , }i i ?s address (1,3) , so much time is wasted in finding 1 3{ , }i i ?s corresponding address (1,3) . If there are less addresses before the address (1,3) , not so much time will be needed. Therefore using sampling method can save much time, if the sample is carefully chosen, the sample can represent the population, and then the Hash table that comes from the sample can represent that comes from the population, the 2-itemsets with high frequency in sample?s Hash table are liable to be the one with high frequency in population?s Hash table. So we propose a method that combines sampling method and N-Hash algorithm to improve the efficiency of N-Hash algorithm, the method is called sampling based N-Hash algorithm. To search for frequent itemsets, the sampling based N-Hash algorithm works as follows:  Step 1 Carefully draw a sample S  from the population P , usually by random sampling;  Step 2  Use N-Hash algorithm to deal with the sample S to  get a Hash table, denoted as Hash table SH ;  Step 3  Rank the Hash table SH with respect to the frequency of bucket content in order to make the bucket address with high frequency lie in the former and that with low frequency the latter, then we get a new Hash table SRH ;  Step 4 Based on SRH , Use N-Hash algorithm to deal with the rest sample of the population P , i.e. P S? , when finished, get a Hash table denoted as PH ;  Step 5 Obtain frequent itemsets according to predetermined minimum support count.

D. Demonstration of Sampling Based N-Hash algorithm In the above step 1, in practice, draw sample usually by  random method, but our sample is rather special here, the reason why we do like this is that our main purpose is to make it convenient to demonstrate how our algorithm works in a simple way, and convenient to show our algorithm?s efficiency.

After our above explanation, suppose we have database to be dealt with shown in Table 4.

TABLE IV.      DATABASE USED TO SHOW SAMPLING BASED N-HASH ALGORITHM  TID List of item_IDs 1 i1, i2, i5 2 i2, i4 3 i2, i3 4 i1, i2, i4 5 i1, i3 6 i2, i3 7 i1, i3 8 i1, i2, i3, i5 9 i1, i2, i3 10 i1, i2, i5 11 i2, i4 12 i2, i3 13 i1, i2, i4 14 i1, i3 15 i2, i3 16 i1, i3 17 i1, i2, i3, i5 18 i1, i2, i3  We use sampling based N-Hash algorithm to get Hash table denoted as PH  with respect to database in Table 4  Step 1 Draw a sample S  from the population P , shown in Table 4;  Step 2  Use N-Hash algorithm to deal with the sample S to  get a Hash table, denoted as Hash table SH , shown in Table 3;  Step 3  Rank the Hash table SH with respect to the frequency of bucket content in order to make the bucket address with the high frequent content lie in the former and that with low frequent content the latter, get a new Hash table  SRH , shown in Table 5; Step 4 Based on SRH , Use N-Hash algorithm to deal with  the rest sample of the population P , i.e. P S? , when finished, get a Hash table denoted as PH , shown in Table 6;  Step 5 Obtain frequent itemsets according to predetermined minimum support count. If we set support count as 6, we find that 2-itemset 1 2{ , }i i , 2 3{ , }i i  and 1 3{ , }i i are frequent.

In this example, comparing with N-Hash, approximately 24.6% matching times ((86-69)/69?24.6%) has been saved in dealing with the transactions in P-S.

