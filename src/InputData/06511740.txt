A SLA-Based Method for Big-Data Transfers with  Multi-Criteria Optimization Constraints for IaaS

Abstract?When one pays for a Cloud Service he wants the Service to be compliant with standards and to respect his needs (respect the Service Level Agreement). When manipulating big- data (like picture collections, satellite images or digital libraries), transfers must be optimized. The Cloud Infrastructure itself utilizes big-data transfers while migrating Virtual Machines inside an Infrastructure as a Service system. In this paper we discuss data transfers in the cloud, which affects performance in the case of Virtual Machine migration and of user submitted big- data transfers, by moving resources where they are needed. We suggest a scheduling policy and offer two greedy scheduling algorithms that minimize individual transfer times. This is mostly important for being able to have a High Performance Computing environment inside the cloud, for scenarios in which individual data transfer times need to be but when the data is transferred is not an issue. We present an empirical validation based on simulation experiments.

Keywords?Data Transfer, SLA, Optimization, IaaS, Cloud Computing, Scheduling

I.  INTRODUCTION Cloud Computing has brought a new paradigm in  Computer Science: one in which, for the first time, processing power is infinite. Users can use the elasticity of the cloud to compute difficult problems without the need for extensive hardware setup. But this elasticity comes with a cost: Virtual Machines need to be created, migrated or replicated; Back-ups need to be created on the fly. All of these operations take certain amounts of time, depending on the infrastructure the cloud provider has to offer, and the Service Level Agreement constraints, which cannot be ignored.

In this paper we discuss big-data transfers in the cloud.

They affect performance in the case of user submitted data transfers, by moving resources to the virtual machine on which they are needed and in the migration of Virtual Machine migration.

An efficient data transfer mechanism will provide a High Performance Computing environment inside the cloud and it will provide more transparency to the user in the migration process of a Virtual Machine from one part of the cloud to the other. How Virtual Machine migration, copy and saving its current state affects performance is studied in [1]. It has been shown that these operations have a big effect on the performance and on the amount of time in which a virtual machine stays inactive.

With the growth in storage capacity and processing power, we can predict that Moore Law will continue to be obeyed and that Virtual Machine size inside the cloud will grow. This makes Virtual Machine migration even more difficult, by requiring bigger amounts of bandwidth and by raising the amount of downtime while a migration is in progress. This problem is even more important in the case of hybrid Clouds where the interconnection between individual clouds tends to have smaller amounts of bandwidth and a more unreliable connection.

Virtual Machine migration, Virtual Machine deployment and Big-data transfers in which the data needs to be transferred as fast as possible, but when the data is transferred is not relevant, raise the need for a policy where individual transfer time needs to be minimized, even at the cost of total transfer time.

For this we offer a greedy scheduling algorithm that will minimize the transfer duration for individual transfers, inside a hybrid cloud topology. To improve on this initial algorithm we add a priority, given throw the SLA, to each transfer that needs to be executed and modify the algorithm to account for this priority.

We test the algorithms inside a simulation environment with a simulator we implemented for this particular problem.

We describe the simulator in the following sections.



II. VIRTUAL MACHINE MIGRATION INSIDE A HYBRID CLOUD  In the context of big-data transfers, a few big questions need to be answered in order to have an efficient cloud environment: when and where to migrate. In this context, an efficient data migration method, focusing on the minimum global time, is presented in [2]. The method however does not try to minimize individual migrations duration. In [3] is proposed a system that detects the need of VM migration, in the context of a SLA violation. The system is called Sandpiper and it also estimates the amount of additional resources needed. In [4] two migrations models are described: offline and online. The offline scheduling model has as main target the minimization of the maximum bandwidth usage on all links for all time slots of a planning period. In the online scheduling model, the scheduler has to make fast decisions and the migrations are revealed to the migration scheduler in an a priori undefined sequence. Jung et al. treats in [5] the data    mining parallelization by considering the data transfer delay between two computing nodes. This delay is estimated by using the auto-regressive moving average filter. In [6], the impacts of two resource reservation methods are tested: reservation in source machine and reservation in target machine. Experimental results proved that the resource reservation in target machines is needed, in order to avoid the migration failure, and the performance overheads of live migration are affected by memory size, CPU, and workload types.

None of these papers tries to minimize the duration of individual virtual machine migration. The time it takes to suspend the virtual machine, to transfer its data, which can have tens of GBs, and to start the virtual machine may bring a significant downtime to the clients system. If the virtual machine size grows this problem becomes even more pressing.

We estimate that scheduling data transfers, so that they do not overlap, will reduce individual transfers duration and thus will shorten downtimes experienced by the end user. In the following sections we will prove these concepts.



III. EXPERIMENTAL SCENARIOS A Cloud offers to the end user the needed processing  power, using its dynamically allocation mechanisms of resources to match the user?s needs. The distribution of CPU, memory, bandwidth and storage is made in a transparent way for each user but cannot exceed the physical capabilities of the network. It also cannot offer smaller latency.

The power of balancing the resources over the cloud is a key feature when a High Performance Computing is implemented over a Cloud domain. Cloud providers can offer a fully redundant infrastructure for HPC application and simulate a full mesh distributed network. This is achieved by using a layered topology described in [4] and [7], where the physical hosts are connected to the first level, called ?Edge Switch Level?, typically through a 1Gb/s connection. The edge switches are connected to the core switches through a 10Gb/s, and the core switches to the routers with the same 10Gb/s connection. As a result of the high number of ports a switch can support this topology can create a full mesh connection with a large number of physical units. Multiple routers are used to connect to different areas of the Internet or to interconnect with other Clouds. This topology can be visualized in Fig. 1.

Fig. 1. Cloud Network Topology   Taking into consideration the described topology, our  simulation is based on multiple Cloud domains with physical machines connected in a full mesh network. Looking at the whole map of domains, they are also connected directly or over the Internet in a full mesh topology. In our simulation we presume each cloud has three physical processing units and one routing unit, that is unable to create messages but can forward them. In the presented topology, four Clouds with the described infrastructure are interconnected. We presume an ideal environment and we do not take into consideration real life events, like temporary disconnects or bandwidth variation over time. Bandwidth and latency are constant over time and all connections have the same bandwidth. The network topology can be observed in Fig. 2. The physical machines  have a number of messages with random destinations inside the hybrid cloud network. Messages are broken into several packets and the packets are scheduled for delivery. Over one connection two packets can be sent in every time unit, one in each direction.

The interactions (over Internet) between customers and providers increase in an exponential way, so the key to a profitable PaaS is user density per resources (physical or virtual machine). The scenario proposed in Fig. 1 highlights the model of Cloud System Networks. Scientific applications (addressing data intensive services or computationally intensive processing) follow the distributed computing paradigm (Cloud Computing) addressing different fields: medicine, physics, astronomy, chemistry, and even economy.

Fig. 2. The network topology     Fig. 3. General Schema

IV. SIMULATOR DESIGN The general flow is: the customer makes a request for a  certain service respecting a specific SLA; the broker receives the SLA and may decide to divide it in many sub SLAs. For example, the client will require three nodes, each with a specific CPU and Memory requirements. In this case, the broker will realize three others SLAs, for each node in part, and will try to schedule it on different hosts. The broker may schedule it in the private cloud or in the public one. The private and public clouds are connected with each other.

Depending on where the task is scheduled Virtual Machines might need to be migrated and a substantial amount of time is spent in the migration process.

The user could also interact with the broker through an API and let it know Big-data transfers need to be done. The broker would then take the SLA into account and schedule the transfer as needed. This requires a minimum guaranteed bandwidth on all the connections so that the scheduling can be done correctly. We analyze the effects of big data transfers and virtual machine migration. The need to migrate and analyzing when a migration should be executed or a transfer is studied in [3].

To test our scheduling algorithm for big data transfers like virtual machine migration we have created our own simulator that has support for multiple clouds. Its schema is described in Fig. 4. The simulator goes through a few stages, it loads the node topology and then it generates messages. Each message    represents a Big-data transfer such as a Virtual Machine migration, it has a source and a destination, and other data may be added. The message is then taken by the simulator and split into packets and then ordered in an outgoing queue. The rest of the simulator deals with the actual transfers. The simulator takes into account bandwidth and any other needed statistics and generates logs at the end of each simulation.

Fig. 4. Simulator design  It is important to mention that the modules in the figure are interchangeable, ergo if one wants to write a different scheduler, that individual module can be changed.



V. GREEDY SCHEDULING WITH AND WITHOUT PRIORITY We have implemented three scheduling algorithms inside  our simulator. The first one is the simplest one: it does not perform any scheduling at all, it simply sends the packets in a first come first serve order. This is how standard networking devices work and how all transfers are treated in a real life environment. This algorithm sends the packet as it receives, so it should have a smaller arrival time for all packets than any other algorithm. Because of the way the packets can get mixed, individual message arrival time can be high.

Our first proposal is the greedy scheduling algorithm. This algorithm get the transfer requests in a first come first serve order and sets a time interval in which they can be sent. This interval is reserved on all the connections the packet has to go through (in our case there is a maximum of three hops to destination, because of the full mesh infrastructure). This is done until there are no more messages to schedule, taking into account the previously reserved time frames for each individual connection. It is important that we treat connections individually because some may prove to be a bottleneck. For instance the connections between individual clouds need to send more messages than connections inside the cloud. This way, even if the connection from a physical machine to a router is unused, the connection between the routers can be oversaturated. There is no point in scheduling the migration  until the transfers that are currently running between the routers ends, even if the connection to the router is unused.

Fig. 5. Individual Transit Time comparison between Greedy and FCFS  We extended this algorithm by adding a priority marker to the messages, a priority that is set through the Service Level Agreement. It has values from 0 to 10, with higher values for better priorities.  This priority is similar to the one some Cloud providers offer for CPU. The virtual machines have a higher priority when using the CPU and get to use it for longer periods of time. The messages are sorted according to the priority and afterwards scheduled. The bigger the priority, the earlier the packets will be scheduled.



VI. SIMULATION RESULTS We have simulated the algorithms that were described in  the previous section. The first come first serve (FCFS) algorithm was used, as a comparison for the proposed greedy scheduling algorithm. This can be seen in Fig. 5, where we compared the transit time for individual packets in the two versions. We tested with a message size of 30 packets and 1 packet per each connection. It is clearly visible how the greedy scheduling transit time per message is about 30 time parts (it differs if the message is sent in the same cloud or in a different    cloud). The default method has bigger transit times for most of the messages. This proves that the scheduling mechanism works and provides the expected results.

In the next figure (Fig. 6), we compared the messages arrival time between the greedy scheduler and FCFS. The results are comparable. In some cases Greedy is slower, but the goal of this algorithm is to obtain smaller transit times and not arrival times. Other papers, like [2],[4] or [6]  treat the problem of arrival time. Although we do not try in any way to minimize arrival times, the graph in figure 6 shows how the results are comparable and the proposed algorithm doesn?t add significant latencies.

We also propose the greedy scheduling with priority, which acts the same way as the classical greedy but takes into account the message priority, priority given by the SLA. This priority is negotiated between the user and the cloud, the higher the priority is the higher the price paid for the service will be.

Fig. 7 compares the arrival times of a messages sequence, for Classical Greedy and Greedy with priority. By analyzing the trend lines, it can be observed that the Greedy Scheduling with Priority manages to deliver the messages with higher priority faster than the others.

In this context, Cloud computing has the potential to be the major solution for scalability, mobility, reliability, fault tolerance and security for business and academic environments in the same time.

As computing needs increase, multiple administrative domains will manage the resources in order to ensure the QoS and guarantee the SLA. Considering this, finding a way to a smart Cloud infrastructure will consider dependable management services to control the infrastructure and provide essential system-level functionality.

It is important to outline that the two algorithms only try to minimize the transit time. Both accomplished this goal and are best fitted for environments in which transition times need to be minimized and arrival times for most messages can be sacrificed.

Ergo the algorithms are best suited for virtual machine migrations and users big-data transfers that can be delayed but need to be executed in the shortest time possible. This happens when the user handles data that can be modified until the transfer starts.

Fig. 6. Individual Arrival Time comparison between Greedy and FCFS     Fig. 7. Comparison between greedy with and without Priority

VII. CONCLUSION AND FUTURE WORK This paper highlights the problem of data transfers in the  cloud, transfers that affect cloud performance by moving resources to the virtual machine on which they are needed. We described a method for big-data transfer optimization based on network characteristics and we considered the constraints introduced by SLA. We proposed two greedy scheduling algorithms and we compared them to a standard First Come First Served, packet transfer order.  Several simulation experiments highlight the obtained improvement.

Having a scheduler for transfers that takes into account global transfers can bring an improvement to applications and to the user experience. We believe that better algorithms may be obtained that further improves on the scheduling so that cloud resource usage is minimized. We also considered using the discussed algorithms for Cloud user data transfers, for this to be practical an API needs to be provided to the Cloud user, throw which to schedule only the big-data transfers. The API should offer some sort of feedback, which can be utilized by the user to further optimize it?s processing.

