Naive Bayes Associative Classification of Mammographic Data

Abstract- In this paper we focus on a new model, named ANB (Associative Naive Bayes) model. ANB model extend the modeling flexibility of well known Naive Bayes (NB) models by introducing rules generated by associative classifier. The model  consists of two layers: an input layer and an internal layer. We propose an associative classifier algorithm (AAC), relaxing the condition of independence of attributes in NB, for generating rules and learning network parameter and a simple algorithm  for training ANB models in the context of classification.

Experimental results show that the learned models can significantly improve classification accuracy as compared to NB.

Keywords-Naive Bayes classifier; association rule; associative classification; Bayes theorem

I. INTRODUCTION  Worldwide, breast cancer is the most common type of cancer among women and the leading cause of cancer deaths in women today. In United States current breast cancer lifetime risk is estimated at 12.7% for all women [9]. Among Asian and Pacific Islander women incident rates continued to increase at 1.5% per year (89 out of every 100,000) but are still significantly lower than white women [9]. Early detection and treatment of breast cancer while it is still small improves a woman's chances of survival before they can be felt. Currently, mammography remains one of the best screening methods used by radiologists for early detection of cancerous tumors. It plays a major role in early detection, detecting about 75% of cancers at least a year before they can be felt [13].

Several computer aided diagnosis (CAD) have been successfully developed to build breast cancer classifiers that can help in early detection of malignancy. CAD can help reduce the number of false positives and therefore reduce the number of unnecessary biopsies. The most common tasks performed by a CAD system are the classification task.

Classifier such as Bayesian network [3], [8], artificial neural network [11], NB classifier [3], [14], associative classifier [12], [6] have been successfully used to predict breast cancer by several researchers. One of the simplest, and yet most consistently well performing set of classifiers is the NB models [5].

NB classifier is a simple probabilistic classification scheme based on Bayes rule of conditional probability.

Classification with NB is the task of predicting the class of an instance from a set of attributes describing that instance and assumes that all attributes are conditionally independent  978-1-4244-7662-6/$26.00 CO 2010 IEEE 276  Siri krishan Wasan Department of Mathematics, Jamia Millia Islamia  New Delhi, India e-mail: skwasan@yahoo.com  given the class label. However, this assumption is clearly violated in many real world problems. To resolve this problem, methods for handling the conditional dependence between the attributes have become a lively research area.

Effort has been made by introducing NB variants [7], [14].

Baye's theorem gives a mathematical representation of how the conditional probability of events  HI' H 2, ........ H m (called hypothesis) that are pair wise disjoint is related to the event E (called evidence) that gives information about which hypothesis are correct.

Associative classification is a classification process, where data mining techniques association rule mining and classification are used. In this process association rules are generated and analyzed for use in classification.

Let D be a dataset. Each record in D is described by n  attributes A = {AI' A2 , .......... An } where n denote the number of the attributes. The dataset also has a target  attribute C = {CPC2, .......... Cn } , which is called the class attribute. The class attribute C is considered separately from A . An item P is an attribute value pair of  the form (Ai' v) where Ai is an attribute taking a value v.

A data record X= (xPx2, ........ xn) satisfies an item P = (Ap v) , if and only if Xi = Vi where Xi is the value of the ilh attribute of X . In associative classification, a rule is of the form P I /\ P 2 ........... PI ? C where the rule antecedent is a conjunction of items  PPP2 .............. Pn(l?n) and consequent is the  associated class label C .

For a given rule R , the percentage of the records in D  satisfying the rule antecedent that also have the class label C is called the confidence of R . The percentage of tuples in D satisfying the rule antecedent and having class label C is called the support of R .

Receiver operating characteristic (ROC), is a graphical plot of the sensitivity, or true positives, vs. (1 - specificity), or false positives, for a binary classifier system [5]. The formula for sensitivity and specificity are:  Sensitivity = TP/(TP + FN), Specificity = TN/(TN + FP) Here TP stands for true positive, FP for false positives,  FN for false negatives. In a ROC curve the true positive rate (Sensitivity) is plotted in function of the false positive rate     (I-Specificity) for different cut-off points of a parameter.

Each point on the ROC plot represents a sensitivity/specificity pair corresponding to a particular decision threshold. The area under the ROC curve is a measure of how well a parameter can distinguish between two diagnostic groups (diseased/normal).

In this paper, associative classification is used with NB classifier to develop a model ANB (Associative Naive Bayes) classifier for classifying breast cancer mammographic data. In our paper we assume that the attributes are not conditionally independent. All correlated itemsets with the class label are generated in the form of rules using AAC algorithm. Rules are introduced to relax some of the independence condition of the NB classifier. It is observed that application of these rules in creating the structure of NB and allowing only those features present in the rules as input can improve the development and performance of a classifier. For evaluation of the performance of ANB model and NB model we created receiver operating characteristic (ROC) curves and calculated the area under the ROC curve for comparison.

NB is a simple form of Bayesian network that is widely used for classification and clustering [5]. Bayesian network is a directed acyclic graph with joint probability distribution, where there is a 1-1 correspondence between nodes in the graph and random variables in the probability distribution that is factorized according to the structure of the graph [4].

Bayesian network classifiers have been evaluated as potential tools for the diagnosis of breast cancer [3]. In [8] bayesian network computer model is used to accurately predict the probability of breast cancer on the basis of risk factors and mammographic appearance of microcalcifications, by using probabilistic relationships between breast disease and mammography findings to estimate the risk of malignancy. Breast cancer models which use BN theory such as [3], design a multi-view causal model which incorporates image analysis knowledge and domain knowledge of mammographic screening, aiming to obtain an understandable domain description, in terms of the variables represented and the mechanisms under which those variables are related to one another. This model established a clear relation between features extracted from the application of the single view CAD system on mammograms.

Our model is created and tested using the data from UCI repository [10], and name of the dataset is mammographic mass data. We have used the same dataset in our earlier paper [17] for classifying the data using neural network and associative classifier. The dataset consists of 961 instances with 516 classified as benign and 445 as malignant. This database contains BI-RADS assessment, patient's age and three BI-RADS attributes together with the ground truth i.e., benign and malignant those have been identified on full field digital mammograms collected at the Institute of Radiology of the University Erlangen-Nuremberg between 2003 and 2006. In our approach AAC algorithm is used to generate rules to create NB classifier model called ANB classifier model to classify breast cancer mammographic data. The ANB model is a tree shape two layer Bayesian   network. The class variable C is the root of the tree, the attributes are at leaves and the rules are all internal. The use of rules allows conditional dependencies to be encoded in the model. Rules are introduced as parents of attribute values. Several network models are created setting a starting minimum support of 5% and confidence of 50%. Using ten? fold cross validation the network model in trained and tested.



II. NAiVE BAYES CLASSIFIER  Baye's theorem gives a mathematical representation of how the conditional probability of events  HI,H2, ........ Hm (called hypothesis) that are pair wise disjoint is related to the event E (called evidence) that gives information about which hypothesis are correct. That  is to fmd the conditional probabilities P( Hi I E) for i = 1,2, . ?  ? ,m [18].

We have, P(Hi I E) = P( Hi n E)  (1) P(E)  P(Hi n E) = P(HJP(E I HJ (2) Since only one of the events HI' H2 , ........ H m can  occur, the probability of E can be written as P(E) = P(HI nE)+ ..... +P(Hm nE)  using (1) the above equation can be written as  P(E) = P(HI)P(E I HI) + ..... + P(Hm)P(E I Hm) (3) Using equation (1), (2) and (3), Bayes theorem is,  P(Hi I E) = mP(HJP(E I HJ (4) 'LP(HJP(E I HJ i=I  NB classifier is a simple probabilistic classification scheme based on Bayes rule of conditional probability. It is an independent feature model [4]. It works on the assumption that its attribute are conditionally independent given the class label. It learns from the training data, from the conditional probability of each attribute values given the class label.

The conditional independence assumption of Naive  Bayes between the various attribute values A; ,  i = 1,2, ? .. , n; n ;::: 1 ; determines that each attributes Ai' is conditionally independent of every attribute A j , for i '* j , given the class variable. In other words:  P(Ai I C,AJ= P(Ai I C) (5) The expression for the probability that class  C = <  CI, ...... Cm > will take on its ,di possible value, according to Bayes rule, is    2010 International Conforence on Educational and Network Technology (ICENT 2010)  p(Ci I AI, ... An) = m P(CJP(AI' .... An I Ci)  (6)  Ip(Cj)P(AI,????An I Cj) j=1 Using equation (1), n  P(Ci)I1 peAk I Ci) p(Ci I AI'???? )= -m-----:::..;k=""- In--?  Ip(cj)I1 peAk I Cj)  m n  j=1 k=1  Where,Z = Ip(Cj)I1 peAk I C) j=1 k=1 P( Ci) = prior probability and, P( Ak I Ci) = feature probability distribution  (7)  Learning task of this model is to estimate two sets of parameter class prior probability and feature probability distribution. Classification using this NB probability model is done by picking the most probable hypothesis which is also known as the maximization criterion. We have the classifier function as follows: n NB(AI,???AJ=argmaxP(CJI1P(Ak I CJ (9) cjeC k=1  The structure of the NB is graphically shown in Fig. 1,  where C is the class variable and AI' ... An are the attributes.

Each attributes AI' ... An is the child of the parent node C .

Figure I. Structure of NaIve Bayes.



III. ASSOCIATIVE NAIVE BAYES CLASSIFIER  In this section, we create a model ANB using AAC algorithm and NB classifier. The complete set of class association rules is determined from the training data set using associative classifier algorithm. The training data set is divided into IO-parts and for each part class association rules are generated setting a starting minimum support of 5% and minimum confidence of 50%. For each rules generated ANB model is created.

A. Association Rule Generation  Given a training dataset D with k classes and minimum support, the dataset is scan and frequent I-item set are generated. The frequent I-item set are sorted in support descending order. And items not satisfying the minimum support threshold are removed from the data sets. The remaining frequent items are arranged in the form of vertical  TABLE I. TRAINING DATA SET  SI.no Rl R2 R3 C I Al BI D3 CI 2 A2 B3 04 CI 3 A2 B3 04 C2 4 Al B3 02 C2 5 Al B4 D2 C2 6 Al B2 03 C2 7 A2 BI 03 CI 8 A2 B4 D1 CI 9 Al B4 D3 CI  10 A2 BI 02 C2 11 A2 B2 D2 C2 12 A2 BI 01 CI 13 A2 BI D1 CI 14 Al B2 D2 C2 15 Al B2 02 C2  data representation [1].

Example, let D be data sets containing 12 elements as  shown in TABLE I, here D is described by three attributes Rl, R2, R3 and a class label C . There are 15 records in this datasets i.e., IDI = 15 . The attributes Rl has values AI, A2 ; R2 has values Bl, B2, B3, B4 and R3 has values Cl, C2, C3 and C 4 . The class label C has values Cl and C2. The support threshold is taken as 20% and confidence as 80%.

First AAC algorithm scan the data set D once, fmds the items satisfying the minimum support value in D. The set is F = {AI, A2, Bl, B2, Dl, D2, D3} called the frequent item set. Then AAC Classifier sorts the items in F in support descending order, i.e., F-Sorted = {A2,Dl,Al,Bl,B2,D3,D2} . All other attribute values, which fail the support threshold, are removed from the data set D i.e., {B3, D2} . The remaining attribute values are arranged in the form of vertical data format w.r.t.

F-Sorted as shown in TABLE II. From TABLE II intersect column A2 = {2,3,7,8,1O,11,12,13} and Cl = {1,3,4 ,5,8,12,13} and we get {3,8,12,13} [1].

Since I A2 n Cll= 4 satisfies the minimum support threshold Al is inserted in Cl set. Each column in TABLE II intersect with the class column and generated two sets Cl and C2 i.e., Cl = {A2,Dl,Al,Bl} and C2 = {B2, D3, D2} . For each intersection of the attribute value column with the class column prior probability and posterior probability of the attribute value given the class is     calculated and stored in a probability table. Then for each class sets possible combination of the item sets are found and itemsets satisfying the minimum support threshold are inserted in CR-tree [2] for rule generation and at the same time posterior probability of the itemsets given the class label are calculated and stored in the probability table.

TABLE II. REFINED DATA  A2 DI Al BI B2 D3 D2 CI C2 2 I I I 2 6 2 I 2 3 3 4 3 4 9 II 3 6 7 4 5 5 9 10 14 4 7 8 5 6 7 10 15 5 9  10 7 9 8 II 8 10 II 8 14 12 14 12 II 12 12 15 13 15 13 14 13 13 15  Rule pruning is done similar to CMAR algorithm [2] and a set of class association rules are generated. Once, a set of class association rules are generated, conditional probability of each rule from the probability table are stored in a conditional probability table. From the example in Table I the rules generated are: (Rl) {D3} ? {C2} confidence = 100.0%, sup(R) = 33.33%, sup(A) = 33.33%, sup(C) = 53.33% (R2) {AI, D3} ? {C2} confidence = 100.0%, sup(R) = 26.67%, sup(A) = 26.67%, sup(C) = 53.33% (R3) {B2, D3} ? {C2} confidence = 100.0%, sup(R) = 26.67%, sup(A) = 26.67%, sup(C) = 53.33% (R4) {AI, Dl} ? {Cl} confidence = 100.0%, sup(R) = 20.00%, sup(A) = 20.00%, sup(C) = 46.67% Where: sup(R) = support for rule, sup(A) = support for antecedent and sup(C) = support for consequent.

The conditional probability table of the rules generated is shown in TABLE III. Thus, with the associative classification rule AAC algorithm generates conditional probability of each rule from the training datasets.

TABLE III. CONDITIONAL PROBABILITY TABLE  Rule CI C2  RI 0 1.00  R2 0 1.00  R3 0 1.00  R4 1.00 0  B. ANBModel ANB network model consists of two layers an input  layer and an internal layer and has a central node, called the class node C. Nodes in the input layer are represented by one characteristic from each rule. Each rule consists of attribute values and each attribute value in the rule is called a characteristic. Thus number of input nodes is equal to the number of characteristics in the rules; the number of internal   nodes is equal to the number of rules. The network model is shown in Fig. 2.

Structure of ANB model:  (1) Input node: Last layer of the network. In this layer each input nodes is input by one characteristic from each of the rules, i.e. each node represents one characteristic.

(2) Internal node: Second layer of the network which is the neighborhood of the input layer. In this layer each internal node is connected with the characteristic of each rule. The number of nodes in this layer is equal to the number of rules.

(3) Root node: The class variable is the root node it is connected with the internal node.

Figure 2. Structure of ANB model.

In an ANB model as shown in Fig. 2 the class variable C is the central node and the attribute values are at the input layer; the rules RI, R2, R3 and R 4 are all internal. The use of rules allows conditional dependencies to be encoded in the model. For instance by introducing a rule R3 as parent of the attribute value Al and D3 as shown in Fig. 2, we can represent the dependence statement Al is not conditionally independent to D3 given C . An ANB model the correlation among the attribute variables given the class by using the rules generated by AAC algorithm.

C. Learning ANB model The model is learned using 10- fold cross validation. In  the learning process for each instance ( in training data sets,  possible combination of items in ( = {(p .. 1m} are found and each item combinations are compared with items in the rules and if a rule is found matching the item combination then the conditional probability of the rule is assigned to the rule node. For example given an instance ( = {AI, B2, D3} , possible combination of the instance ( are {AI} , {B2} , {D3} , {AI,B2} , {AI,D3} , {B2, D3} , {AI, B2, D3} . From the model in Fig. 3 item combination {D3} , {AI,D3} and {B2,D3} matches with the item combination in rule RI, R2, and R3, then the posterior probability of RI, R2, and R3 given Care:    2010 International Coriference on Educational and Network Technology (ICENT 2010)  P(Rll C)= P(D31 C), P(R21 C) = P(Al,D31 C),  andP(R31 C) = P(B2,D31 C) Since rule R 4 has no matching item combination  posterior probability of R 4 is calculated as, P(R4 I C) = K ' where k is number of attribute values.

Then the output class probability is computed by,  P(C I t) = P(C)P(Rll C)P(R21 C)P(R31 C)P(R41 C) For each rule Ri, i = 1,2, ... n; n ? 1 , where n is the  number of rules, the output class probability is computed by, n  P(C I t) = p(C)I1 P(Ri Ie) (10) i=1  When the accuracy on the training set is higher than a given accuracy threshold, the mining process is stopped.

Accuracy threshold is set at 95%.



IV. EV ALVA TION OF THE CLASSIFIER  We experiment on mammographic data taken from UCI repository [10], and name of the dataset is mammographic mass data. The dataset consists of 961 instances with 516 classified as benign and 445 as malignant. This database contains BI-RADS assessment, patient's age and three BI? RADS attributes together with the ground truth i.e. benign and malignant those have been identified on full field digital mammograms collected at the Institute of Radiology of the University Erlangen-Nuremberg between 2003 and 2006.

BI-RADS (Breast Imaging Reporting and Data system), developed by the American College of Radiology provides a standardized classification for mammographic studies. The system demonstrates good correlation with the likelihood of breast malignancy. The BI-RADS system can inform family physicians about key fmdings, identify appropriate follow? up and management [15]. The BI-RADS attributes are mass shape, mass margin and mass density [10]. Depiction of fme micro calcification and subtle soft-tissue masses at high? quality mammography is the key to detection of early breast cancer [16]. It has its own characteristics and can be used as a clue to classify masses. Masses can be circumscribed, micro lobulated, round, oval, lobular, irregular etc and radiologist will need to take a good look to fmd the suspicious lesions. Therefore in order to classify a mass, the BI-RADS attributes recorded were used as an input feature.

There are three BI-RADS attributes- they are mass shape, mass margin and mass density. The masses are mass shape: round, oval, lobular and irregular, mass margin: circumscribed, microlobulated, obscured, ill-defmed, speculated and mass density: high, iso, low, fat-containing with the class attributes benign (non--cancerous) and malignant (cancerous).

Data quality and evaluation can be found out easily by knowing the relationships between the attributes. Discovery of relationships between the attributes leads to data cleaning   rules and improved its constraints. These are carried out by analyzing rules between the attributes.

Therefore cleaning process is carried out on the dataset.

The dataset undergo normal statistical cleaning process where all the attributes are distributed normally to check outliers, extreme, noisy or missing values. These values are replaced with the attribute's mean or average depending on its suitability. All the continuous variables are converted into discrete variables. Histogram and normal curve graph were projected to ensure that all data are normally distributed.

Out of the 961 instances two third of the data items are taken for training and one third of the data items for testing Le., 640 data items for training and 321 data items for testing. The model is learned using 10- fold cross validation.

When the accuracy on the training set is higher than a given accuracy threshold, the mining process is stopped. Accuracy threshold is set at 95%.

Our experimental result shows that the accuracy of classifying the test instance is 85.312% with root mean squared error rate of 0.35719. As compared to NB classifier in weka the accuracy percentage is 82.19 with root mean squared error rate of 0.3741.

We also present ROC curves for visualizing classifier performance. It describes the relations between two indices, sensitivity and specificity.

The ROC plot as shown in Fig. 3 shows False Positive rate (X axis), the probability of incorrectly diagnosing a case as positive when its true state is negative against True Positive rate (Yaxis) and the probability of correctly diagnosing a positive case across all decision levels for the  0.9 ? O.B '> ., ? 0.7 ? 0.6 ? 0.5 , ? I ? 0.4  :  ? 0.3 ? 2 0.2  I-  0.1  Figure 3.

0.2 0.4 0.6 O.B False positive rate (1 - Specificity)  No disaimination  -O--ANB  -....... NAIVEBAYES  ROC curve of ANB and Naive Bayes.

diagnostic test. Ideally the curve climb quickly toward the top-left meaning the test correctly identifies cases. The diagonal grey line is a guideline for a test that is unable to correctly identifying cases.

ROC curve for our model ANB and NB are shown in Fig. 3. Area under the ROC curve is an important criterion about classifier performance. AUC (Area Under the Curve) for our model is 0.92 and for Naive Bayes is 0.90.

The experiment is made on a computer with a single 1.73 GHZ Core (TM)2-CPU and 1280 MB memory and AAC algorithm is written in java and ANB classifier model were designed trained and tested in weka.



V. CONCLUSION  We have presented a new model ANB for classifying mammography mass data from UCI repository dataset and our experimental result have shown that accuracy performance of the model ANB reaches 85.312% then 82.19% for NB. Our model also improves on error rate. We find that in ROC curve AUC for ANB is higher than NB.

Thus our model increases classification performance when compared to NB model.

