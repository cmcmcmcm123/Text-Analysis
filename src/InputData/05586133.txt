The 2009 Mario AI Competition

Abstract? This paper describes the 2009 Mario AI Competi- tion, which was run in association with the IEEE Games Inno- vation Conference and the IEEE Symposium on Computational Intelligence and Games. The focus of the competition was on developing controllers that could play a version of Super Mario Bros as well as possible. We describe the motivations for holding this competition, the challenges associated with developing artificial intelligence for platform games, the software and API developed for the competition, the competition rules and organization, the submitted controllers and the results. We conclude the paper by discussing what the outcomes of the competition can teach us both about developing platform game AI and about organizing game AI competitions. The first two authors are the organizers of the competition, while the third author is the winner of the competition.

Keywords: Mario, platform games, competitions, A*, evolutionary algorithms

I. INTRODUCTION  For the past several years, a number of game-related competitions have been organized in conjunction with major and artificial intelligence for games (Game AI). In these competitions, competitors are invited to submit their best controllers or strategies for a particular game; the controllers are then ranked based on how well they play the game, alone or in competition with other controllers. The competitions are based on popular board games (such as Go and Othello) or video games (such as Pac-Man, Unreal Tournament and various car racing games [1], [2]).

In most of these competitions, competitors submit con- trollers that interface to an API built by the organizers of the competition. The winner of the competition becomes the person or team that submitted the controller that played the game best, either on its own (for single-player games such as Pac-Man) or against others (in adversarial games such as Go).

Usually, prizes of a few hundred US dollars are associated with each competition, and a certificate is always awarded.

There is no requirement that the submitted controllers be based on any particular type of algorithm, but in many cases the winners turn out to include computational intelligence (typically neural networks and/or evolutionary computation) in one form or another. The submitting teams tend to comprise both students, faculty members and persons not currently in academia (e.g. working as software developers).

There are several reasons for holding such competitions as part of the regular events organized by the computational intelligence community. A main motivation is to improve  JT is with IT University of Copenhagen, Rued Langgaards Vej 7, 2300 Copenhagen S, Denmark. SK is with IDSIA, Galleria 2, 6928 Manno-Lugano, Switzerland. RB is with Imperial College of Science and Technology, London, United Kingdom. Emails: julian@togelius.com, sergey@idsia.ch, robin.baumgarten06@doc.ic.ac.uk  benchmarking of learning and other AI algorithms. Bench- marking is frequently done using very simple testbed prob- lems, that might or might not capture the complexity of real- world problems. When researchers report results on more complex problems, the technical complexities of access- ing, running and interfacing to the benchmarking software might prevent independent validation of and comparison with the published results. Here, competitions have the role of providing software, interfaces and scoring procedures to fairly and independently evaluate competing algorithms and development methodologies.

Another strong incentive for running these competitions is that they motivate researchers. Existing algorithms get applied to new areas, and the effort needed to participate in a competition is (or at least, should be) less than it takes to write new experimental software, do experiments and write a completely new paper. Competitions might even bring new researchers into the computational intelligence fields, both academics and non-academics. One of the reasons for this is that game-based competitions simply look cool.

The particular competition described in this paper is similar in aims and organization to some other game-related competitions (in particular the simulated car racing compe- titions) but differs in that it is built on a platform game, and thus relates to the particular challenges an agent faces while playing such games.

A. AI for platform games  Platform games can be defined as games where the player controls a character/avatar, usually with humanoid form, in an environment characterized by differences in altitude be- tween surfaces (?platforms?) interspersed by holes/gaps. The character can typically move horizontally (walk) and jump, and sometimes perform other actions as well; the game world features gravity, meaning that it is seldom straightforward to negotiate large gaps or altitude differences.

To our best knowledge, there have not been any previous competitions focusing on platform game AI. The only pub- lished paper on AI for platform games we know of is a recent paper by the first two authors of the current paper, where we described experiments in evolving neural network controllers for the same game as was used in the competition, using an earlier version of the API [3]. Some other papers have described uses of AI techniques for automatic generation of levels for platform games [4], [5], [6].

Most commercial platform games incorporate little or no AI. The main reason for this is probably that most platform games are not adversarial; a single player controls a single character who makes its way through a sequence of levels, with his success dependent only on the player?s skill. The     obstacles that have to be overcome typically revolve around the environment (gaps to be jumped over, items to be found etc) and NPC enemies; however, in most platform games these enemies move according to preset patterns or simple homing behaviours.

Though apparently an under-studied topic, artificial intelli- gence for controlling the player character in platform games is certainly not without interest. From a game development perspective, it would be valuable to be able to automatically create controllers that play in the style of particular human players. This could be used both to guide players when they get stuck (cf. Nintendo?s recent ?Demo Play? feature, introduced to cope with the increasingly diverse demographic distribution of players) and to automatically test new game levels and features as part of an algorithm to automatically tune or create content for a platform game.

From an AI and reinforcement learning perspective, plat- form games represent interesting challenges as they have high-dimensional state and observation spaces and relatively high-dimensional action spaces, and require the execution of different skills in sequence. Further, they can be made into good testbeds as they can typically be executed much faster than real time and tuned to different difficulty levels. We will go into more detail on this in the next section, where we describe the specific platform game used in this competition.



II. INFINITE MARIO BROS  The competition uses a modified version of Markus Pers- son?s Infinite Mario Bros, which is a public domain clone of Nintendo?s classic platform game Super Mario Bros. The original Infinite Mario Bros is playable on the web, where Java source code is also available1.

The gameplay in Super Mario Bros consists in moving the player-controlled character, Mario, through two-dimensional levels, which are viewed sideways. Mario can walk and run to the right and left, jump, and (depending on which state he is in) shoot fireballs. Gravity acts on Mario, making it necessary to jump over holes to get past them. Mario can be in one of three states: Small, Big (can crush some objects by jumping into them from below), and Fire (can shoot fireballs).

The main goal of each level is to get to the end of the level, which means traversing it from left to right. Auxiliary goals include collecting as many as possible of the coins that are scattered around the level, finishing the level as fast as possible, and collecting the highest score, which in part depends on number of collected coins and killed enemies.

Complicating matters is the presence of holes and moving enemies. If Mario falls down a hole, he loses a life. If he touches an enemy, he gets hurt; this means losing a life if he is currently in the Small state. Otherwise, his state degrades from Fire to Big or from Big to Small. However, if he jumps and lands on an enemy, different things could happen. Most enemies (e.g. goombas, cannon balls) die from this treatment; others (e.g. piranha plants) are not vulnerable to this and proceed to hurt Mario; finally, turtles withdraw into their  1http://www.mojang.com/notch/mario/  shells if jumped on, and these shells can then be picked up by Mario and thrown at other enemies to kill them.

Certain items are scattered around the levels, either out in the open, or hidden inside blocks of brick and only appearing when Mario jumps at these blocks from below so that he smashes his head into them. Available items include coins, mushrooms which make Mario grow Big, and flowers which make Mario turn into the Fire state if he is already Big.

A. Automatic level generation  While implementing most features of Super Mario Bros, the standout feature of Infinite Mario Bros is the automatic generation of levels. Every time a new game is started, levels are randomly generated by traversing a fixed width and adding features (such as blocks, gaps and opponents) according to certain heuristics. The level generation can be parameterized, including the desired difficulty of the level, which affects the number and placement of holes, enemies and obstacles. In our modified version of Infinite Mario Bros we can specify the random seed of the level generator, making sure that any particular level can be recreated by simply using the same see. Unfortunately, the current level generation algorithm is somewhat limited; for example, it cannot produce levels that include dead ends, which would require back-tracking to get out of.

B. Challenges for AIs (and humans)  Several features make Super Mario Bros particularly inter- esting from an AI perspective. The most important of these is the potentially very rich and high-dimensional environment representation. When a human player plays the game, he views a small part of the current level from the side, with the screen centered on Mario. Still, this view often includes dozens of objects such as brick blocks, enemies and collectable items. The static environment (grass, pipes, brick blocks etc.) and the coins are laid out in a grid (of which the standard screen covers approximately 22 ? 22 cells), whereas moving items (most enemies, as well as the mushroom power-ups) move continuously at pixel resolution.

The action space, while discrete, is also rather large. In the original Nintendo game, the player controls Mario with a D-pad (up, down, right, left) and two buttons (A, B). The A button initiates a jump (the height of the jump is determined partly by how long it is pressed); the B button initiates running mode and, if Mario is in the Fire state, shoots a fireball. Disregarding the unused up direction, this means that the information to be supplied by the controller at each time step is five bits, yielding 25 = 32 possible actions, though some of these are nonsensical (e.g. left together with right).

Another interesting feature is that there is a smooth learn- ing curve between levels, both in terms of which behaviours are necessary and their required degree of refinement. For example, to complete a very simple Mario level (with no enemies and only small and few holes and obstacles) it might be enough to keep walking right and jumping whenever there is something (hole or obstacle) immediately in front of Mario. A controller that does this should be easy to    // always the same dimensionality 22x22 // always centered on the agent public byte[][] getCompleteObservation(); public byte[][] getEnemiesObservation(); public byte[][] getLevelSceneObservation(); public float[] getMarioFloatPos(); public float[] getEnemiesFloatPos(); public boolean isMarioOnGround(); public boolean mayMarioJump();  Fig. 1. The Environment Java interface, which contains the observation, i.e the information the controller can use to decide which action to take.

learn. To complete the same level while collecting as many as possible of the coins present on the same level likely demands some planning skills, such as smashing a power- up block to retrieve a mushroom that makes Mario Big so that he can retrieve the coins hidden behind a brick block, and jumping up on a platform to collect the coins there and then going back to collect the coins hidden under it. More advanced levels, including most of those in the original Super Mario Bros game, require a varied behaviour repertoire just to complete. These levels might include concentrations of enemies of different kinds which can only be passed by timing Mario?s passage precisely; arrangements of holes and platforms that require complicated sequences of jumps to pass; dead ends that require backtracking; and so on.



III. COMPETITION API  In order to be able to run the competition (and in order to use the game for other experiments), the organizers modified Infinite Mario Bros rather heavily and constructed an API that would enable it to be easily interfaced to learning algorithms and competitors? controllers. The modifications included removing the real-time element of the game so that it can be ?stepped? forward by the learning algorithm, re- moving the dependency on graphical output, and substantial refactoring (the developer of the game did not anticipate that the game would be turned into an RL benchmark). Each time step, which corresponds to 40 milliseconds of simulated time (an update frequency of 25 fps), the controller receives a description of the environment, and outputs an action. The resulting software is a single-threaded Java application that can easily be run on any major hardware architecture and operating system, with the key methods that a controller needs to implement specified in a single Java interface file (see figure 1). On an iMac from 2007, 5 ? 20 full levels can be played per second (several thousand times faster than real-time). A TCP interface for controllers is also provided.

Figure 2 shows the size and layout relative to Mario of the sensor grid. For each grid cell, the controller can choose to read the complete observation (using the getCompleteObser- vation() method) that returns an integer value representing exactly what occupies that part of the environment (e.g. a section of a pipe), or one of the simplified observations (getEnemiesObservation() and getLevelSceneObservation()) that simply returns a zero or one signifying the presence of an enemy or an impassable part of the environment.

Fig. 2. Visualization of the granularity of the sensor grid. The top six environment sensors would output 0 and the lower three input 1. All of the enemy sensors would output 0, as even if all 49 enemy sensors were consulted none of them would reach all the way to the body of the turtle, which is four blocks below Mario. None of the simplified observations register the coins, but the complete observation would. Note that larger sensor grids can be used (up to 22 ? 22), if the controller can handle the information.



IV. COMPETITION ORGANIZATION AND RULES  The organization and rules of the competition sought to fulfill the following objectives:  1) Ease of participation. We wanted researchers of dif- ferent kinds and of different levels of experience to be able to participate, students as well as withered professors who haven?t written much code in a decade.

2) Transparency. We wanted as much as possible to be publicly known about the competing entries, the benchmark software, the organization of the competi- tion etc. This can be seen as an end in itself, but a more transparent competition also makes it easier to detect cheaters, to exploit the scientific results of the competition and to reuse the software developed for it.

3) Ease of finding a winner. We wanted it to be unam- biguously clear how to rank all submitted entries and who won the competition.

4) Depth of challenge. We wanted there to be a real score difference between controllers of different quality, both at the top and the bottom of the high-score table.

The competition web page hosts the rules, the down- loadable software and the final results of the competition2.

Additionally, a Google Group was set up to which all technical and rules questions were to be posted, so that they came to the attention of and could be answered by both organizers and other competitors3, and where the organizers posted news about the competition. The searchable archive of the discussion group functions as a repository of technical information about the competition.

Competitors were free to submit controllers written in any programming language and using any development method-  2http://julian.togelius.com/mariocompetition2009 3http://groups.google.com/group/marioai    ology, as long as they could interface to an unmodified version of the competition software and control Mario in real time on a standard desktop PC running Mac OS X or Windows XP. For competitors using only Java, there was a standardized submission format. Any submission that didn?t follow this format needed to be accompanied by detailed instructions for how to run it. Additionally, the submission needed to be accompanied by the score the competitors had recorded themselves using the CompetitionScore class that was part of the competition software, so that the organizers could verify that the submission ran as intended on their sys- tems. We also urged each competitor to submit a description of how the controller works as a text file.

Competitors were urged to submit their controllers early, and then re-submit improved versions. This was so that any problems that would have disqualified the controllers could be detected and rectified and the controllers resubmitted. No submissions or resubmissions at all were accepted after the deadline (about a week before each competition event). The most common problems that mandated resubmission were:  ? The controller used a nonstandard submission format, and no running instructions were provided.

? The controller was submitted together with and entan- gled in a complete version of the competition software.

? The controller timed out, i.e. took more than 40 ms per time step on average on some level.

A. Scoring  All entries were scored before the conference through running them on 10 levels of increasing difficulty, and using the total distance travelled on these levels as the score. The scoring procedure was deterministic, as the same random seed was used for all controllers, except in the few cases where the controller was nondeterministic. The CompetitionScore method uses a supplied random number seed to generate the levels. Competitors were asked to score their own submissions with seed 0 so that this score could be verified by the organizers, but the seed used for the competition scoring was not generated until after the submission deadline, so that competitors could not optimize their controllers for beating a particular sequence of levels.

For the second phase of the competition (the CIG phase) we discovered some time before the submission deadline that two of the submitted controllers were able to clear all levels for some random seeds. We therefore modified the CompetitionScore class so as to make it possible to differentiate better between high-scoring controllers. First of all, we increased the number of levels to 40, and varied the length of the levels stochastically, so that controllers could not be optimized for a fixed level length. In case two controllers still cleared all 40 levels, we defined three tie- breakers: game-time (not clock-time) left at the end of all 40 levels, number of total kills, and mode sum (the sum of all Mario modes at the end of levels, where 0=small, 1=big and 2=fire; a high mode sum indicates that the player has taken little damage). So if two controllers both cleared all levels,  that one that took the least time to do so would win, and if both took the same time the most efficient killer would win etc.



V. MEDIA STRATEGY AND COVERAGE The competition got off of to a slow start. The organizers  did not put a final version of the competition software online until relatively late, and neglected advertising the competition until about a month and a half before the deadline of the first phase. Once it occurred to the organizers to advertise their competition, they first tried the usual channels, mainly mailing lists for academics working within computational intelligence. One of the organizers then posted a link to the competition on his Facebook profile, which led one of his friends to submit the link to the social media site Digg. This link was instantly voted up on the front page of the site, and a vigorous discussion ensued on the link?s comment page.

This also meant 20000 visitors to the competition website in the first day, some of which joined the discussion group and announced their intention to participate. Seeing the success of this, the organizers submitted a story about the competition to the technology news site Slashdot. Slashdot is regularly read by mainstream and technology news media, which meant that within days there were stories about the competition in (among others) New Scientist4, MSNBC5 and Le Monde6.

At this point, Robin Baumgarten had already finished a first version of his controller, and posted a video on YouTube of his controller guiding Mario through a level, complete with a visualization of the alternate path the A* algorithm were evaluating at each time step7. The video was apparently so enjoyable to watch that it quickly reached the top lists, and has at the time of writing been viewed over 600000 times. The success of this video was a bit of a double-edged sword; it drew hordes of people to the competition website, and led to several announcements of intention to participate (though far from everybody who said they would eventually submitted a controller), but it also caused some competitors to drop out as Robin?s controller was so impressive that they considered themselves to not stand a chance to win.



VI. SUBMITTED CONTROLLERS This section describes the controllers that were submitted  to the competition. We describe the versions of the controllers that were submitted to the CIG phase of the competition; in several cases, preliminary versions were submitted to the ICE-GIC phase. The winning controller is described in some detail, whereas the other controllers are given shorter descriptions due to space consideration. In some cases, a controller is either very similar to an existing controller or very little information was provided by the competitor; those controllers are described only very briefly here.

4http://www.newscientist.com/article/dn17560-race-is-on-to-evolve-the- ultimate-mario.html  5http://www.msnbc.msn.com/id/32451009/ns/technology and science- science/  6http://www.lemonde.fr/technologies/article/2009/08/07/quand-c-est-l- ordinateur-qui-joue-a-mario 1226413 651865.html  7http://www.youtube.com/watch?v=DlkMs4ZHHr8    A. Robin Baumgarten  As the goal of the competition was to complete as many levels as possible, as fast as possible, without regarding points or other bonuses, the problem can be seen as a path optimisation problem: What is the quickest route through the environment that doesn?t get Mario killed? A* search is a well known, simple and fast algorithm to find shortest paths which seemed perfect for the Mario competition. As it turned out, A* search is fast and flexible enough to find routes through the generated levels in real-time.

The implementation process can be separated into three phases: (a) building a physics simulation including world states and object movement, (b) using this simulation in an A* planning algorithm, and (c) optimising the search engine to fulfill real-time requirements with partial information.

a) Simulating the Game Physics: Due to being open- source, the entire physics engine of Infinite Mario Bros is directly accessible and can be used to simulate future world states that correlate very closely with the actual future world state. This was achieved by copying the entire physics engine to the controller and removing all unnecessary parts, such as rendering and some navigation code.

Associating simulated states of enemies with the received coordinates from the API provided an implementation chal- lenge. This association is needed because the API provides only the location and not the current speed, which has to be derived from consecutive recordings. Without accurate enemy behaviour simulation, this can be difficult, especially if there are several enemies in a close area.

b) A* for Infinite Mario: The A* search algorithm is a widely used best-first graph search algorithm that finds a path with the lowest cost between a pre-defined start node and one out of possibly several goal-nodes[7]. A* uses a heuristic that estimates the remaining distance to the goal nodes in order to determine the sequence of nodes that are searched by the algorithm. It does so by adding the estimated remaining distance to the previous path cost from the start node to the current node. This heuristic should be admissible (not overestimate this distance) for optimality to be guaranteed.

However, if only near-optimality is required, this constraint can be relaxed to speed up path-finding. With a heuristic that overestimates by x, the path will be at most x too long[8].

To implement an A* search for Infinite Mario, we have to define what a node and a neighbour of a node is, which nodes are goal nodes, and how the heuristic estimates the remaining distance.

A node is defined by the entire world-state at a given mo- ment in time, mainly characterised through Mario?s position, speed, and state. Furthermore, (re)movable objects in the environment have to be taken into account, such as enemies, power-ups, coins and boxes. The only influence we have on this environment is interaction through actions that Mario can perform. These Mario actions consist of combinations of movements such as left, right, duck, jump and fire.

Neighbours of a node are given by the world state after one (further) simulation step, applying an action that Mario  Fig. 3. Illustration of node generation in A* search. Each neighbour node in the search is given by a combination of possible input commands.

Fig. 4. Illustration of node placement in A* search in Infinite Mario.

The best node in the previous simulation step (right, speed) is inaccessible because it contains an obstacle. Thus the next best node is chosen, which results in a jump over the obstacle. The search process continues by generating all possible neighbours. The speed of Mario at the current node distorts the positions of all following nodes.

executes during this simulation step. See figures 3 and 4 for an illustration of a typical scenario, figure 5 for an in-game visualization. Note how backtracking takes place when an obstacle is discovered, and how the location of the subsequent neighbours is influenced by the current speed of Mario in figure 4.

As only a small window of the world around Mario is visible in the competition API, the level structure outside of this window is unknown. Therefore it does not make sense to plan further ahead than the edge of the current window.

Thus, a goal node for our planner is defined as any node where Mario is outside (or just before) the right border of the window of the known environment.

In the standard implementation of A*, the heuristic esti- mates the remaining distance to the target. In Infinite Mario, one could just translate this as the horizontal distance to the right border of the window. However, this does not lead to a very accurate heuristic, as it ignores the current speed of Mario. To make sure the bot has an admissible (i.e., underestimating) heuristic, it would have to assume Mario runs with maximum speed towards the goal, which is often    Fig. 5. Visualization of the future paths considered by the A* controller.

Each read line shows a possible future trajectory for Mario, taking the dynamic nature of the world into account.

not the case. Instead, a more accurate representation of the distance to the goal can be given by simulating the time required to reach the right border of the window. Here, the current speed of Mario is taken into account. The quickest solution to get to the goal would then be to accelerate as fast as possible, and taking the required time as a heuristic.

Similarly, the previous distance of a node to the starting node is simply the time it took to reach the current node.

c) Variable Optimisation: While the A* search al- gorithm is quite solid and guarantees optimality, certain restrictions need to be put on its execution time to stay within the allowed 40ms for each game update. These restrictions will likely lead to a non-optimal solution, so careful testing has to be undertaken to ensure that the search terminates.

The first restriction used was to stop looking for solutions when the time-limit had been reached, and using the node with the best heuristic so far as a goal node. The linear level design created by the level generator in Infinite Mario favors this approach, as it does not feature dead ends that would require back-tracking of suboptimal paths.

The requirement that the heuristic has to be admissible has also been relaxed. A slightly overestimating heuristic in- creases the speed of the algorithm in expense of accuracy[8].

In detail, the estimation of the remaining distance can be multiplied with a factor w >= 1. Experimentation with different values for w, led to an optimal factor of w = 1.11.

Another factor that has an effect on the processing time required by A* is the frequency of plan recalculation. As mentioned above, limited information requires a recalculation of the plan once new information becomes available, i.e., obstacles or enemies enter the window of visible information.

Experimentation indicated that the best balance between planning ahead and restarting the planning process to in- corporate new information is given when the plan is re- created every two game updates. Planning longer in advance occasionally led to reacting too late to previously unknown threats, resulting in a lost life or slowdown of Mario.

B. Other A*-based controllers  Two other controllers were submitted that were based on the A* algorithm. One was submitted by Peter Lawford and the other by a team consisting of Andy Sloane, Caleb Anderson and Peter Burns (we will refer to this team with Andy?s name in the score tables). These submission were inspired by Robin?s controller and used a similar overall approach, but differed significantly in implementation.

C. Other hand-coded controllers  The majority of submitted controllers were hand-coded and did (to our best knowledge) not use any learning al- gorithms, nor much internal simulation of the environment.

Most of these continuously run rightwards but use heuristics to decide when and how to jump. Some were built on one of the standard heuristic controllers supplied with the competition software. The following are very (due to space limitations) brief characterizations of each:  1) Trond Ellingsen: Rule-based controller. Determines a ?danger level? of each gap or enemy and acts based on this.

2) Sergio Lopez: Rule-based. Answers the questions ?should I jump?? and ?which type of jump?? heuristically, by evaluating danger value and possible landing points.

3) Spencer Schumann: A standard reactive heuristic con- troller from the example software augmented with a calcula- tion of desired jump length based on internal simulation of Mario?s movement. Incomplete tracking of enemy positions.

4) Mario Perez: Subsumption controller, inspired by con- trollers used in behaviour-based robotics.

5) Michal Tulacek: Finite state machine with four states: walk-forward, walk-backward, jump and jump-hole.

6) Rafel Oliveira: Reactive controller; did not submit any documentation.

7) Glenn Hartmann: Based on an example controller.

Shoots continuously, jumps whenever needed.

D. Learning-based controllers  A number of controllers were submitted that were devel- oped using learning algorithms in one way or another. These controllers exhibit a fascinating diversity; still, due to space considerations and paucity of submitted documentation, also these controllers will only be described summarily.

1) Matthew Erickson: Controller represented as expres- sion tree, evolved with fairly standard crossover-heavy Ge- netic Programming, using CompetitionScore as fitness. The tree evaluates to four boolean values: left/right, jump, run and duck. Nonterminal nodes where chosen among standard arithmetic and conditional functions. The terminals were simple hard-coded feature detectors.

2) Douglas Hawkins: Based on a stack-based virtual machine, evolved with a genetic algorithm.

3) Alexandru Paler: An intriguing combination of imita- tion learning, based on data acquired from human playing, and path-finding. A* is used to find the route to the end of the screen; the intermediate positions form inputs to a neural network (trained on human playing) which returns the number and type of key presses necessary to get there.

4) Sergey Polikarpov: Based on the ?Cyberneuron? archi- tecture8 and trained with a form of reinforcement learning.

A number of action sequences are generated, and each is associated with a neuron; this neuron is penalized or rewarded depending on Mario?s performance while the action sequence is being executed.

5) Erek Speed: Rule-based controller, evolved with a GA.

Maps the whole observation space (22? 22) onto the action space, resulting in a genome of more than 100 Mb.



VII. RESULTS The first phase of the competition was associated with  the ICE-GIC conference in London, and the results of the competition presented at the conference. The results are presented in table I, and show that Robin Baumgarten?s controller performed best, very closely followed by Peter Lawford?s controller and closely followed by Andy Sloane et al.?s controller. We also include a simple evolved neural network controller and a very simple hard-coded heuristic controller (ForwardJumpingAgent which was included with the competition software and served as inspiration for some of the competitors) for comparison; none of the agents that were not based on A* outperformed the heuristic controller.

TABLE I RESULTS OF THE ICE-GIC PHASE OF THE COMPETITION.

Competitor progress ms/step Robin Baumgarten 17264 5.62 Peter Lawford 17261 6.99 Andy Sloane 16219 15.19 Sergio Lopez 12439 0.04 Mario Perez 8952 0.03 Rafael Oliveira 8251 ?

Michael Tulacek 6668 0.03 Erek Speed 2896 0.03 Glenn Hartmann 1170 0.06 Evolved neural net 7805 0.04 ForwardJumpingAgent 9361 0.0007  The second phase of the competition was associated with the CIG conference in Milan, Italy, and the results presented there. For this phase, we had changed the scoring procedure as detailed in section IV-A. A wise move, as both Robin Baumgarten?s and Peter Lawford?s agent managed to finish all of the levels, and Andy Sloane et al.?s came very close. In compliance with our own rules, Robin rather than Peter was declared the winner because of it being faster (having more in-game time left at the of all levels). It should be noted that Peter?s controller was better at killing enemies, though.

The best controller that was not based on A*, that of Trond Ellingsen, scored less than half of the A* agents.

The best agent developed using some form of learning or optimization, that of Matthew Erickson, was even further down the list. This suggests a massive victory of classic AI approaches over CI techniques. (At least as long as one does not care much about computation time; if score is divided by average time taken per time step, the extremely simple heuristic ForwardJumpingAgent wins the competition...)  8http://arxiv.org/abs/0907.0229

VIII. DISCUSSION AND ANALYSIS  While the objective for the competitors was to design or learn a controller that played Infinite Mario Bros as well as possible, the objective for the organizers was to organize a competition that accurately tested the efficacy of various controller representations and learning algorithms for controlling an agent in a platform game. Here we remark on what we have learned in each of these respects, using the software in your teaching and the future of the competition.

A. AI for platform games  By far the most surprising outcome of the competition was how well the A*-based controllers performed compared to all other controller architectures, including controllers based on learning algorithms. As A* is a commonly used algorithm for path-finding in many types of commercial computer games (e.g. RTS and MMORPG games), one could see this as a victory over ?classical? AI over more fancy CI techniques which are rarely used in the games industry. However, one could also observe that the type of levels generated by the level generator are much less demanding and deceiving than those found in the real Super Mario Bros games and other similar games. All the levels could be cleared by constantly running right and jumping at the right moments, there were no hidden objects and passages, and in particular there were no dead ends that would require backtracking. All the A*- based agents consume considerably more processing time when in front of vertical walls, where most action sequences would not lead to the right end of the screen, suggesting that A* would break down when faced with a dead end.

While the sort of search in game-state space that the A* algorithm provides is likely to be an important component in any agent capable of playing arbitrary Mario levels, it will likely need to be complemented by some other mechanism for higher-level planning, and the architecture will probably benefit from tuning by e.g. evolutionary algorithms. Further, the playing style exhibited by the A*-based agents is nothing like that exhibited by a human player (the creepy exactness and absence of deliberation seems part of what made the YouTube video of Robin?s agent so popular). How to create a controller that can (learn to) play a platform game in a human-like style is an industrially relevant (cf. Demo Play) problem which has not been addressed by this competition.

B. Competition organization  Looking at the objectives enumerated in section IV, we consider that the first two objectives (ease of participation and transparency) have been fulfilled, the third (ease of finding a winner) could have been met better and that we largely failed at meeting the fourth (depth of challenge).

Ease of participation was mainly achieved through having a simple web page, simple interfaces and letting all com- petition software be open source. Participation was greatly increased through the very successful media campaign, built on social media. Transparency was achieved through forcing all submissions to be open source and publishing them on    TABLE II RESULTS OF THE ICE-GIC PHASE OF THE COMPETITION. EXPLANATION OF THE ACRONYMS IN THE ?APROACH? COLUMN: RB: RULE-BASED, GP:  GENETIC PROGRAMMING, NN: NEURAL NETWORK, SM: STATE MACHINE, LRS: LAYERED CONTROLLER, GA: GENETIC ALGORITHM.

Competitor approach progress levels time left kills mode Robin Baumgarten A* 46564.8 40 4878 373 76 Peter Lawford A* 46564.8 40 4841 421 69 Andy Sloane A* 44735.5 38 4822 294 67 Trond Ellingsen RB 20599.2 11 5510 201 22 Sergio Lopez RB 18240.3 11 5119 83 17 Spencer Schumann RB 17010.5 8 6493 99 24 Matthew Erickson GP 12676.3 7 6017 80 37 Douglas Hawkins GP 12407.0 8 6190 90 32 Sergey Polikarpov NN 12203.3 3 6303 67 38 Mario Perez SM, Lrs 12060.2 4 4497 170 23 Alexandru Paler NN, A* 7358.9 3 4401 69 43 Michael Tulacek SM 6571.8 3 5965 52 14 Rafael Oliveira RB 6314.2 1 6692 36 9 Glenn Hartmann RB 1060.0 0 1134 8 71 Erek Speed GA out of memory  the web site after the end of the competition. However, many competitors did not describe their agents in detail.

In future competitions, the structure of such descriptions should be specified, and submissions that are not followed by satisfactory descriptions should be disqualified.

C. Using the Mario AI Competition in your own teaching  The Mario AI Competition web page, complete with the competition software, rules and all submitted controllers, will remain in place for the foreseeable future. We actively encourage use of the rules and software for your own events, and have noted that at least one local Mario AI Competition has already launched (at UC San Diego). Additionally, the software is used for class projects in a number of AI courses around the world. When organizing such events, it is worth remembering that the existing Google Group and its archive can serve as a useful technical resource, and that the result tables in this paper provide a useful point of reference. We appreciate if any such events link back to the original Mario AI Competition web page, and students are encouraged to submit their agents to the next iteration of the competition.

D. Future competitions  The 2010 Mario AI Championship, like the 2009 compe- tition, uses a single website9 but is divided into three tracks:  1) The gameplay track: Similarly to the 2009 competition, this track is aimed at producing the controller that gets furthest on a sequence of levels. However, the competition software is modified so that some of the levels are substan- tially harder than the hardest levels in last year?s competition.

2) The learning track: This track is similar to the game- play track, but favours controllers that perform online learn- ing. Each controller will be tested a large number (e.g. 1000)  9http://www.marioai.org  of times on a single level, but only the score on the last attempt will count. The level will contain e.g. hidden blocks, shortcuts and dead ends. Thus, the scoring will reward controllers that learn the ins and outs of a particular level.

3) The level generation track: The level generation track differs substantially from the other tracks as what is tested is not controllers for the agent, but level generators that create new Mario levels that should be fun for particular players.

The generated levels will be evaluated by letting a set of human game testers play them live at the competition event.

