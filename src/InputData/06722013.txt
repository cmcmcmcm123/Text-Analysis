A Study on Classification in Imbalanced and Partially-Labelled Data Streams

Abstract?The domain of radio astronomy is currently facing significant computational challenges, foremost amongst which are those posed by the development of the world?s largest radio telescope, the Square Kilometre Array (SKA). Preliminary specifications for this instrument suggest that the final design will incorporate between 2000 and 3000 individual 15 metre receiving dishes, which together can be expected to produce a data rate of many TB/s. Given such a high data rate, it becomes crucial to consider how this information will be processed and stored to maximise its scientific utility. In this paper, we consider one possible data processing scenario for the SKA, for the purposes of an all-sky pulsar survey. In particular we treat the selection of promising signals from the SKA processing pipeline as a data stream classification problem. We consider the feasibility of classifying signals that arrive via an unlabelled and heavily class imbalanced data stream, using currently available algorithms and frameworks. Our results indicate that existing stream learners exhibit unacceptably low recall on real astronomical data when used in standard configuration; however, good false positive performance and comparable accuracy to static learners, suggests they have definite potential as an on-line solution to this particular big data challenge.

Index Terms?Data Streams, Classification, Imbalanced Learn- ing, Unlabelled Data, Astroinformatics

I. INTRODUCTION  Data streams have arisen naturally from the ever increasing volumes of data being generated by modern computational systems. Typically rapidly generated, temporally ordered, and infeasible to store in their entirety, data streams pose a sig- nificant challenge to those seeking to unlock the knowledge they contain. In recent years, considerable research effort has been expended towards identifying and solving these problems, leading to the development of many effective data stream learners. However the main focus of this work has been upon learning from streams that possess a reasonably balanced class distribution, and completely labelled or in some cases partially labelled data. In situations where the class balance is heavily skewed, or where it is unrealistic to expect more than a small fraction of the stream to be labelled, we face a new set of challenges that undermine the effectiveness of existing approaches. In this paper, the results of an empirical investigation are presented which demonstrate how the per- formance of Hoeffding bound based data stream classifiers, can degrade when faced with an increasingly imbalanced data  stream. Motivated by a real world problem faced by the radio astronomy community, this paper seeks to add to data stream research efforts, by drawing increased attention to a learning scenario that is likely to become commonplace as data streams appear in ever more diverse domains.



II. MOTIVATION  Radio Pulsars are extremely dense, rapidly rotating stellar remnants formed during the collapse of massive stars. They are a somewhat rare phenomena, which happen to provide a unique environment in which to perform numerous astro- physical experiments [1]. These scientific opportunities have motivated the search for pulsars using large radio telescopes.

In order to find pulsars, the signals arriving at the receiver of a radio telescope must be fed into a computational pipeline, designed to search for periodic broadband radio emission.

Those periodic signals possessing a signal-to-noise ratio above a predefined threshold value (based on domain knowledge), are considered to be pulsar ?candidates? worthy of further investi- gation. The search pipeline itself is made up of many process- ing components. Broadly speaking these can be separated into those that act as data processors which clean and correct the data (e.g. excision of radio frequency interference) and search routines tasked with isolating signals of interest. Crucially the volume of data moving through such search pipelines has been increasing steadily for some time. Indeed as observed in [2], pulsar survey data capture rates have exhibited a level of growth closely describable by Moore?s Law. This growth reflects advances in technology that have enabled various improvements in survey specifications, from increased survey sampling rates to finer frequency resolution. However there is an inverse relationship between the sampling rate of a pulsar survey and the data capture rate. As the sampling period decreases (thereby increasing sensitivity to shorter period emissions) the data capture rate increases. When coupled with finer frequency resolution and wider observational bandwidths, this relationship creates successive data storage and processing challenges. Similarly an inverse relation exists between the sampling rate of a survey, and the number of signals meeting the candidate selection criteria. As the majority of signals captured at the receiver of a telescope are attributable to noise   DOI    DOI 10.1109/SMC.2013.260     or interference; nearly the entire set of selected candidates are of no scientific value. Separating those candidates which are likely to lead to discovery from those which are spurious is therefore becoming increasingly difficult to do.

A. The Square Kilometre Array  The next generation of radio telescope, the Square Kilo- metre Array (SKA) [3] is currently under development by an international consortium. Due to begin science operations in the next decade, the SKA will produce data at an unprece- dented rate, firmly ushering in the era of exascale computing.

For pulsar survey operations alone the data rate is predicted to be between 0.43 ? 1.45 TB/s [4]. This is many orders of magnitude greater than previous surveys. Using existing technology, it is practically impossible to acquire the hardware and supporting infrastructure required for storing all this data permanently due to financial restrictions. Hence future SKA surveys must either a) utilise technological advances that make it feasible to store such large quantities of data at low cost, or b) process the data in real-time, storing only a fraction of the data for off-line analysis. Although one cannot say with certainty, it appears that real-time analysis is the only tractable and cost effective long term solution to this problem [4]. If this is indeed the case, then in the future candidate selection will have to be performed on-line, and in real-time. In the absence of an existing real-time selection system or the means to deal with the impending increase in candidates; the possibility exists that important discoveries could be delayed or possibly missed due to inadequate supporting computational infrastructure. Given that present day searches for radio transients1 are already being conducted in real time [5], it seems likely that the real-time scenario will win out.



III. PROBLEM DEFINITION  We consider the selection of candidate pulsars as a two-class data stream classification problem. We define the completely unlabelled input stream S = {(xi), ..., (xn), ...}, i = 1, ..., n as the stream of candidates emerging from a pulsar search pipeline, under a discrete time model. Each candidate in the stream xi ? X is defined as xi = {(y1i ), ..., (ymi )}, where each yji ? R for j = 1, ...,m is a summary statistic or measure that describes the candidate xi. We specify binary candidate labels L = {0, 1}, where l is an individual label such that l ? L (i.e. uninteresting = 0, interesting = 1). Our goal is to learn a function f : X ?? L which maps each candidate to its correct label producing the set of labelled candidates C = {(xi, f(xi)), ..., (xn, f(xn))}. From C the set of positively labelled candidates (those instances with l = 1) can be obtained, which should be recommended for inspection.

As the stream is completely unlabelled, only those candidates selected are ever likely to receive their correct labels (with the remainder necessarily discarded). Even then only a fraction of these may receive labels given the candidate volume, and certainly only after a substantial delay. Thus this scenario  1Typically transients are short non-repeating bursts of high energy radio emission.

can be thought of in terms of two distinct streams. The first from the telescope, the second a biased feedback stream containing data labelled by experts. Crucially the primary stream emerging from the telescope will be heavily imbalanced in favour of the negative class. The Parkes Multibeam Pulsar Survey (PMPS) for example has so far yielded 742 pulsar discoveries [6], from ? 8 million candidates [7]; giving rise to a class balance of roughly +1 : -11,000. If we make the conservative assumption that the class balance of the PMPS is maintained by the SKA, then we can expect to collect some ?200,000,000 negative candidates (given that current estimates suggest the SKA will detect approximately 20,000 new pulsars in total [4] ). A hypothetical classifier with perfect recall (1) and an accuracy (2) of .99 applied to these SKA candidates, would still select 2 million for further analysis.

Recall = True Positives  True Positives + False Negatives (1)  Accuracy = True Positives + True Negatives  Positives + Negatives (2)  Precision = True Positives  True Positives + False Positives (3)  False Positive Rate = False Positives  False Positives + True Negatives (4)  This is far too many for an expert to analyse, and infeasible to follow up on given the costs associated with telescope time. An ideal classifier must therefore maintain a high rate of recall (as missing positive instances is acutely costly given their rarity) and significantly reduce the false positive rate (4).

We summarise the five key data mining challenges for our candidate classifier as follows:  1) Imbalanced class distribution: the ratio of positive to negative instances will be greatly imbalanced.

2) Nonstationarity : Although the negative class is com- prised primarily of noise, various types of interference will be subject to gradual changes over long time scales, and abrupt changes over short ones.

3) Unlabelled data: the data in the stream is completely unlabelled. Drift detection must be performed on un- labelled data, without the prospect of obtaining correct labels within a reasonable time.

4) Real-time processing: The SKA will produce a high throughput data stream. It is unrealistic for much more than a small sample of this data to be stored, hence processing must likely be done in real-time.



IV. RELATED WORK  A. Candidate Selection  Candidate selection has typically involved the expert se- lection of candidate signals via some form of summary user interface. During the Swinburne Intermediate Latitude Pulsar Survey [8] and PMPS [9], graphical tools were developed which allowed large numbers of candidates to be filtered. A later reprocessing of the PMPS spawned the more sophisticated REAPER tool, which enabled candidates to be viewed via a customizable graphical plot [10] ultimately leading to the     discovery of 128 pulsars. A new version of this tool called JREAPER was then developed and led to the discovery of 28 pulsars [11]. More recently machine learning techniques have been used to filter candidates. In [7] a multi-layered perceptron (MLP) was used to perform an automated re-analysis of a data sample taken from the PMPS. The implementation was capable of recalling up to 93% of the pulsars present in PMPS test samples. Other methods utilised include Gaussian Mixture models as applied in [12], to rank pulsar candidates from the Fermi 2FGL catalogue.

B. Imbalanced & Unlabelled Streams  The problem of class imbalance is usually tackled by either assigning different costs to training examples thereby reweight- ing the balance, or by resampling the original dataset to achieve a desired balance. Resampling can be done in many ways, either by over-sampling the minority class or under-sampling the majority class [13]. The SMOTE algorithm developed by Chawla et al. [13] mixes both of these approaches. Empirical results suggest that SMOTE can successfully improve the accuracy of classifiers such as C4.5 and Na??ve Bayes on the minority class. In [14] Chen et al. develop their recursive weighted ensemble approach (REA) for classifying nonstation- ary imbalanced data streams. REA adaptively pushes minority class examples into the current data chunk to balance the class distribution. An in-depth discussion of the problems associated with learning from imbalanced data can be found in [15].

In terms of unlabelled streams, the Semi-supervised learning paradigm [16] has typically been applied when large data sets are continually produced or when the labelling of the data is prohibitively costly [17]. Some representative works in this area include: the decision tree based algorithm for classifying concept drifting and unlabelled data streams called SUN developed in [18], [19]; the relational K-means based transfer semi-supervised SVM (RK-TS3VM) developed for classifying unlabelled drifting data streams [20], and the CSL- Stream algorithm (Concurrent Learning of Data Streams) that clusters and classifies data at the same time [21].



V. EXPERIMENTS  Amongst the most successful of the single model data stream classifiers has been the very fast decision tree (VFDT) [22]. VFDT is an incremental any time decision tree learner that uses constant memory, and permits model updates in time proportional to the tree depth and data dimensionality.

VFDTs low runtime and space complexity therefore make it ideal for use on data streams, particularly as VFDT?s can be dynamically pruned if required to operate under strict memory constraints. The output of a VFDT is also guaranteed to be asymptotically similar to a conventional learning algorithm.

VFDT achieves this by using the Hoeffding bound (5) to choose with a high probability, those split attributes that would have been selected given access to all the data as in the non- streamed scenario. By calculating the observed mean r? of an attribute, the bound is able to determine with confidence 1? ?  Dataset Instances Attributes / Type Balance Pulsar 11,219,848 22 / Continuous, numerical +1 : -7500 Skin 245,057 3 / Discrete, numerical +1 : -5 MiniBoone 130,065 50 / Continuous, numerical +2 : -5 Magic 19,020 10 / Continuous, numerical +1 : -2  TABLE I CHARACTERISTICS OF THE DATA SETS USED.

(where ? is user supplied), that the true mean of an attribute r is at least r? ? ? where,  ? =  ? R2 ln(1/?)  2n . (5)  Although not intended to operate on unlabelled data, we begin looking at ways to solve the candidate selection problem by testing the effectiveness of existing algorithms such as the VFDT. Using the MOA stream mining framework [23], we test an implementation of the VFDT called the HoeffdingTree [23] on imbalanced and unlabelled streams. We compare the perfor- mance of the HoeffdingTree to three static classifiers including a standard decision tree, a SVM and Na??ve Bayes. Our tests were designed to reveal how well the HoeffdingTree performs if i) made to learn from only the examples in the stream and ii) pre-trained before being taken on-line. Tests of other VFDT tree based algorithms including the HoeffdingOptionTree [24], AdaHoeffdingOptionTree2 , and finally OzaBag[25] and Oza- Boost[25] both using the HoeffdingTree have also been com- pleted (in preparation). In each case the parameters used for these algorithms were set to the defaults provided by MOA.

A. Data  In order to test the various data stream classifiers, four datasets were utilised (see Table I). The first and largest dataset consisted of pulsar candidates obtained during the High Time Resolution Universe Survey (HTRU) [26]3. Candidates are represented by 22 continuous numerical attributes, each of which is a summary statistic that describes it in some way. The data set contains only 1,611 positive and 2,593 negative examples correctly labelled by human annotators. The remainder of the dataset is na??vely assumed to be negative.

However we can constrain the number of actual positives incorrectly labelled in this dataset based on Monte Carlo simulations undertaken by Keith [26] and later by Levin [27], to some small fraction of approximately 879-916 instances.

Also of the known 1,108 pulsars in the survey region, 725 have been re-detected in this data. Thus together the number of re-detections and expected discoveries falls within the range of the 1,611 certain positive examples already labelled.

The remaining datasets used included the Skin Segmentation, MiniBoone and the Magic Gamma Telescope datasets. These are obtainable from the UCI machine learning repository [28].

B. Methodology  Three distinct types of test were undertaken for this work.

The first established a baseline level of performance on our  2This is a HoeffdingOptionTree with the leaves modified so they store an estimation of the current error.

3This data is currently not publicly accessible.

data, using static classifiers under a traditional static learning scenario. Here performance on our data sets was assessed using all of the data and stratified 5-fold cross validation (train on 4 folds, test on 1). To test the algorithms under the two streaming scenarios on the other hand (no pre-training vs.

pre-trained), the data sets were firstly shuffled (except for the temporally ordered pulsar data) and then randomly sampled in order to generate different levels of class imbalance 4, whilst also varying the proportion of instances labelled 5. The no pre-training scenario tested only the stream classifiers under a test then train model (test on an instance, then train on an instance), such that each of the sampling permutations was treated as a single stream of data. Each test was repeated ten times for a given balance and labelling, allowing results to be averaged over multiple runs. For the pre-trained scenario, the same sampling and test procedure was used except that training and test sets were generated. The learners were then trained prior to being taken on-line for testing following the same test then train approach. All training sets were uniform in size and configuration, containing 200 positive and 1000 negative instances (+1 : -5 balance). The class distribution of these training sets is similar to the test distribution of three out of our four data sets (see Table I), the exception being the pulsar data which has a very different test distribution. For the pre-trained tests both static and stream classifiers were used, allowing for a comparison between the performance of static and streamed learners trained on the same small sample of data.

The test framework which executed these tests is summarised in Figure 1. As we assume a discrete time model, for each instance xi arriving at time step t, a class prediction is made at time step t + 1. The learner is trained at time step t + 2 only if the instance was labelled. Following each prediction a count of true positives, false positives, false negatives and true negatives is updated, by checking the prediction against the correct label of xi. If xi was unlabelled in the stream, then the correct label is obtained from a meta data file, and then compared to the prediction as before. Thus each prediction is evaluated during testing. However overall statistics are not computed until the end of an individual test run.

Evaluating classifier performance on imbalanced data is known to be difficult, particularly as many metrics are sensitive to the underlying class distribution [15]. We therefore keep track of multiple assessment metrics which include amongst others the F-Score (6) and the G-Mean [15] (7). The F-Score is sensitive to the class distribution given its dependence on precision (3). Thus when the data set is large and imbalanced, the numerator of the F-Score equation remains small while the denominator becomes increasingly large. Thus low precision can cause the F-Score to obtain a small value even given a high level of recall (e.g. with recall = .99 and precision = .01, the F1 = .0099) . As the G-Mean is not sensitive to the class distribution in the same way, we present the two together to  4The levels used were: 1.0, 0.5, 0.3, 0.25, 0.2, 0.1, 0.04, 0.02, 0.01, 0.008, 0.004, 0.002, 0.001, 0.0004, 0.0002, 0.00013, 0.0001 (e.g. 0.1 = +1 : -10).

5The labelling proportions used: 0%,1%,5%,10%,25%,50%,75%,99% and 100%.

Fig. 1. An overview of the test framework.

provide a representative impression of performance.

F-Score = 2? Precision ? Recall Precision + Recall  (6)  G-Mean =  ? TP  TP + FN ? TN  TN + FP (7)  C. Results  In Table II baseline accuracy results for static classifiers tested in a traditional learning scenario, are compared to those achieved by the stream classifiers under some representative imbalanced class scenarios. Static learners perform well on our data sets, achieving higher levels of accuracy than the stream learners when the class distribution is reasonably balanced.

As the class imbalance increases however, the accuracy of the stream learners tends towards 1, surpassing the accuracy levels achieved by the static classifiers. This increase in accuracy be- comes statistically significant for all data sets (using a signifi- cance level of 0.05) when the imbalance reaches ? 1+ : ?125.

Accuracy tending towards 1 in this manner, appears to happen due to the presence of Na??ve Bayes classifiers at each leaf of the Hoeffding tree. As each leaf contains a count of those positive and negative examples arriving there, a fully labelled data stream which is heavily imbalanced, will cause the count of negatives at these leaves to increase disproportionately to the positives. This skews the Na??ve Bayes predictions towards the negative class. As most examples in the stream are negative, by consistently predicting negative in this way the Hoeffding tree is effectively optimising classifier accuracy.

Stream Accuracy Balance 1.0 0.1 0.01 0.001 0.0001 Pulsar .9174 .9759 .9949 .9992 .9999 Skin .9892 .9860 .9940 .9987 .9996 MiniBoone .9949 .9995 0.9996 .9999 .9998 Magic .7154 .9083 .9897 .9990 -  Static Accuracy DT SVM NB .9999 .9999* .9676 .9992 .9291 .9239 .9999 .9279 .2827 .8503 .7916 .7269  TABLE II ACCURACY OF THE HoeffdingTree WITHOUT PRE-TRAINING ON A 100% LABELLED DATA STREAM, VERSUS ACCURACY OF STATIC CLASSIFIERS TRAINED AND TESTED USING STRATIFIED 5-FOLD CROSS VALIDATION (ALL DATA). * HERE A 90% TRAIN, 10% TEST STRATEGY WAS USED.

However the improvement in accuracy comes at the expense of reduced recall. Across all of the datasets tested, recall rates     Recall Balance 1.0 0.5 0.1 0.01 0.001 0.0001 Pulsar .86/.87 .83/.86 .81/.82 .74/.74 .64/.61 .06/.04 Skin .99/ .99 .98/.98 .93/.93 .45/.32 .08/.07 .03/0 MiniBoone .98/.99 .99/.99 .98/.96 .96/.84 .74/.46 .73/.02 Magic .84/.53 .13/.31 .008/.008 .007/0 0/0 -  TABLE III RECALL RESULTS FOR THE HoeffdingTree WITH AND WITHOUT  PRE-TRAINING (WITH/WITHOUT), ON A 50% LABELLED DATA STREAM.

consistently dropped as the class imbalance increased. This is a side effect of almost always predicating the negative class. This effect can be seen clearly in Table III. Even when 50% of the stream is labelled (which is unrealistic for a SKA scenario) recall rates tend toward zero which is unacceptable for our problem domain. Figure 2 also shows the same drop in recall rates observed when testing a non pre-trained Hoeffding tree on pulsar data. In this case recall rates are zero when no labels are available, and slowly increase as more labelled data appears in the stream. Static classifiers trained on only a small sample of data (200 positive and 1000 negative instances) achieved higher recall rates when treating the complete pulsar data stream as a static data set. Recall rates were ? .91 for C4.5 and ? .88 for the SVM respectively. Though both of these static classifiers return far too many false positives. C4.5 returned on average ? 138, 000 and the SVM ? 87, 000.

Fig. 2. Recall rates of the non pre-trained Hoeffding tree on pulsar data as the labelling and class balance is altered.

Compared to a static classifier, the Hoeffding tree has a very low false positive return rate when the stream is completely labelled. The previously described preference for applying neg- ative classifications when the stream is imbalanced, means that few positive predictions are ever made. Thus false positives become extremely rare. For the most imbalanced streams only  37000 instances became a false positive. As the proportion of labelled items in the stream is reduced, the false positive rate increases - slightly for the non pre-trained classifiers, but greatly for those pre-trained. This can be seen in Table IV which shows the false positive rates obtained on unlabelled streams. In row one a false positive return rate of .02 equates to ? 225, 000 false positives, much worse than the static classifiers. It appears that without labels to learn from, the Hoeffding tree maintains an imprecise model similar to the  static classifiers, as both are trained on the same sample of data.

False positive rate Balance 1.0 0.5 0.1 0.01 0.001 0.0001 Pulsar .03 .02 .01 .01 .03 .02 Skin .02 .02 .02 .03 .02 .02 MiniBoone .01 .005 .02 .004 .004 .1 Magic 0 0 0 0 0 -  TABLE IV FALSE POSITIVE RATES FOR THE HoeffdingTree ON UNLABELLED  STREAMS, TRAINED ON 200 POSITIVE & 1000 NEGATIVE INSTANCES.

In comparing the pre-trained and non pre-trained stream classifiers, a statistically significant difference in accuracy was observed. The results are summarised in Tables V and

VI. Non-pre-trained classifiers cannot build a model from an unlabelled stream, thus their accuracy for these streams is zero. On the remaining streams, the pre-trained classifiers initially outperform the non pre-trained, particularly in terms of recall. However as the the size of the data sets and the proportion of labelling increases, the non-pre-trained classifiers achieve higher levels of accuracy, once again by favouring negative classifications. Despite this difference in accuracy, the pre-trained classifiers actually maintain higher recall rates throughout. Though asymptotically their performance does appear to approach that of the non pre-trained classifiers as the number of instances classified increases. Thus it appears that pre-training a classifier only delays the inevitable favouring of the negative class if a large class imbalance exists. In summary these results show that those stream learners we tested, optimise accuracy on imbalanced data sets by almost always predicting the majority class. The net effect of this preference for the majority negative class in our case is to reduce recall and false positive rates, given that the minority class is very rarely predicted. Pre-training a classifier off-line on a sample of training data does in the short term improve recall rates, however this effect is only short lived.



VI. CONCLUSION  In this paper we have found that the recall capabilities of the Hoeffding tree classifier, deteriorate when faced heavily class imbalanced data streams. The imbalance appears to skew the class predictions of the Na??ve Bayes classifiers at the leaves of the tree, toward the majority class. This has the effect of improving classifier accuracy, whilst also degrading recall. We have found that pre-training a classifier before taking it on-line does initially improve the recall rate, however the class imbalance makes this effect inherently short term.

Although these result suggests that VFDT based classifiers such as the Hoeffding tree are unsuitable for solving the candidate selection problem, their capacity to maintain low false positive return rates makes them appealing, particularly as we explore ways in which the VFDT approach could be modified to accommodate imbalanced streams and thereby increase the recall rate. Future work will expand on the results of this investigation, and test other data stream classifiers not exclusively based on the Hoeffding bound. We also intend to     Balance +1 : -10 +1 : -100 +1 : -1,000 +1 : -10,000 Labelling (%) 0 50 75 100 0 50 75 100 0 50 75 100 0 50 75 100 Pulsar 0/0 .9/.87 .9/.86 .91/.97 0/0 .86/.76 .85/.75 .86/.76 0/0 .78/.59 .79/.6 .79/.63 0/0 .29/.14 .37/.22 .42/.26 Skin 0/0 .96/.90 .96/.92 .97/.93 0/0 .56/.45 .62/.52 .68/.60 0/0 .22/.13 .21/ .12 .23/.10 0/0 0/0 .02/0.1 .07/0.1 MiniBoone 0/0 .98/.98 .99/.99 .99/.99 0/0 .92/.9 .93/.92 .95/.95 0/0 .67/.56 .77/.69 .75/.69 0/0 .05/.04 .05/0.4 0/0 Magic 0/0 .07/.02 .07/.02 .07/.02 0/0 .05/.03 .04/.03 .07/.03 0/0 0/0 0/0 0/0 - - - -  TABLE V G-MEAN/F1 SCORE RESULTS FOR THE HoeffdingTree CLASSIFIER ON THE TEST DATASETS WITHOUT PRE-TRAINING.

Balance +1 : -10 +1 : -100 +1 : -1,000 +1 : -10,000 Labelling (%) 0 50 75 100 0 50 75 100 0 50 75 100 0 50 75 100 Pulsar .92/.87 .9/.86 .89/.85 .92/.87 .92/.57 .86/.77 .86/.75 .88/.75 .92/.1 .8/.6 .77/.6 .8/.61 .92/.02 .23/.08 .29/.12 .32/.15 Skin .9/.83 .96/.89 .97/.89 .97/.93 .9/.42 .67/.55 .7/.58 .73/.64 .91/.09 .28/.01 .24/.08 .2/.07 .91/.01 .11/.01 .13/.03 .12/.04 MiniBoone .79/.94 .99/.99 .99/.99 .99/.99 .76/.83 .98/.97 .98/.97 .98/.98 .59/.54 .83/.76 .95/.91 .9/.85 .84/.56 .83/.74 .96/.92 .71/.72 Magic 0/0 .06/.02 .05/.01 .1/.04 0/0 0/0 .01/.03 .01/.03 0/0 0/0 0/0 0/0 - - - -  TABLE VI G-MEAN/F1 SCORE RESULTS FOR THE HoeffdingTree CLASSIFIER WHEN TRAINED BEFORE CLASSIFYING THE STREAM.

analyse the attributes that describe a candidate pulsar, with the aim of removing redundant features which may improve classifier accuracy, and reduce the dimensionality of the data.

