

Abstract-The problem of deriving association rules from data was first formulated in [9] and is called the ?market-basket problem?. This paper presents an efficient version of APRIORI algorithm for mining multi-level association rules in large databases to solve market-basket problem. Our algorithm, named DEPTH FIRST MULTI-LEVEL APRIORI (DFMLA), uses the benefits of multi-leveled databases, by using the information gained by studying items from one concept level for the study of the items from the following concept levels.

Keywords: data mining, support constrains, knowledge discovery in databases, multi-level databases, multi-level association rules mining

I. INTRODUCTION  Data Mining refers to extracting or ?mining? knowledge from large amount of data. Data mining is the nontrivial extraction of implicit, previously unknown, and potentially useful information from data as shows Dunham in [5] and Frawley et al. in [6]. Through the extraction of knowledge in databases, large databases serve as rich, reliable sources for knowledge retrieval and verification, and the discovered knowledge can be applied to information management, decision making, process control and many other applications.

The aim of data mining is the discovery of patterns within data stored in databases. Mining for association rules is a data mining method that lends itself to formulating conditional statements such as ?if customers buy product x  then they also buy product y ?.

Therefore, data mining has been considered as one of the most important and challenge research areas. Some researchers like Strikant and Agrawal, in [9] and Rajkumar et al. in [10] found that for many applications, it is difficult to find strong and interesting associations among data items at the primitive levels of abstraction due to the sparsity of data.

However, many strong associations discovered at rather high concept levels are common sense knowledge. Therefore, a mining system with the capabilities to mine association rules at multiple levels of abstraction and traverse easily among different abstraction spaces is more desirable like Han et al.

in [8] and Rajkumar et al. in [10] indicate.



II. ASSOCIATION RULES MINING  Association rule discovery has emerged as an important  problem in knowledge discovery and data mining. The association mining task consists of identifying the frequent itemsets, and then forming conditional implication rules among them.

Finding frequent itemsets is one of the most investigated fields of data mining.

The problem was first resented in [1]. The subsequent paper of Agrawal, [2] is considered as one of the most important contributions to the subject. Its main algorithm, APRIORI, not only influenced the association rule mining community, but it affected other data mining fields as well.

Association rule and frequent itemset mining became an ideally researched area, and hence faster and faster algorithms have been presented. Numerous of them are APRIORI based algorithms or APRIORI modifications.

We theoretically and experimentally analyze APRIORI which is the most established algorithm for frequent itemset mining.

One of the most well-studied problems in data mining is mining for association rules in market basket data.

Association rules, whose significance is measured via support and confidence, are intended to identify rules of the type, "A customer purchasing item A often also purchases item B."  Since the introduction of association rules, many algorithms have been developed to perform the computationally very intensive task of association rule mining. During recent years there has been the tendency in research to concentrate on developing algorithms for specialized tasks, e.g. for mining optimized rules or incrementally updating rule sets. Here we return to the "classic" problem, namely the efficient generation of all association rules that exist in a given set of transactions with respect to minimum support and minimum confidence.

We try to solve the market basket problem using association rules mining algorithms. In this paper we discuss the depth first [8] implementation of APRIORI [1], one of the fastest known data mining algorithms to find all frequent itemsets in a large database, i.e., all sets that are contained in at least minsup transactions from the original database. Here      SOFA 2009 ? 3rd International Workshop on Soft Computing Applications ? 29 July ? 1 August ? Szeged (Hungary) ? Arad (Romania)  minsup is a threshold value given in advance. There exist many implementations of APRIORI [6, 11].



III. PROBLEM STATEMENT   Frequent itemset mining came from efforts to discover  useful patterns in customers? transaction databases. A customers? transaction database is a sequence of transactions (T = _t1,..., tn_), where each transaction is an itemset (ti I). An itemset with k elements is called a k-itemset. In the rest of the paper we make the (realistic) assumption that the items are from an ordered set, and transactions are stored as sorted itemsets. The support of an itemset X in T, denoted as suppT(X), is the number of those transactions that contain X, i.e. suppT(X) = |{tj : X ? tj}|. An itemset is frequent if its support is greater than a support threshold, originally denoted by min supp. The frequent itemset mining problem is to find all frequent itemset in a given transaction database.

The first, and maybe the most important solution for finding frequent itemsets, is the APRIORI algorithm [2].

Later faster and more sophisticated algorithms have been suggested, most of them being modifications of APRIORI.

Therefore if we improve the APRIORI algorithm then we improve a whole family of algorithms. We assume that the reader is familiar with APRIORI [2] and we turn our attention to its central data structure.

A typical example of association rule mining is market based analysis presented by Dunham [5]. This process analyzes customer buying habits by finding association between the different items that customers place in their ?market basket?. The discovery of such association can help retailers develop marketing strategies by gaining insight into witch items of frequently purchased together by customers.

We assume every 1-itemset to be frequent; this can be effected by the first step of the algorithms we are looking at, which might be considered as preprocessing.

A ?database query? is defined as a question of the form ?Does customer _ buy product _?? (or ?Does transaction  has item _??), posed to the original database. Note that we have __ database queries in the ?preprocessing? phase in which the supports of the 1-itemsets are computed and ordered: every field of the database is inspected once.

The number of database queries for DFMLA equals:   For instance, if customers buying milk, how likely are they  to also buy bread on the same trip to super market? Such information can lead to increased sale by helping retailers do selective marketing and plan there self space. For example, placing milk and bread with in close proximity may further encourage the sale of these items together within visit to the store.

Let I= {I1, ..., Im }   be a set of attributes called items. A subset X ?  I is called an itemset. A k-itemset is an itemset that contains k items.

Let the database D= {T1 ,? ,Tn } be a multi-set of transactions, where each transaction Ti , I ? { 1, ? , n } , is an itemset [3]. A transaction T contains an itemset X if X ? T. Each itemset has a certain statistical significance called support or frequency. The support of an itemset X is the fraction of transactions in the database D containing itemset X, i.e.

||  |}|{| )( ...

D  TXDT XS  ?? =    (2)   An association rule is an implication X --> Y, where X ?  I, Y ?  I, and X  Y = ? .  X is called the antecedent and Y is called the consequent of the rule. The rule X --> Y holds with confidence C  if  C is the fraction of transactions containing X that also contain Y , i.e.:    )(  )( ),(  XS  YXS YXC =    (3)   The confidence denotes a rule?s strength. A confidence  threshold Cmin is used to exclude rules that are not strong enough to be interesting. Accordingly, there is a support threshold Smin that excludes all rules whose number of transactions containing the union of the antecedent and the consequent is below a certain amount. Itemsets with minimum support are called frequent or large itemsets.

The two kinds of thresholds must not be confused. The support threshold is defined over itemsets. When applied to association rules, it describes the minimum percentage of transactions containing all items that appear in the rule. The confidence threshold specifies the minimum probability for a consequent to be true if the antecedent is true. A confidence of nearly 100% characterizes only very stringent rules.



IV. MULTI-LEVEL ASSOCIATION RULES   Based on the concept hierarchy and some existing  algorithms for mining single-level association rules, a group of efficient algorithms for mining multilevel association rules are proposed by Han et al. [7], [8] and Rajkumar et al. [10]  We assume that the database contains: 1) an item data set which contains the description of  each item in I in the form of Ai; description, where Ai is the product?s code.

2) a transaction data set, T = { T1, ?, Tn}, which consists of a set of transactions Ti= { Ap ,.., Aq}  in the format of  (TID, Ai); Where  TID is  a  transaction identifier and Ai is an item from the item data set.

Rajkumar et al. [10] show that in multi-level databases use hierarchy-information encoded transaction table instead of the original transaction table. This is useful when we are interested in only a portion of the transaction database such as food, instead of all the items. This way we can first collect the relevant set of data and then work repeatedly on the task-     Mirela Pater, Daniela E. Popescu ? Market-Basket Problem Solved With Depth First Multi-Level Apriori Mining Algorithm  relevant set. Thus in the transaction table each item is encoded as a sequence of digits.

Example: The item Pepsi Cola is encoded as ?0201? according to the table 1 below. The first digit ?0? represents ?Food? at the first level, the 2nd digit represents ?Drinks? in the 2nd level, 3rd digit is ?Cola? in the 3rd level and, the 4th digit ?1? represents product ?1? in the ?Cola? category. The information required to create such a database is either implicit  ?Pepsi Cola? is obviously part of the ?cola? category which is part of the ?drinks? category, etc., or it is provided by the user. Some information, like price or brand can be used to transform items into categories. For example White Bread can become a category which contains ?1$ White Bread? or ?Dorbob? White Bread   Table 1.Item codes and descriptions                The problem of finding association rules can be stated as follows. Given a set I of items, a database D of itemsets, a support threshold Smin, and a confidence threshold Cmin, find all association rules  X --> Y in  D  that have support  X Y  Smin  and confidence  C( X,Y )  Cmin.

Fig.1. Hierarchy-based multi-level database  We generally are not interested in all implications but only  those that are important. Here importance usually is measured by two features called support and confidence.

The support of an item (or set of items) is the percentage of transactions in which that item (or items) occurs. The confidence or strength for an association rule X -> Y is the ratio of the number of transactions that contains X U Y to the number of transactions that contain X.

In the multilevel database the item data set will be entered like in the table 1. This table also contains category codes and  a description for each code (category or item) which is only needed for the final display. Item?s quantity, price, brand or other information should be present but are not considered in this paper.

Table 2.Transactions components    Transaction Products  T1 Bread White, Milk 1.5%, Coca Cola, Prigat  T2 Bread White, Milk 1,5%, Milk 2%, Prigat  T3 Milk 2%, Coca Cola  T4 Milk 2%, Pepsi Cola  T5 Bread Wheat, Coca Cola, Milk 2%, Prigat   Table 3.Transactions data set                    From the tables 1 and 2 we can deduce that the  transactions are represented in table 3: Multi-level rules are rules in which the concepts or  constants may be at multiple conceptual levels in conceptual hierarchies.

The problem is to discover the rules at multiple conceptual levels, i.e. multiple-level rules. Most rule discovery methods find primitive levels which only involve concepts at the primitive level. In multi-level databases this methods are extended by finding rules at different levels as opposed to rules at one specific level.

Thus, we have the ability to focus our attention on discovering informative rules between categories, or even between an item and a category, not just rules between items.

The support, the confidence and other mining rules can be used in new ways in a multilevel database.

For example the support of a category is the percentage of transactions in which that category occurs. Support for the Milk category:  %100100*  100*  ..

)..(.

S(Milk) ===  transnrTotal  productsMilkwithtranzNr    Code Description 0 Food 00 Bread 01 Milk 02 Drinks 000 White (Bread) 001 Wheat (Bread) 010 1,5% (Milk) 011 2% (Milk) 020 Cola 021 Prigat 0200 Coca (Cola) 0201 Pepsi (Cola)  Transaction Item Code T1 000 T1 010 T1 0200 T1 021 T2 000 T2 010 T2 011 T2 021 T3 011 T3 0200 T4 011 T4 0201 T5 001 T5 0200 T5 011 T5 021     SOFA 2009 ? 3rd International Workshop on Soft Computing Applications ? 29 July ? 1 August ? Szeged (Hungary) ? Arad (Romania)  One has to consider that the support of a category may not be the sum of the supports for the descendants. However, the support for a category can?t be lower than the support for one of its descendants.

For example, because a customer may have bought more than one item from the same category (see transaction T2: milk 1.5% and milk 2%) in the same transaction, the support for the ?milk category is counted only once, and it?s smaller (by 1) than the sum of the supports of these items.

If a category doesn?t have minimum support then its descendants won?t meet minimum support either. This is because the support for a category is lower or equal to the sum of the supports for its descendants and it?s obvious that if the sum of sup1 and s2 is lower than min_sup than neither s1 nor s2 can be bigger than min_sup.

Fig.2. Rule Inheritance

IV. DEPTH FIRST MULTI-LEVEL APRIORI  ALGORITHM (DFMLA)  Description. This is an improvement of the Apriori  algorithm presented by Agrawal and Strikant [2] in witch there are generally not interested in all implications but only those that are important like Han and Fu demonstrated [8].

Here importance is measured by support.

The algorithm is for multi-level databases which gives us the advantage of using a progressive deepening method that is developed by extending the Apriori algorithm for mining single-level association rules in [8]. The method first finds frequent data items at the topmost level and then progressively deepens the mining process into their frequent descendants at lower concept levels.

At first, this algorithm views the database as single-level which contains only itemsets from the first concept level. It uses the original Apriori algorithm to find association rules between these itemsets (which, in our example, are categories).

Each time we find a new rule between 2 itemsets (f g) we test it for importance (by support and confidence). If it doesn?t pass the importance threshold then there won?t be association rules between their descendents.

However if it does pass the importance tests, it means that there may be rules between their descendants. We take all the descendants of these itemsets from the next concept level and try to find a rule f2 g2 (where f2 and g2 are subsets of f, g from the previous concept level). If we find a rule we immediately continue at next concept level in the same way until we don?t find interesting rules.

Implementation In implementation, this algorithm is like the Apriori algorithm for single-level databases presented in [2]. The only difference is that after the importance tests, if  the rule between the 2 itemsets passes, it does the steps described in the following recursive function:   function CheckDescendants(k, lvl, f, g ) { for (f2=1; f2<nr.of.itemsets.descented.from.f.on.lvl+1;  f2++) for (g2=1;  g2<nr.of.itemsets.descented.from.g.on.lvl+1; g2++) if (f2 g2 rule is interesting) { Add.Itemset(f2,g2) to L[k, lvl+1]; CheckDescented( k, lvl+1, f2, g2);   //if f2 and g2  exist?} }  Where: - k ? nr of items from a k-itemset - f,g are the itemsets from the f g rule - lvl ? level where f,g are found  This function must be called before trying to find another  rule at the same level (lvl) because we need to know exactly which subsets (f and g) were used in this rule.

Because f2 and g2 are derivations of the f and g subsets, the transactions (T(k,lvl+1)) in which they are found are actually some (or all) of the transactions in which f and g are found (T(k,lvl)).

Thus, when the ?CheckDescendants()? function is called, we can send it a selection (or just the IDs) of transactions from T(k,lvl). The function won?t have to scan the entire database to check for importance, it will just scan the selection of transactions. In fact, as we progress deeper, from on concept level (lvl) to the one following it (lvl+1), the number of transactions should be smaller and smaller and scanning will be faster and faster  In implementation, to obtain T(k,lvl+1) we filter, from the previous transaction selection, only the transactions which contain items common to the f and g itemsets.



V. PERFORMANCE STUDY & COMPARISONS  These algorithms don?t try to find rules between all  subsets from the k-itemsets (k>1) that remained after the prune step, [8] find rules between derivations of the subsets from the rules that were previously found to be interesting.

For example, if they find a rule at the first concept level, a rule between categories, they go deeper and try to find which subcategories that rule apply to.

If a rule doesn?t pass the importance threshold, then there won?t be important rules between subcategories at the following concept levels.

To make this advantage more noticeable, we will compare the first algorithm to a similar multi-level Apriori algorithm described in [7]. This algorithm uses the candidate generation algorithm in Apriori on each concept level. When it finishes with one level it moves on to the next level. We can say that this is a width-first algorithm and that the algorithms presented in this paper are depth-first.

Mirela Pater, Daniela E. Popescu ? Market-Basket Problem Solved With Depth First Multi-Level Apriori Mining Algorithm  On the database presented in the following example, DFMLA algorithm will test the importance of the rules 1 2, 1.1 2.1, 1.1.1 2.1.1, 1 3 and 2 3. Rule 1 3 and 2 3 won?t pass the importance test and thus, the DFMLA algorithm won?t test rules 1.1 3.1, 1.1 3.2, 2.1 3.1 etc, contrary to the other algorithm which will eventually test the rules when it gets to the respective level.

Fig.3.Multi-level Apriori vs. DFMLA. Performance comparison   An important disadvantage that DFMLA algorithm has is its memory usage. In a database with many concept levels, when we get to the last level, TIDs for each previous level will be retained.

The studies were made on a multi-level database, created as presented at section 4. The database had 45000 database entries, 7500 transactions, each one having no more than 15 items; 750 distinct items which were encoded on 10 concept levels.

The experiments were conducted at a Pentium-IV machine with 512 MB memory at 2.8 GHz, running Red Hat Linux 7.3. The program was implemented in Java.



VI. CONCLUSIONS  The scope of data-mining has been broadened by the study  on the mining on multiple-level rules. The mining of multiple level rules can provide more information for the users and enhance the flexibility and power of data-mining systems.

This algorithm finds new rules by using the knowledge gained from previously found rules. If a rule fails at the first concept level many rules from the following concept levels won?t be studied and thus,  the more concept levels a database has, the faster it will be to get results compared to other algorithms.

Storing the database in the primary memory is no longer a problem. On the other hand, storing the candidates causes trouble in situations, where a dense database is considered with a small support threshold. This is the case for any algorithm using candidates. Therefore, it would be desirable to look for a method which stores candidates in secondary memory. This is an obvious topic for future research.

Our conclusion is that DFMLA is a simple, practical, straight forward and fast algorithm for finding all frequent itemsets.

