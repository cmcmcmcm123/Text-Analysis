2010 2nd International Asia Conference on Informatics in Control, Automation and Robotics

Abstract?This dissertation proposes a new algorithm of distributed mining association rules using the improved Apriori algorithm, based on analyses and introduction of the basic concepts and algorithms of mining association rules and mining association rules in distributed databases. Using improved Apriori algorithm to directly produce all of local frequent itemset in each crunode, rather than iteratively selecting candidate itemset. Then gather all of local multifarious itemset to broadcast to the general node, producing the global frequent itemset of association rules. In the process, the data is no longer saved with the affair ID as the key word. We take the item ID as the new key word. The performance of the improved Apriori algorithm has been improved through cutting down the store space. While the general node gathers all of local frequent itemset to select the global frequent itemset, it needs only a broadcast probably, needing three broadcasts worst. This raised the efficiency of the new algorithm of Association Rules in Distributed Database System.

Index Terms?association rules, distributed database, data mining

I. INTRODUCTION With the widely application of the Internet/Intranet and  the sophisticated of distributed technologies, database and data warehouse has been applied in the distributed environment of amounts of data. As the data are located in different places, an efficient distributed algorithm is needed to mining the large number of distributed data. The proprietary mining algorithm is corresponding to the characteristics of association rules mining in distributed database system. At present, the researchers of data mining have done some research on this. There are many distributed mining algorithm of association rules and the improved ones which has mined the valuable association rules from the distributed database system. But the traffic of generating candidate set is too heavy directly affecting the efficiency of the algorithm especially faced enormous databases or sparse data. In this paper, based on the analysis the theory of association rules and the apriori algorithm we proposed an improved apriori algorithm to create local frequent itemset rapidly, and then generate the global frequent itemsets only through a communication.



II. ASSOCIATION RULES AND IMPROVED ALGORITHM  A. Association Rules Concept Association Rule provides a kind of very simple but  worthy description form for the rule mode in data mining.

Association Rule is one of the most popular methods descript partial mode of data mining. It simply presents the rate of some particular affairs in database taking place together, particularly is applicable to a sparse data sets.

Define 1Item: It is a field of the transaction database.

We usually use small letter i  [1]  m  Define 2 Transaction: It is corresponding to a record of the database. Transaction usually is marked as small letter t  to mark item.

i. ti={i1,i2,?,ip}. Each transaction has an only identifier called TID. The whole set of transaction ti constitutes a database D. D= {t1,t2,?,tn Define 3 Itemset: It is the sets of whole item in a transaction database. And it is marked as capital I.

Any sub sets x of I is itemset in D. I={i  }.

1,i2,?,im Define 4 the Dimension of itemset: The count of items in an itemset is called the dimension of the itemset. The itemset with dimension K is marked as K-Itemset.

}.

Define 5 Association Rule: The association rule is a contain model looking like X Y. There into, X I,Y I,and X . Its meaning is the emergence of X causing the emergence of Y.X Y separately is the premise and the conclusion of the association rule.

Define 6 Support: The support of association rule X Y in transaction database is a ratio. The ratio is between the count of itemset which contains X and Y, and the count of all of itemset. That marks support(X Y). That is the percent of the itemset containing X and Y at the same time in the transaction database. If item t=XUY, then the support of item t support (t) =support(X Y).The more high the support of association rule is, the more high the frequency of association rule?s and itemset? emergence is. The minimum support appointed by the customer according to needing is marked as MinSup.

2010 2nd International Asia Conference on Informatics in Control, Automation and Robotics   Research on Algorithm of Association Rules in Distributed Database System  Lijuan Zhou Information Engineering College  Capital Normal University Beijing, China  Zlj87@tom.com  Shuang Li Information Engineering College  Capital Normal University Beijing, China  lishuang924@163.com  Mingsheng Xu Information Engineering College  Capital Normal University Beijing, China  xmsh1987@gmail.com  Abstract?This dissertation proposes a new algorithm of distributed mining association rules using the improved Apriori algorithm, based on analyses and introduction of the basic concepts and algorithms of mining association rules and mining association rules in distributed databases. Using improved Apriori algorithm to directly produce all of local frequent itemset in each crunode, rather than iteratively selecting candidate itemset. Then gather all of local multifarious itemset to broadcast to the general node, producing the global frequent itemset of association rules. In the process, the data is no longer saved with the affair ID as the key word. We take the item ID as the new key word. The performance of the improved Apriori algorithm has been improved through cutting down the store space. While the general node gathers all of local frequent itemset to select the global frequent itemset, it needs only a broadcast probably, needing three broadcasts worst. This raised the efficiency of the new algorithm of Association Rules in Distributed Database System.

Index Terms?association rules, distributed database, data mining

I. INTRODUCTION With the widely application of the Internet/Intranet and  the sophisticated of distributed technologies, database and data warehouse has been applied in the distributed environment of amounts of data. As the data are located in different places, an efficient distributed algorithm is needed to mining the large number of distributed data. The proprietary mining algorithm is corresponding to the characteristics of association rules mining in distributed database system. At present, the researchers of data mining have done some research on this. There are many distributed mining algorithm of association rules and the improved ones which has mined the valuable association rules from the distributed database system. But the traffic of generating candidate set is too heavy directly affecting the efficiency of the algorithm especially faced enormous databases or sparse data. In this paper, based on the analysis the theory of association rules and the apriori algorithm we proposed an improved apriori algorithm to create local frequent itemset rapidly, and then generate the global frequent itemsets only through a communication.



II. ASSOCIATION RULES AND IMPROVED ALGORITHM  A. Association Rules Concept Association Rule provides a kind of very simple but  worthy description form for the rule mode in data mining.

Association Rule is one of the most popular methods descript partial mode of data mining. It simply presents the rate of some particular affairs in database taking place together, particularly is applicable to a sparse data sets.

Define 1Item: It is a field of the transaction database.

We usually use small letter i  [1]  m  Define 2 Transaction: It is corresponding to a record of the database. Transaction usually is marked as small letter t  to mark item.

i. ti={i1,i2,?,ip}. Each transaction has an only identifier called TID. The whole set of transaction ti constitutes a database D. D= {t1,t2,?,tn Define 3 Itemset: It is the sets of whole item in a transaction database. And it is marked as capital I.

Any sub sets x of I is itemset in D. I={i  }.

1,i2,?,im Define 4 the Dimension of itemset: The count of items in an itemset is called the dimension of the itemset. The itemset with dimension K is marked as K-Itemset.

}.

Define 5 Association Rule: The association rule is a contain model looking like X Y. There into, X I,Y I,and X . Its meaning is the emergence of X causing the emergence of Y.X Y separately is the premise and the conclusion of the association rule.

Define 6 Support: The support of association rule X Y in transaction database is a ratio. The ratio is between the count of itemset which contains X and Y, and the count of all of itemset. That marks support(X Y). That is the percent of the itemset containing X and Y at the same time in the transaction database. If item t=XUY, then the support of item t support (t) =support(X Y).The more high the support of association rule is, the more high the frequency of association rule?s and itemset? emergence is. The minimum support appointed by the customer according to needing is marked as MinSup.

|{ | , } |( ) ( ) | |  ti X Y ti ti Dsupport X Y p X Y D  Define 7 Confidence: It is the ratio between the count of transaction containing X and Y and the count of transaction containing X. That is marked as confidence(X Y). Confidence is the percent of the transaction sets containing X and Y at the same time in the transaction database. This is a conditional probability. The higher the confidence of association rule is, the more dependable the rule is. The minimum confidence appointed by the customer usually is marked as MinConf.

|{ | , } | ( )( ) ( | ) |{ | , } | ( ) ti X Y ti ti D support X Yconfidence X Y p Y X  tj X tj tj D support X  Define 8 Frequent Itemset: That means the itemset, whose support is not lower than the minimum support (MinSup).

Define 9 Strong rule and Weak rule: If support(X Y) Y)  Y as a strong rule, otherwise mark it as a weak rule.

B. Apriori Algorithm and Related Algorithm Analyse In 1994 Agrawal etc. put forward famous Apriori  algorithm according to the property of association rule: the sub sets of the frequent itemset is also frequent itemset, the supersets of non-frequent itemset is also non-frequent itemset. The algorithm each time makes use of k-frequent itemset carrying on conjunction to get k+1 candidate itemset.

Then get k+1 frequent itemset through cutting. So keep on, until there is not frequent itemset. The Apriori algorithm has to firstly produce frequent itemset with small count. Next through iteration produces frequent itemset with great count.

So when Apriori algorithm produces each length frequent itemset, it has to scan the database for a time. In addition, it must produce candidate itemset at first. Thus the time of scanning the database is up to the count of the largest frequent itemset.

C. Basic Thought and Process of Improved Algorithm Literature [3] introduces two pieces of inference as  follows:  Inference 1: If the count of item in K-frequent itemset LK is smaller than or equal to K, then the itemset is the sets of the largest frequent itemset.

Inference 2: The count of item in the largest frequent itemset LK is smaller than or equal to the largest count K of items, which satisfies support in all of transactions.

The New_Apriori algorithm makes use of above-mentioned inference, at first making sure the count K of item of the largest frequent itemset. It directly gets the largest frequent itemset LK  As follows introduce the New_Apriori algorithm with an example:  among these transactions, whose count of item is larger than or equal to K. Then consider these transactions one by one in order, whose count of item is k-1 and k-2.Dine the frequent itemset, which does not belong to the largest frequent itemset with the largest count of item.

Table 1 is an original affair database. Each affair T represents a student's record; the I1-I7  Scan the database to take down C[n], C[1]=1, C[2]=5, C[3]=8, C[4]=4, C[5]=2;and the Flag1mark of each transaction. At the same time, change data storage structure, taking item as the key word:  means different attributes respectively. Minimum support min_support =4.

TABLE I. ORIGINAL AFFAIR DATABASE  Affair Item Flag1  T I1 1,I 27  T2 I        2,I3,I 34  T I3 4,I5,I 37  T I4 2,I4,I5 3  T I5 5,I6,I 37  T I6 2,I 24  T I7 3,I5,I 37  T I8 2,I4,I5,I 46  T I9 5,I 27  T I10 2,I4,I 35  T I11 4,I 27  T I12 1,I2,I3,I4,I 55  T I13 2,I4,I5,I6,I 57  T I14 2,I3,I 34  T I15 4,I5,I 37  T I16 3,I 24  T I17 1,I3,I5,I7 4  T I18 2,I4,I5,I 46  T I19 15  T I20 2,I4,I5,I 46  Because C [5] <min_support, C [4]>min_support, the largest count k of item of the largest frequent assumed before is 4. Now from the tansformed data storage select the item with Flag2 into the transaction whether its Flag1i is larger than k. If such transaction?s count satisfies the minimum support, take down temporarily. Then get the lately small database.

TABLE II. DATA STORAGE TAKING ITEM AS KEY WORD  Item Affair Flag2  I T1 1,T12,T 317  I2 T      2,T4,T6,T8,T10,T12,T13,T14,T18,T 1020  I T3 2,T7,T12,T14,T16,T 617  I T4 2 ,T3,T4,T6,T8,T10,T11,T12,T13,T14,T15,T16,T1  8,T20  I T5 3 ,T4,T5,T7,T8,T9,T10,T12,T13,T15,T17,T18,T19  ,T 1420 I T6 5,T8,T13,T18,T 520  I T7 1,T3,T5,T7,T9,T11,T13,T15,T 917  |{ | , } |( ) ( ) | |  ti X Y ti ti Dsupport X Y p X Y D  Define 7 Confidence: It is the ratio between the count of transaction containing X and Y and the count of transaction containing X. That is marked as confidence(X Y). Confidence is the percent of the transaction sets containing X and Y at the same time in the transaction database. This is a conditional probability. The higher the confidence of association rule is, the more dependable the rule is. The minimum confidence appointed by the customer usually is marked as MinConf.

|{ | , } | ( )( ) ( | ) |{ | , } | ( ) ti X Y ti ti D support X Yconfidence X Y p Y X  tj X tj tj D support X  Define 8 Frequent Itemset: That means the itemset, whose support is not lower than the minimum support (MinSup).

Define 9 Strong rule and Weak rule: If support(X Y) Y)  Y as a strong rule, otherwise mark it as a weak rule.

B. Apriori Algorithm and Related Algorithm Analyse In 1994 Agrawal etc. put forward famous Apriori  algorithm according to the property of association rule: the sub sets of the frequent itemset is also frequent itemset, the supersets of non-frequent itemset is also non-frequent itemset. The algorithm each time makes use of k-frequent itemset carrying on conjunction to get k+1 candidate itemset.

Then get k+1 frequent itemset through cutting. So keep on, until there is not frequent itemset. The Apriori algorithm has to firstly produce frequent itemset with small count. Next through iteration produces frequent itemset with great count.

So when Apriori algorithm produces each length frequent itemset, it has to scan the database for a time. In addition, it must produce candidate itemset at first. Thus the time of scanning the database is up to the count of the largest frequent itemset.

C. Basic Thought and Process of Improved Algorithm Literature [3] introduces two pieces of inference as  follows:  Inference 1: If the count of item in K-frequent itemset LK is smaller than or equal to K, then the itemset is the sets of the largest frequent itemset.

Inference 2: The count of item in the largest frequent itemset LK is smaller than or equal to the largest count K of items, which satisfies support in all of transactions.

The New_Apriori algorithm makes use of above-mentioned inference, at first making sure the count K of item of the largest frequent itemset. It directly gets the largest frequent itemset LK  As follows introduce the New_Apriori algorithm with an example:  among these transactions, whose count of item is larger than or equal to K. Then consider these transactions one by one in order, whose count of item is k-1 and k-2.Dine the frequent itemset, which does not belong to the largest frequent itemset with the largest count of item.

Table 1 is an original affair database. Each affair T represents a student's record; the I1-I7  Scan the database to take down C[n], C[1]=1, C[2]=5, C[3]=8, C[4]=4, C[5]=2;and the Flag1mark of each transaction. At the same time, change data storage structure, taking item as the key word:  means different attributes respectively. Minimum support min_support =4.

TABLE I. ORIGINAL AFFAIR DATABASE  Affair Item Flag1  T I1 1,I 27  T2 I        2,I3,I 34  T I3 4,I5,I 37  T I4 2,I4,I5 3  T I5 5,I6,I 37  T I6 2,I 24  T I7 3,I5,I 37  T I8 2,I4,I5,I 46  T I9 5,I 27  T I10 2,I4,I 35  T I11 4,I 27  T I12 1,I2,I3,I4,I 55  T I13 2,I4,I5,I6,I 57  T I14 2,I3,I 34  T I15 4,I5,I 37  T I16 3,I 24  T I17 1,I3,I5,I7 4  T I18 2,I4,I5,I 46  T I19 15  T I20 2,I4,I5,I 46  Because C [5] <min_support, C [4]>min_support, the largest count k of item of the largest frequent assumed before is 4. Now from the tansformed data storage select the item with Flag2 into the transaction whether its Flag1i is larger than k. If such transaction?s count satisfies the minimum support, take down temporarily. Then get the lately small database.

TABLE II. DATA STORAGE TAKING ITEM AS KEY WORD  Item Affair Flag2  I T1 1,T12,T 317  I2 T      2,T4,T6,T8,T10,T12,T13,T14,T18,T 1020  I T3 2,T7,T12,T14,T16,T 617  I T4 2 ,T3,T4,T6,T8,T10,T11,T12,T13,T14,T15,T16,T1  8,T20  I T5 3 ,T4,T5,T7,T8,T9,T10,T12,T13,T15,T17,T18,T19  ,T 1420 I T6 5,T8,T13,T18,T 520  I T7 1,T3,T5,T7,T9,T11,T13,T15,T 917    Now try to get U2 4 5 6. If the count of item in the intersection is larger than or equal to min_support, then {I2, I4, I5, I6} is the largest frequent itemset according to the inference in literature[3]. We get the intersection {T8, T13, T18, T20}, and the count of item is 4. This satisfies the minimum support min_support. Thus it can be seen, {I2, I4, I5, I6} is the largest frequent itemset. The record is< I2, I4, I5, I6, 4>. In the record, the number 4 expresses the amount of elements of the intersection. In other words, there are 4 records including I2, I4, I5, I6  TABLE III. THE LATELY SMALL DATABASE  at the same time.

Item Affair  I2 T      8,T12,T13,T18,T20       (U2)  I T4 8,T12,T13,T18,T20       (U4)  I T5 8,T12,T13,T17,T18,T20    (U5)  I T6 8,T13,T18,T20       (U6)  Owing to there are just I2, I4, I5, I6four items in this experiment, it is equal to the largest count k of item of the largest frequent assumed. So in the process of solving, when we try to get U2 4=Ua, if the count of Ua?s item is smaller than 4, |Ua  According to the specialty of Apriori algorithm: All of the sun sets of frequent itemset are frequent itemset, we can from the largest frequent itemset{ I  |<4, there is no need continue to get the intersection. Because the count doesn?t satisfy the minimum support, at this time the count of considering item is smaller than k. Then there isn?t the largest frequent itemset. If we don?t find 4-frequent itemset, then the largest count k of item of the largest frequent assumed decrease 1.Get the largest frequent itemset with the method above-mentioned.

2, I4, I5, I6} get part of 2-frequent itemset{I2, I4},{I2, I5},{I2, I6},{I4,I5},{I4,I6},{I5,I6} and part of 3-frequent itemset { I2, I4, I5}etc. We can get the intersection in the same: record < I2, I4, 5>, < I2, I5, 5>, < I2, I6, 4>, < I4,I5  And then continue to try to get the largest frequent itemset to database. We just need to take the k as 2 or 3. In this experiment , at last we find all of the largest frequent itemset{ I  , 5>etc.

2, I4, I5, I6,{I3,I4},{I4,I7},{I5,I7}. And {I2, I4, I5, I6

III. BASIC THOUGHT AND PROCESS OF DISTRIBUTION DATA MINING  } is the largest frequent itemset with the largest count of item.

Distributed data mining algorithms can be divided into two categories, data distribution algorithm (DD algorithm).and the count distributed algorithm (CD algorithm).

The basic idea of DD algorithm is to designate the candidate set evenly distributed among the nodes. This can reduces the number of candidate set to be scanned when the nodes increases. The problem is that the traffic between the nodes can reduce the efficiency of the algorithm.

CD algorithm is a typical parallel algorithm based on apriori algorithm. Each sub-database scans the database independently to calculate the support of each candidate set, and then set the sum of the supports of the entire candidate set as the global support. If the global support is greater than the mini-support, we regard the itemset as the global frequent itemset. CD algorithm begins the next scan through  the next synchronization point after the end of each scan.

With the increase of the nodes, the traffic of the algorithm will be increasing quickly and the efficiency of the algorithm will be significantly reduced.

In order to reduce the traffic between the various partitions, D.W.Chenung proposed a fast association rule mining algorithm based on distributed system FDM. The algorithm first run apriori algorithm to find the local frequent itemset in each partition, and then changed the sum degree of the support between the global frequent itemset. The data size needed to be handled was reduced by generating a smaller number of candidate set.

In this paper, the global frequent sets can be acquired between 1time and 3 times based on the improved apriori algorithm.



IV. DISTRIBUTED ASSOCIATION MINING BASED ON THE IMPROVED APRIORI ALGORITHM  In this paper, we assume that the system contains three nodes S1, S2 and S3. The database is divided into DB1, DB2 and DB3. The data in the global database contain 60 transactions and each local database has 20 transactions.

Also we assume that the mini-support degree s=20%.DB1 acquire all the local frequent itemset < I2, I4, 5>, < I2, I5, 5>, < I2, I6, 4>, < I4, I5, 5> in the DB1 through the improved apriori algorithm. Similarly DB2 and DB3  TABLE IV. ALL OF LOCAL FREQUENT ITEMSET  respectively obtain their own local frequent itemset through the improved apriori algorithm. Then, all of local frequent itemset are broadcast to the node S and summarized at the node S.



X.sup1 X.sup2 X.sup3  I2,I4         I5 3,I4      I5 3,I4      6  I2,I6 I      4        2,I4      I4 3,I5      4  ? I2,I6      10  I2,I4,I5,I6 4  I3,I4          4  I4,I7          4  I5,I7          7  There are 60 records in the database. The mini-support degree is 20%.So if the support of the itemset satisfy 60*20%=12, the itemset is a global frequent set. The calculation of the support of the itemset is divided into four cases:  As {I3, I4}. { I3, I4  As {I  } is local frequent itemset in each sub-database so it must accord with the mini-support requirement. It must be global frequent itemset.

2, I6}. { I2, I6  As {I  } is not the local frequent itemset in all the sub-database, but the support of it satisfies the requirement of the mini-support. It is a global frequent itemset.

2, I4}. { I2, I4} has two local supports sup1, sup2, and sup1+sup2=9<12, then it doesn?t accord with the mini-support requirement. But 12-9<4*(3-2),and 12 represents the global mini-support, 9 represents the current mini-support, 4 represents the local mini-support, 3 represents the total count of nodes, and 2 represents the count of local support. { I2, I4} may be a global frequent  Now try to get U2 4 5 6. If the count of item in the intersection is larger than or equal to min_support, then {I2, I4, I5, I6} is the largest frequent itemset according to the inference in literature[3]. We get the intersection {T8, T13, T18, T20}, and the count of item is 4. This satisfies the minimum support min_support. Thus it can be seen, {I2, I4, I5, I6} is the largest frequent itemset. The record is< I2, I4, I5, I6, 4>. In the record, the number 4 expresses the amount of elements of the intersection. In other words, there are 4 records including I2, I4, I5, I6  TABLE III. THE LATELY SMALL DATABASE  at the same time.

Item Affair  I2 T      8,T12,T13,T18,T20       (U2)  I T4 8,T12,T13,T18,T20       (U4)  I T5 8,T12,T13,T17,T18,T20    (U5)  I T6 8,T13,T18,T20       (U6)  Owing to there are just I2, I4, I5, I6four items in this experiment, it is equal to the largest count k of item of the largest frequent assumed. So in the process of solving, when we try to get U2 4=Ua, if the count of Ua?s item is smaller than 4, |Ua  According to the specialty of Apriori algorithm: All of the sun sets of frequent itemset are frequent itemset, we can from the largest frequent itemset{ I  |<4, there is no need continue to get the intersection. Because the count doesn?t satisfy the minimum support, at this time the count of considering item is smaller than k. Then there isn?t the largest frequent itemset. If we don?t find 4-frequent itemset, then the largest count k of item of the largest frequent assumed decrease 1.Get the largest frequent itemset with the method above-mentioned.

2, I4, I5, I6} get part of 2-frequent itemset{I2, I4},{I2, I5},{I2, I6},{I4,I5},{I4,I6},{I5,I6} and part of 3-frequent itemset { I2, I4, I5}etc. We can get the intersection in the same: record < I2, I4, 5>, < I2, I5, 5>, < I2, I6, 4>, < I4,I5  And then continue to try to get the largest frequent itemset to database. We just need to take the k as 2 or 3. In this experiment , at last we find all of the largest frequent itemset{ I  , 5>etc.

2, I4, I5, I6,{I3,I4},{I4,I7},{I5,I7}. And {I2, I4, I5, I6

III. BASIC THOUGHT AND PROCESS OF DISTRIBUTION DATA MINING  } is the largest frequent itemset with the largest count of item.

Distributed data mining algorithms can be divided into two categories, data distribution algorithm (DD algorithm).and the count distributed algorithm (CD algorithm).

The basic idea of DD algorithm is to designate the candidate set evenly distributed among the nodes. This can reduces the number of candidate set to be scanned when the nodes increases. The problem is that the traffic between the nodes can reduce the efficiency of the algorithm.

CD algorithm is a typical parallel algorithm based on apriori algorithm. Each sub-database scans the database independently to calculate the support of each candidate set, and then set the sum of the supports of the entire candidate set as the global support. If the global support is greater than the mini-support, we regard the itemset as the global frequent itemset. CD algorithm begins the next scan through  the next synchronization point after the end of each scan.

With the increase of the nodes, the traffic of the algorithm will be increasing quickly and the efficiency of the algorithm will be significantly reduced.

In order to reduce the traffic between the various partitions, D.W.Chenung proposed a fast association rule mining algorithm based on distributed system FDM. The algorithm first run apriori algorithm to find the local frequent itemset in each partition, and then changed the sum degree of the support between the global frequent itemset. The data size needed to be handled was reduced by generating a smaller number of candidate set.

In this paper, the global frequent sets can be acquired between 1time and 3 times based on the improved apriori algorithm.



IV. DISTRIBUTED ASSOCIATION MINING BASED ON THE IMPROVED APRIORI ALGORITHM  In this paper, we assume that the system contains three nodes S1, S2 and S3. The database is divided into DB1, DB2 and DB3. The data in the global database contain 60 transactions and each local database has 20 transactions.

Also we assume that the mini-support degree s=20%.DB1 acquire all the local frequent itemset < I2, I4, 5>, < I2, I5, 5>, < I2, I6, 4>, < I4, I5, 5> in the DB1 through the improved apriori algorithm. Similarly DB2 and DB3  TABLE IV. ALL OF LOCAL FREQUENT ITEMSET  respectively obtain their own local frequent itemset through the improved apriori algorithm. Then, all of local frequent itemset are broadcast to the node S and summarized at the node S.



X.sup1 X.sup2 X.sup3  I2,I4         I5 3,I4      I5 3,I4      6  I2,I6 I      4        2,I4      I4 3,I5      4  ? I2,I6      10  I2,I4,I5,I6 4  I3,I4          4  I4,I7          4  I5,I7          7  There are 60 records in the database. The mini-support degree is 20%.So if the support of the itemset satisfy 60*20%=12, the itemset is a global frequent set. The calculation of the support of the itemset is divided into four cases:  As {I3, I4}. { I3, I4  As {I  } is local frequent itemset in each sub-database so it must accord with the mini-support requirement. It must be global frequent itemset.

2, I6}. { I2, I6  As {I  } is not the local frequent itemset in all the sub-database, but the support of it satisfies the requirement of the mini-support. It is a global frequent itemset.

2, I4}. { I2, I4} has two local supports sup1, sup2, and sup1+sup2=9<12, then it doesn?t accord with the mini-support requirement. But 12-9<4*(3-2),and 12 represents the global mini-support, 9 represents the current mini-support, 4 represents the local mini-support, 3 represents the total count of nodes, and 2 represents the count of local support. { I2, I4} may be a global frequent    itemset. So the node S broadcasts { I2, I4} to S1, S2, S3. If the sum of all of local support accords with the global mini-support, { I2, I4} is a global frequent itemset.

As {I3, I5}. { I3, I5} has one local support sup3=4. This doesn?t satisfy the requirement of the mini-support. At the same time, 12-4 =4*(3-1), { I3, I5

V. CONCLUSIONS  } can not be a global frequent itemset.

At last, we have mined all of the global frequent itemset.

That is to say to mine the association rules in distributed database.

The traditional algorithms create the local candidate sets firstly, and then determine whether the local frequent itemsets is the global frequent itemsets by the traffic between the nodes. The most different between the proposed algorithm and the traditional algorithms is that it firstly generates all the local frequent itemsets at each node and then communicates to the top point. At the top point, there are four cases to deal with all the local frequent itemsets. For the fastest case, the determination could be made by the completion of round-trip communications. At the same time, all the operations of this algorithm are completed not by traditional data storage but by a new data storage in which the item is considered as the keyword. The method can save storage space, especially for sparse data. So the support can be calculated by the intersection of the transaction sets, which is much easier than by accessing to the database.

Finally, the association rules in the distributed database are mined.

