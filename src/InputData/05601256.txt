TCP-to-SCTP Translation using Dynamic Rules-table

Abstract?Using the application of bulk data transfer, we investigate the performance of existing TCP-to-SCTP Translation shim scheme under the scenario of shim rules-table is static (i.e., manual). We find that a potential problem named stale shim rules-table value caused by shim rules-table static configuration, which induces TCP-to-SCTP Translation does not startup in time. Based on this, we present a suggestion called Dynamic Rules-Table (DRT) for TCP-to-SCTP Translation to implement the real-time startup and to improve the throughput of SCTP association. A demonstrate using simulation is provided showing how application performance in terms of responsiveness and throughput using the shim and SCTP is equivalent to or better than performance when application operate using TCP-to- SCTP translation shim as originally designed.

Keywords-TCP-to-SCTP Translation; shim rules-table; Dynamic Rules-Table (DRT)

I. INTRODUCTION SCTP [1] is a connection-oriented, reliable, messaged based,  general purpose transport protocol with congestion control similar to that used by TCP, supporting advanced features unavailable such as fault tolerance [1] and concurrent multipath transfer (CMT) [2] in TCP or UDP.

By now, the motivation for TCP-to-SCTP Translation, design and implementation of shim layer, including the general approach taken, the functionality of the major components, the implementation details and the experimental evaluation performed on the shim and the results have been first described in [3].

Our research focuses on the policy selection of shim rules- table configuration for transparent TCP-to-SCTP translation.

Our goal is to find a new scheme of shim rules-table configuration to further improve the performance of TCP-to- SCTP translation by modify original scheme of TCP-to-SCTP translation. This paper introduces a transparent TCP-to-SCTP translation using Dynamic Rules-Table (DRT) to prevent the potential problem named stale shim rules-table value and to avoid  that TCP-to-SCTP translation does not startup in time caused by stale refresh to rules-table.

The rest of the paper is organized as follows. Section II describes current scheme of transparent TCP-to-SCTP translation shim layer, emphasize on the disadvantage of shim rules-table static configuration scheme, and then presents Dynamic Rules-Table (DRT) scheme. And a series of simulations and results are discussed in Section III. We conclude the paper with future work in Section IV.



II. DYNAMIC RULES-TABLE (DRT)  A. Problem Description Figure 1 illustrates the relationship between a normal TCP  socket and its hidden SCTP socket, as well as the corresponding protocol switch structures [3].

Figure 1.  Normal TCP socket with hidden SCTP socket  The basic building block of the shim control system is a rule object, consisting of an IP address, a subnet mask and two port numbers.

Previously, the shim rules-table is only refreashed using manual, the original command list of shim rules-table is illustrated in the following table.

TABLE I.   COMMAND LIST FOR SHIM RULES-TABLE  command list Usage:shimrules-[AD]-c<chain>-p<policy><rule specification> shimrules-L[-c<chain>][-p<policy>] shimrules-[FH] shimrules-G-c<chain> shimrules-S-c<chain>-p<policy> Commands: -A Add rule -D Delete rule -L List rules -F Flush all rules -G Get default policy -S Set default policy -H Show usage information     Table I shows shim rules-table is only configured ordinarily, which induces blindness for an association of TCP-to-SCTP translation shim layer.

For example, here we assume remote-enable is {IP address 10.1.2.0/24, Port 21} in shim rules-table, the shim layer attempt to startup TCP-to-SCTP translation when application call remote IP address 10.1.2.20, if remote server does not support SCTP (i.e., support TCP only), in the best case ( i.e., minimum RTO = 1s, PMR = 5 ), it will take at least (1 + 2 + 4 + 8 + 16 + 32) = 63s to detect failure. In the worst case (i.e., maximum RTO = 1s, PMR = 5), failure detection requires 6*60 = 360s!

B. Update Command List and Modify Shim-rule Structure In FreeBSD, most tunable operating system parameters are  implemented as sysctls. The sysctl interface allows an administrator to configure the system?s kernel variables dynamically [4]. Since network protocols typically have a large number of tunable parameters, the sysctl interface facilitates adjusting system properties without requiring editing of source code and kernel recompilation. We use sysctls for many of the shim?s dynamic configuration variables.

As table II shows, in our design, two new commands such as {-C Auto configuration} and {-B forbidden Auto Dynamic Configuration} are added, and two new correspond socket options such as { SO_SHIM_RULE_AUTO_EN } and { SO_SHIM_RULE_AUTO_DIS } are also added as table III shows.

TABLE II.  COMMAND LIST FOR SHIM RULES-TABLE WITH ADDED COMMAND ?C AND COMMAND  -B  command list ? ? -C Auto configuration -B forbidden Auto Dynamic Configuration  TABLE III.  ADDED SOCKET OPTIONS  Socket selection                               System call             Function ?                                  ?                                 ? ?                                  ?                                 ? SO_SHIM_RULE_AUTO_EN  setsockopt ? ? sosetopt ? ? SO_SHIM_RULE_AUTO_DIS      setsockopt? ?      setsockopt? ? {SO_SHIM_RULE_AUTO_EN} indicates startup auto  configuration process, and {SO_SHIM_RULE_AUTO_DIS} shows dis-startup auto configuration process.   After {-C Auto configuration} is configured by shimrules tool, Setsockopt () in the kernel will add a flag of {SO_SHIM_RULE_AUTO_EN} in the header of shimrules_config_queue, then this flag will notifiy shimrules_auto_config, so connect ( ) stores inquired shim rules content in the queue. The queue stuucture of shimrules_config_queue is illustrated  in Figure 3.

Here, original shim_rule structure is modified by us, four fields shcu as date, date_call_times, fail_times and call_time are added. Date field indicates shim_rule is refeshed currently, date_call_times field shows call count today, fail_times indicates failure count today, it shows some rules will be deleted from shim_rule and add to remote_disable when    Figure 2.  Queue structure for Shimrules_config_queue  C. Implementation of TCP-to-SCTP translation using DRT    Figure 3.  A typical Client / Server Connection  for SRDC operation

III. PERFORMANCE ANALYSIS AND EVALUATIONS  A. Simulation Setup For our experiments, we measure the total time required to  transfer files of various sizes using the SSH suite?s SCP tool when running over a normal TCP connection and when running over the shim using an SCTP association. We compare the transfer times for TCP and the existing shim rules-table scheme at a variety of loss rates.

Each experiment required three nodes: a server machine running the SSH/SCP service, an SCP client, and an intermediate node running Dummynet [5] to simulate bandwidth, propagation delay, and loss rate configurations. The    intermediate Dummynet router node was configured with a tail-drop queue of 50 packets and performed uniform random loss at the rates described above. Each node was a Pentium 4 system running FreeBSD 4.10 with a KAME kernel supporting SCTP. The SCTP version used was patch level 25, released in February 2005. To prevent the experiments with the shim from naturally taking advantage of the ability of SCTP multihoming and using other paths not part of the simulation topology, we disabled all interfaces besides the ones attached to the Dummynet-simulated network on the client and server systems before beginning the experiments. Disabling the alternate interfaces allowed for a fair comparison between TCP and the shim because SCTP was restricted to the simulated network, and was unable to use any alternate paths between the client and server nodes.

Each run of the experiment involved measuring the total time required to issue the SCP command to retrieve a single file on the client system, and then transfer the entire file from the server, including the SSH key exchange overhead. We used a public-key authentication configuration rather than passwords with SSH to allow the experiments to be run in a non- interactive batch mode. Every combination of file size and loss rate was run with the 1.5 Mbps/35 ms bandwidth-delay configuration a total of 30 times, except the 50K file experiments which were run 90 times per loss rate due to higher variance in the transfer times. Thus, each data point in the graphs shown in Section III-B is the average of 30 (or 90) runs of the same file size/loss rate configuration.

B. Experimental Results Figures 4, 5, 6, and 7 display our recorded transfer times for  each simulated loss rate for 50 KB, 500 KB, 5 MB, and 25 MB files, respectively. In situations where SCTP is available on both peer endpoints, the shim adds no significant communications overhead beyond the inherent differences in the transport layer protocols being used or substituted. The case where SCTP is unavailable on the remote system can be a source of overhead during the connection establishment process because at least one additional RTT is required to  50 KB Transfer: 1.5 Mbps/35 ms           0% 2% 4% 6% 8% 10% Loss Rate  Fi le  T ra  ns fe  r t im  e (s  ec )  TCP Shim/SCTP DRT    Figure 4.  Transfer Time vs. Loss Rate for 50 KB file transfer over 1.5 Mbps/35 ms delay link in different polices.

500 KB Transfer: 1.5 Mbps/35 ms           0% 2% 4% 6% 8% 10% Loss Rate  Fi le  T ra  ns fe  r t im  e (s  ec )  TCP Shim/SCTP DRT    Figure 5.  Transfer Time vs. Loss Rate for 500 KB file transfer over 1.5 Mbps/35 ms delay link in different polices.

5 MB Transfer: 1.5 Mbps/35 ms          0% 2% 4% 6% 8% 10% Loss Rate  Fi le  T ra  ns fe  r t im  e (s  ec )  TCP Shim/SCTP DRT    Figure 6.  Transfer Time vs. Loss Rate for 5 MB file transfer over 1.5 Mbps/35 ms delay link in different polices.

25 MB Transfer: 1.5 Mbps/35 ms          0% 2% 4% 6% 8% 10% Loss Rate  Fi le  T ra  ns fe  r t im  e (s  ec )  TCP  Shim/SCTP  DRT    Figure 7.  Transfer Time vs. Loss Rate for 25 MB file transfer over 1.5 Mbps/35 ms delay link in different polices.

detect that SCTP is unavailable before reverting to TCP and starting data transfer. However, because the nodes in our experiment support SCTP and both the SSH/SCP client and server have the shim enabled, no additional overhead is introduced by using the shim. The differences in transfer times are entirely due to the specific features and implementations of the underlying transport protocols, not the translation process.

The graphs yield two main observations about application throughput over a TCP connection versus the shim with an SCTP association. First, in situations without any network- induced loss, TCP and the shim perform with different rules- table policy such as existing and we support approximately equivalently. Although not visible in the graphs, TCP had a slight edge with average transfer times between 4 and 240 ms faster than the shim at 0 percent loss across all four file sizes tested. This difference is likely a result of SCTP?s more complex four-way association establishment handshake compared to TCP?s three-way handshake, and SCTP?s more expensive CRC32 checksum. The second observation is that for all runs with loss rates greater than 1 percent, the shim running over SCTP outperforms TCP by an increasing margin as loss rates increase. The trend of the shim providing better application throughput at all loss rates greater than 1 percent holds across all files sizes, with longer transfers (i.e., 25 MB files) seeing a greater improvement than short transfers (i.e., 50 KB files). We argue that the greater throughput afforded by the shim in high loss conditions is due to the advanced congestion control features in SCTP, such as Limited Transmit, Appropriate Byte Counting, and Selective Acknowledgments, that are not present in FreeBSD 4.10?s version of TCP (New Reno) [6]. Our results from experimenting with SCP over the shim confirm the same trends at high loss rates as earlier work [6] which experimented with various implementations of FTP running over SCTP. We speculate that if TCP were to incorporate the same congestion control features that SCTP currently supports, throughput for applications running over both the shim and normal TCP connections would be similar at all loss rates.

We feel our experimental results show the transparent TCP- to-SCTP translation shim with DRT policy is technically feasible, functions effectively with common network applications under realistic conditions, and provides performance (measured in terms of application throughput) that is equivalent to or better than what is possible using TCP and existing TCP-to-SCTP translation scheme.



IV. CONCLUSIONS AND FUTURE WORK The transparent TCP-to-SCTP translation shim allows  legacy TCP applications to enjoy SCTP?s multihoming  advantages (i.e., fault tolerance and potentially concurrent multipath transfer) without requiring any modifications to the legacy applications themselves. Additionally, in non- multihoming situations, the shim encourages increased SCTP deployment by providing a path for gradual migration from legacy TCP applications to native SCTP applications, solving the incremental deployment problem typically experienced with new protocols.

Experimental results presented in Section III illustrate that not only is the shim approach an interesting theoretical concept, but that the shim with DRT policy is technically feasible in practice with real applications under typical network conditions. The hope of this work is that users and developers alike will begin to appreciate how the advanced features provided by SCTP can be valuable for network applications.

By ensuring that existing legacy TCP applications can seamlessly interact with new SCTP counterpart applications, the shim encourages innovation and the increased deployment of SCTP throughout the Internet.

In the future work, it would be interesting to looking for a new scheme of transparent TCP-to-SCTP translation shim using parallel TCP and SCTP connection when TCP connection and SCTP association co-exists.

