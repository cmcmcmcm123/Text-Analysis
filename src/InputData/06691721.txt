Searching Inter-disciplinary Scientific Big Data based on Latent Correlation Analysis

Abstract?In this paper, a novel cross-database search system (Cross-DB) is proposed. The aim of Cross-DB is to facilitate the search of interdisciplinary-correlated datasets from large- scale, multi-domain and heterogeneous data repositories. With conventional systems or portals for searching scientific datasets, the scientists must know the relation between the datasets in advance or must find their relations manually. In Cross-DB, the datasets search process is based on discovering an optimal combination of their multiple and latent associations such as spatio-temporal, ontological, and citational correlations based on evolutionary computing.

The basic concepts of Cross-DB are introduced as well as its main components. Comparisons with an existing search engine based on a massive datasets repository demonstrate the feasibility and the correctness of the proposed framework. We show that offering to the user a full set composed of correlated datasets is a useful alternative to the classical ranking methods. Experimental result shows that our system can overachieve conventional portal search in terms of relevance and novelty.

Keywords?Big Data, Evolutionary Computation, Search En- gine, e-Science, Serendipity.



I. INTRODUCTION  The proliferation of affordable computing resources, com- bined with the pervasive deployment of sensors and large scale instruments capable of capturing huge amounts of data, has enabled the rapid growth of data-driven computational science, also known as e-Science, as defined in the framework of the fourth paradigm [1]. This has led to an explosion in the availability of scientific datasets [2], including raw data taken directly from measuring instruments and also derived data from computational models and simulations [3]. These datasets can be stored on-line in large volume in public or private repositories, and be made accessible to users within a scientific community or beyond, in an effort to foster inter-organizational and inter-disciplinary research that can accelerate scientific discovery [4], [5].

Finding datasets for scientific work requires to discover them across a wide range of domains inter-related, such as an interdisciplinary research about a particular natural phenom- ena. By using current systems or portals for searching datasets it is not possible to find relationships among them, if the user does not have any prior knowledge about the data attributes and terminology.

There have been many efforts to create search engines suitable for discovering scientific datasets. One of the biggest scientific data repository is known as the World Data System (WDS) [6]. The system includes data from more than hundreds repositories. Among the other data repositories, we can note the Global Change Master Directory [7] or the National Oceanic and Atmospheric Administration Climate Services [8].

Although widely used by scientists, all of them implement keyword-based search engines, especifically, they allow data- attribute search.

In this paper, a new search engine is introduced named ?Cross-DB?, for Cross-Database search system. Its specifici- ties are: 1) It is especially oriented for datasets discovery, facilitating the data access and their usability. 2) It provides high quality-ensured data and information especially for the scientists since it is capable to find the related and correlated datasets.

The main contributions of this paper are listed as follows:  ? A model and architecture for discovering appropriate and comprehensive sets of datasets based on multi- correlational optimization process, which considers the combination of spatiotemporal, ontological and citational correlations.

? Based on a genetic algorithm enhanced with original and specific genetic operators a quality function is constructed to effectively optimize and determine the quality score for a set of datasets and not for an individual dataset.



II. CROSS-DB SYSTEM ARCHITECTURE  Fig. 1 shows the architecture of the Cross-DB search system. Basically it is composed of three layers. The upper layer correspond to the application, where the web-search interface is located and where users have direct interaction by performing queries and visualizing the results. The middle layer is the correlation engine that is composed of a complex- join process which integrates several types of correlations in order to find not only the objects satisfying the user?s query, but also all the other objects that are somehow linked to those objects. The typical objects are datasets, web docu- ments, microblogs, etc. Initially, for simplicity, only datasets are considered in this paper. In the lower layer, the system     Fig. 1: Conceptual Architecture of Cross-DB Search System  collects the description of the objects (metadata) by harvesting several digital repositories. Then, metadata is used to find the correlations.

1) Spatio-Temporal Correlation: The spatio-temporal cor- relation represents the possibility to find datasets in a surround- ing area and/or a specific point of time. Consider two datasets a and b with a geographical area represented as a rectangle R1 and R2 for each dataset, respectively. Let (x1, y1) be the location of the bottom-left corner of R1 and (x2, y2) be the lo- cation of its top-right corner. Similarly, let (x3, y3) and (x4, y4) be the respective corner locations for R2. The intersection of R1 and R2 will be a another rectangle R3 whose bottom-left corner is at (max(x1, x3),max(y1, y3)) and top-right corner at (min(x2, x4),min(y2, y4)). If max(x1, x3) > min(x2, x4) or max(y1, y3) > min(y2, y4) then R3 does not exist, i.e.

R1 and R2 do not intersect. Then, the spatial correlation CS between datasets a and b is defined by: CS(a, b) = 12 [  R3 R1  +R3R2 ].

If both overlap completely, the result is 1. In case there is no intersection, the result is 0.

For temporal correlation, the starting and ending time of the capturing data is considered. For instance, STa and ETa represent the starting time and ending time for dataset a, respectively. Temporal correlation CT between datasets a and b is defined as:  CT (a, b) =  [  t  ETa ? STa + t  ETb ? STb ] (1)  where t is the overlapping time between datasets a and b.

Finally the spatio-temporal correlation between datasets a and  b is defined by:  CST (a, b) = CS(a, b) + CT (a, b)  (2)  2) Ontological Correlation: The ontological correlation aims at finding datasets in related subjects and disciplines. It is computed as follows:  1. Clustering the datasets according to their similarity. That is, every dataset should belong to one or more clusters. 2. With the knowledge of an expert, each cluster is labeled and mapped to a specific class or concept in the ontology. As our aim is to find scientific datasets, an appropriate public scientific ontology is the Semantic Web for Earth and Environmental Terminology ontology (SWEET) [9]. 3. Using a conventional algorithm, the score similarity between the nodes of the ontology is computed. In this paper, the ILIADS algorithm [10] is utilized. That is, all the connections among the nodes in the ontology have a score. For instance, the score for the connection from node x to node y is represented as S(vx, vy).

Then the ontological correlation between datasets a and b can be calculated, as follows: 1. Find the label of the cluster Ca and Cb for each dataset, respectively. 2. For Ca and Cb find the class or concept associated, say va and vb. 3. Calculate the distance between va and vb by using the geodesic distance [11] which is the number of edges of the shortest path that connect them.

Let d(va, vb) denote the distance between va and vb where va, vb ? V . Obviously if va = vb, then d(va, vb) = 0.

Otherwise d(va, vb) = S(va, vb), assuming that va and vb are first-degree distant.

Therefore d(va, vb) is defined as:  d(va, vb) =  n(n? 1) ? i,j  d(vi, vj) (3)  where n is the number of vertices in the path between va and vb. Finally, the ontological correlation between datasets a and b is defined as: CONT (a, b) = ed(va,vb) where e is the base of the natural logarithm.

3) Citational Correlation: Data Citation (DC) refers to the relationship between web documents and datasets. Citational correlation is used to discover set of datasets that are frequently cited together[12]. It allows users to understand which datasets have been used for what purpose, or in what other prior works.

The citational information is harvested together with the metadata and stored in a specific repository named Data Citation Archive (Fig. 1). Then, a process for association rule discovery [13] is executed on this repository to find the text- to-data associations between typical text-attributes contained in web documents and typical data-attributes contained in scientific databases. Each association rule has its support value and the hub score for each text-attribute. Citational correlation between datasets a and b is calculated as follows: 1.

Compute STPk: the strength of each path between datasets a and b, as follows: STPk = hubi ? suppi,j where k ? K, K is the number of paths that exist between two datasets, hubi is the hub score of text-attribute i, and suppi,j is the support value of the rule that associates text-attribute i with data-attribute j. 2.

Then, count the total strength of all paths:TSTP =  ? k STPk.

Finally the citational correlation between datasets a and b is defined by: CDC(a, b) = 2 ? [ 11+eTSTP ? 12 ] where e is the base of the natural logarithm.



III. COMPLEX-JOIN SEARCH ENGINE  The design of the Complex-Join search engine is based on a genetic algorithm (GA). This choice is motivated by the following reasons. First, the search space consists of all the datasets (DS) contained in the system. Hence, exploring the search space for relevant datasets can be seen as a highly dimensional problem that possesses a huge amount of local optima. GAs are known for their robustness and their capability to rapidly explore noisy and large search spaces. Second, the stochastic nature of GA allows the search to keep and to discover diversity, independently of the initial query.

A. Analogy with Graph Theory  In the proposed GA, a candidate solution is a finite collection of datasets lying in the semi-finite space ?. If we see ? as the graph ? = (V,E) where V is the set of nodes (datasets) and E is the set of edges that symbolize the correlations between the datasets, then a candidate solution can be represented as the complete graph S = (V ?, E?) where V ? ? V , {V ?} = {v?1, . . . , v?|V ?|}, and {E?} = {e?1, . . . , e?|E?|}.

Since the links can be seen as a distance measurement between two nodes, and that the distance is in this case the strength of the correlations that links the datasets, we will refer to the graphs as ?correlational graphs?. Each edge of E possesses NC components, ei = {ei1, . . . , eiNC}, (i = 1, . . . , |E|), that correspond to NC types of correlations.

B. Problem Definition and GA Objective Function  Given the set ?, the goal of the GA is to determine the optimal set S? = (V ?, E?) such that V ? ? V and 1. At least one type of correlations that connect one node to another is above a threshold thres corr for each node. 2. The overall correlations that group all the nodes together is as high as possible. 3. The number of nodes that compose V ? is as big as possible. 4. The diversity present in V ? is as high as possible.

Formally, the Complex-Join seeks an optimal or near- optimal solution of the nonconvex optimization problem maxS?? f(S), where S is a candidate solution and f is a real- valued function that is also used as the GA objective function:  f(S) = ia?MCS(S)+ib?P LENGTH(S)+ic?DIV (S), (4)  where MCS is the mean correlation strength defined as  MCS = ?NC  j=1 pj ? ?S,j with ?S = ?|E?|  i=1 ei |E?| =  {?S,1, . . . , ?S,NC} and pj ? [0, 1], j = 1, . . . , NC. MSC is thus the average of the correlation values for each type of correlation, scaled by a preference factor p specified by the user. P LENGTH is the size of the biggest connected com- ponent of S. The connections of graph S can be represented by the adjacency matrix A(S), such that the term  aij =  { 1, if max(e?{ij}) ? thres corr, 0, otherwise.

We then define for our case: Given S = (V ?, E?), a subgraph C = (VC , EC) is a maximally connected component if C is connected and for all vertices u such that u ? V and u /? VC there is no vertex v ? VC for which auv = 1. DIV measures the diversity in terms of provenance of the set of nodes V ?. In Equation (4), ia, ib and ic are the importance factors attributed by the user to each objective.



IV. EXPERIMENTAL RESULTS  WDS Pangaea Portal as well as Cross-DB have been queried using the same keywords for comparison. The frame- work adopted in this paper is as follows. First, a list of ten arbitrary keywords mainly inspired from environmental science has been created. By using each keyword, Cross-DB and WDS have been queried separately and the amount of retrieved DS has been recorded (in |Cross-DB| and |WDS|, respectively).

The number of common DS to, as well as the number of DS uniquely retrieved by both systems have also been tracked (within |X ? Y |, |X \ Y | and |Y \X|, respectively, assuming X = Cross-DB and Y = WDS). Let (X \ Y ) and (Y \X) be the sets that represent the DS unique to Cross-DB and WDS, respectively. For each DS contained in the sets, three experts were asked to judge whether the DS were relevant or not.

Let F be an arbitrary set of DS. The relevance of a DS i of the set F is written r(Fi) and takes the value 1 when Fi is considered relevant, 0 otherwise. Then the relevance of the set F is equal to  ?|F | i=1 r(Fi)/|F |, where |F | represents the  number of elements in F .

Similarly, we have also evaluated the serendipity of Cross- DB. The serendipity is defined in [14] as being a measure of the extend to which a DS is considered as attractive and surprising for the user. Therefore, a serendipitous DS should not have been expected by the user but should also be useful.

TABLE I: Comparison Results between Cross-DB and WDS in terms of Relevance  Query Number of DS retrieved Number of DS retrieved |X ? Y | |X \ Y | r(X \ Y ) |Y \ X| r(Y \ X) by Cross-DB = |X| by WDS = |Y |  1. global warming 74 61 33 41 74.8% 28 42.9% 2. marine biology 53 114 38 15 40.0% 76 10.6% 3. ice sheet 147 432 136 11 63.8% 296 6.2% 4. pollution 344 90 62 282 78.2% 28 20.6% 5. el nino 184 22 18 166 76.3% 4 41.7% 6. seasonality 481 337 122 359 60.4% 215 14.2% 7. aromatic hydrocarbon 68 53 41 27 82.5% 12 32.8% 8. climate change 488 579 403 85 83.7% 176 15.0% 9. enzyme 82 101 65 17 65.1% 36 11.3% 10. heat flux 439 376 287 152 42.6% 89 14.4% Mean 236 216.5 120.5 115.5 66.7% 96 21.0%  TABLE II: Serendipity Evaluation of Cross-DB  Keyword |X \ Y | serend(X \ Y ) 1. global warming 41 79.7% 2. marine biology 15 39.8% 3. ice sheet 11 72.7% 4. pollution 282 61.1% 5. el nino 166 68.4% 6. seasonality 359 55.5% 7. aromatic hydrocarbon 27 47.6% 8. climate change 85 72.6% 9. enzyme 17 66.9% 10. heat flux 152 47.5% Mean 115.5 61.2%  By assuming that the set Y = WDS contains all the DS that are expected by the user, then the originality or unexpect- edness provided by Cross-DB lies in the set (X \ Y ) with X = Cross-DB. Serendipity of an arbitrary set F is calculated with the following formulae: serend(F ) =  ?|F | i=1 u(Fi)/|F |,  where u(Fi) is a function that evaluates the usefulness of the DS i of the set F . The averaged serendipity score for each keyword are reported in Table II.



V. CONCLUSIONS AND FUTURE WORK  A framework for discovering correlated datasets from large-scale scientific data has been proposed. It provides scientists with a quick selection tool and with improved comprehensibility with the results. It relies on an enhanced GA as the core of the search engine which performs a multi- optimization of several types of correlations among datasets.

Empirical comparisons between Cross-DB and WDS reveals that the result sets contains a proportion of common returned DS. However, it is the original DS unique to each system that highlight their characteristics. Indeed, our experimentations demonstrate that while WDS focuses on displaying documents that simply match a given keyword, without much more strat- egy, Cross-DB on the other hand emphasizes on the relations that exists between the DS contained in the result set. For future work, a lot of improvements are considered over the current proposed system. 1. Let the users perform queries by example data. That is, the users are allowed to discover latent correlations among their own datasets and external datasets provided by the system. 2. Since scalability is one of the most important issue to obtain more interesting results quickly, a user-defined harvester method has to be developed to include many different large and heterogeneous data repositories. 3.

Query processing can be extended by a concept dictionary for aggregating diversity for better performance. Moreover, sophisticated Natural Language Processing and text mining techniques may be added so that query processing can better capture the user?s intention.

