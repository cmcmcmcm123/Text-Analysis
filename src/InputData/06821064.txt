Smart Task Distributor for MapReduce on Cloud  Computing

Abstract?A MapReduce system is widely used to implement the large-scale computation on cloud computing. A MapReduce system currently defines computation resources in a node as a roughly configurable slot number, and distributes tasks over nodes according to the slot number. However, a MapReduce system may make computation resources of clusters in the underutilization or overutilization condition, because a task of different applications unlikely uses the same computation resources and because a node may have different CPUs with different capabilities. A MapReduce system can use Smart Task Distributor (STD) proposed in this paper to solve the problem of the computation resource underutilization or overutilization in clusters. Technically, a MapReduce system can use STD to smartly distribute tasks over nodes in clusters, because STD on the one hand gradually assigns tasks to a node in order to fully utilize computation resources in the node and on the other hand dynamically estimates the remaining computation resources in the node for toggling the assignment of tasks on demand without overloading it. In experiments, a MapReduce system is proved to get better performances with STD than with other ways.

Keywords?STD; Computation Resources; Overutilization; Underutilization; MapReduce; Cloud Computing

I.  INTRODUCTION MapReduce [1] is the key to the success of cloud  computing today. As a programming model, MapReduce can easily distribute over nodes in clusters for an application that is programmed according to the model. MapReduce allows application developers who are not familiar with distributed and paralleled programming to easily develop applications on cloud computing. All MapReduce needs is application developers to implement their applications with a Map function (a.k.a. Mapper) and a Reduce function (a.k.a. Reducer). At runtime, MapReduce uses the runtime system to automatically distribute Mappers and Reducers over nodes in clusters on behalf of application developers. Because MapReduce highly relies on the runtime system, the design of the runtime system greatly affects the performance of applications on cloud computing.

On cloud computing, the runtime system partitions input files into multiple blocks at the initiation time of an application.

Next, the runtime system finds a node that has available computation resources and then distributes a Mapper of the application over the node. When running a Mapper on a node, the runtime system provides the Mapper with blocks of input  files, e.g. loading them from the master node or a distributed file system such as the Google File System (GFS) [2] and the Hadoop Distributed File System (HDFS) [3]. After a Mapper ends execution and generates output files (a.k.a. intermediate files), the runtime system finds a node that has available computation resources to run a Reducer and forwards the intermediate files from the Mapper to the Reducer. Finally, the runtime system collects, sorts, and merges the output files generated by Reducers in all nodes as the result of the application.

However, a runtime system in the existing MapReduce prototypes, e.g. Hadoop [3], defines computation resources in a node as a roughly configurable slot number. For example, a runtime system may define a node to have two slots capable of running two Mappers or Reducers if the node has a dual-core CPU in it. Currently, a runtime system leaves the configuration of slot number in a node to the administrator of the runtime system or the designer of the prototype. Usually, a runtime system has a default value and no administrator wants to change it arbitrarily. Because a Mapper or Reducer of different applications unlikely uses the same computation resources and because a node may have different CPUs with different capabilities, a runtime system may make computation resources of clusters in the underutilization or overutilization condition if distributing Mappers or Reducers over nodes according to the roughly configurable slot number.

A runtime system maybe incurs the problem of computation resource underutilization if occupying slots in a node with I/O-bound tasks [4]. For example, a runtime system may distribute to a node the tasks that consume very few computation resources, so the node almost idles even though its slots are all used. Conversely, a runtime system maybe incurs the problem of computation resource overutilization if occupying slots in a node with CPU-bound tasks [5]. For example, a runtime system may distribute to a node the tasks that consume many computation resources, so the node is too overloaded to give the underlying operating system or the runtime system enough computation resources for performing certain important routines. Furthermore, when two nodes have CPUs with different capabilities but are configured the same slot number just according to the core number, a runtime system may incur the problem of computation resource underutilization in one node but incur the problem of computation resource overutilization in the other node.

DOI 10.1109/CLOUDCOM-ASIA.2013.110    DOI 10.1109/CLOUDCOM-ASIA.2013.110    DOI 10.1109/CLOUDCOM-ASIA.2013.110     In this paper, Smart Task Distributor (STD) is proposed to solve the problem of the computation resource underutilization or overutilization in clusters on cloud computing. STD is a module working on a master node to help the runtime system smartly distribute tasks over nodes in clusters. STD is heuristically designed to distribute tasks over nodes according to the Additive-Increase/Multiplicative-Decrease (AIMD) algorithm [6]. On the one hand, STD can gradually assign tasks to a node in order to fully utilize computation resources in a node. On the other hand, STD can dynamically estimate the remaining computation resources in a node and toggle the assignment of tasks on demand without overloading the node.

Accordingly, STD can prevent nodes from working in the underutilization or overutilization condition. In experiments, STD is tested with several canonical applications in nodes having different slot numbers. Besides, STD is compared with a General Task Distributor (GTD) that assigns a task to the node having the most available slots, and an algorithm (referred to as IDLEST) that assigns a task to the node having the most computation resources (i.e. the idlest node).

According to experiment observations, STD outperforms GTD and IDLEST which are the two algorithms widely used in the existing runtime systems.

We briefly highlight the contributions of this paper as follows.

1. Although there are several algorithms capable of estimating computation resources in a node in literature, e.g.

history-based [7] or Single Exponential Smoothing (SES) [8] algorithms, we are the first one to propose estimating computation resources based on the AIMD algorithm [6], a well-known algorithm used by TCP to estimate available network bandwidth.

2. We use STD based on the AIMD algorithm to simultaneously avoid the problems of computation resource underutilization and overutilization in clusters on cloud computing.

3. We implement STD on a general runtime system and test it with several canonical applications to verify its practicability.

4. We compare STD with GTD and IDLEST in experiments to confirm that STD outperforms the other algorithms.

This paper is organized as follows. Section 2 reviews MapReduce. Section 3 introduces Smart Task Distributor (STD). Section 4 presents experiment results. Section 5 concludes this paper.



II. MAPREDUCE MapReduce [1] is a programming model proposed by  Google to compute big data in clusters on cloud computing.

MapReduce attracts application developers because the programming model is simple. MapReduce only needs application developers to develop their applications with a Map function (a.k.a. Mapper) and a Reduce function (a.k.a.

Reducer). At runtime, MapReduce relies on the runtime system to read input data, distribute Mappers and Reducers over nodes in clusters, provide Mappers with input data, forward  intermediate data from Mappers to Reducers, and collect outputs of Reducers as the result of the application. Because the runtime system handles the issues of distributing applications over nodes, managing tasks, and collecting results on behalf of application developers, MapReduce gradually becomes a de facto standard in programming applications on cloud computing.

Word Count [1] is a typical application often used to explain MapReduce and its runtime system. Word Count needs application developers to develop a Mapper that processes input data tokenized by the runtime system and generates a key and value pair for each word in input data. When reading a word ?good? in input data, for example, Word Count uses a Mapper to generate ?good 1? as intermediate data stored in local disks. In a Reducer, Word Count can receive intermediate data forwarded by the runtime system from certain Mappers, and sum all identical words as the count of the word. For example, Word Count can generate ?good 10? as a part of the final result if getting ten strings of ?good 1? in intermediate data. Finally, Word Count can get the counts of all words in input data after the runtime system collects outputs from all Reducers.

In MapReduce, the runtime system plays a critical role. The runtime system has to read and partition input data into blocks, and then tokenizes data in the blocks before giving tokenized data to Mappers. During the execution of a Mapper, the runtime system collects its outputs, i.e. intermediate data, and forwards intermediate data to the corresponding Reducers over networks. Finally, the runtime system has to collect, sort, and merge outputs from all Reducers as the result of the application. The runtime system manages the tasks, i.e.

instances of Mappers and Reducers, and has a great impact on the performance of an application when distributing tasks over nodes in clusters.

In the existing MapReduce prototypes, the runtime system uses a master to distribute tasks over nodes according to available slot numbers in nodes. If a node is defined to have two slots, for example, the runtime system never gives the node more than two tasks. However, the runtime system hardly gives an application a better performance according to the method of quantifying computation resources with slot numbers, because different CPUs have different capabilities. Merely relying on slot numbers based on the core number in a CPU, the runtime system may accidentally make nodes in the condition of computation resource overutilization or underutilization, for example, by giving four tasks to a node having a low-end quad- core CPU but two tasks to a node having a high-end dual-core CPU.



III. SMART TASK DISTRIBUTOR (STD)  A. Overview Smart Task Distributor (STD) is a module working in a  master of the runtime system and distributes tasks over nodes according to the Additive-Increase/Multiplicative-Decrease (AIMD) [6] algorithm. STD periodically collects CPU utilization statistics from nodes and uses the AIMD algorithm to estimate future CPU utilization of each node. When having a     task ready to run, STD gives the task to the node that is estimated to have the least CPU utilization in the future.

We can use Fig. 1 to explain STD and compare it with a General Task Distributor (GTD) that assigns a task to the node having the most available slots, and an algorithm (referred to as IDLEST) that assigns a task to the node having the most computation resources (i.e. the idlest node). We know that GTD finds the node having the highest slot number because GTD uses slot numbers to quantify computation resources in nodes. If a node already has 90% CPU utilization, for example, we know that GTD still assigns a new task to the node instead of other nodes because the node has the most slots among nodes. Conversely, we know that IDLEST will assign a new task to a node having 65% CPU utilization because the node has the lowest CPU utilization. If the node uses a low-end CPU, we think that assigning a new task to the node will further degrade the performance because the node may easily be overloaded. In STD, we probably assign a new task to a node having 80% CPU utilization because the node is estimated to have the least CPU utilization in the future.

Fig. 1, STD Overview  B. Theory STD uses the Additive-Increase/Multiplicative-Decrease  (AIMD) [6] algorithm to distribute tasks over nodes in clusters.

STD assumes that each node has uncertain computation resources for tasks of applications because other factors probably exist to compete with the tasks for computation resources, e.g. threads or processes belonging to the runtime system or the operating system. STD uses the AIMD algorithm illustrated in Fig. 2 to estimate future computation resources in a node.

Periodically, STD collects current CPU utilization statistics from a node and calculates the maximum CPU cost of running a task. STD embodies the maximum CPU cost of running a task with the maximum increment, i.e., from the CPU utilization statistics before running a task to the CPU utilization statistics after running a task. STD uses the maximum CPU cost observed in the past to dynamically estimate future CPU utilization of the node. If current CPU utilization is smaller  than future CPU utilization of a node, STD increases future CPU utilization of the node by the maximum CPU cost of running a task and approves of the assignment of a new task to the node. If current CPU utilization is not smaller than future CPU utilization of a node, STD decreases future CPU utilization of the node by a half but does not give the node a new task. Finally, STD corrects future CPU utilization of the node to avoid overestimating and underestimating it, i.e. the statements of limiting it to the value between 1% and 90% in Fig. 2.

Fig. 2, AIMD in STD

IV. EXPERIMENTS  A. Implementation We implement a general runtime system with the PHP  language [9] to serve MapReduce applications. We program the general runtime system to have the basic functions as the existing runtime systems, i.e. reading input files from a master, running a Mapper on a node, running a Reducer on a node, providing a Mapper with input data, forwarding intermediate data from Mappers to the corresponding Reducers according to a key-length-based hash function, and collecting outputs from all Reducers as the result of the application. Accordingly, we provide Mappers of an application with input files stored in disks of a master at the beginning of the application execution, and then collect outputs from all Reducers to save them in disks of a master at the end of the application execution.

We implement three algorithms in the master of the general runtime system. We faithfully follow the proposal in this paper to implement STD capable of distributing tasks over nodes according to the AIMD algorithm. Next, we implement GTD capable of finding the node that has the most available slots.

We implement IDLEST capable of finding the node that has the most computation resources (i.e. the idlest node). For meeting the requirement of STD and IDLEST, we implement a process in each node to periodically broadcast its CPU utilization statistics over networks, so a master can receive CPU utilization statistics from all nodes in clusters. Currently, we configure the broadcast interval of CPU utilization statistics to 2 seconds.

B. Configuration We construct a cluster with 9 identical computers that have  an AMD Phenom II X6 1055T CPU, 4 GB RAM, and a 2 TB 7200 rpm HD for each of them. We install Apache 2.2.17, PHP 5.3.8, and Windows 7 on each of the 9 computers and connect them to each other with Gigabit Ethernet. Among the 9 computers, we select a computer to run a master of the runtime system, 4 computers to run Mappers, and 4 computers to run Reducers. We prepare a 512 MB input file holding random data in a disk of the master and upload data to Mappers on demand at runtime. We configure each Mapper to process 8 MB data each time. We observe performances of GTD, IDLEST, and STD when configuring different slot numbers in the runtime system at the computers responsible for running Mappers and Reducers.

C. Word Count Performance  First, Word Count [1] is taken to evaluate performances of GTD, IDLEST, and STD because it is the typical application widely used to test a MapReduce system. Word Count uses a Mapper to emit a string ?word, 1? for each word found in input data and uses a Reducer to merge intermediate data. Finally, Word Count collects outputs of Reducers in nodes to get the result of the application. Word Count generates much intermediate data from Mappers to Reducers over networks.

Fig. 3, Word Count Performance in GTD, IDLEST, and STD  In Fig. 3, we observe that the three algorithms get results similar to each other when the task number per node (a.k.a. slot number per node) is configured less than 8. When the task number per node is configured larger than 8, we note that performances of GTD and IDLEST begin to degrade greatly because they make the runtime system dispatch many tasks to overload nodes according to the inappropriate configuration.

Even though IDLEST can find the idlest node among all nodes to run a task, we find that its performance degrades too because the workload of an extra task may overload the idlest node.

Conversely, we observe that STD does not continue dispatching a task to a node once it predicts that the addition of the extra task may overload a node. As a result, we observe that the performance of STD does not degrade greatly according to the inappropriate configuration of task number per node.

Fig. 4, Word Count Standard Deviation in GTD, IDLEST, and STD  The standard deviation [10] in GTD, IDLEST, and STD is observed because the appearance of various standard deviations between Mapper and Reducer numbers implies the appearance of strugglers [11] in a MapReduce system and because strugglers have negative impacts on performances. In Fig. 4, the standard deviation in GTD is not uniform because many Reducers are created when the task number per node is configured less than 8 and because many Mappers are created when the task number per node is configured larger than 8 in comparison to IDLEST and STD. Conversely, the standard deviation in IDLEST is much less than in GTD according to the summation of Mapper and Reducer numbers, although Reducers are still much more than Mappers. Nevertheless, the standard deviation in STD is the least among the three ways, which corresponds to the observations in Fig. 3.

D. Quick Sort Performance   Fig. 5, Quick Sort Performance in GTD, IDLEST, and STD     Quick Sort [12] uses a ?divide and conquer? policy to sort numbers. First, Quick Sort uses a Mapper to put each number into the corresponding intermediate file according to its digital length. Second, Quick Sort in a Reducer sorts numbers in intermediate data belonging to its responsibility with a recursion-based algorithm. Finally, Quick Sort merges outputs from all Reducers as the result of the application. Quick Sort costs much CPU time in Reducers because they are responsible for sorting numbers, while Mappers merely classify and put numbers into different intermediate files.

In Fig. 5, we observe that Quick Sort can get the best performance with the configuration of 8 task number per node for the three algorithms. However, we note that GTD and IDLEST both have performances to degrade suddenly when task number per node is increased, because GTD and IDLEST make the runtime system continue dispatching tasks to nodes and overload the nodes. Although IDLEST can find the idlest node among all nodes to run a new task, we observe that it still can not avoid overloading a node to degrade the performance.

Conversely, we witness that STD seldom degrades its performance even though task number per node is inappropriately configured (e.g. 16 tasks per node), because STD can predict the workload of a node with the addition of an extra task to smartly determine whether the task should be dispatched to the node or not.

Fig. 6, Quick Sort Standard Deviation in GTD, IDLEST, and STD  According to Fig. 6, the standard deviation in GTD not only has the highest value among the three algorithms but also has the most variances among the three algorithms. Specifically, the standard deviation in GTD indicates that the runtime system creates more Mappers than Reducers when task number per node is larger than 8. Similarly, the standard deviation in IDLEST has a high value and shows a great difference between the number of Mappers and the number of Reducers when task number per node is larger than 8. Conversely, the standard  deviation in STD has a lower value and makes the runtime system create nearly identical numbers of Mappers and Reducers in each configuration of task number per node. The standard deviation in Fig. 6 clearly shows why STD makes Quick Sort have better performances than GTD and IDLEST in Fig. 5.



V. CONCLUSIONS In this paper, Smart Task Distributor (STD) is proposed to  solve the problem of the computation resource underutilization or overutilization in clusters on cloud computing. STD is designed as a module to work on a master node for helping a MapReduce runtime system smartly distribute tasks over nodes in clusters. STD can dispatch tasks to nodes in clusters according to the Additive-Increase/Multiplicative-Decrease (AIMD) algorithm. STD can gradually assign tasks to a node in order to fully utilize computation resources in a node. STD can dynamically estimate the remaining computation resources in a node and toggle the assignment of tasks on demand without overloading the node. Eventually, STD can prevent nodes from working in the underutilization or overutilization condition. In experiments, STD is tested with several canonical applications in nodes having different slot numbers. Besides, STD is compared with a General Task Distributor (GTD) that assigns a task to the node having the most available slots, and an algorithm (IDLEST) that assigns a task to the node having the most computation resources (i.e. the idlest node). According to experiments, STD greatly outperforms GTD and IDLEST when slot numbers of nodes are inappropriately configured.

