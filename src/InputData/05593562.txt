A new algorithm of association rules mining

Abstract?To reduce the number of candidate itemsets and the times of scanning database, and to fast generate candidate itemsets and compute support, this paper proposes an algorithm of association rules mining based on attribute vector, which is suitable for mining any frequent itemsets. The algorithm generates candidate itemsets by computing nonvoid proper subset of attributes items, it uses ascending value and descending value to compute nonvoid proper subset of the weights of attributes items, the method may be used to reduce the number of candidate itemsets to improve efficiency of generating candidate itemsets. And the algorithm gains support by computing attribute vector module, the method may be used to reduce the time of scanning database, and so the algorithm only need scan once database to search all frequent itemsets. The experiment indicates that the efficiency of the algorithm is faster and more efficient than presented algorithms of congener association rules mining.

Index Terms?data mining, association rules, attribute vector, proper subset, attributes items weights

I.  INTRODUCTION Apriori mainly wants to solve two key problems: one is  how to reduce the number of candidate itemsets and the times of scanning database, the other is how to fast generate candidate itemsets and compute support. Aiming to the first problem, people present some algorithms, DMFIA as in [1] and IDMFIA as in [2]. And then, to solve the second problem, some algorithms based on binary were presented, B-Apriori as in [3], B_ARDSM as in [4] and B_UDMA as in [5]. These algorithms improved the method of generating candidate itemsets and computing support by binary logic operation to indeed improve efficiency. However, they need repeated scan database when computing support. These algorithms based on binary are inefficient because they still use traditional way of generating candidate itemsets and computing support.

Therefore, this paper proposes an algorithm of association rules mining based on attribute vector, denoted by ARMBAV, which is suitable for mining any long frequent itemsets.



II. BASE NOTIONS AND PROPERTIES Let I= {i1, i2?im} be a set of attribute items, suppose each  ik has been changed into Boolean quantity from multiattribute.

Definition 1 Binary Transaction (BT), a transaction is expressed as binary, binary transaction of transaction T is expressed as BT= (b1 b2?bm), bk?  [0, 1], k=1?m, if ik?T, and then bk =1, otherwise bk =0.

Example, let I={1,2,3,4,5,6} be a set of attribute items, if T={2,5,6}, and then BT=(010011)2.

Definition 2 Transaction Weights (TW), it is an integer, its value is equal to decimal integer of binary transaction.

Example, if BT = (010101)2, and then TW (BT) =21.

Definition 3 Relation of transaction weights accords with relation of transaction set.

Example, suppose transaction weights of a transaction T1 is denoted by TW1, transaction weights of a transaction T2 is denoted by TW2. If T1 ? T2, and TW1 ? TW2, namely, TW1 & TW2 = TW1. TW1 is regarded as subset of TW2, which is regarded as superset of TW1.

Definition 4 Attribute Vector (AV), it is constituted by the m-order integer, m is called vector dimension, and each integer is called vectorial component.

Example, 3-AV= {23, 45, 33}, here 23, 45 and 33, they are also called vectorial component.

Definition 5 Vectorial Component Module (VCM), it is an integer, which is equal to the number of ?1? contained by binary code of vectorial component.

Example, here, 23 is one of vectorial component of above 3-AV, VCM (23) = VCM (010111) = 4.

Definition 6 Attribute Vector Module (AVM), it is an integer, which is equal to the summation of vectorial component module.

Example, 3-AV= {23, 45, 33}, AVM (3-AV) = VCM (23) + VCM (45) + VCM (33) = 4 + 4 + 2 = 10.

Definition 7 Attribute Vectors Cross Product (AVCP), the result of operation is a vector. Each component is the value that components corresponding to all Attribute Vectors will operate ?and? logical operation.

Example, if here are two Attribute Vectors, 3-AV1= {23, 45, 33}, 3-AV2= {20, 36, 35}, and AVCP (3-AV1, 3-AV2) is equal to the result: {23&20, 45&36, 33& 35} = {20, 36, 33}.

Property 1 Let p and q be binary transactions with m bits, let Tp be a transaction about p, let Tq be a transaction about q, and p & q=p, let F be frequent itemsets.

If Tq ? F, then Tp ? F.

If Tp ? F, then Tq ? F.

Computer Science & Education Hefei, China. August 24?27, 2010   ThP1.8

III. ASSOCIATION RULES MINING ALGORITHM BASED ON ATTRIBUTE VECTOR  Suppose there are N transactions in database, which contains m attributes item, and these items are Boolean variable. There are n different transactions in database.

A. The method of generating candidate itemsets We let I = {i1, i2?im} be a set of attribute items, If we let I  be a transaction, and TW (I) = 2m-1, this value is also denoted by the weights of attributes items. According to this knowledge as in [6], the algorithm computes nonvoid proper subset of the weights of attributes items to generate candidate itemsets by ascending value and descending value, this course is expressed as follows:  Min= 1, Max= 2m-2.

The course of generating candidate itemsets by ascending value is expressed as follows:  VMin=1, 2, 3...

The course of generating candidate itemsets by descending value is expressed as follows:  VMax= 2m-2, 2m-3, 2m-4?  Example let I= {a, b, c, d}, and then TW (I) = 24-1=15, namely, 15 is the weights of attributes items. Let NPS be a nonvoid proper subset, and all NPS of the weights of attributes items is expressed as follows:  Min=1, Max=14.

NPS1=1, it denote candidate itemsets {d}.

NPS2=14, it denote candidate itemsets {a, b, c}.

NPS3=2, it denote candidate itemsets {c}.

NPS4=13, it denote candidate itemsets {a, b, d}.

NPS5=3, it denote candidate itemsets {c, d}.

NPS6=12, it denote candidate itemsets {a, b}.

NPS7=4, it denote candidate itemsets {b}.

NPS8=11, it denote candidate itemsets {a, c, d}.

NPS9=5, it denote candidate itemsets {b, d}.

NPS10=10, it denote candidate itemsets {a, c}.

NPS11=6, it denote candidate itemsets {b, c}.

NPS12=9, it denote candidate itemsets {a, d}.

NPS13=7, it denote candidate itemsets {b, c, d}.

NPS14=8, it denote candidate itemsets {a}.

According to the example, we know all candidate itemsets were generated by computing nonvoid proper subset of the weights of attributes items.

B. The method of computing support According to this knowledge as in [7], the method of  computing attribute vector module is used to gain support by the algorithm, which is expressed as follows:  Step1: Turning original transaction into Binary Transaction according to definition 1.

Step2: Aiming to the column corresponding to each attribute item, its corresponding Binary code is turned into integer in fragment, and these integers will constitute Attribute Vector of attribute item according to definition 4.

Step3: Each attribute item in candidate itemsets has own Attribute Vector, and then we will use these Attribute Vectors to operation by Attribute Vectors Cross Product. Finally, a new Attribute Vector will be gained.

Step4: Computing Attribute Vector Module of new Attribute Vector according to definition 6. The result is equal to support of candidate itemsets.

For example, there are ten transactions in database. The course of computing support is expressed as follows:  Step1: Turning original transaction into Binary Transaction.

T1: {a, b, e, f}, BT1 = (110011)2;  T2: {b, c, d, e}, BT2 = (011110)2;  T3: {a, b, d, e, f}, BT3 = (110111)2;  T4: {a, b, c, d, e, f}, BT4 = (111111)2;  T5: {c, d, e, f}, BT5 = (001111)2;  T6: {a, c, d, f}, BT6 = (101101)2;  T7: {a, c, d, e, f}, BT7 = (101111)2;  T8: {a, b, c, e, f}, BT8 = (111011)2;  T9: {b, d, e, f}, BT9 = (010111)2;  T10: {a, b, c, d, f}, BT10 = (111101)2;  Setp2: Computing Attribute Vector of each attribute item in database is expressed as table I.

TABLE I.  COMPUTING ATTRIBUTE VECTOR  No. a b c d e f BT1 1 1 0 0 1 1 BT2 0 1 1 1 1 0 BT3 1 1 0 1 1 1 BT4 1 1 1 1 1 1 BT5 0 0 1 1 1 1  component 1 22 30 11 15 31 23 BT6 1 0 1 1 0 1 BT7 1 0 1 1 1 1 BT8 1 1 1 0 1 1 BT9 0 1 0 1 1 1 BT10 1 1 1 1 0 1  component 2 29 7 29 27 14 31 Attribute Vectors  {22, 29}  {30, 7}  {11, 29}  {15, 27}  {31, 14}  {23, 31}  Step3-step4: Let candidate = {b, e, f} be a candidate itemsets, and then we use these three Attribute Vector of attribute item to compute support by Attribute Vectors Cross Product, denoted by Support (candidate):  AV ({b}) = {30, 7};  AV ({e}) = {31, 14};   ThP1.8      AV ({f}) = {23, 31};  AVCP ({b}, {e}, {f}}) = {30&31&23, 7&14&31}  = {22, 6};  Support (candidate) = AVM (AVCP ({b}, {e}, {f}}))  = AVM ({22, 6})  =VCM (22) +VCM (6)  = 3 + 2 = 5.

C. The program of generating frequent itemsets Let I= {i1, i2?im} be a set of attribute items, we define  some signs expressed as follows:  D: saving all Binary Transactions.

F1: saving frequent itemsets by ascending value.

F2: saving frequent itemsets by descending value.

NF: saving non frequent itemsets by ascending value.

(1) Min =1, Max=2m-2;  (2) While (Min<Max) {  (3)  If ((Min ? F1) && (NF ? Min) {  (4)   If (Support (Min)>=minimal support)  (5)     Write Min to F1 and delete its subset in F1;  (6)   Else  (7)     Write Min to NF;  (8)  }  (9)  If ((Max ? F2) && (NF ? Max) {  (10)    If (Support (Max)>=minimal support)  (11)     Write Max to F2 and delete its subset in F1 ;}  (12)  }  (13)  Min++;  (14)  Max-- ;}  (15) Output F1 and F2;  Input: a Candidate;  Output: support of Candidate;  Support (int Candidate)  (1)  int i = 0, j = 0, Support = 0, exit = 0;  (2)  int [] Set = new int[Row];  (3)  i = 0;  (4)  While ((i < Column) && (Exit == 0)) {  (5)     If ((Candidate & 1) == 1) {  (6)        Exit = 1;  (7)        For (j = 0; j < Row; j++) {  (8)         Set[j] = D [j, i] ;}  (9)     }  (10)     Else {  (11)         Candidate = Candidate >> 1;  (12)         i++;}  (13)   }  (14)   j = i;  (15)   While ((j < Column) && (Candidate! = 0)) {  (16)       If ((Candidate & 1) == 1) {  (17)         For (i = 0; i < Row; i++) {  (18)           Set[i] = Set[i] & D [i, j] ;}  (19)       }  (20)       Candidate = Candidate >> 1;  (21)       j++;}  (22)   For (i = 0; i < Row; i++) {  (23)     Support = Support + VCM (Set[i]) ;}  (24)  Return Support;

IV. COMPARING THE CAPABILITY OF ALGORITHMS Here we use B-Apriori as in [3] and B_ARDSM as in [4] to  compare with ARMBAV presented by this paper.

A. Analyzing the Capability of Algorithms B-Apriori: The algorithm generates candidate itemsets  which contain the number of items from fewness to many.

However, the algorithm use traditional method to compute support of candidate, namely, it need scan once database when computing once support, and so there are many times of scanning database in the course of mining association rules, the efficiency of algorithm would be affected because of this.

B_ARDSM: Generating candidate frequent digital transaction DTL of which is from maximum and minimum to middle. The algorithm is suitable for mining general database where frequent itemsets aren?t confined, but the method of computing support is the same as B-Apriori, the algorithm scans once database when computing support. And so it need scan all these transaction several times, and so the efficiency of algorithm still isn?t fast and efficient.

ARMBAV: The algorithm is suitable for mining any frequent itemsets. It generates candidate itemsets by computing nonvoid proper subset of attributes items, it uses ascending value and descending value to compute nonvoid proper subset of the weights of attributes items, the method is used to reduce the number of candidate itemsets to improve efficiency of generating candidate itemsets. The algorithm gains support by computing attribute vector module, the method is used to reduce the time of scanning database, and so the algorithm only need scan once database to search all frequent itemsets.

B. Comparing the experiment of algorithms Now we use result of experiment to testify above analyses  and conclusion. Above three algorithms are used to generate   ThP1.8      frequent itemsets from these transaction weights, which are from 3 to 8191, these transactions don?t include any single items, and so the number of attributes is expressed as m=13, the sum of transactions is expressed as n=8178.

Our experimental circumstances are expressed as follow: Intel(R) Celeron(R) M CPU 420  1.60 GHz, 1.24G, language of the procedure is Visual C# 2005.NET, OS is Windows XP Professional.

The experimental result is expressed as Fig. 1 and 2, where support is absolute value. The runtime of algorithms B-Apriori and ARMBAV is expressed as Fig. 3 as support and length of itemsets change. The runtime of algorithms B_ARDSM and ARMBAV is expressed as Fig. 4 as support and length of itemsets change.

Figure 1.  The experimental result   Figure 2.  The experimental result   Figure 3.  Runtime of algorithms B-Apriori and ARMBAV   Figure 4.  Runtime of algorithms B_ARDSM and ARMBAV

V. CONCLUSION This paper proposes an algorithm of association rules  mining based on attribute vector, which is suitable for mining any long frequent itemsets. The experiment indicates its efficiency is faster and more efficient than presented congener algorithms.

