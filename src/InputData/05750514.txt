Optimization of Asociation Rules with Genetic Algorithms

Abstract?As databases grow in size, the interpretation of data within them, is increasingly difficult to obtain, and often only stored as backup of various transactions of records. In these data there is new information which is present, but requires a higher level of processing. The outcome of this processing are association rules which are used to detect events that occur together in a set of data. To find these rules, a very popular algorithm in Data Mining is Apriori, which aims to reduce the search space and increase efficiency. In this paper, we will introduce new metrics like interestingness and completeness with derived applications from Artificial Intelligence, ?Genetic Algorithms?, in order to reduce the search space and execution time.

Keywords-data mining; asociation rules;artificial intelligence.



I. INTRODUCTION ?According to investigators two basic goals of Data  Mining [3] are prediction and description. Prediction uses variables in the database to predict unknown features of interest. Description focuses on the discovery of patterns that describe data and subsequent submission to the interpretation of the user. There are several techniques that describe these objectives. Some of them can be classified into the following categories: classification, clustering, association rules mining, analysis and discovery of sequential patterns.? [5].

This research will consider the Mining Association Rules for Knowledge Discovery in conjunction with genetic algo- rithms.

A. Rules of Association Since its introduction in 1993 the task of association rule  mining has received a great deal of attention. Today the mining of these rules is one of the most popular pattern discovery methods in KDD.

An association rule is an expression X=?Y, where X and Y are sets of items. The meaning of these rules is quite intuitive: Given a database D of transactions, where each transaction T ? D includes a set of items, X =? Y expresses that whenever a transaction T contains X then T probably contains Y also. The rule confidence is defined as the percentage of transactions containing X plus Y in  relation to the total number of transactions that contain X.

That is, the confidence can be understood as the rule of conditional probability p(Y ? T / X ? T).

The support of an association rule is the percentage / fraction of records containing X and Y on total number of records in the database. Support (s) = support (X =? Y) = P(A ? B).

The idea of mining association rules originates from the analysis of data from a market basket. A person buying goods x1 and x2 could also buy another product with probability c%. Direct application to business problems, along with inherent intelligibility, even for non-experts in data mining make the association rule mining a popular method.

The main motivation for using genetic algorithms (fur- thermore referred to as GAs) in the discovery of high- level prediction rules is that they perform a global search and cope better with attribute interaction than the greedy rule induction algorithms often used in data mining. The improvements associated to the use of GAs, as we will see in this paper, definitely help the rule based systems used for classification as described in results and conclusions.

B. The Problem  Obtaining mining association rules yields two main problems which need to be dealt with: First, the algorithmic complexity. The number of rules increases exponentially with the number of items present in the database.

Fortunately, today?s algorithms are able to prune efficiently this vast search space based on thresholds for quality measures of the rules [4]. Second, rules of interest should be collected from within the set of generated rules. This approach also implies a high algorithmic cost because the number of rules which are generated is very large, i.e., typically over 100,000 rules. However, the percentage of usable rules is frequently only a small fraction [4].

DOI 10.1109/SCCC.2010.32     Most of the research in the area focuses on reducing computation times, but also takes into account the quality of the generated rules. In this proposal, which will also cover both issues, the main idea will be to implement the algorithm used in Data Mining, Apriori, but in a more limited form to reduce the cost of generating itemsets, and furthermore apply GAs to optimize the quality of rules based on the itemsets obtained by our version of Apriori.



II. GENETIC ALGORITHMS  As discussed in [6], in general the main motivation for using GAs in the discovery of high-level prediction rules is that they perform a global search and cope better with at- tribute interaction than the greedy rule induction algorithms often used in data mining. This section of the paper discusses several aspects of GAs for rule discovery. The main areas of discussion include individual representation of rules, genetic operators involved and the choice of Fitness function.

A. Individual Representation  The first thing to consider is that an association rule is an implication of the type X ? Y, where all the attributes represented by X, are called antecedent and attributes represented by Y is called consequent. Therefore, an association rule can be represented by n attributes in the antecedent and m attributes in the consequent.

To represent an individual, we will use the Java ArrayList structure, which allows us to store a variable number of attributes. In consequence, it is never necessary to know how many attributes the rule has.

Rule class is defined, which contains: ? <String> ArrayList attributes: Contains all the  attributes of an association rule .

? <String> ArrayList values: Contains all allocation values of the rule, in which ?00? denotes presence of the attribute in the antecedent of the rule and ?11? denotes presence of the attribute in the consequent of the rule.

Considering the above, an association rule will be repre- sented as it is shown in Figure 1.

B. Genetic Operators  1) Crossover: Based on the classic genetic operators, we propose a new implementation, in order to obtain a greater variety of rules, given the representation of the rule:  ? Select two random elements from the rule container.

Figure 1. Chromosome Representation  ? Of those elements, some number of attributes of the antecedent and of the consequent are randomly selected for crossover.

? Perform the exchange of the selected attributes, maintaining the values of the attributes that were not selected in the previous stage.

For example, consider the following rules:  ? R1 :AB?CD ? R2 :EFG?HIJ  Applying the steps described in the previous stage we have:  ? In R1, two items from the antecedent and one item from the consequent will be randomly selected .

? From R2, one antecedent item and two consequent items will be interchanged.

? Then, we randomly select elements of the antecedent and consequent items, for example, AB for the antecedent and D for the consequent in R1 and FH for the antecedent and J for the consequent in R2.

Crossover is applied between both rules and the two new rules are spawned:  ? R ?  1 : F?CHJ ? R  ?  2 : EGAB?ID  The new operator is obtained by randomizing the selection of the sections to interchange and further enriching the classical crossover genetic operator. It adds variability on the size of the rule. By using a crossover operator as defined above, it can be noted that unlike A-Priori, which generates the k-itemsets - a process that can take a long time-, M-GARM may obtain rules with a high number of attributes with a single application of the crossover operator.

Note that validations are considered for the new rules, to avoid falling into logical inconsistencies such as redundancy attribute, empty rules, etc.

2) Mutation: For the mutation, a new strategy was also implemented. It is defined as follows:  ? Select a random element from the rule container.

? Randomly select what it mutates: antecedent or consequent. Select and delete the corresponding attribute. If the resulting rule is empty in the antecedent or the consequent, then generate a new random rule in its place.

? Update rule container, inserting the mutated rule.

Since the probability of mutation is low, mutated rules automatically decrease the number of attributes. When this occurs, the quality of the rule will probably diminish greatly, therefore, it is likely that at the time of selection such rule is discarded.

3) Evaluation: For evaluation of the rules we consider two new metrics: the factor of interest (INF) and completeness factor (CF).

Figure 2 shows the confusion matrix which defines the performance of an association rule.

Figure 2. Confusion Matrix  The abbreviation of the labels used in the confusion matrix have the following meaning:  ? TP: True Positive ? Number of attributes satisfying antecedent and consequent.

? FP: False Positive ? Number of attributes satisfying antecedent but no consequent.

? FN: False Negative? Number of attributes satisfying consequent but not antecedent.

? TN: True Negative ? Number of attributes not satisfying antecedent nor consequent.

In our implementation, we used some quality measures based on the previous definitions:  ? Interestingness Factor (INF): TPTP?FP  ? Completeness Factor (CF): TPTP?FN  Now, based on the previously mentioned measures and [5], the fitness function is defined as:  Fitness = TP 2  (TP ? FP )(TP ? FN)  The general idea is to maximize the number of rules which comply with X ? Y. This occurs when the quality function is close to 1, yielding values of FN and FP close to 0 for the optimal case.

4) Selection: For selection, a fitness criterion is used.

Boundaries around the value of 1 are defined by the user, for example [0.8 .. 1.2] , to only maintain high-quality rules.

5) Termination Criterion: If we consider a convergence criterion for termination, the execution time might be very large, which would affect the optimization. On the other hand, a defined number of iterations will be the chosen criterion, since it is a user-defined parameter and gives mobility to the algorithm.



III. SCOPE  The proposed algorithm allows us to find Boolean association rules, using metrics of interest and completeness.

However, the generation of the random population is strongly dependent on the problem. Furthermore, this algorithm can be used only for discrete nominal cases, ongoing cases will be the subject of further work.



IV. RESULTS  A. Example Dataset  Consider a dataset from the Machine Learning Repository, University of California, Irvine (UCI).

? Title: Evaluation of Automotive.

? No attributes: 6.

? No instances: 1728  For a demonstration of its utility, the analyzed database was generated synthetically. A car evaluation database was derived from a simple hierarchical decision model originally developed for the demonstration of DEX1[2].

1An Expert System Shell for Multi-Attribute Decision Making     The model evaluates cars according to the following concept structure:  ? CAR: car acceptability.

? PRICE: overall price.

? Buying: buying price.

? Maint : price of the maintenance.

? TECH: technical characteristics.

? Doors : number of doors.

? Persons: capacity in terms of persons to carry.

? Lug boot: the size of luggage boot.

? Safety: estimated safety of the car.

? Attribute values2:  ? Price:very high(A),high(B),medium(C),low(D).

? Maintenance:very high(E),high(F),medium(G),  low(H).

? Doors: 2(I),3(J),4(K),more(L).

? Persons: 2(M),4(N),more(O).

? Lug boot: small(P),medium(Q),big(R).

? Safety: low(S), medium(T),high(U).

? Acceptability:  unacceptable(V),acceptable(W),good(X), very good(Y).

Figure 3 shows a sample of the original dataset and the modified dataset. Each line represents a transaction.

Figure 3. Original and custom dataset  B. Quality of the Association Rules  For this analysis, we show the rules obtained on an initial population of 10 individuals and 10 iterations of the algorithm. Figure 4 shows the rules that were found.

2Are coded with a letter of the alphabet to distinguish them in the implementation of the dataset.

Figure 4. Retrieved Rules

V. FUTURE WORK  Data mining is a young science, so this line of research has yet to bear much fruit. In regard to the M-GARM algorithm, further work may be done on it, some examples being:  ? Improving the algorithm, incorporating extended at- tribute analysis, and also allows attributes of different types of data.

? P-GARM, a distributed version of the algorithm M-GARM, which incorporates the division of tasks for the analysis of the dataset is also object of future work.

? Implementation of metaheuristics such as Ant Colony, Simulated Annealing, etc.

? Incorporate searching and analysis of negative association rules like X ? ? Y which is of high importance nowadays. For example, in seismology, solving the problem could determine relations like Tsunami ? ? Earthquake, considering the large amount of data related to the topic.

? Retrieve and process data from research companies, providing the ability to model datasets from their transactional databases, to obtain datasets as real as possible, with a large number of attributes and records.



VI. CONCLUSIONS  This research is based on a thorough study of mining association rules in data mining, and has resulted in the implementation of a new algorithm, M-GARM, which is proposed as an alternative to the Apriori algorithm to search for strong association rules.

From the standpoint of computational complexity, it was calculated and tested that the algorithm M-GARM is more efficient than A-Priori since it has a linear complexity relative to the number of transactions times the number of parameters analyzed in the dataset. However, the number of     generated rules, are clearly dependent on the quality criteria that are set by metrics of interest and completeness, which are calculated within the genetic algorithm at the selection phase.

Moreover, concerning the power of genetic operators, our definition of crossover and mutation work very efficiently.

Tests show that optimal settings for probabilities are of 80% for crossover and 1% for mutation.

Tests were performed on large datasets with many parameters, but the calculation of the quality function created a significant bottleneck when the size of the random population was increased. There are no major differences with A-Priori at this point.

The quality of the resulting rules, was considerably higher due to an adequate evaluation function that correctly relates attributes in addition to long associations which are achieved by the crossover operator.

A factor which also influences a correct exploitation of GAs is the randomness of the initial population.

Data mining is a discipline that is still evolving and will certainly contribute to knowledge discovery. Integration with ERP and Business Intelligence will be very helpful to narrow the search space to obtain more structured information.

