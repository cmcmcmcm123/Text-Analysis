A CLOUD PLATFORM FOR FLOW-BASED ANALYSIS OF  LARGE-SCALE NETWORK TRAFFIC

Abstract  Traditional flow-based network monitoring and analyzing tools are challenged with large-scale network traffic for the big size of data.  This paper proposes a Cloud Platform for large-scale network analysis. It exploits HDFS and DataX to deal with the storage of heterogeneous big data. And the MapReduce model is applied to concurrently compute the analyzing and correlating on large amount of network flow data. Flow-based techniques are employed in collecting network data to make possible an application level analysis of network usage. Experimental results show that the performance of the platform is greatly improved compared with that of the traditional RDBMS application.

1 Introduction  In order to provide a good quality of service, network administrators need to maintain a healthy state of the network, by measuring the IP traffic between network nodes, analyzing customer behavior and collecting statistics data about important business network bandwidth, as well as proper network planning and evaluating. They need timely and accurate flow monitoring and statistical tools on network traffic and the business hosted upon it.

Generally there are three kinds of ways to monitor the network traffic: 1) Common network monitoring tools, such as MRTG, Cacti.

These tools use the SNMP protocol to monitor port state and calculate port traffic. However, these methods can only monitor indicators such as the bytes counter of packets entering and leaving a single port. The traffic information is too rough and it can?t be understood what the traffic carries and where the traffic flows to.

2) Packet capturing software, such as PCAP or DPI (Deep Packet Inspection). By unpacking and investigate the details of each packet, these analytical methods can learn more about the network traffic data, and can even replay network traffic.

However, due to the huge amount of data, the unpacking analysis method is costly. And it often creates new points of failure to the network. Therefore, it is difficult to deploy.

3) Flow-based analysis software, such as CISCO NetFlow.

By extracting information from the flow, it can help analyze the network application ? which is made up of flows -- in the network traffic. Those flows, if enriched by information such as associated radius log and DNS log data, can also be exploited to analyze the customer behavior. Flow-based  measurement approach strikes a balance on the granularity and the efficiency of traffic monitoring. Moreover, it has the advantages of investment saving and fast deployment.

Although flow-based network monitoring has many advantages over other monitoring methods, analysis and processing of the data is still a problem, mainly due to the extremely large amount of traffic data when facing with large-scale network traffic monitoring. Especially when a long term traffic data should be analyzed, the analysis task is hardly to be done with the traditional data storage and RDBMS technology.

This paper proposes a Cloud network analyzing platform to deal with ?flow? of data analysis and processing. It uses general-purpose and low-cost hardware and software to build a cluster solution. This system copes well with the storage of big network flow and the analysis and statistics on big data. It can be used to analyze large-scale network traffic.

2 Why Is the Network Analyzer Flow-Based  2.1 Network Flow A network flow can be defined in many ways. Cisco?s standard NetFlow version 5 defines a flow as a unidirectional sequence of packets that share all of the following 7 values: Ingress interface (SNMP ifIndex) Source IP address Destination IP address IP protocol Source port for UDP or TCP, 0 for other protocols Destination port for UDP or TCP, type and code for ICMP, or 0 for other protocols IP Type of Service The above definition of flows is also used for IPv6, and a similar one is used for MPLS and Ethernet flows.

NetFlow is a network protocol developed by Cisco Systems for IP traffic information collection. NetFlow has become an industry standard for traffic monitoring and is supported on various platforms. Advanced NetFlow or IPFIX implementations like Cisco Flexible NetFlow also allow user-defined flow keys.

NetFlow was initially implemented by Cisco, and described in an ?informational? document that was not on the standards track: RFC 3954 ? Cisco Systems NetFlow Services Export Version 9. The NetFlow protocol itself has been superseded by Internet Protocol Flow Information eXport (IPFIX). Based on the NetFlow Version 9 implementation, IPFIX is on the IETF standards track with RFC 5101, RFC 5102, etc., which were published in 2008.

Many vendors other than Cisco provide a NetFlow equivalent technology on their routers and switches, among them some  259 ICSSC 2013    typical examples are listed in table 1.

Table1. Some Typical NetFlow Equivalent Technology.

Protocol vendors Jflow or cflowd Juniper Networks  NetStream Huawei Technologies Cflowd Alcatel-Lucent  AppFlow Citrix  2.2 Data Enrichment The NetFlow data should be enriched with various data sources to deal with various network problems.

Multiple data sources provide valuable enrichment for the whole picture of what?s occurring on a network. A centralized platform shall be provided to quickly and easily search, analyze, correlate and report across multiple data types. Here are the examples: Enriching NetFlow data with RADIUS Logs?RADIUS Logs is helpful to ensure the authenticity of network stakeholders.

Since IP address assignments can change after each connection, malicious activity that occurs one day may be difficult to be traced back into the affected systems once the assignments changed. Attaching RADIUS logs along with NetFlow and other sources would enable this kind of tracing.

Enriching NetFlow Data with DNS Logs?DNS logs can be used to detect abnormalities and unusual instances of DNS.

For example, traffic communicating directly with an IP address with no corresponding DNS lookup would be suspicious and need further investigation.

At least two fields would be enriched to the flow data, i.e., the user identity and the domain name of the site.

2.3 Advantages of Flow-Based Analysis The distributive feature of modern enterprises raises a series of operative and infrastructural problems to network administrators. Network administrators are often required to diagnose network problems quickly. But they are always lack of the global visibility to find the root cause. To deal with these problems, IT administrators need to clearly observe how traffic traverses. They have to monitor and record activities in order to understand how the traffic interacts among network, applications and users. Only with effective network analyzer, it would be possible to have a complete picture of network traffic.

Using the flow-based traffic analysis tools, the health state of the network can be easily and efficiently monitored: 1) Support analyzing network flows by customized strategies.

The analysis can help identify top talkers and dialogue, and then recognize which users and applications have utilized the maximum bandwidth, and so on.

2) Understand the traffic trends and usage patterns. By viewing the trend of network traffic, the top-level application and peak usage time is able to be determined.

3) Mark specific applications as special flows. A combination of ports and protocols is used to mark applications as exceptions. That means these ?marked? applications shall not be restricted so that the traffic they?ve caused in the flow statement can be specifically highlighted out.

4) Detect the bandwidth usage of specific user groups, e.g., the utilization rate and the bandwidth or applications assigned  to each organization (regions, countries, or provinces) defined by IP addresses.

5) Perform user behavior analysis, such as network traffic characteristics of specific groups of users, or the access pattern to specific network application from certain user group.

6) Improve the accuracy of resource calculation by the use of real-time bandwidth and network usage statistics.

3 Cloud Platform for the Network Analyzer  Generally for large-scale network traffic, the time window for the analysis cannot be too big, and so is for the correlation analysis of other data sources (such as RADIUS, DNS). The root of these problems lies in the huge amount of data. If taking into account the correlation of Radius, DNS (and others) with network flow data, the amount of data will be heavily multiplied.

Traditional flow-based monitoring and analysis system is running in the centralized high-performance computers, rather than a distributed system. A prominent problem to be encountered in this mode is: how to store the data, and how to analyze and do statistics upon them.

The SAN is the traditional data storage system. In order to improve performance, data is stored in the FC-SAN. With the continuous expansion of the data amount, such a solution will face the scalability problem. And with limited fiber-switch port number, the total capacity of this solution will eventually reach the upper limit. For data analysis, even with the performance tuning, traditional database will spent too much time dealing with TBs? data.

In this paper, a Cloud computing platform is introduced to try to solve the big data problem in flow-based monitoring and analysis system.

Figure1. Architecture of the Cloud platform for large-scale network analysis.

Figure 1 shows the architecture of the flow measurement and analysis system. Flow data from routers are delivered to Collector nodes, as well as the Radius logs and DNS logs are.

The Collector nodes will correlate the flow data with the log data, and then use the data integration tools (DataX) to load the data into a distributed file system. Processing nodes are managed by the master node to process the data.

Each Collector node is equipped with a flow collector a log     file reader and a DataX library . Each processing node is equipped with a distributed cluster file system, and a MapReduce library and a Hive library. A flow collector receives flow packets, correlates with the log data, then stores them into local files system. The DataX then loads files from local disk to the cluster file system. NetFlow packets from routers or monitoring servers are usually sent to cluster nodes in unicast. Flow data are enriched with the log data and saved periodically into files associated with each flow-exporting router, and then uploaded to the cluster file system. The flow collector uses the NetFlow as the collecting and processing tool. DataX will load the data in the local file system into the HDFS. Mapper and Reducer (or Hive) will analyze flow data with Hadoop MapReduce library.

3.1 Distributed Computing with ?Map-Reduce? Model Hadoop is employed for the Cloud network analysis system.

It provides open MapReduce software framework and distributed file system. HDFS is suitable for handling very large files with the streaming data access pattern that is a write-once and read-any pattern. In HDFS, a name node operates management of the file system metadata and provides management and control services, while a data node supplies block storage and retrieval services. A name node at the master site will perform recovery and automatic backup of name nodes. The block size (64 MB by default) and the number of replicated blocks in HDFS could be customized, according to the fault-tolerance policy.

To meet the customized purpose of flow analysis, appropriate Mapper and Reducer programs should be implemented. The map-reduce programming model was introduced by Google in 2004, in a paper describing the architecture along with some limited aspects of their non-public implementations.

One of the primary advantages of programming in the map-reduce model is that it abstracts away, and even eliminates ? to some degree -- the complexity of writing a concurrent program. The programmer implements two functions: map and reduce. The framework takes care of invoking these functions on the input data and scheduling parallel execution of them across any number of computation nodes.

3.2 Data Integration with Datax Input and output data for the network analyzer are data of heterogeneous. They may be a text file, a binary file or some data from RDBMS. The Cloud platform uses a high performance big data integration tool ?DataX? to access and integrate different data.

Figure2. Integration of heterogeneous big data with DataX.

DataX has some specific features: 1) High-speed switching between heterogeneous database / file system data.

2) Using Framework + plugin architecture to build a Framework that deals with buffering, flow controlling, concurrency, and loading the context of high-speed data, exchanging most of the technical problems, providing a simple interaction interface with widget.

3) Run mode: Stand-alone, data transfer process is completed within a single process, the entire memory operation does not read and write disk, nor the IPC.

4) Open framework, developers can develop a new plug-in to quickly support new database / file system.

In our experimental platform, the corresponding data file reader plug-in has been developed to load data into HDFS.

3.3 Flow Analyzing A network flow analysis requirement can typically be expressed as a query consisting of one of more of the following: A filter expression defining what flow records should be included in the processing.

A set Q = {Q1,...,Qm} of aggregation queries, each defining a set of flow record fields to aggregate on.

A sort condition to indicate how the end result should be sorted, such as sorting by the total number of bytes.

A limit statement to restrict the number of returned rows, typically indicating the lines that he or she is mostly interested.

To complete the query, the MapReduce programming model is exploited. In this model, the computation takes a set of input key/value pairs, and produces a set of output key/value pairs. Map and Reduce are two basic functions in the MapReduce computation. A user writes the Map method that takes an input pair and produces intermediate key/value pairs.

The Hadoop MapReduce library will group the intermediate values according to the keys. Also, a user writes the Reduce method to merge the intermediate values for smaller values.

To implement various data analysis programs with MapReduce, appropriate input key/value pairs for each analysis program have to be defined. For example, to analyze the behavior of a message sender, the user id and the octets counter can be defined as the key-value pairs.

With MapReduce, typical flow analysis functions provided by well-known Internet traffic statistics programs can be realized without too much effort. Figure 3 shows the procedure of the     MapReduce-based program that performs mail-transform analysis of flow data.

Flow data(HDFS) timestamp,srcip,dstip,srcport,dstport,ifindex,TOS,pr  otocal,userid,domain_name  Read each line Parse the  userid,octets Filter(dstport=80)  Add result set  userA,64 userB,64  userA,256 userC,128  Intermediate pairs  Read the tempdata  userA,[64,256] userB,[64]  userC,[128] reduce  userA,320 userB,64  userC,128  Reducer   Figure3. A MapReduce program for mail analysis.

1) Input flow files: After flow data from flow probes is stored on the local disk, the raw NetFlow v5 files are moved to the HDFS. As the current Hadoop mapper supports only text files as the input format, NetFlow files are converted to text-format ones. As the size of text-format flow files is much larger than that of binary-format ones, binary flow files have to be supported as the inputs for Mapper. Otherwise, the gzip-compressed text flow files could be used for the input format.

2) Mapper: The flow mapper reads each flow record that is split by new lines. A flow record has attributes of timestamp, IP addresses, port, protocol, flag, octet count, packet count, and interface numbers. After reading a flow record, we filter out necessary flow attributes for a flow analysis job. As shown in Fig. 3, when the flow analysis job sums up octet counts per user_id, we set key/value pairs as (user_id, octets).

The flow map task will write its temporary results onto the local disk.

3) Reducer: The flow reducer will be called with the inputs as intermediate values generated by flow mappers. As in the mail transfer example, a value list of octets belonging to the user_id will be summed up. After merging octet values associated with the user, the flow reducer writes the octet value for each user_id.

4 Experimental Results  In order to evaluate the performance of flow analysis with MapReduce, a small Hadoop testbed including a master node and four data nodes is built up. Each node has quad-core 2.83 GHz CPU, 4 GB memory and 1.5 TB hard disk. HDFS is used as the distributed file system. All Hadoop nodes are connected with Gigabit Ethernet cards.

Flow-tools are used to collect NetFlow v5 packets sent by a router. Then, exported flows are saved into a file every five minutes. Flow data on a network consisting of 100 interfaces was captured for months, and a few TB of traffic data was collected to test the performance of the program.

The performance of traditional database applications was compared with that of MapReduce program. The experiments showed that the MapReduce program performance has been greatly improved when the data amount changed from a few  GB to a few TB.

Additionally, we test the hive framework. Hive provides a query language, HiveQL, that is similar to SQL and more general than the Pig query language. All data sets processed by Hive are exposed as tables. These tables can be processed by selection, insertion, joining or other operations which are often associated with relational algebra. For example, a Hive query can be written like this way: SELECT flows.src_addr, SUM (flows.packets), SUM (flows.bytes) FROM flows GROUP BY flows.src_addr; Similarly to SQL, Hive supports grouping of records and calculating sums and record counts for the aggregated data, a feature that has been utilized in the example above. The listing above also illustrates the similarity between SQL and HiveQL, and shows that HiveQL can be considerably less verbose than Pig Latin. Although it is not shown in the example, Hive also allows filtering, sorting and limiting of the result set.

5 Conclusions  This paper proposes a Cloud Platform for large-scale network analysis. It exploits HDFS and DataX to deal with the storage of heterogeneous big data. And the MapReduce model is applied to concurrently compute the analyzing and correlating on large amount of network flow data. Experimental results shows that the performance of the platform is greatly improved compared with the traditional RDBMS application.

