Measures of Class Membership in Association Rule based Classification

Abstract  In this work, we focus on the measures of class member- ship defined for classifiers based on associative classification rules. We revisit the ?2 test for defining an effective measure.

For comparison, we adapt the weight of evidence (Wang and Wong TKDE 2003) for a system based on the notions of support and confidence. Some variants of those measures are also defined. The effect of the defined measures is verified through the experimentation on categorical UCI datasets.

1. Introduction  The association rule based classification has received extensive attention in data mining and machine learning research [1], [2], [3], [4], [5], [6], [7]. The approach consists of two main steps: (i) Extraction of a set of class-association rules, called a classifier, from the learning data, and (ii) Selecting significant rules in this set for classifying unseen data. In general, the high quality rules are defined on the notions of support and confidence. The support of a rule shows how frequent the rule classifies correctly the learning data, and the confidence represents the precision of the rule.

Further heuristics to enhance the quality of selected rules based on statistical tools, such as the chi-square test [2], the correlation coefficient [6], have been proposed.

Instead of basing on support and confidence, another approach to select interesting rules is to base on residual analysis in statistics [7]. In this approach, a pattern is consid- ered significant if its frequency of occurrence is significantly different from its expected occurrences, using a test based on the notion of standardized residual. For classifying an unseen object, the effect of each class label on the object is considered in a similar manner, using a measure called the weight of evidence. This is a competitive measure in classification.

In this work, we focus in the measures to quantify the effect of a class label on an unseen object, using the rules in the classifier built on small size key itemsets. We revisit the ?2 test to define an effective measure. We adapt the weight of evidence in the context of classification based on the notions of support and confidence. To improve those  measures, some variants are defined. The effect of those measures and of the variants is verified and compared with the classical measure defined on the notion of confidence, through the experimentation on categorical UCI datasets [8].

2. Preliminaries  2.1. Dataset and Association Rule  A dataset is a set of objects (or transactions). Each object is represented by an identifier and a list of valued attributes; each valued attribute is called an item. Let I be the set of all items of a dataset D. A subset of I is called an itemset.

An itemset consisting of k items is called a k-itemset. Class labels are special items for defining the class of objects.

The support of an itemset I with respect to a dataset D, denoted by sup(I), is the number of the objects in D that have all the items of I . In practice, one is interested in itemsets with support larger than some threshold minsup.

These itemsets are called frequent itemsets.

An association rule (AR) is an expression of the form X ? Y , where X and Y are itemsets and X ? Y = ?.

2.2. Class-association Rule  A class-association rule (CAR) is an expression of the form X ? C, where X is an itemset and C a class label.

Let r be a CAR X ? C. An object O is covered by r (or O satisfies r) if O has all the items of X . An object O is correctly classified by r if O satisfies r and C is actually the class label of O. The support of r with respect to a dataset D, denoted by sup(r), is the number of the objects in D that are correctly classified by r. The confidence of r is defined by conf(r) = sup(r)/sup(X). A rule with confidence 1 is called an exact rule.

Example 1: For the training dataset represented in Table 1, a ? C is a CAR with support 2 and confidence 0.67, because sup(aC) = 2 and sup(a) = 3.

Over CARs, the precedence order [3] is a partial order defined as follows. Given two CARs r and r?, r ? r? (r precedes r?) if  ? conf(r?) < conf(r), or   DOI 10.1109/WAINA.2009.81     Table 1. A training dataset  Oid Itemset Class label 1 acd C 2 abe C 3 abd C?  ? conf(r) = conf(r?) and sup(r?) < sup(r), or ? conf(r) = conf(r?) and sup(r) = sup(r?) and the  number of items in LHS(r) (the left-hand side of r) is less than the number of items in LHS(r?).

An itemset I is called a key itemset (or a minimal generator) [9] if ?I ? ? I, sup(I ?) = sup(I) implies I = I ?.

An association rule X ? Y , with X and Y being key itemsets, represents an equivalent class of rules, with respect to the confidence and support. Based on this property, classifiers can be built with only key itemsets.

3. Related Work  Using an Apriori-like algorithm [10], CBA [3] extracts class association rules (CARs) and sorts them in the prece- dence order. In this order, CARs are selected for building the classifier. A CAR is selected if it classifies correctly at least a training object. Once a CAR is selected, all the objects that are covered by the rule are discarded from the selection process. As a consequence, each training object is covered by at most one CAR of the built classifier.

CMAR [2] is developed on the ideas of CBA, but it uses FP-growth for computing CARs. In contrast to CBA, (i) CMAR selects only positively correlated CARs for the classifier, using the ?2 test, and (ii) CMAR allows each training object to be covered by several CARs.

HARMONY [5] uses the same strategy as FP-growth for computing CARs. An important difference from CBA and CMAR is that, during the CAR mining process, HARMONY maintains for each training object a top-k list of the highest confidence rules mined so far (k ? 1) that classify correctly the object. At the end of the process, those rules are grouped by class label to form the classifier.

For classifying an object O, CBA searches the classifier, in the precedence order, for the first rule that covers the object to predict the class label. If such a rule does not exist, CBA predicts the majority class label. CMAR and HARMONY follow the vote model. For each class label, HARMONY computes the score of the top-k highest confidence rules that cover O. It predicts the class label with the highest score.

The sum of the confidences is a score. CMAR defines a measure called the weighted ?2 as the score.

In HPWR [7], significant patterns (itemsets) is defined on the deviation between the observed and expected frequency of occurrences. For building classifiers, classification rules are measured in a similar manner. For a rule r = P ? C,  using the conditional probability, HPWR defines the weight of evidence of P in favor of C by  W (C/?C|P ) = logPr(C|P ) Pr(C)  ? logPr(?C|P ) Pr(?C)  (1)  = log Pr(P |C) Pr(P |?C)  (2)  The weight of evidence is positive if P provides positive evidence supporting C, otherwise, it is negative or zero.

For classifying an object O, the rules that cover O are grouped by class label. The sum of the weights of evidence is computed for each group. The object is classified with the class label that has the largest sum.

4. Class Membership Measures  4.1. Confidence  The confidence of rules is a natural measure of class membership. Each rule that covers an object represents a conditional probability space the object can go into. Using the confidence measure, there are two main strategies for classifying an unseen object. CBA looks for the first rule of the classifier, in the precedence order, that covers the object to predict the class of the object. If such a rule does not exist, CBA predicts the majority class. With such a simple strategy of classification, CBA is a very good classifier. HARMONY is also a very competitive classifier with the classification based on the confidence of rules. For classifying an object, the best strategy of HARMONY is to compute, for each class label, the sum of confidences of all the rules of the classifier that cover the object (?conf(ri)). The class label that has the largest sum is the predicted label.

4.2. C-coefficient  In CMAR, only positively correlated rules are selected for building classifiers. Those rules are determined by the ?2 test. Given a rule r : P ? C, let ? T be the number of the objects in the dataset.

? p, c and pc be respectively the number of the objects  in the dataset that are covered by r, the number of the objects in the dataset that have the class label C, and the number of the objects in the dataset that are correctly classified by r.

The ?2 value of r is defined as follows. Let  o1 = pc, o2 = p?o1, o3 = c?o1, o4 = T ? c?o2 (3)  n1 = p? c T  , n2 = p? (T ? c)  T , (4)  n3 = (T ? p)? c  T , n4 =  (T ? p)? (T ? c) T  (5)     ?2 = (o1 ? n1)2  n1 +  (o2 ? n2)2  n2 +  (o3 ? n3)2  n3 +  (o4 ? n4)2  n4 (6)  The authors of [2] remarked that in the vote model, the application of Equation 6 to define the effect of a class label does not yield a good result, because Equation 6 may be favorable to minority classes. They proposed another sophisticated measure that integrates both information of correlation and popularity, called the weighted ?2. This measure yields the best results in the experimentation of CMAR. However, the authors also remarked that it is hard to verify theoretically the soundness or effect of this measure.

We have another observation. Through the expressions in Equations 3, 4, 5, we can show that  (o1 ? n1)2 = (o2 ? n2)2 = (o3 ? n3)2 = (o4 ? n4)2 (7)  for any rule built from the dataset, and  o1 + o2 + o3 + o4 = n1 + n2 + n3 + n4 = T (8)  That is, for any rule P ? C, the numerators of the fractions in Equation 6 are identical, and the sum of the denominators is constant and equal to T . Hence,  ?2 = (o1 ? n1)2( n1  + n2  + n3  + n4  ) (9)  If o1 = n1, then from equation 7, o1 = n1, o2 = n2, o3 = n3 and o4 = n4, and ?2 is minimal, ?2 = 0. We have the same result if o2 = n2 or o3 = n3 or o4 = n4. Conversely, if ?2  is minimal, i.e., ?2 = 0, then o1 = n1, o2 = n2, o3 = n3 and o4 = n4. Otherwise, if the classification by r = P ? C is well distinct from the classification by the equi-probable distribution, either much better or much worse, then ?2 tends to the maximal value. In the context of this work where the classifier is built with the rules that classify correctly the training objects with the highest confidence, it is the case where the classification by P ? C is much better than the classification by the equi-probable distribution. Hence, ?2  has the similar property as the information gain in sense of the weight of evidence of HPWR [7].

However, the experimentation shows that ?2 is not com- petitive measure. In addition, by Equation 9, for a non-zero value of o1 ? n1, (i.e., o1 6= n1), if one among ni, i = 1..4, tends to zero, then ?2 tends to the maximal value. This is the case of minority classes.

To improve it, firstly, we delete the last two terms of ?2, because they concern only the objects that are not covered by r. Let  ??2 = (o1 ? n1)2  n1 +  (o2 ? n2)2  n2 (10)  ??2 preserves the above property of ?2.

Secondly, in order to deal appropriately with minority  classes, we shift Equation 10 to the scale of the dataset to  define a measure, called C-favor, to estimate the favor of P to a class C. We denote  o?1 = conf(r)? T, o?2 = T ? o?1, (11)  ?r =  ? (o?1 ? c)2  c +  (o?2 ? (T ? c))2 T ? c  (12)  ?r still has the similar property as the information gain and can be used to measure the class membership. Let O be an object represented by an itemset X . Let the rules with class label C that cover O be ri = Pi ? C, i = 1, ..., k.

We define the C-favor of X by  FC(X) = ?i=1,k?ri (13)  To classify O, we compute FC(X) for each class label C, and O is predicted in the class that has the largest FC(X).

As the confidence is an important measure, we think that the combination of confidence and FC can be a better measure. The combined measure, denoted by cFC(X), is defined as follows. For each class label C,  cFC(X) = FC(X)? ?i=1,kconf(ri) (14)  The class label that corresponds to the largest value cFC(X) is used to predict O. To verify the effect of FC and cFC , we shall compare them with a measure that we get by adapting the weight of evidence (HPWR [7]) in the context of the systems based on the notions of support and confidence.

4.3. Weight of evidence  For a rule r = P ? C, HPWR [7] defines a measure of evidence provided by P in favor of C as the difference in gain of information when predicting C on P instead of predicting some other class label. Remind that, in Section 3, this measures is defined by Equation 2. HPWR applies this measure in the context of significant association patterns defined by standardized residual. In the context of frequent itemsets defined on the notions of support and confidence, the confidence of r, conf(r) = sup(r)/sup(P ), corre- sponds to the conditional probability Pr(C|P ). Moreover, we have  Pr(P |C) = Pr(C|P )? Pr(P ) Pr(C)  (15)  Hence, Pr(P |C) P (P |?C)  = conf(r)? (1? rsup(C)) (1? conf(r))? rsup(C)  (16)  where rsup(C) = sup(C)/T , the relative support of C with respect to the number of the objects of the dataset.

Let ri = Xi ? C, i = 1, ..., k be the rules that cover an object O represented by an itemset X . Then the adapted weight of evidence of X in favor of C is  EC(X) = log conf(r1)? (1? rsup(C)) (1? conf(r1))? rsup(C)  +     ...+ log conf(rk)? (1? rsup(C)) (1? conf(rk))? rsup(C)  (17)  In this work, we omit the condition Xi ? Xj = ?,?i 6= j, 1 ? i, j ? k, required in [7], because checking this condition is a cost operation and also because when the omission is applied to all class labels, each one can share almost the same effect.

For classifying O, the adapted weight of evidence of X is computed in favor of each class label. The object O is predicted in the class that corresponds to the highest weight of evidence.

4.4. Variants  It is easy to see that when conf(r) = 1 Equation 16 becomes infinite, as well as W (C/?C|P ). In such a case, from Equation 17, the class label of any exact rule can be selected for the prediction without consideration of the information gain. Under this observation, we define a variant of the weight of evidence as follows. The expression  log conf(r)? (1? rsup(C)) (1? conf(r))? rsup(C)  (18)  is modified into  log(1 + T.conf(r)  c )? log(1 + T.(1? conf(r))  1? c ) (19)  Let vEC(X) denote the variant of the weight of evidence as defined in Equation 17, but use Equation 19 in the place of Equation 18. This variant preserves the property of the weight of evidence and allows to compare the gain of information of exact rules.

Finally, we consider the combination of the measures EC(X) and vEC(X) with the measure by the sum of con- fidences to try to improve them. These combined measures are denoted respectively cEC(X) and cvEC(X).

cEC(X) = EC(X)? ?i=1,kconf(ri) (20)  cvEC(X) = vEC(X)? ?i=1,kconf(ri) (21)  5. Implementation  We apply those measures of class membership to a method for building classifiers [11] to classify unseen objects. This method builds classifiers on small key itemsets, using a prefix tree structure for extracting class association rules.

It has the following particularities.

? It can adapt the level-wise computation of Apriori, but it is different from Apriori: by enumerating the itemsets and counting their occurrences in the prefix tree, it combines the two phases, generating candidates and computing their support, into one phase.

? It builds the classifiers with the highest confidence rules that classify correctly each training object, as in  HARMONY. In contrast to HARMONY, it postpones the search for those rules until the end of the class association rule extraction process.

? Another contrast to HARMONY is that we do not consider the itemsets of all sizes, but we limit the maximal size of key itemsets to 5. Under this limit, we can explore key itemsets with low supports. In fact, we apply the support constraint, using minsup, only to i-itemsets, with i ? 2.

This constraint is not applied to itemsets with size > 2, excepting those with support 1. Though itemsets with very small supports can be considered, only rules with maximal confidences and supports that classify correctly each training object, are added to the classifier.

6. Experimental Results  Table 2. The 23 UCI datasets.

13 small datasets Dataset #obj #item #class anneal 798 106 5 auto 205 142 7  breast 699 48 2 glass 214 52 7 heart 303 53 5  hepatitis 155 58 2 horseColic 368 94 2 ionosphere 351 104 2  iris 150 23 3 pimaIndians 768 42 2  ticTacToe 958 29 2 wine 178 68 3 zoo 101 43 7  10 large datasets adult 48482 131 2 chess 28056 66 18  connect 67557 66 3 led7 3200 24 10  letRecog 20000 106 26 mushroom 8124 127 2  nursery 12960 32 5 pageBlocks 5473 55 5 penDigits 10992 90 10 waveform 5000 100 3  The program for verifying the effect of the class mem- bership measures is implemented in C and experimented on a laptop with a Pentium 4, 1.7 GHz mobile processor and 768 MB memory, running Linux 9.1. Let us call SIM the implemented program. For comparison, we evaluate the measures on the same 23 UCI categorical datasets (13 small and 10 large datasets) obtained from the author of [8], using the 10-fold cross validation. Table 2 represents the characteristics of these 23 UCI datasets.

To have an idea on the effect of the measures, we use the experimental results of HARMONY as the reference. The reasons are:     Table 3. Experimentation on 13 small datasets  HAR. SIM Dataset ?conf ?conf FC cFC EC cEC vEC cvEC Sz anneal 94.04 94.27 91.35 92.58 95.17 94.94 92.02 93.37 4 auto 72.00 76.50 78.50 78.00 77.00 77.00 78.50 78.00 3  breast 91.18 90.14 92.03 92.03 93.19 91.88 92.17 93.19 3 glass 68.58 71.90 70.48 71.90 71.43 71.90 71.43 71.43 5 heart 56.33 59.00 56.67 58.67 58.67 58.67 58.00 58.67 5  hepatitis 82.01 82.67 81.33 82.00 84.67 85.33 82.67 80.67 3 horseColic 81.68 83.89 83.61 83.89 83.61 83.89 83.89 83.89 4 ionosphere 89.15 90.00 90.86 90.29 90.00 90.00 90.29 90.29 4  iris 93.98 94.00 94.00 94.00 94.00 94.00 94.00 94.00 3 pimaIndians 69.22 69.08 73.42 73.95 73.29 73.82 73.42 73.42 3  ticTacToe 96.42 97.89 98.11 98.21 97.89 97.89 98.21 98.63 4 wine 90.57 91.76 91.18 91.18 91.76 91.76 91.18 91.18 4 zoo 89.00 92.00 93.00 93.00 92.00 92.00 93.00 92.00 3  Total/Avg. 82.63 84.08 84.20 84.59 84.82 84.85 84.52 84.52  Table 4. Experimentation on 10 large datasets  HAR. SIM Dataset ?conf ?conf FC cFC EC cEC vEC cvEC Sz  adult 83.40 84.23 82.02 83.73 83.41 84.29 82.61 83.78 3 chess 44.93 60.57 61.58 61.79 60.80 60.56 61.70 61.76 5  connect 77.30 77.67 78.24 78.88 79.72 79.21 78.88 79.45 4 led7 74.35 74.37 74.19 74.22 74.22 74.31 74.42 74.44 5  letRecog 70.82 71.29 71.54 71.47 70.82 71.25 69.74 72.34 4 mushroom 100 100 100 100 100 100 100 100 3  nursery 92.94 98.33 96.71 97.85 98.77 98.66 97.21 97.85 5 pageBlocks 91.17 90.93 91.52 92.08 91.74 92.08 91.52 91.90 4 penDigits 96.03 97.05 97.02 97.02 97.02 97.04 97.02 97.02 4 waveform 77.92 79.86 79.78 79.82 75.72 76.24 79.78 79.78 3 Total/Avg. 80.89 83.45 83.26 83.69 83.22 83.36 83.27 83.83  (i) HARMONY is a good classification system, its per- formance is well compared [6] with the important systems such as FOIL [12], CPAR [13], and SVM [14].

(ii) SIM shares with HARMONY on many points in the method for building classifiers and is also different from HARMONY on many other points.

(iii) Thanks to the authors of HARMONY, we get the runnable program of HARMONY and can use it to experi- ment HARMONY and SIM on the same UCI datasets and on the same computer.

For the experiments conducted in this work, the parameter setting for HARMONY is done as described in [6]: for the 13 small datasets, minsup = 10, and for the 10 large datasets, minsup = 50; items are sorted in the correlation coefficient ascending order (the order with which HARMONY gets the best results in general). In particular, for the dense datasets connect and ionosphere, only the items with supports no greater than 20, 000 and 190 respectively are considered to generate class association rules. The same consideration is applied to SIM.

Tables 3 and 4 represent the experimental results of the classification accuracy of HARMONY (abbreviated by  HAR.) and SIM using the measures ?conf , FC , cFC , the adapted weight of evidences (EC) and its variants (cEC , vEC , cvEC). In these tables, the column Sz represents the maximal size of itemsets extracted by SIM .

7. Discussions and Conclusion  Notice that the results represented in column HAR. (HAR- MONY) of Tables 3 and 4 are different and better than what was reported in [6]. The differences can be due to the method for dividing data randomly in the 10-fold cross validation.

On the results of the experiments conducted in this work, Tables 3 and 4 show that:  ? For the same measure ?conf , on average the classifica- tion by SIM is about 1.45% and 2.56% more accurate than that by HARMONY for the 13 small and 10 large datasets, respectively. The good accuracy of SIM can be explained by the selection of CARs having the maximal confidence and support among those having small supports. These rules may be very specific. However, as they are built on the small size key itemsets, they are in general not specific.

? FC (the measure based on ?2 revisited) and EC (the adapted weight of evidence) are comparable to ?conf .

Moreover, we can say that FC is comparable to EC and the modified version of EC (i.e. vEC) is correct and effective.

? The combinations of FC , EC , and vEC with ?conf improve slightly FC , EC , vEC , and ?conf . We can say that the sum of confidences is an important measure and these combined measures are useful, in particular in the context where the accuracies of the classification by the sum of confidence, by FC , and by EC are almost very good.

For conclusions, firstly, the adapted weight of evidence is a good class membership measure built on the gain of information. FC , a measure built on the revisited ?2 test, provides another view of information gain. It is comparable to the adapted weight of evidence. The sum of confidence is a simple and natural measure with the good performance.

The combinations of the sum of confidence with the pre- vious measures are interesting and useful to improve their performance.

Next, based on the average accuracy values of different measures, we recommend to use the combined measures, be- cause the average accuracy values of the combined measures are in general better than that of the non-combined measures.

Finally, through the results on each dataset, between the combined measures based on ?2 and the weight of evidence, we suggest the following propositions.

? For the 13 small datasets, though the average accuracy  value of cEC is slightly better than that of cFC , we can observe that cFC is much often wins cEC . Indeed, cEC wins cFC on only 3 datasets while cFC wins cEC on 6 datasets. Hence, we can recommend cFC for the small datasets.

? For the 10 large datasets, the average accuracy value of cvEC is slightly better than that of cFC , and cvEC wins cFC on 4 datasets and cFC wins cvEC 3 datasets.

Hence, we can recommend cvEC for large datasets.

