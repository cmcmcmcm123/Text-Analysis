An Iterative Algorithm for Differentially Private  Histogram Publication

Abstract?Traditionally, the accuracy of range queries in  differentially private histogram publication was boosted by obtaining the Best Linear Unbiased Estimator (BLUE) of the nodes? noisy values in the Differential Privacy Range Tree (DPRT). However, exist works require DPRT be perfect k-ary, which means it is not suitable for differentially private histogram publication under arbitrary tree structure In this paper, we propose an efficient iterative algorithm GBLUE for differential privacy histogram publication based on any tree structure. The correctness of GBLUE is guaranteed by theoretical analysis and experimental demonstration. Experimental results show that GBLUE is effective and feasible.

Keywords?differential privacy; iterative algorithm; histogram publication

I. INTRODUCTION In real life, many research institutions or organizations will  release data for statistics and scientific research. How to ensure the utility of published data while not leaking private information of individuals has become a very popular topic in database research field[1,2].

In the past ten years, people have put forward some privacy preserving data publishing models. However, most of the existing privacy protection models are based on anonymization, such as k-anonymity [3], l-diversity [4] and t-closeness [5] and so on. These models require special attack assumptions and some background knowledge, so it has great limitations. After that, Dwork et al proposed a more robust privacy protection model-differential privacy model [6,7].The model will provide strong privacy guarantee by adding random noise to the released data. No matter what background knowledge the attacker has, he can not identify whether a record is in the original dataset or not. Recently, Many researchers focus on data dissemination in differential privacy and do some meaningful research work, especially in the field of differential privacy histogram publication. However, the differential privacy protection level and the availability of publishing  histogram data is a contradiction. Therefore, how to publish privacy security data while assuring high data quality is the core issue of differential privacy data publication [8].



II. RELATED WORKS Histogram publication is one general and widely used  method for releasing data under differential privacy. The basic idea of publishing is to divide the table into several disjoint sets based on one or more attributes, and each bar in the histogram represent the number of records whose attribute value are in the specific range. Histograms can visually represent data with information about the data distribution, and support statistical queries. In recent years, researchers made a number of valuable researches in it.

Dwork [6,7,8] has worked on differential privacy and published series of papers on differential privacy data releasing.

Most researches focus on the statistical queries on the database, and achieve differential privacy by simply adding Laplace noise to the original results. We review some major work under the framework of differential privacy. Xiao [9] developed a technique enforced ?-differential privacy with economical cost by wavelet transform. Graham Cormode et al[10] releases a compact summary of the noisy sparse data with the same privacy guarantee by sampling and filtering, which also works for [9]. [11] using the clustering technique proposed an alternative optimization interval counting queries, and [12] using greedy strategy to achieve clustering to improve the efficiency of clustering.

Hay et al [13] proposed one method based on the best linear unbiased estimator of the noisy data in perfect k forks differential privacy range tree. It is with high accuracy in range count query and consistency. However, the existing differential privacy histogram publication algorithm based on best linear unbiased estimation only applies to a perfect differential privacy range tree with k fork. This paper intends to design method based on the best linear unbiased estimation that   DOI 10.1109/CLOUDCOM-ASIA.2013.41    DOI 10.1109/CLOUDCOM-ASIA.2013.41     applies for arbitrary tree structure for differential privacy range tree in differential privacy histogram publication, and analyze its feasibility and effectiveness with similar algorithms.



III. PRELIMINARIES In this section, we will give the background of the problem,  the detail definition of ?-differential privacy and will describe Hay?s algorithm [13] and some necessary notions to understand this algorithm. We will also point out the defect of Hay?s algorithm in this section.

Assume that there is one relational table T, which has n records. Users will perform some range query operations, which are as follows:  Select count(*) from T where T.a?[x,y]  It is obvious that T can be represented by a histogram M with several bars, each bar Xi will represented by the number of records within one spicific attribute range.

DEFINITION 1. (?-differential privacy) [6]A randomized algorithm ? satisfied ?-differential privacy, if and only if (i) for any two tables T1 and T2 that differ only in one tuple, and (ii) for any output O of ?, we have  P[?(T1) = O] ? e ? ? P[?(T2 ) = O]  (1)  When running the randomized algorithm ? that satisfies ?- differential privacy, the probability distribution of the output is almost the same between these two tables. Which provide strong privacy guarantee on whether Mary is in the database or not. By Dwork?s theory [5], we can use the Laplace mechanism [5] to release M under differential privacy, which by adding noise with Laplace distribution. A Laplace distribution has a probability function:  ( ) 1  p e ?  ?? ?  ? =  (2)  By equation (2), we get that the Laplace distribution has an expectation 0 and a variance 2?2. After adding noise, the releasing version histogram M* is accomplished. If we keep ? constant, the magnitude ? depends on the sensitivity of the query sequences, which is defined as follows.

DEFINITION 2. (Sensitivity [5]). Let F be a set of functions, such that the output of each function f?F is a real number. The sensitivity of F is defined as  S(F ) = max T1,T 2  ( f (T1) ? f (T2 ) f ?F ? )  (3)  T1 and T2 are any two tables that differ in only one tuple.

If F is the function set that maps T to M, F will contain m functions in total and each of them output a real number entry in M respectively, then we have  THEOREM 1. ([5]) Let F be a set of functions with a sensitivity S(F). Let ? be an algorithm that adds independent noise to the output of each function in F, such that the noise follows a Laplace distribution with magnitude ?. Then, ? satisfies    (S(F)/ ?)-differential privacy.

A bigger ? will lead to a better privacy preserving but a lower quality of releasing data and vice versa. In hay?s algorithm, it maps the histogram to one perfect k-range tree. And then compute the new value of each node with the noisy ones. Here k-range tree is defined as follows  DEFINITION 3. (k-range tree) For given statistical histogram M?k-range tree satisfies:  (1) k is no less than 2  (2) Each Tree Node x holds domain interval [L(x),R(x)]  and the count sum from L(x) to R(x), we will use  Seg(x)={L(x),L(x)+1?R(x)} to denote [L(x),R(x)]. The node  with |Seg(x)|=1 is leaf node  (3) Node x has some child nodes, Son(x)  if x is leaf node, then Son(x) is empty  (4)  For one non-leaf node x:  1) |Seg(x)| ? k, Split |Seg(x)| leaf node  2) Otherwise, split k son node such that       Here follows one example with 2-range tree when the  histogram has 5 bars  Seg( y) y?Son( x ) ? = Seg(x)  ?  ? ?  ?  ? ?? Seg( y) Seg(z)?  y ,z?Son( x )?y<>z  = ? ?  ? ?  ?  ? ?  | Seg(x) k  ?  ? ?  ?  ? ? ? Seg(x)  y?Son( x )  ? | Seg(x)  k ?  ? ?  ?  ? ?       Figure 1 2-Range Tree with 5 elements in histogram  Every node x is associated with one fx takes T as input and output the count sum between[L(x), R(x)]. If T1 and T2 are differ in one entry, then for some specific leaf node x, |fx(T1)-fx(T2)|=1, moreover all its ancestor node y will have |fy(T1)-fy(T2)|=1, so S(F) is the farthest distance from one leaf node to root in k-range tree. For example, in figure 1, S(F) = 4.

For histogram, we know that S(F)=1 since adding/deleting one record will lead to one bar to change by 1. For the range count query Q which may cover O(n) bars, will have the variance O(2n?2). In one word, public the histogram directly after adding the noise will lead to great error on range count query.

We can construct the k-range tree on the histogram. It is obvious that S(F) =O(logn). By theorem 1, adding Laplace with parameter S(F)/? to each tree node will guarantee ?-differential privacy.  The k-range tree after adding Laplace noise will be mentioned as k-Differential Privacy Range Tree (DPRT). The following figure demonstrate how we process with the histogram.

For one range count query Q=[L,R], query operation on k- DPRT is to find such node set A that:  Figure 2 Process of constructing 2-DPRT with 5 elements  (1) Seg(x)  x?A ? = Q  (2) Seg( y) Seg(z)?  y ,z?A?y<>z  = ?  (3) The size of A is minimized In this paper we define that each node in A is covered by Q.

k-DPRT is with O(logn) level. For any query Q, at most O(k) node will be covered in each level, so the variance of the answer will be  2? 2  x?A  ? = 2 A ? 2 =O(2k logn? 2 ) =O(k? 2 logn)    which could dramatically improve the accuracy for range count query.

However, inconsistency will occur due to the noise.

Suppose the user have three queries Q(0)=[1,2], Q(1)=[3,4], Q(2)=[1,4] and the k-DPRT returns with P(0)=3.1, P(1)=0.5, P(2)=23. It is obviously that P(0) + P(1) is not equal to P(2).

Hay?s work [13] could boosting the accuracy of range count query with consistency. However it only apply for the histograms that could map to a perfetc k-DPRT, which may lead to massive padding and reduce the performance.

A. Local Best Linear Unbiased Estimator(BLUE) on k- DPRT Given one k-DPRT with exactly k+1 nodes. The value of  node root and its son nodes Son(root)={i1,i2?ik} after adding Laplace noise will be  ? ?  ? ? ?  ?+=  ? ?  ? ? ?  ?+?? ?  ? ?? ?  ? =  ?  ? ?  ?  ?  )()(  )()(  )( )(  ~  )( )(  ~  FSLapXih  FSLapXrooth  iL rootSoni  rootSoni iL    h ?  = [h ?  (i1),h ?  (i 2 )...h ?  (ik )] T    h ~  = [h ~  (i1),h ~  (i 2 )...h ~  (ik )] T    indicates the BLUE and original disturbed value of these nodes.

THEOREM 2. The BLUE for k-DPRT with exactly k+1 nodes is  h ?  (root) = h ~  (root)? ?root  h ?  (i) i?Son(root )  = h ~  (i)+ ?root   ?root =  h ~  (root)? h ~  (i) i?Son(root )  ?  k +1   by [13] , Here follows the algorithm to solve the Local Best Linear Unbiased Estimator (LBLUE)       Algorithm 1. LBLUE  Input: k-DPRT node x  Output?BLUE of all k+1 nodes  1 if x?Son(root), return its value. Otherwise go to step 2  2  k ? Son(x) ,h ?  (x) = h ~  (x)? ?root  with    ? x = h ~  (x)? h ~  (i) i?Son(x )  ?  k +1   3 for i?Son(x) do h  ?  (i) = h ~  (i)+ ?root  B. Iterative algorithm GBLUE based on LBLUE We run the LBLUE algorithm from button to up in k-  DPRT with the following algorithm  Algorithm 2. LBLUE-ADJUST  (Local Best Linear Unbiased Estimator ADJUST) algorithm  Input: k-DPRT node x  Output?adjusted value of all nodes  1 if x?Son(root), return its value. Otherwise go to step 2  2 for i?Son(x) do LBLUE-ADJUST(i)?  3 LBLUE(x)    Algorithm 3. GBLUE  (Global Best Linear Unbiased Estimator)algorithm  Input: k-DPRT T, threshold ?  Output?BLUE of all nodes  1 if T satisfied consistency within threshold ?, then return.

Otherwise go to step 2  2 LBLUE-ADJUST(root), go to step 1    C. Convergence of the GBLUE DEFINITION 4 (Incompatible value) After iterating for m times, the largest gap between the value of one non-leaf node and the sum of its sons? value is defined as ?(m)   Gapm (x) = h ?  m (x)? h ?  m (i) i?Son(x )  ?  ?(m) =max x?T {Gapm (x)}    In particular Gapm(y)=0 for leaf node y. h ?  m (x)  is the value of  node x after m iteration, and in particular  h ?  0 (x) = h ~  (x) .

DEFINITION 5. (depth of tree) The depth of tree is defined as  Dep(x) = 0, x ? Leaf (root)  Dep(x) = max y?Son(x )  {Dep(y)}+1,otherwise  ? ? ?  ??   Leaf(root) indicates the set of root?s son nodes.

LEMMA 1. GBLUE will converge to the BLUE of k-DPRT nodes? value Proof. For the value of node in k-DPRT, we have  ? ?  ? ? ?  ?+?? ?  ? ?? ?  ? = ?  ? ? )()(  )( )(  ~ FSLapXyh ySoni  iL    all the variable is with Laplace noise, which are with  expectation zero and are uncorrelated and have equal variances.

By Gauss?Markov theorem, the BLUE of the value is given by  the ordinary least squares (OLS) estimator  min h ?  (i)? h ~  (i) ? ? ?  ? ? ?  i  ?  h ?  ( j) j?Leaf (i)  ? = h ?  (i)  (4)  which will derive the following optimization problem  min h ?  ( j) j?Leaf (i)  ? ? h ~  (i) ?  ? ??  ?  ? ??   i  ?  Is is one convex function, so the partial derivative of h ?  (i) i?Leaf (root )    should be 0 at the answer  0)()(2)()(2 )( )(:  ~  )(:  ~  )( =?  ? ?  ? ? ? ?=??  ?  ? ?? ?  ? ?=  ?  ? ?? ?  ?  ?  ? ?  ?  ? jLeafijjLeafij jLeafk  jhjhjhkh ih  f     ??  ??  ?  = )(:  ~  )(: )()(  jLeafijjLeafij jhjh  (5)  If Dep(root)=1, then one single LBLUE-ADJUST will find the BLUE.

We only consider nodes whose depth is larger than 1 in the following.

In LBLUE, each node x will be subtracted by ? x  on its value while every son node of it will be added by ? x  on their value. So after LBLUE, equation 4 will always hold.

Now let?s review the LBLUE-ADJUST , here follows one example that we adjust the value of node x .

Figure 3 update the value in the m-th iteration   Node x is with Dep(x)?2. In Figure 3(a) Gapm y?Son(x )  (y) = 0  however after adjusting the Gapm y?Son(x )  (y) = ? x  and they will  not change again in the same iteration. So we can have }{max)(  2)( xxDep m ?=?  ? and ? y Dep(y)=0  = 0  for leaf node. Let  Size(y) = Son(y)  then  ? y Dep(y)=1  =  h ?  m (y)? h ?  m (x) x?Son(y)  ?  Size(y)+1 = Gapm (y)  Size(y)+1  ? y Dep(y)=1  ? ?(m)  Size(y)+1 < ?(m)     Since the adjusting is button-up, we must adjusting every  node in Son( y)  before adjusting y  1)(  )(  1)(  )()( )()(  2)( +  ?+  = +  ? ? ?  ? ? ? ???  =? ??  ??  ??  ? ySize  yGap  ySize  xhyh ySonx  xm ySonx  xmm  yDep y    ? y Dep(y)?2  =  Gapm (y)+ ? x x?Son(y)  ?  Size(y)+1 ?  Gapm (y) + ? x x?Son(y)  ?  Size(y)+1 ?  ?(m)+ ?x x?Son(y)  ?  Size(y)+1   By mathematical induction we could get  ? y Dep(y)?1  < ?(m)  )(}{max)1( 2)(  mm xxDep ?<?=+? ? so every single adjusting (iteration) will always guarantee  equation 4, and since the )(}{max)1(  2)( mm xxDep ?<?=+? ?  is descent, equation 5 will hold finally. So after the iteration, it will converge on the answer on the OLS, which means it will converge on the BLUE of k-DPRT nodes? value.



IV. EXPERIMENTS  This section will evaluate the algorithm running efficiency and  accuracy by experiments. This GBLUE algorithm is compared  to the previous work of Boost algorithm [13], and the literature  [14] pointed out that for random range count queries, perfect  12-ary tree is a more appropriate choice. In this paper, we  choose traditional two and twelve forks range tree with Boost  algorithm (hereinafter respectively referred Boost-2 with  Boost-12) and 12-range tree with GBLUE algorithm  (hereinafter referred to as GBLUE). For real data set, we run  GBLUE and Boost-2, Boost-12 algorithm and compare theirs  time and space efficiency; At last we compare and analyze the  accuracy on range count query.

The data from [15] is the access records to Amazon.com  from 3/1/2005 to 8/31/2010. In this experiment we will use the  random range count queries with length 2i (i ? 0). For any  range count query length, generating 500 random range queries.

We will add noise for 50 times and the error is evaluated  through the mean square error of average valuation.

The experimental platform is 1.8Ghz Intel Core i5 and  4GB Ram in MAC OS X 10.8.3. The algorithm is implemented  by C++ and the figures are plotted  by MATLAB 7.0. Here  follows the statistic about the dataset.

Table 1 statistics on the dataset  Dataset Data size Parted by Size after partition  Amazon 716064 Day 2010    We run GBLUE, Boost-2 and Boost-12 on the dataset. We will  run them 50 times and get their average running time under the  thresh hold ?=10-3 and ?=10-6     Figure 4 Program runtime in Amazon   We could see that GBLUE runs fast on the dataset.

However with less thresh hold, the running time of GBLUE is much longer. The difference in ? do not make big different in running time for all three algorithms.

GBLUE will terminate only when ?m<?. Fewer threshold means more iterations, which lead to longer running time.

However Boost-2 and Boost-12?s running time is not associated with ? and ?.

In the experiment we chose ?=10-6 and run GBLUE, Boost- 2 and Boost-12 on the dataset Amazon. For random interval query calculates the average standard deviation, the experimental results are shown in Figure 5.

Figure 5 Range queries error in Amazon   We could see from the experiment, the accuracy of Boost-2  is below the GBLUE and Boost-12 in the dataset. On Amazon, GBLUE is better than Boost-12. GBLUE has the lowest error among the three algorithms demonstrate its effectiveness.



V. CONCLUSION In this paper, we present one iterative algorithm GBLUE in  publishing histograms under differential privacy condition. We strictly prove that GBLUE will converge on the best linear unbiased estimator of the noisy data in k-range tree and provide high consistency guarantee. We analyze the advantages of it by comparing with existing solutions. Finally, the experimental results demonstrate that GBLUE has outstanding performance at range-count queries on differential privacy histograms.

GBLUE works for any tree-like data structure. If the dataset could be store in such data structure, the GBLUE should apply. Therefore we should study how GBLUE could be extended to other data type or even to multi-conditioned query in the future work.

