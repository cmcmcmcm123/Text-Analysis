Horizontal Format Data Mining with

Abstract  Analysing the data warehouses to foresee the patterns of  the transactions often needs high computational power  and memory space due to the huge set of past history of  the data transactions. Apriori algorithm is a mostly  learned and implemented algorithm that mines the data  warehouses to find the associations. Frequent item set  mining with vertical data format has been proposed as an  improvement over the basic Apriori algorithm.

In this paper we are proposing an algorithm as an  alternative to Apriori algorithm, which will use bitmap  indices in conjunction with a horizontal format data set  converted to a vertical format data structure to mine  frequent itemsets leveraging efficiencies of bitmap based  operations and vertical format data orientation.

Categories and Subject Descriptors  [Data Mining] Association Rule, Apriori, Bitmap Indices.

[Data Analysis] Data warehousing, Data Analysis.

General Terms - Data Analysis and Mining  Keywords - Data mining, Association Rule, Apriori, Vertical  format mining, Bitmap Indices

I. INTRODUCTION  Data Mining is an emerging concept or a tool in database  concepts, which is young, yet powerful and promising [1, 2].

Knowledge discovery, prediction, clustering, and  classifications through pattern recognition are some of the key  design aspects of data mining. Most of the existing techniques  on the associations are based on analysing the data  warehouses - the existing bulk data of transaction history.

Given the existence of a set of items, association rules enable  the prediction of the existence of one or more other items  based on the knowledge gathered by classifying the data  warehouses. Association rules are commonly used in mining  web usage [3], intrusion detection [4], and bioinformatics [5].

Statistical bias caused by suggesting a hypothesis by non-  representative data or a narrow sample to match the  hypothesis is defined as data-snooping bias, which leads to a  wrong decision in calculations, so these factors also should be  taken in to consideration [6]. Apriori algorithm [1] is  considered the fundamental algorithm for mining for  associations. Hybrid algorithms converging the classification  and association rule mining have also been suggested [7].

With the explosive growth of the data base size, scalability for  the data mining algorithms becomes crucial to be able to work  with very large data sets effectively. In this paper, a hybrid  frequent item-set mining method extending Apriori algorithm  is proposed.



II. PRELIMINARIES  Finding frequent item sets plays an important role in data  mining as the first step in determining association rules.

A. MARKET BASKET ANALYSIS  Association rule learning is mostly explained by market  basket analysis [1] ? retrieving the association between the  items that customers purchase. Here products or sets of items  (item-sets) which occur in many transactions are found.

TABLE I - TRANSACTIONAL DATA  TID List of item_IDs  T100 I1, I2, I5  T200 I2, I4  T300 I2, I3  T400 I1, I2, I4  T500 I1, I3  T600 I2, I3  T700 I1, I3  T800 I1, I2, I3, I5  T900 I1, I2, I3  Each of the patterns of behaviour of buyers identified  through this analysis can be used to place the goods in the  shops or restructure pages in the catalogues.

A set consists of i goods, called i-set-element (i-item-set).

The percentage of transactions containing a given set, called  the support (provision) set.  It is believed that in order for a set  of interest, its support must be above a user-defined minimum,  such sets are called 'frequent'. Table 1 [8] describes several  transactions (T100, T200, etc), stored in a relational database.

Corresponding column mentions the relevant list of item ids  for the particular transaction. As an example ?T200?  transaction contains ?I2? and ?I4? item ids.

In frequent item-set mining, we derive rules based on two  measurements called minimum support and the minimum  confidence that reflect the usefulness and certainty of the  discovered rules. Typically, association rules are considered  interesting if they satisfy both a minimum support threshold  and a minimum confidence threshold [1].

B. ALGORITHM APRIORI  This algorithm defines several stages. At the i-th stage  identifies all frequent i-element sets. Each stage consists of  two steps: the formation of candidates (candidate generation)  and the counting of candidates (candidate counting).  At the  step of forming the candidates, algorithm creates a lot of  candidates from the i-element sets.  At the step of counting  candidates algorithm scans the database of transactions,  computing support sets of candidates.  After scanning, the  algorithm discards the candidates, ensuring that less than a  user-defined minimum and saves only the common i-element  sets.

During the 1 st  phase of the selected set of candidates  contains all 1-element sets.  Algorithm calculates their support  during the step counting candidates. Thus, after the first phase  all frequent 1-element sets are known. Reasoning in a straight  line, a candidate can "burn" all pairs of goods. However,  Apriori reduces the number of sets of candidate sifting - a  priori - those candidates who may not be frequent, based on  information received at previous stages of the information on  which of the sets are the most abundant.  Screenings are based  on the simple assumption that if the set is frequent, all its  subsets must also be frequent. Thus, before the counting of  candidates step algorithm can reject any candidate set, a  subset of which is not frequent. This process is continued until  the number of frequent n-item-sets becomes zero, where n  determines the no. of children in the item-set.

C. ALGORITHM OPTIMIZATION  1)  PRUNING  Mannila et al. proposed pruning techniques to reduce the  size of candidate set [9] as an optimization. Pruning strategy  for the algorithm is based on a characteristic: a frequent item-  set is a set where all its subsets are frequent.

2)  DYNAMIC HASHING AND PRUNING  Park et al. suggested a hash based algorithm (DHP), which  generates candidate large itemsets efficiently, while reducing  the size of the transaction data base [10]. Number of candidate  itemsets generated by DHP is smaller than that generated  using existing algorithms, making it efficient for large  itemsets, specifically for the large 2-itemsets.

3)  PARALLEL DATA MINING  Park et al., extended DHP into a parallel data mining  algorithm, facilitating parallel generation of the candidate  itemsets and parallel determination of large itemsets [11].

4)  TRANSACTION REDUCTION  Agrawal et al. proposed this algorithm to reduce the  number of transactions scanned in the future iterations [12,  13]. A transaction that does not contain any frequent k-  itemsets can not contain any frequent (K +1) itemsets, hence  can be removed.

5)  PARTITION  Savasere et al. designed a Partition-based highly parallel  algorithm [14], which logically divides the data base into  several disjoint blocks. Each considered separately a sub-  block and it generates all the frequent sets. Sub-block size is  chosen to allow each sub-block be placed in the main  memory, each stage scanned only once.

6)  SAMPLING  A subset of the sample is taken for mining in the sampling  based mining techniques. Toivonen further developed this  [15], which significantly reduced the I /O costs, often with  inaccurate results and distortions in the data.

7)  DYNAMIC ITEMSET COUNTING  Dynamic itemset counting technology [16] proposed by  Brin et al. marks the starting point of the database divided into  blocks. The results of algorithm need fewer database scans  than Apriori. ECLAT (Equivalence CLASS Transformation)  algorithm developed by Zaki [17] is used to mine efficiently  using vertical data formats.

8)  VERTICAL FORMAT DATA MINING  Apriori algorithm mines frequent patterns from a horizontal  data format which represents the items categorized into  particular transactions, where vertical data format represents  data as transactions categorized into particular items. So for a  particular item, there is a set of corresponding transaction ids.

Instead of horizontal data format, Apriori can be extended to  use vertical data format for efficient mining.

Vertical format data mining only has to parse the dataset  once to get the itemsets. For the itemset generation from the  nd  itemset it only needs to refer the previous itemset. So this  eliminates the need to parse through the dataset each time to  count the frequency of itemsets, for each round. Hence  vertical format mining is more efficient than its horizontal  from, which has motivated general purpose algorithms such as  VIPER [18], based on the vertical format representation.



III. HORIZONTAL FORMAT DATA MINING WITH EXTENDED  BITMAPS  We propose the algorithm 'Horizontal Format Data Mining  with Extended Bitmaps', for mining a data set in a horizontal  format, by converting it to a data structure of vertical format  along with bitmaps indices to store the itemset data.

?. PARSING THE DATASET TO CREATE 1ST ITEMSET AND BITMAPS  Here, the first and only parsing of the dataset is done.

1. Creating the master array.

2. Creating bitmaps relevant to each item in master  array     a ? masterArray; in ? Item n; b ? associatedItemArray; c ? BitSet Fig. 1: Main data storage structure of the extended vertical data mining.

As in Fig. 1, masterArray is an array of Item objects of all  items in the dataset. Under each item, there is an array of other  items occurring together with it in transactions which we call  from, which we refer from now on as AssociatedItems. Under  each AssociatedItem, we store a bit vector indicating the  presence of the AssociatedItem for every transaction where  the Item is found. Basically length of bit vectors give the  number of transactions that its root Item in masterArray is  found. If the AssociatedItem is in the transaction, bit vector  value for that transaction will be 1 (true), otherwise 0 (false).

B. REMOVE REDUNDANT ASSOCIATED ITEMS.

Here for better memory utilisation, the redundant duplicate  mappings (AssociatedItems) are pruned from the main data  structure such as, Item i1 mapped to AssociatedItem i2 and  Item i2 mapped to AssociatedItem i1 in the masterArray.

C. PRUNING ITEMSETS ACCORDING TO APRIORI PROPERTY.

Association mining is carried out solely in this step, in three  major phases. The core logic of Apriori property is used, but  implementation is done using the manipulation of bit vectors.

1st frequency itemset extraction - The items not satisfying  the minimum support are removed from the masterArray [1].

2nd frequency itemset extraction - Here we iterate through  the associatedItemArrays for each Item in masterArray. For  each AssociatedItem, the cardinality of its bit vector is  compared with the minimum support value. AssociatedItems  having less than the minimum support are removed.

Extracting frequency itemsets above two - For mining  frequency item sets of three or above, we start intersecting bit  vectors of AssociatedItems, compare result bit vectors'  cardinality with minimum support value, and remove the  AssociatedItems as Apriori property suggests. This will take  place for increasing frequency itemset values and stop when  no more frequent item sets are present. In this recurring step,  two main data structures [Fig. 2, Fig. 3] are used.

Fig. 2: Pattern Structure  Pattern structure Fig. 2, is used to store each resultant bit  vector from intersection of bit vectors and the itemset ids for  those intersected bit vectors' belonging items. As Pattern is  also a representation of item sets, we can use it as a frequent  itemset itself.

Fig. 3: Candidates Structure  Candidates structure Fig. 3, is used to store the frequent  item sets of each level of itemset size. Here each candidate  level is an array of patterns. Each level indicates the frequency  itemset level, hence frequent itemsets of length one will be  under level one, frequent itemsets of length two will be under  level two and so on. Once the above steps are completed, the  frequent item sets are retrieved by iterating through the  candidate structure.



IV. EXPERIMENTAL STUDY  The algorithm was implemented and benchmarked in a  system with 2 GB memory and 2.5 Ghz core 2 Duo Processor,  against an implementation of Apriori algorithm for chosen  datasets [19], along with different values of minimum support,  as both the algorithms mine the datasets in horizontal form.

TABLE II: T= 10; I = 4; D = 10K  Minimum  Support (%)  T10I4D10K  Apriori (sec) HFDM-EB (sec)  0.75 1934.4 10.6  1 1365.4 10.5  2 238.4 10.4  5 1.6 9.6  FIG. 4: T= 10; I = 4; D = 10K  TABLE III: T = 10; I = 4; D = 100K  Minimum  Support (%)  T10I4D100K  Apriori (sec) HFDM-EB (sec)  0.75 21039.1 134.2  1 12600.5 132.1  2 2365.1 131.7  5 1.3 132.1     FIG. 5: T= 10; I = 4; D = 100K  TABLE IV: T = 40; I = 10; D = 100K  Minimum  Support (%)  T40I10D100K  Apriori (sec) HFDM-EB (sec)  5 232 10.9  10 19 10.8  20 0.5 10.6  40 0.3 10.4  FIG. 6: T = 40; I = 10; D = 100K  Table II, Table III, and Table IV compare the performance  of  the algorithms, where the respective graphs are shown in  Fig. 4, Fig. 5, and Fig. 6. In most of the cases the proposed  algorithm showed a significant improvement over the Apriori,  while the Apriori implementation was more efficient at high  support levels. Here the results of our algorithm are dominated  by the time to build the bitmap and not time taken to mine it.

The time taken to build the bitmap is independent of the  number of frequent item sets. But Apriori's time drops  drastically when the number of frequent items is low. Thus  Apriori was seen having a higher performance at high support  levels where number of frequent item sets found is low.



V. CONCLUSION AND FUTURE WORK  In this paper we have proposed an efficient algorithm for  Association Rule Mining, which recovers the associations as  the Apriori and the other existing algorithms do. We  implemented it in Java, which may be more efficient, if  implemented in C or a lower level language.

For each data item, a bitmap is created for each associated  item. If there are n associated items for a data item, then the  number of candidate sets generated is n(n-1). So there can be  redundant bitmaps created for the same data item pairs.

Currently redundant pairs are pruned after creating the vertical  format. But, if there is a dynamic pruning mechanism to prune  redundant data item pairs while creating the vertical format  memory can be well optimized.

The algorithm lends well to Map Reduce like distributed  data mining since mining of each data item is independent of  others. So this algorithm can be enhanced to work in a  distributed environment with or without a shared memory.

For some larger data sets that are having many items per  transaction, the algorithm fails to withstand due to utilizing  prohibitive amount of memory. It can be mitigated by using  compressed bitmap [20] implementation instead of plain  bitmaps, so the memory is utilized better.

