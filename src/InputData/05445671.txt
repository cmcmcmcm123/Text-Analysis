Automatic Topic(s) Identification from Learning material: An Ontological Approach

Abstract? The ability to judge the relevance of topics and related sources in information-rich environments is a key to success when scanning online learning environments. A Learner may be looking for learning materials explaining given topic or exercises on the topic. Any learning material may cover multiple topics related to multiple subject domains. This paper presents ontological approach for identifying major topics, covered in the learning material. Along with the topics, the subject and discipline to which those topics belongs to and relevance of the topic in the learning material as compared to other topics present in the same document are also discovered. Domain ontology is developed to retrieve the topics covered in the document. We present an evaluation against a manually categorized topics as well as author?s judgment of relevance of the topics discovered by our system. Evaluation results show that the technique presented by us is effective in identifying topics and subtopics covered in a single learning document.  This work is part of research conducted on developing a web service for automatic semantic annotation of learning material.

Keywords-Topic identification, ontology, semantic annotation

I.  INTRODUCTION With energetic development of the Internet, especially the web page interaction technology, online learning object repositories have become more and more realistic and popular in the past ten years. The reach of Internet to student community has increased its acceptability. Learning object repositories are essentially storage and retrieval systems for learning objects where the learning objects are organized so as to be easily searchable. These repositories enable sharing and reuse of learning materials by different users. Different learners have different requirements and therefore require different learning contents. One of the key issues in using learning objects is their identification by search engines. This is usually facilitated by assigning descriptive metadata to the learning object. These learning object metadata is stored along with learning objects in learning object repository. This information background is needed for querying services to perform accurate queries for learning object retrieval. The work presented in this paper is part of Development of web service for automatic semantic annotation of learning material. With respect to  personalization and adaptability in learning systems, we have identified certain essential metadata elements and topic(s) identification is one of them. A learning material may contain topics and terms related to multiple subjects and domains. Thus topic(s) identification along with subject and relevance of each topic will assist in effective search and recommendation for a learner for a personalized learning management system. A topic refers to a particular subject that is discussed about in a piece of text [1]. It is the common theme that semantically links together pieces of text that may be physically disconnected [2]. Identification of topics covered in the document and subject domain of important terms covered in the document will help in meaningful and need based retrieval of document by a learner. Researchers have indicated that automatic metadata generation is more efficient, less costly, and more consistent than human-oriented processing [3][4][5][6][7][8][9]. This paper showcases our work of automatic retrieval of topics, subtopics and relevance of each topic. We view learning material as explanation of concepts. In the first section, this paper discusses related work done by researchers in topic identification. In the later sections our approach is discussed, followed by evaluation, results and conclusion.



II. RELATED WORK Researchers have tried to generate topics from different sources of information like research papers, news articles, multiparty dialogues etc. Majority of them are focused on collection of material from various sources and identifying topics from them to categorize them. Most of the approachs of topic identification are based on natural language processing for named entity extraction, association rule data mining [10], clustering of association rules [10], topic keyword clusters using the k-nearest neighbor graph and the keyword clustering function [11] and information retrieval techniques. Machine learning techniques have been used to combine linguistic, statistical and position information to identify topic levels for headline in the text [12].  Shuhua Liu has proposed generation of topics by partitioning the text [13]. Further partitions are made to form smaller groups of text segments that represent sub-topics, which may contain one or a few paragraphs.   Miura Nakayama has proposed Random Projection, one of the dimension reduction techniques. He has examined and verified a topic word model in which topic can be identified by means of   DOI 10.1109/ICCEA.2010.221     word distribution under same author [17].  Wordnet and Latent Semantic Indexing based model for dimension reduction has been proposed by V?clav Sn??el et. al [20].

Kino Coursey et. al. has presented an unsupervised system for automatic topic identification, which relies on a biased graph centrality algorithm applied on a graph built from Wikipedia [21].



III. OUR APPRAOCH Our observation of learning material states that it can  cover concepts belonging to multiple subjects/topics/areas.

For example, a document related to Computer Science subject may contain certain concepts which in turn are related with the Mathematics, Statistics or Management.

Numerous learning objects thus may belong to multiple disciplines and in turn multiple subjects. The novelty of our tool is that we have tried to identify concepts belonging to different topic or subject areas in same document.  With each identified topic and concepts covered under it, we have reported its relevance as compared to other topics present in the same document with the help of subject domain ontology. This identification will prove helpful to intelligent learning systems while presenting learning documents to learner according to his needs. For example, learning material explaining six sigma concepts will be related to Total Quality Management,  as well as Statistics as Six Sigma concept is based on Standard Deviation. A  student will not able to comprehend Six Sigma concept till he understands Standard Deviation. Thus Intelligent learning management system should make learner alert of the relation between concepts related to multiple subject areas.

Similarly learning object explaining Deadlock detection will discuss about cycle in a graph which actually in detail will be enclosed under data structures subject. An intelligent eLearning system can search learning materials corresponding to the topics and its relevance extracted by our tool, (supplemented as part of metadata) and can present its suggestion to a learner.

Figure 1.  Topic Extraction  Our approach of Topic extraction employs a multistage process, first it identifies key concepts within a document, and then these terms are compared with terms present in domain ontology. In the next stage, topics/subtopics from domain ontology containing one or more terms as siblings are listed out. Finally the subjects and disciplines, to which retrieved topic belongs to are identified and displayed.

A. Domain Ontology   Ontology [19] is a specification of an abstract, simplified view of the world that we wish to represent for some purpose. This view is called conceptualization. Therefore, ontology defines a set of representational terms that typically include concepts and relations.  The system stores the domain knowledge of various disciplines and their subjects in the form of ontology. The domain ontology is an ontological structure of the topics and terms in a particular subject domain together with the relationships between those topics and terms. Figure 2 gives an overview of created domain ontology with multiple layers.

Figure 2.  Subject Domain Ontology  The top layer contains disciplines like Computer Science.

Separate ontologies for other disciplines like Mathematics, Statistics, Management, Electronics etc are created.  The second layer contains Subjects under that discipline. The third layer contains the broad topics covered under that subject. Topics again can contain sub-topics. These subtopics may be again represented by collection of various terms. We have represented domain ontology using Ontology web language. We developed ontologies in the Prot?g? Ontology Editor [14] and used the Web Ontology Language (OWL) to express the ontologies (Prot?g? OWL).

Figure 3 shows the snapshot of prot?g? editor.

Figure 3.  Subject terms as entities in Prot?g? editor    The development of the domain ontology incurs cost in terms of both time and manual effort. But we have observed that once it is developed, its presence will help in effective and standardized automatic generation of metadata and achieving higher precision level in the retrieval process. The benefits of educational use of ontologies have been identified by many researchers [15][16]. Many researchers have used these ontologies in various forms and for various purposes. For this study, we have chosen to annotate the learning material belonging to Computer Science discipline.

We have build domain ontology for four subjects, Operating systems, Database management systems, Data Structures and Statistics. To evaluate the system, ontology corresponding to certain portion of Operations management subject is also developed. The concept and terms required to create domain ontology are identified through three different techniques (1) manually, through back-of-the-book index of some of the major texts (2) through a semi-automatic technique by parsing the Wikipedia pages and (3) Semantically parsing ebooks.

B. Topic Extraction process   This phase skims a document for noun phrases to understand the semantics of the document; we have used Stanford Parser [13] to grammatically tag the content extracted from html document. Some noise like stop words is filtered from the phrases generated. Once list of Noun phrases is finalized, for each generated Noun phrase a vector of formatting features applied on it is created.  If formatting feature is applied on the phrase, then certain weight is assigned to the phrase [18]. In the next step frequency of word and position of word in document are given importance and each noun phrase generated is ranked.

The lists of the generated keyphrases are displayed for user approval. Figure 4 and 5 shows User interface for the topic  extraction tool and Figure 6 shows the list of system generated key phrases ready for user approval.  Input to the system can be either html or docx file.

Figure 4.  User Interface    Figure 5.  Keyphrase Approval from user     Figure 6.  Keyphrase comparison with domain ontology entities     These approved key phrases are compared to the terms present in the domain ontologies. Collection of these terms represents certain topic/subtopic of some subject/domain to which it belongs. For each subtopics, topics, subject and discipline are retrieved. OWL Parser is developed to retrieve parent, child and siblings. Input to parser is collection of keyphrases. Parser parses owl file and retrieves topics/subtopics which contains collection of all or some of those keyphrases as siblings. Figure 6 shows the output. It has been observed while testing that most of the learning materials cover or use concepts from various subjects and domains. Thus it is important at this stage to figure out the topics which actually characterize the document. The relevance of document with respect to topic(s) /subtopic(s) enclosed in the learning material is calculated as Total number of its key terms identified under the topic/subtopic divided by Total number of key terms identified in the entire document. Document can be characterized using the topic with higher relevance. Figure 7 shows topics/subtopics and its relevance in the document.

Figure 7.  Relevance of topic with respect to document content

IV. EVALUATION :RELEVANT VS. EXTRACTED TOPICS In this study, we have randomly collected sample of 200 learning material belonging to different subjects like Operating systems, Database management systems, Data and File structures, Statistics and Total quality management.

The documents were processed by our tool and topics/subtopics along with their corresponding subject domains and significance pertaining to the document were extracted.  Evaluation of this study was done in two phases.

In first phase subject experts were asked to list down the topics/subtopic(s) covered in the document. These topics/subtopic(s) were then compared with system generated output. Topics and subtopics exactly matching, partially matching and topics/subtopics that are not found are then counted. The result of the counting is as shown in figure 8.

Figure 8.  Evaluation Results  The measure used to evaluate the results was the F-score, defined as     In this study, the main concern is that, how many of the suggested topics/subtopics are correct (precision) and how many of the manually assigned topics/subtopics are retrieved (recall). As the proportion of correctly suggested topics is considered equally important as the amount of total topics extracted by the system, ? was assigned the value 1, thus giving precision and recall equal weights.

Precision = Number of topic(s)/subtopic(s) identified correctly by the system / Total topic(s)/subtopic(s) generated by the system.

Recall is calculated as Number of topic(s)/subtopic(s) identified correctly by the system / Number of topic(s)/subtopic(s) identified by the authors.

The results are shown in the next section. In the second phase, agreement of authors was sought for the topic(s)/subtopic(s) that were found by the system but were not listed out by authors in the first phase of evaluation.

Average topics/subtopics extracted by the system is 6.05, while the average topics/subtopics per document listed by subject experts is 3.76.  Figure 9 shows the agree/disagree listing of the topics/subtopics extra found by the tool.

Figure 9.  Sample result of second phase of evalution

V. RESULTS We have evaluated our tool in two ways viz., Strict and Lenient. In strict evaluation, while calculating total number of topics/subtopics identified correctly by the system, we are considering partially matched topics/subtopics as topics/subtopics not found. In lenient evaluation, we are considering partially matched topics/subtopics as fully matched topics/subtopics. Table 1 shows precision, recall and F-score in both the evaluations.

TABLE 1: EVALUATION RESULTS  Strict Evaluation  Precision Recall F-score  0.5057541 0.8015656 0.63       Lenient Evaluation  Precision Recall F-score  0.6273393 0.9609625 0.76  In the evaluation of second phase, it was found that in 82.3% cases authors have agreed with the system generated topics/subtopics which they missed out while listing down the topics.



VI. CONCLUSION Thus, this paper showcases that ontological approach to identify topics from the learning document is an effective strategy. Precision and Recall of our results proves the effectiveness of the tool. Second phase of the evaluation also signifies that at times author is not able to identify the topics or a subtopic from a document if its coverage or relevance is very less in the document. But the system can identify those topics. Identification of the topic relevance can be used as a metadata which can prove useful in presenting the learning documents according to the needs of the learner by intelligent learning management systems.

