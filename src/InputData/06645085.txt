Figure 1:  The Test Data Lifecycle

Abstract?It is estimated that in 2012 most mid-size companies in the USA generate the equivalent data of the US Library of Congress in 1 year. As a company, Wal-Mart creates the equivalent of 50 million filing cabinets worth of data every hour. While these numbers seem incredible, the trend for most companies is an increasing volume of data generation and storage.

Test Data generated by Automatic Test Equipment (ATE) in R&D, manufacturing and Repair environments is no exception to this increased volume of data. The challenge of this enormous amount of Test Data is how to provide people with effective ways to make decisions from it. Data visualization through charts, graphs and reports has been, historically, one of the more effective ways to provide actionable intelligence because humans can readily make decisions based on patterns and comparisons. But as data volume goes up, even this method is reaching its limits. When one starts to combine large datasets like Manufacturing Test Data and Repair Data together, data visualization becomes problematic. More sophisticated algorithmic, machine learning and predictive approaches become critical.

In this paper, we will explore the experiences of using predictive algorithms on "Big Data" from both Manufacturing Test and Repair Test environments in the complex mission critical aerospace industry. By effectively using datasets from different functional areas, we will be looking at applying SPC techniques to answer new questions about the correlation of Repair test data and manufacturing data with the end goal to predict number of returns in the future and minimize product escapes.



I. INTRODUCTION The manufacturing of modern aerospace and medical  device products generates enormous amounts of raw test data.

Some of IntraStage?s customers produce more than 400 Gigabytes of test data in one year. To put this amount of data in perspective, 25 of these types of companies can generate an amount of information equivalent to that which is currently stored in the US Library of Congress (10 TB).  And, today, there are over 10,000 aerospace manufacturing companies in the USA alone.  This is truly BIG Data, and begs the question: what does a company do with Big Data once it has been  compiled?  How will engineers sift through the data to find the evidence to help them make critical business decisions?

And, most importantly: how will Big Data be leveraged to provide a data model that predicts product failures before they?ve even happened?



II. DEFINING BIG DATA IN TEST  A. Volumes of data, Voluminous Sources Big Data in Test comes from all aspects from the products  lifecycle.  This includes but is not limited to: Suppliers R&D NPI Reliability Manufacturing/Rework RMA/Repair Field  There a number of interdependencies on these different types of data.  For example, an issue found in RMA (Returned Materials Authorization) can be reported back into R&D and Manufacturing.  This issue could be related to a bad component found years later in the field, necessitating long- term storage of data.  R&D and/or Manufacturing may need to go back to the supplier to gather test data on parts tested 10 years earlier to fully understand a current issue.  As a product     Figure 2:  As products become more complex, data volume climbs  Figure 1: Lotus 1-2-3 DOS Version  evolves and moves through its life-cycle, it is vital for the stakeholders of the different relevant parties to see the life- cycle as a contiguous series of easily accessible and analyzed events rather than separate, distinct silos.

B. The Profusion of Data Formats Data captured at any stage of the product life cycle can  take on many different types and a virtually infinite number of formats.  This range of data includes but is not limited to:  Manually Entered Data o Paper (including log books, paper  travelers, certificates etc?) o Scanned hard-copy documents in PDF,  TIF, JPG, etc.

o MS Word, Excel files or other proprietary  format Automated Test Equipment (ATE) generated data  o Parametric results XML, Text, CSV, XLS, Binary or other custom formats Databases (SQL Server, Oracle, Access, MySQL ?.)  o Waveform Data 2D, 3D data waveform ?snapshots? typically stored as images or CSV  o Long term Logged data such as Reliability data  Typically stored in large text/CSV files based on cycle or period.

Often contains multiple sensors with a time stamp associated with a number of measurements.

o Binary attachments High Resolution Images or Video Sound or voice files Signature images Other support documents  This list is by no means exhaustive but illustrates the many different types of data that can be and are commonly acquired at the many different sources or stages of a mission-critical product?s life.

C. With Greater Product Complexity Comes Greater Demands for Data  With the design of electronic components and products becoming more sophisticated and complex every day, there is a direct correlation on the amount of data that is acquired during test.  In addition, the cost for storage of data is getting steadily cheaper. With standard, reliable 2TB hard drives currently retailing for less than $100, engineers with vision and foresight feel encouraged to store as much data as  possible, whether or not there is a current and clear plan to analyze and use the data.



III. TYPES OF ANALYSIS: EARLY ANALYSIS TECHNIQUES Data visualization is relatively recent phenomenon.  Tables  were first created during the first century for analyzing tabular data.  But it was not until the 18th and 19th centuries that the first graphs and charts (such as bar charts and histograms) were invented by William Playfair.  Now, in the information age, we have computers with amazing processing power combined with modern graphing capability to produce rich data visualization.

A. The Evolution of Modern Analysis Before computers became commonplace appliances,  desktop analysis was performed using hand written spreadsheets, with the calculations totaled using calculators.

This technique dominated the way information was shared and analyzed.  Fortunately, the amount of data generated and available for analysis was, by and large, relatively limited.  As technology advanced, the ability (and need) to analyze data quickly evolved.  This was the genesis of software packages such as Lotus 1-2-3 (released in 1983) and MS Excel.  These products revolutionized the way data could be stored and analyzed.

Soon, more advanced programs were published with the express purpose of analyzing data in many different ways.

Many of these software packages took advantage of this data    Figure 4:  Typical web-based reports  now in digital format instead of relying on the information worker having to re-enter this data. Products such as Crystal Reports, Active Reports and so forth allowed users to connect to spreadsheets and do far more detailed and custom analysis of their data.  Information consumers all across organizations were able to use their desktop computers to access reports on their network to get to a ?single version of the truth?.

This evolution of reporting tools coincided with the storage of the data becoming far more efficiently stored as well as more readily accessible.  The modern database has been a major enabler of the reporting technologies.  Packages such as MS SQL Server, Oracle, DB2, Access and so forth are able to store far more normalized information than a spreadsheet could ever hope to.  Databases allowed companies to:  Securely store their data in one repository Normalize disparate datasets from different silos Store data more efficiently than flat files or spreadsheets Allow information workers access to this data based on permissions Drastically speed up data retrieval time  These two technologies (Reporting and Database), combined with the rapid growth in internet usage and literacy, changed the way people interacted and consumed data.  Since the mid-1990s, web based reports have been adopted as the de facto medium of data dissemination for sophisticated organizations.  Information workers from C-Level executives, to Ops Managers, to Product Managers, to Test Managers and o have insisted that all of their corporate metrics be viewed using web based reporting.  This explosion in reports being viewed, modified, and shared from anywhere on any device started in the early 2000s and has continued at a feverish pace.

This new paradigm of business analysis has brought an onslaught of new capabilities including:  ? Ad Hoc Web Based Reports  ? OLAP (Online Analytical Processing)  ? Dashboard technologies  o KPI (Key Performance Indicators) o Self-service analysis  ? Automation features such as Subscriptions and Alerts  These modern analysis systems, such as IntraStage, which can store Test Data across all stages of a products? life cycle and allow information workers at any level to visualize the metrics that help them achieve their short- and long-term goals.

To do this effectively software must take full advantage of these very recent technologies.



IV. THE (PREDICTIVE) LINE ON THE HORIZON Now that technology has advanced enough to allow on-  demand and automatic reporting on data generated mere milliseconds before, and alerts such as real-time SPC notifications are being distributed directly to critical users? smart phones or email inboxes, one could assume that we must have solved nearly all of our problems in manufacturing, and the only time a product now gets returned is through an act of god.  This, unfortunately and obviously, is not quite the case.

Although our tools and technologies are getting better and better for analyzing this data, Operations, Engineering and Management are and can count on being inundated with more and more data.   As we have seen, this data is streaming from multiple locations throughout the products? lifecycle.

Hundreds of product lines at many different stages in their lifecycle are being tested against thousands of different metrics at each station, results in millions and millions of attributes with billions and billions of data points.

How can an organization keep up with all of this data?

How do we need to adapt and reshape our current thinking?

These modern database and reporting technologies do a fantastic job at showing us what is happening now, what happened in the past, and we can do some very limited forecasting into the future.  Humans can look at reports, dashboards in Excel or on the Web and, using our eyes, can look for outliers, seek root causes and correlate.

Figure 5  Figure 6:  Variable Clustering  However, what if we could model the future far more effectively?  What if we could use data from one process (like RMA) to determine the outcome of another process (like Manufacturing)?

Let?s take, for example, an Engine Control Unit (ECU) on a modern jet engine.  Consider that this component undergoes an incredibly thorough amount of testing in Manufacturing.  The ECU is recognized as one of the most complicated and critical pieces of equipment on a modern airliner.  Like all man-made artifacts, failure can be expected during its lifecycle.  If and when it fails it will be sent to a RMA facility where it will typically undergo the same types of testing, such as ?Final ATP,? that was originally performed in manufacturing.

Thus, for every part that fails, there will be at least two records of test-one record from manufacturing and one record from RMA.  Date and times of these tests can and often will be separated by months or years, particularly in high-cost, high- reliability aerospace products. The equipment used for test will also tend to differ in instrument composition and human operators.  In addition, some of the testing variables, limits, attributes, etc. may have changed over the years. However, most of the key measurements will still be present on both datasets.  And it is this commonality and correlation of key, critical measurements that allows the visionary engineer to derive the next level of analysis from the data-analysis that allows the organization to be proactive about potential quality issues instead of reacting to disasters.

At IntraStage we are studying machine learning techniques to increase the accuracy of predicting failures in the field?

What if we could say that certain serial numbers of ECUs that left the manufacturing facility after passing through Final ATP were five times more likely to fail in the field over the first 10 years of life?  What if one of these ECUs was on Air Force One?

Our process includes taking a grouping of 10 failures from RMA and automatically capturing all the test results for the RMA?d product.  Once we have identified and compiled the RMA data, our next step is to go back to the original Manufacturing data for those 10 failures.

Now, we cluster the different attributes for our manufacturing data for those serial numbers that failed.  By  looking for the biggest clusters (like Work Order or Station or Operator or Lot Number or Component Supplier ID) we can visually identify the most likely underlying causes of failure.

While it?s easy for a human eye to look at one or a few attributes, the power of automated reporting and analytics allows data mining specialists to cluster off of multiple variables.

Creating this predictive data model is dependent upon several variables.  One bedrock requirement is to have normalized, scrubbed data from serialized components, including a set of components that failed (of course, products that went through a formal RMA process would provide more robust data). In addition, if similar testing is done in both RMA and manufacturing (for instance, most RMA departments in Aerospace and Medical Devices have the same Automated Testing Equipment as the manufacturing line-and the products are run through that ATE as part of the RMA acceptance test), the data model is more robust. Having data from serialized products that failed in the field and having access to the original manufacturing data allows us to build a prototype predictive model. It?s also useful to have data that is as attribute-rich as possible.

However, with more sets of failure data, the model will evolve and become more accurate.  Thus, it?s necessary to iterate the model multiple times. Each new iteration would include new RMA serial numbers and corresponding manufacturing data for those serial numbers.  As the data fed into the model grows, the data mining personnel will have to tweak the model to account for each RMA iteration?s failures.

Start with control group of known failures. The goal then is to tune the model and you believe it to be fairly accurate for the failures you have, then you can take that model and run it against the other manufacturing data to see where the probabilities of failure lie.

And once that data model has been tweaked and refined, significant value can be recognized for the organization. The analysts or engineers responsible for the predictive model can provide forecasts of future failures to business unit owners. By posing the question: ?These certain products have not failed yet, but are at risk of failing? and having a solid justification for that probability of failure, the engineer/analyst can affect real    change and give executives the information necessary to prevent disastrous product failures in the field.

The phrase ?Big Data? is relatively new, but the task of analyzing and utilizing ever-increasing amounts of information is a challenge that has been with the medical device and  aerospace industries for years.  And dealing with that challenge will only grow in importance in the future.  Failures will always happen. Recalls will always happen. But developing an early-warning mechanism via machine learning that can ameliorate the risk of disaster will become of paramount importance.

