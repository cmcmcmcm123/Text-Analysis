Improving the Performance of a Parallel Data Mining  Algorithm using Communication Message Scheduling

Abstract-In this paper, we propose several strategies for  communication scheduling that can help increase the speed of parallel data mining algorithm. The proposed method based on the smart scheduling that enable parallel data mining application to fully utilize the interconnection infrastructure for communication. We compare the proposed strategies with a non communication scheduling algorithm on 2 to 32 cores multicore cluster systems using simulation. The experimental results show that a significantly performance improvement can be gained when a smart communication scheduling is included in the data mining algorithm.



I. INTRODUCTION AND RELATED WORKS  One of the key challenges that human society are facing is how to extract the meaning out of the massive amount of data being produced and collected every day. Data mining technique is frequently used to extract novel significant or interesting knowledge that is implicit in these large volumes of data. Association rule mining [1], [2] is one of the most important techniques in data mining. This technique is used to extract significant patterns from transaction databases and generates rules used in many decision support systems.

Anyway, the mining of large volumes of data is very compute intensive. Therefore, parallel computing is used to improve the performance of association rule mining applications.

There are three broad strategies for parallelizing association rule mining algorithms [3]. Firstly, task parallelism (or control parallelism) is exploited by having each process executes different operations on the database. Secondly, SPMD parallelism can be employed by having a set of processes executes the same algorithm in parallel on difference partitions of the database [4], [5], [6]. Thirdly, independent parallelism is exploited when processes are executed in parallel in an independent way; generally each process has access to the whole database [10].

All of these strategies must be implemented on the parallel computing systems. Among the most popular is shared memory multiprocessor system [6] and distributed memory multicomputer system [5], [7]. Most of the works address how an association rule mining algorithm will perform on an ideal distributed system. Nevertheless, parallel data mining applications usually generates a lot of communication messages. Thus, a proper optimization of these communications on an infrastructure has a potential to  substantially increase the performance of a parallel data mining application.

In this paper, we propose that the proper scheduling of communication message that consider the right sequence of message transmission, message size, and interconnection network utilization will yield a great performance enhancement to parallel data mining application. The parallel FI-growth data mining is analyzed and used to demonstrate the proposed concept. Using simulation, it can be shown that by properly schedule the communication message and interconnection switch usage in data mining applications, substantial improvement in performance can be attained.

The rest of the paper is organized as follows: Section 2 provides the system model. Section 3 presents the designing the scheduling strategies. Section 4 describes the experiments and discusses the results. Finally, Section 5 concludes the paper and discusses some future development.



II. SYSTEM MODEL  In this paper, the parallel FI-growth program is modeled using a directed acyclic graph (DAG). Given a set of tasks T = {T0, T1, ... ,Tn} to be executed, a directed acyclic graph can be defined as G= (V, E), where V is a set of nodes {v1,v2, ?,vn} and E is the set of directed edges {eij}. The edge eij in the DAG connecting nodes vi and vj represents the communication and precedence constraints among the nodes. The weight of an edge denoted by W(eij) is the communication cost of the edge.

(See the task graph in Fig. 1). A node vi in the DAG corresponds to task Ti. Each node of the task graph has a weight W(vi) that represents the computing cost.

In this application model, we assume that a task is an indivisible unit of computation, and that a processor executes one task at a time. When all input data to a task have arrived to it, the task can be started to execution.

To express resource contentions, we assume that only a limited number of communications can pass from the processor into the network and from the network into the processor at one communication in time.

Figure 1. The parallel FI-growth task graph (left) and Machine model (right)  In this work, we use a multicore cluster system as the target  architecture. A multicore cluster system is a set machine M = {m1, m2, ?, m|M|} of |M| machines and each machine mi include a set processor Pi = {p1, p2, ?, p|pi|} of |Pi|  processor.

Each processor pj of machine mi has multiple core Ci = {c1, c2, ?, c|Cij|} of  |Cij| cores.

In Fig. 1, multicore cluster system is a tree-structure with cores (c) as leaves, processors (p) as intermediate nodes being a parent for cores, machines (m) as intermediate nodes combining processors and the entire machine or system (S) as root node. Let S = (V, E, WV, WE), where V = {v1, v2, ?, vn} is a set of nodes, and E is a set of undirected edges. V represents the set of cores in the system, and an edge eij ? E represent communication link between core vi and vj. WV(vi) and WE(eij) is the computational cost of core vi and the communication cost between core vi and vj, respectively.

In this paper, a multi-core cluster system will have a hierarchy of communication networks. First, an inter-core communication using the shared-cache between cores on the same processor. Second, an inter-processor communication using shared-memory between cores on the different processor.

Finally, an inter-machine communication based on message passing between machines. In this work, we assume that the bandwidth between cores is 20 GBps, bandwidth between processors is 10 GBps, and bandwidth between machines is 1 GBps. It is assumed that the number of processors and the number of tasks are static and known beforehand.  In this paper, we use an all-to-all personalized communication to illustrate that a proper communication scheduling of FI-growth data mining can substantially increase the speed of the application.



III. PROPOSED COMMUNICATION SCHEDULING CONCEPT  In the following subsection, various scheduling strategies are proposed for parallel FI-growth data mining. Then, their impacts on application performance are studied using simulation.

A. Non-Bandwidth Scheduling In this work, we assume that each task will have all its  communication messages queued prior to the real transmission.

Thus, a schedule is created for communication task by assigning them a priority. We proposed three ways of determining the priority of a node as follows.

L-Max: The task with maximum message size or maximum communication weight, max(W(eij)), will be scheduled first.

L-Min: The task with minimum message size or minimum communication weight, min(W(eij)), will be scheduled first.

L-MaxMin: The task with maximum message size will be scheduled first followed by a task with minimum message size.

Maximum message size and minimum message size will be scheduled alternately.

B. Bandwidth Scheduling In order to further enhance the scheduling strategy for  parallel FI-growth application, the communication bandwidth utilization of the interconnection network is considered. In general, interconnection network in a cluster usually consists of a switch fabric and a network links to computation node with a certain bandwidth such as 1Gbps. When more than one communication sessions are initiated to a single target node, the switch to node link bandwidth will be shared. The bandwidth is then reduced and more latency is inserted into the computation.

To avoid this problem, we proposed that the communication should be schedule. In our scheme, the execution consists of two step; nodes partitioning and communication scheduling.

These two steps execute alternately. For each step, the execution will proceed as follows:  Partitioning step: A set of n node is divided into two balanced partitions and then recursively divide the two partitions until the number of node of each partition is one.

Communication step: In each partitioning step, all nodes in each partition will be scheduled to send/receive message to/from all nodes within another partition. The number of communication phases will be equal to number of nodes in the divided partition. Total of communication phases is n-1.  An example of how the partitioning works is given in Fig. 2.

Partition#1 (0 1 2 3)  (4  5 6 7) Comm.

Sender  0 1 2 3  4  5 6 7 Phase Receiver 4 5 6 7  0  1 2 3 I 5 6 7 4  1  2 3 0 II 6 7 4 5  2  3 0 1 III 7 4 5 6  3  0 1 2 IV Partition#2 (0 1) (2 3)  (4  5) (6 7) Sender  0 1 2 3  4  5 6 7 Receiver 2 3 0 1  6  7 4 5 V 3 2 1 0  7  6 5 4 VI Partition#3 (0) (1) (2) (3)  (4)  (5) (6) (7) Sender  0 1 2 3  4  5 6 7 Receiver 1 0 3 2  5  4 7 6 VII   Figure 2. Example of results of bandwidth scheduling strategy of 8 nodes                               p  m  p  c c c c  p  m  p  c c c c  S    From the example in Fig. 2, it is a result of bandwidth scheduling strategy of 8 nodes. In Patition#1, there are two partitions, (0, 1, 2, 3) and (4, 5, 6, 7). There are 4 nodes per partition. The number of communication phases is also equal to the number of nodes per partition. Let an order pair (a, b) denotes the communication from node a to node b. Then, the 8 scheduled communication in the first communication phase (Comm. Phase I) is given by a set: {(0,4), (1,5), (2,6), (3,7), (4,0), (5,1), (6,2), (7,3)} and the total number of communication in this phase is 7.

The benefit of our proposed strategy is that the communication between a pair of node is contention free.

Hence, the full utilization of available interconnection bandwidth is ensured. The result is an improved in attainable performance and execution speed of the parallel data mining algorithm.



IV. EXPERIMENTAL EVALUATION  In order to generate the test task graph, we mined a 60 million transaction database using the parallel FI-growth program [8]. We utilized the standard ?IBM synthetic data generator? [9] to synthesize a transaction database. We used 1000 unique items to create 60 million records; each has average transaction length of 10. The communication volume associated with each edge varies from 96 bytes to 768.8 Kbytes.

After the test graph is generated, we can run a scheduling simulator on this task graph using different scheduling strategies. To evaluate these scheduling strategies, we ran all of the proposed strategies on various kind of cluster architecture as listed in Table I.

TABLE I MULTI-CORE VARIATION ARCHITECTURE  Total number of cores  # of machines - # of processors per machine - # of cores per processor  32 1-1-32 , 1-2-16 , 1-16-2 , 1-32-1 , 2-1-16 , 2-2-8 , 2-4-4 , 2-8-2 , 2-16-1 , 4-2-4 , 4-4-2 , 8-2-2 , 16-1-2 , 16-2-1, 32-1-1  16 1-1-16 , 1-2-8 , 1-4-4 , 1-8-2 , 1-16-1 , 2-1-8 , 2-2-4 , 2-4-2 , 2-8-1 , 4-1-4 , 4-2-2 ,  4-4-1 , 8-1-2 , 8-2-1 , 16-1-1 8 1-1-8 , 1-2-4 , 1-4-2 , 1-8-1 , 2-1-4 ,  2-2-2 , 2-4-1 , 4-1-2 , 4-2-1 , 8-1-1 4 1-1-4 , 1-2-2 , 1-4-1 , 2-1-2 , 2-2-1 , 4-1-1 2 1-1-2 , 1-2-1 , 2-1-1   All the scheduling strategies in our simulator have been  written in Java programming language. All the experiments are conducted on PC with Core 2 Duo 1.66 GHz CPU, 2.00 GB memory and hard disk 140 GB. The operating system used is Windows Vista.

Max Min MaxMin L-Max L-Min L-MaxMin  Ru n  Ti m  e ( se  c.)  Scheduling Strategies  2 cores  1-1-2  1-2-1  2-1-1   (a)               Max Min MaxMin L-Max L-Min L-MaxMin  Ru n  Ti m  e ( se  c.)  Scheduling Strategies  4 cores 1-1-4  1-2-2  1-4-1  2-1-2  2-2-1  4-1-1   (b)               Max Min MaxMin L-Max L-Min L-MaxMin  Ru n  Ti m  e ( se  c.)  Scheduling Strategies  8 cores 1-1-8  1-2-4  1-4-2  1-8-1  2-1-4  2-2-2  2-4-1  4-1-2  4-2-1  8-1-1   (c)                Max Min MaxMin L-Max L-Min L-MaxMin  Ru n  Ti m  e ( se  c.)  Scheduling Strategies  16 cores 1-1-16 1-2-8 1-4-4 1-8-2 1-16-1 2-1-8 2-2-4 2-4-2 2-8-1 4-1-4 4-2-2 4-4-1 8-1-2 8-2-1 16-1-1   (d)              Max Min MaxMin L-Max L-Min L-MaxMin  Ru n  Ti m  e ( se  c.)  Scheduling Strategies  32 cores 1-1-32 1-2-16 1-16-2 1-32-1 2-1-16 2-2-8 2-4-4 2-8-2 2-16-1 4-2-4 4-4-2 8-2-2 16-1-2 16-2-1 32-1-1   (e)   Figure 3. Run time of parallel FI-growth of the scheduling variation strategies  using (a) 2 cores, (b) 4 cores, (c) 8 cores, (d) 16 cores, and (e) 32 cores on variation architectures    0.0  0.5  1.0  1.5  2.0  2.5  3.0  1-1-2 1-1-4 1-1-8 1-1-16 1-1-32  Ru n  Ti m  e ( se  c.)  Multi-core variation architecture   Figure 4. Run time of parallel FI-growth of the bandwidth scheduling strategy  on variation architectures    Fig. 3a, 3b, 3c, 3d, and 3e illustrate the run times for six scheduling strategies using 2, 4, 8, 16, and 32 cores, respectively. The results clearly show the use of communication bandwidth scheduling (Max, Min, and MaxMin) result in a substantially lower execution time than the non-bandwidth scheduling (L-Max, L-Min, and L- MaxMin).

Fig. 4 shows the run times for bandwidth scheduling strategies using 2, 4, 8, 16, and 32 cores, respectively. The results show that run-time drops rapidly as the number of processors increase from 2 to 8, and decreases more slowly as additional processors are used. This is caused by the additional communication overhead incurred.



V. CONCLUSION  In this paper, we propose a set of communication scheduling strategies that helps reduce the execution time of parallel FI- growth data mining algorithm. The results show that adding bandwidth scheduling in data mining applications can substantially help improve the performance of the application.

Currently, we are working on integrating the computation and communication scheduling into a unified framework that is applicable to a broader class of knowledge discovery algorithm.

This will result in the potential for the development of fast, scalable data mining application that can handle a massive amount of data.

