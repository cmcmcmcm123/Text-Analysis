Robust and sparse estimation of tensor decompositions

Abstract?We propose novel tensor decomposition methods that advocate both properties of sparsity and robustness to outliers. The sparsity enables us to extract some essential features from a big data that are easily interpretable. The robustness ensures the resistance to outliers that appear commonly in high- dimensional data. We first propose a method that generalizes the ridge regression in M -estimation framework for tensor decompositions. The other approach we propose combines the least absolute deviation (LAD) regression and the least absolute shrinkage operator (LASSO) for the CANDECOMP/PARAFAC (CP) tensor decompositions. We also formulate various robust tensor decomposition methods using different loss functions.

The simulation study shows that our robust-sparse methods outperform other general tensor decomposition methods in the presence of outliers.



I. INTRODUCTION  Tensors, being multi-way arrays in a simple definition, accommodate high-dimensional data sets naturally. Various tensor decompositions based on multilinear models are pow- erful tools to explore a high-dimensional data in many areas of science and engineering [8].

The well-known regularization methods such as ridge re- gression or the LASSO [4] [9] can form a sparse estimation for tensor decompositions, which is highly useful in reducing dimensionality, simplifying visualization, and variable selec- tion [15], [6]. It also improves interpretability of models often decreasing the estimation uncertainty.

Outliers often occurring in multi-dimensional data obscure the model assumptions and add difficulty in data analysis.

Most of the tensor decomposition methods are based on the optimization with a least squares criterion which is severely lacking robustness to outliers. Surprisingly, the use of robust estimators has been largely neglected in the tensor (signal processing) research community. We are aware of only few studies on the robust tensor estimation such as [13], [14], and [3], other than some work in the medical imaging.

Instead of using the least squares estimation which is highly sensitive to outliers, robust alternatives can be obtained with different loss functions such as absolute deviation (L1), least trimmed squares (LTS), or M -estimation (e.g., using Huber?s or Tukey?s biweight) loss function [11].

In Section II-A, we first formulate various robust tensor de- composition methods using different loss functions. In Section II-B and II-C, we propose two robust and sparse methods in the  context of CP tensor decompositions extending our previous work (encouraging only sparsity) [6].

The fist method we propose called the CP alternating ridge M -regression (CPA-RMR) is to minimize the penalized robust objective function formed by an M -estimation loss function (Huber?s ?-functions) with the L2-penalty. This CPA-RMR can be used not only as a stand-alone method but also as a reliable method to facilitate good initializations for any alternating tensor decomposition algorithms.

The second method we propose called the CP alternating LAD-LASSO combines the LAD regression and the LASSO in the context of CP tensor decompositions. To our knowledge, our method is the first that incorporates robustness and sparsity simultaneously in tensor decompositions. Such work can be found recently only in other areas of regression analysis [1].

The simulation study shows that our methods depict good estimation of sparsity of factor matrices and give highly improved performance over the conventional tensor decom- position method especially in the presence of outliers.

Notations: A tensor of order d as a d-way array is denoted by a boldface Euler script letter, A whereas low-order tensors such as matrices by A, vectors by a and scalars by a. For a m ? n matrix A, we write its ith row (resp. column) vector by ai (resp. a?i) i.e., a matrix can be expressed as A = (a?1 . . . a?n) = (a1 . . . am)  ? and aij denotes its (i, j)th element. Let ? ? ?2 (resp. ? ? ?1) denote the ?2-norm (resp.

?1-norm) defined as ?A?22 = Tr(AA?) =  ?  i  ?  j a ij (resp.

?A?1 = ?  i  ?  j |aij |) for any matrix A. Let ? denote the outer product, i.e. a ? b = ab? and (a ? b ? c)ijk = aibjck.

A. Sparse CP decompositions  For simplicity, we illustrate all the methods for 3-way tensors, but extensions to multiway tensors are straightforward.

The CANDECOMP/PARAFAC (CP) decomposition [2], [7] approximates a tensor X ? RI?J?K by a predicted tensor X? consisting of a sum of R ? N+ rank-1 tensors [8]:  X = X?+ E = R ?  r=1  ?ra?r ? b?r ? c?r + E (1)  where a?r ? RI , b?r ? RJ and c?r ? RK for r = 1, . . . , R form the unit-norm column vectors of the factor matrices A ? R  I?R, B ? RJ?R, and C ? RK?R and the tensor E ? R  I?J?K contains the error terms.

Unfolding the tensor X along the first mode yields a I?JK- matrix denoted as X(1)  X(1) = A?(C?B)? +E(1), (2) where ? = diag(?1, . . . , ?R) and E(1) denotes the unfolded matrix of E. Note that the Khatri-Rao product for B ? RJ?R and C ? RK?R, C ? B is a JK ? R matrix C ? B = (  c?1 ? b?1 ? ? ? c?R ? b?R )  , where ? denotes the Kronecker product. The mode-2 and mode-3 unfoldings of the tensor X are obtained similarly. For the notational convenience, we write Z = C ? B, X = X(1), n = JK and m = I so that X is m? n matrix and A is a m?R matrix. Let ?r = ?a?r? taking the scales of the columns of A.

The conventional CP decomposition seeks to minimize  Q(A,B,C) = ?X?AZ??22 = m ?  i=1  n ?  j=1  (xij ? z?j ai)2. (3)  To enforce sparsity, we add the L2-norm penalty as in [6] to find the minimum of the penalized objective function:  m ?  i=1  n ?  j=1  (xij ? z?j ai)2 + ?1?A?qq + ?2?B?qq + ?3?C?qq, (4)  where ? = (?1, ?2, ?3)? and ?i ? 0, i ? {1, 2, 3} denote the fixed penalty (shrinkage, regularization) parameters and q ? {1, 2} is the chosen norm of the penalty. A computationally convenient form of (3) for the alternating algorithm (estimating one factor matrix at a time) is  m ?  i=1  { n ?  j=1  (xij ? z?j ai)2 + ?1?ai?qq }  + ?2?B?qq + ?3?C?qq.

Here we first estimate the factor matrix A keeping other factor matrices (C and B) fixed. Then, finding the minimum A? = (a?1 ? ? ? a?m)? is equivalent to solving the m separate penalized L2-objective functions, for each i = 1, . . . ,m:  a?i = min ai  {  n ?  j=1  (xij ? z?j ai)2 + ?1?ai?qq }  . (5)  With the L2-norm (q = 2) for the penalty, A? has a closed form solution A? = XZ(Z?Z + ?1I)?1. This method is called the CP alternating Ridge Regression (CP-ARR) in [6]. If ?i = 0 ?i, then the approach reduces to the well-used CP alternating least squares (CP-ALS) method. With the L1-norm (q = 1) penalty, the solution to (5) is the LASSO estimate, which leads to our method called the CP alternating LASSO in [6].



II. ROBUST AND SPARSE CP-ALS METHODS  A. Robust CP decompositions  Disregarding outliers in data sets or heavy-tailed errors can mislead the analysis and inference. One of the most popular robust approaches may be to replace the L2-loss function by the L1-loss as in [13], [3] yielding an objective function  QL1(A,B,C) = ?X?AZ??1 = m ?  i=1  n ?  j=1  |xij ? z?j ai|.

However, the L1-loss function is not bounded and large outliers can still have a considerable influence on the esti- mation. To achieve better robustness, we can consider the M - estimation [5] type objective function  Q?(A,B,C) =  m ?  i=1  ??2i  n ?  j=1  ? (xij ? z?j ai  ??i  )  , (6)  where ??i is a preliminary robust scale estimate (to be dis- cussed later) and ? is an even (?(e) = ?(?e)) nondecreasing function for e ? 0. The multiplying factor ??2i is used so that Q?(A,B,C) becomes (3) when ?(e) = e2. A popular robust choice for ?(?) is Huber?s ?-function defined as  ?k(e) =  {  2e  2, for |e| ? k k|e| ? 12k2, for |e| > k  (7)  where k is a user-defined tuning constant that affects robust- ness and efficiency of the method. Note that ?k(e) ? ?(e) = e2 as k ? ?. Thus, the solution to minimizing (6) converges to the CP-ALS solution as k ? ?. With k = 0.7317, 85% asymptotic efficiency is attained for Gaussian errors in the multiple regression model. Since Huber?s ?-function is not bounded, Tukey?s biweight ?-function is often preferred.

However, this ?-function is non-convex and causes difficulty in finding a global minimum in the regression setting.

A high-breakdown estimator can be derived by utilizing the LTS loss function. For the vector of squared residuals r2i (a) = (  r2i1(a), . . . , r in(a)  )? where rij(a) = xij ? z?j a, for i =  1, . . . ,m, j = 1, . . . , n, the LTS objective function is  QLTS(A,B,C) =  m ?  i=1  h ?  j=1  (  r i (ai)  )  j:n (8)  where (r2i (a))1:n ? ? ? ? ? (r2i (a))n:n are the order statistics of the squared residuals and ?n/2? ? h ? n.

B. The CP alternating Ridge M -Regression (CPA-RMR)  We propose a robust and sparse CP tensor decompositions using the alternating ridge M-regression abbreviated as CPA- RMR. The ridge regression inducing sparsity is adjusted to the robust version using the M -regression (hence, called RMR).

The objective function (6) with the L2-norm penalty is m ?  i=1  {  ??2i  n ?  j=1  ? (xij ? z?j ai  ??i  )  +?1?ai?22 }  +?2?B?22+?3?C?22.

For B and C fixed, the minimum A? = (a?1 ? ? ? a?m)? can be found by  a?i = min a  {  ??2i  n ?  j=1  ? (xij ? z?j a  ??i  )  + ?1?a?22 }  (9)  for i = 1, . . . ,m. We center xi and columns of Z to have median zero to keep the results invariant. Furthermore, the columns of Z are scaled to have median absolute deviation (MAD) equal to one. After obtaining ai?s for the standardized variables using the efficient ?iteratively (re)weighted ridge     regression (IWRR) algorithm? explained below, we can trans- form the solutions back to the original scale. The initial value of ai (in the IWRR) is a?  (0) i calculated as the LAD regression  estimate when regressing xi on Z:  a? (0) i = LAD(xi,Z) = argmin  a  n ?  j=1  |xij ? z?j a|.

The scale estimate ??i is computed as MAD of residuals:  ??i = 1.4286 ?median{|rij(a?(0)i )|}nj=1 IWRR algorithm. Let ?(e) = ??(e) and w(e) = ?(e)/e  with the convention that w(e) = 0 for e = 0. For example, if Huber?s ?-function is used in (7), then the respective ?- function is ?k(e) = max[?k,min(k, e)]. By setting the derivatives of the objective function in (9) to zero, shows that a?i solves the following estimating equation:  (Z?WiZ+ 2?1I)a?i = Z ? Wixi (10)  where Wi = diag({wij}n1 ) with wij = w (  rij(a?i)/??i )  . This leads us to iterative computation of solving the weighted ridge normal equations:  (Z?Wi,tZ+ 2?1I)a? (t+1) i = Z  ? Wi,txi (11)  where Wi,t = diag({wij,t}nj=1) and wij,t = w (  rij(a? (t) i )/??i  )  and t indicates the iteration step (until convergence).

Following [11], it can be shown that the objective function  (9) descends at each iteration. Thus, for convex problems (e.g., using Huber?s ?), the IWRR algorithm can be used to find the global minimum. It is important to note that (11) can be solved efficiently as follows. Let  xi,t =  (  W 1/2 i,t xi   )  and Zt,?1 =  (  W 1/2 i,t Z? 2?1I  )  ,  where 0 is a R ? 1 vector of zeros, I denotes an R ? R identity matrix and W1/2i,t is a square-root matrix of Wi,t,  i.e., W1/2i,t = diag({ ? wij,t}nj=1). With these notations, (11)  becomes Z ?  t,?1Zt,?1 a? (t+1) i = Z  ?  t,?1xi,t (12)  which is in line with the conventional normal equations. Con- sequently, a?(t+1)i can be solved efficiently without inverting (nor computing) Z?t,?1Zt,?1 .

The procedure for the CPA-RMR method is the same as the one for the CP alternating LAD-LASSO given in Table I except that a?i, b?i, c?i are computed by the IWRR algorithm.

C. The CP alternating LAD-LASSO method  We propose here another method called the CP alternating LAD-LASSO, which is robust with the L1-penalty inducing sparse (exactly zero) estimates. We note that this method is computationally more demanding than the CPA-RMR due to the L1 objective function. The aim is to minimize the objective function QL1(?) with the L1-norm penalty  m ?  i=1  {  n ?  j=1  |xij ? z?j ai|+ ?1?ai?1 }  + ?2?B?1 + ?3?C?1.

TABLE I CP ALTERNATING LAD-LASSO METHOD.

Input: X, rank R, shrinkage parameters ?1, ?2 and ?3.

0) Initialize B and C by B? and C? using CPA-RMR solution.

1) Set X = X(1), Z = C? ? B?, n = JK and m = I . Compute  A? ? = (a?1 ? ? ? a?m) by solving a?i = LADLASSO(xi,Z, ?1)  (using the LAD-LASSO algorithm.) 2) Set X = X(2) and Z = C? ? A?, n = IK and  m = J . Compute B?? = (b?1 ? ? ? b?m) by solving b?i = LADLASSO(xi,Z, ?2).

3) Set X = X(3), Z = B? ? A?, n = IJ and m = K. Compute C?  ? = (c?1 ? ? ? c?m) by solving c?i = LADLASSO(xi,Z, ?3).

4) Repeat steps 1)?3) until ? = ?X?? X?old?2/?X??2, the relative  change between the current and previous CP fits is small.

The minimum A? = (a?1 ? ? ? a?m)? can be found by  a?i ? LADLASSO(xi,Z, ?1)  = min a  {  n ?  j=1  |xij ? z?j a|+ ?1?a?1 }  (13)  for i = 1, . . . ,m, when B and C are fixed. Similarly as in CPA-RMR algorithm, centering the xi and standardizing the columns of Z are done in computation. The CP Alternating LAD-LASSO method is summarized in Table I with the initialization using the CPA-RMR solution.

LAD-LASSO algorithm. The objective function in (13) is ?n  j=1 |xij ? z?j a| + ?R  r=1 |0 ? ?1ar|, where a = (a1, . . . , aR)  ?. This form illustrates that the LAD-LASSO criterion can be recast as the ordinary LAD criterion for augmented data sets:  x ? i =  (  xi   )  and Z?1 = (  Z  ?1I  )  ,  where 0 above is a R-vector of zeros and I denotes an R?R identity matrix. Then, minimizing (13) is equivalent to solving the ordinary LAD estimation for the augmented data sets, namely a?i = LAD(x?i ,Z?1 ).

D. Selection of the shrinkage parameter  We estimate the shrinkage parameter ? using the Bayesian information criteria (BIC)  BIC(?) = 2N ln ?? + w ? df(?) ? lnN (14)  where N = I ? J ? K , ?? is a scale estimate of the residuals rij = (X(1) ? A?Z??)i,j with Z? = C? ? B? (i = 1, . . . , I , j = 1, . . . , JK), df(?) is the degrees of freedom of the model and w ? 1 is a weight that can be assigned by the user. The default is w = 1 and w =  ? 2 was used  in our simulations. Note that w > 1 favors sparsity. In the case of LASSO, the scale is estimated by ??2 = avei,j{r2ij} where rij = xij ? z??j a?i, for LAD-LASSO ?? = avei.j{|rij |} i.e. the mean absolute deviation estimate, and for RMR ??2 = 1.4286 ?mediani,j{|rij |}, the median absolute deviation estimate. Regarding the CP alternating LASSO and LAD- LASSO, df(?) is defined as the sum of the number of non- zero elements in factor matrices (A?, B? and C?). In calculating the number of non-zeros in a factor matrix (e.g. A? after     normalizing the columns to be of unit norm), the element a?ij is considered to be zero if a?ij ? 10?11. In respect to the ridge (M -)regression, df(?) = I ?Tr{Z?(Z??Z?+?1)?1Z??}+ J ?Tr{Z?2(Z??2 Z?2+?2)?1Z??2 }+K ?Tr(Z?3(Z??3 Z?3+?3)?1Z??3 ), where Z?2 = C?? A? and Z?3 = B?? A?.



III. SIMULATIONS  We consider the CP model whose true noise-free three- way tensor X0 is sparse. The observed three-way tensor is generated as X = X0+E, where X0 =  ?R r=1 ?rar?br?cr, E  is the noise tensor and the rank R is assumed to be known. The accuracy of the obtained estimate X? is calculated by the nor- malized mean squared error NMSE(X?) = ?X0?X??22/?X0?22.

In our simulation we set I = 1000, J = 20, K = 20, and R = 3 with one sparse factor matrix A ? R1000?3 whose half of the elements (1500) are randomly chosen to be zero. The other half of the elements of A are independent random deviates from N(0, 1). The entries of factor matrices B ? R20?3 and C ? R20?3 are independently drawn from the N(0, 1) distribution. The columns of A, B and C are then normalized to have unit length and the values of the loadings are ?1 = 1000, ?2 = 500 and ?3 = 500. We generated M = 50 tensors according to the setup above and first added the noise tensor E ? R1000?20?20 from N(0, 1). Then, the heavy-tailed noise tensor E ? R1000?20?20 from the Cauchy distribution with symmetry center 0 and scale parameter 1/2 is added in place of the normal noise tensor for comparison.

The penalty parameter is selected by minimizing the BIC in (14) with the weight w =  ? 2 over a grid of ? = ?1 = ?2 = ?3  values. Note that an equal penalty parameter ? for each factor matrix was used simply for the computational feasibility. Natu- rally, using different penalty parameters for each factor matrix would be more appropriate yielding improved estimates, since only one factor matrix is sparse in this simulation. However, this would require the BIC evaluation over the 3-dimensional grids of all possible (?1, ?2, ?3) combinations and adds more computational burden. For our proposed CPA-RMR method, we considered three values of ? ? {1, 10, 100} representing mild, medium and hard-shrinkage. A small selection of ? can- didates are used for CPA-RMR, since we used this algorithm only to obtain good initializations for the other (LAD)LASSO methods in the simulation study. In the case of LASSO (resp.

LAD-LASSO) we used ? ? {0.001}?{0.01, . . . , 0.09}. (resp.

? ? {0.01} ? {1.0, 1.5, . . .5.0}.)  The results of the study are summarized in Table II. The RER (recovery rate) is another measure for performance which estimates the rate of correctly classified zero/non-zero elements for a sparse factor matrix (A) It is defined as the sum of entries correctly estimated (classified) zeros/non-zeros divided by the number of all entries (in A). Thus, RER = 1 implies the case of perfect classification.

From Table II, both the CP-ALS and the CP alternating LASSO methods poorly estimate the factor matrices for the tensors with the heavy-tailed Cauchy noise, whereas our robust sparse methods, LAD-LASSO and RMR, show excellent per- formance. For the tensors with the normal noise, all the sparse  TABLE II SIMULATION RESULTS FOR GAUSSIAN NOISE (UPPER TABLE) AND  CAUCHY NOISE (LOWER TABLE)  CP Alt.- Classifying zeros RER Average method correct incorrect NMSE (std) LS 0 0 50 % 0.0685 ( 0.1152) RMR 0 0 50 % 0.0312 (0.0608) LASSO 77.3% 3.7 % 86.8% 0.0057 (0.0015) LAD-LASSO 83.60 8.0 % 87.8% 0.0153 (0.0398)  LS 0 50 % 1.0?107 (7 ?107) RMR 0 0 50 % 0.0487 (0.073) LASSO 61.2 % 53.2% 54.0% 1.0 ? 107 (7 ?107) LAD-LASSO 93.40 % 11.7 % 90.9 % 0.0232 (0.044)  methods (CPA-RMR, LASSO, LAD-LASSO) outperform the conventional CP-ALS method. In terms of NMSE values (for the normal noise) the CPA-RMR reveals 2.19 times better accuracy than CP-ALS, the CP alternating LASSO 12.02 times better, and the CP alternating LAD-LASSO 4.47 times better.

The best (NMSE) performance of the LASSO in the case of Normal noise is expected, since the sum of the squared residuals (in the objective function) is optimal for the Normal distribution. It can be noted that the weight w =  ? 2(= 1.414)  on the penalty term produces the higher value of ? (assigning more zeros) for the LAD-LASSO compared to the LASSO.

