Performance Analysis of Parallel Apriori on  Heterogeneous Nodes

Abstract ? The paper analyzes the performance of parallel Apriori Algorithm on heterogeneous nodes with different datasets and over n processors on a commodity cluster of machines. In the Apriori Algorithm all processes need to synchronize after every pass. If any process is assigned more load than other processes in the system, the slowest process will dictate the speed of the program. It is therefore important to ensure that load is equally balanced among all processes. Memory, speed of the processor and cache play a significant role in the processing capacity of the system. The experiments show that nodes with different configurations affect the performance of the parallel apriori algorithm. In order to maximize the efficiency it is required to balance the data set based on the processing speed of the various nodes present in the cluster.

Keywords - Data Mining, Association Rules, Parallel Mining, Apriori Algorithm,  Load Balancing, Commodity Cluster.



I.  INTRODUCTION Data Mining is the efficient discovery of previously  unknown patterns in large databases. One of the most important problems in data mining [1] is discovering association rules. There has been considerable research in designing parallel algorithms for mining association rules [2] [3] [4]. One of the key features of all the previous algorithms is that they require multiple passes over the database. For disk resident databases, this requires reading the database completely for each pass resulting in a large number of disk I/Os. The effort spent in performing just the I/O would be considerable for large databases and would result in poor response times. Efficiency Improvement[6] over the serial Apriori algorithm [1] [2] and the parallel CD algorithm [3] to generate all significant association rules based on the reduction in the time spent on scanning elements in the data set have been worked on. In this paper, we analyze the performance of the algorithm over n processors on a commodity cluster of machines. The extensive experiments carried out show that the parallel algorithm scales well to the number of processors and also improves on the efficiency by effective load balancing.

A. Problem Decomposition The problem of discovering all association rules can be  decomposed into two sub-problems [1]:  1. Find all sets of items (item-sets) that have transaction support above minimum support. The support for an item-set is the number of transactions that contain the item-set. Item- sets with minimum support are called large item-sets, and all others, small item-sets.

2. Use the large item-sets to generate the desired rules. Here is a straightforward algorithm for this task. For every large item- set l, find all non-empty subsets of l. for every such subset a, output a rule of the form a  (l-a) if the ratio of support(l) to support(a) is at least minconf. Consider all subsets of l to generate rules with multiple consequents.



II. DISCOVERING LARGE ITEM-SETS Algorithms for discovering large item-sets make multiple passes over the data [1] [2]. In the first pass, support of individual items are counted and it is determined which of them are large, i.e., has minimum support. Every subsequent pass starts with a seed set of item-sets that are found to be large in the previous pass. Use this seed set for generating new potentially large item-sets, called candidate item-sets, and count the actual support for these candidate item-sets during the pass over the data. At the end of the pass, determine which of the candidate item-sets are actually large, and they become the seed for the next pass. This process continues until no new large item-sets are found.

A. Apriori Algorithm Figure 1 gives an overview of the Apriori Algorithm [1]  [2], using the notation in Table 1. The first pass of the algorithm simply counts item occurrences to determine the large 1-item-sets. A subsequent pass, say pass k, consists of two phases. First, the large item-sets Lk-1 found in the (k-1)th pass are used to generate the candidate item-sets Ck. Next, the dataset is scanned and the support of candidates in Ck is counted. For fast counting, it is required to efficiently determine the candidates in Ck that are contained in a given transaction t.

TABLE1. NOTATIONS  k-item-set An item-set having k items.

Lk Set of frequent k-item-sets(those with  minimum support).

Ck Set of candidate k-item-sets (potentially  frequent item-sets)   DOI 10.1109/ACT.2009.20     Pi Processor with id i Di The dataset local to the processor Pi Cki The candidate set maintained with the  Processor Pi during the kth Pass   1) L1 = { Large 1-item-sets}; 2) k =2; // k represents the pass number 3) while ( Lk-1 <> 0 ) do 4) begin 5)         Ck = new candidates of size k generated from Lk-1; 6)        forall transactions t ? D do 7)             Increment the count of all candidates in Ck that are  contained in t; 8)        Lk = All candidates in Ck with minimum support; 9)       k = k +1; 10) end 11) Answer = UkLk;  Figure 1: Apriori Algorithm  B. Parallel CD Algorithm The CD Algorithm [3] uses a simple principle of allowing  ?redundant computations in parallel on otherwise idle processors to avoid communication?. The first pass is special.

For all other passes k >1, the algorithm works as follows: 1. Each processor Pi generates the complete Ck, using the complete frequent item-set Lk-1 created at the end of pass k-1.

2. Processor Pi makes a pass over its data partition Di and develops local support counts for candidates in Ck.

3. Processor Pi exchanges local Ck counts with all other processors to develop global Ck counts. Processors are forced to synchronize at this step.

4. Each Processor Pi now computes Lk from Ck.

5. Each Processor Pi independently makes the decision to terminate or continue to the next pass.

In the first pass, each Processor Pi dynamically generates its local candidate item-set C1i depending on the items actually present in its local data partition Di. Hence candidates counted by different processors may not be identical and care must be taken in exchanging the local counts to determine the global C1.

C. Performance Consideration Each Processor Pi makes a pass over its data partition Di  reading one tuple at a time and builds Cki. Since in every pass each Processor has to go through all the items in every transaction, this becomes a compute intensive step. After every pass, processes need to synchronize by exchanging counts. This implies that the slowest process will dictate the speed of the algorithm. Hence the data should be divided in such a way that every processor gets equal amount of work so that the efficiency of the parallel algorithm is maximized.



III. PERFORMANCE ANALYSIS In the serial Apriori and the parallel CD algorithm, for every pass made by a Processor Pi, it requires a scan of all the elements of individual transaction read from the data partition.

This affects the scalability of the algorithm as there can be  cases where a transaction does not contain frequent k-item-set in the k+1 pass. If such transactions can be identified then the algorithms would not read the transaction and move ahead with the next transaction. This would save considerable amount of processor time.

The proposed formulation identifies the transactions that contain frequent k-item-sets in the 1st scan and checks whether a transaction should be scanned when reading the database for subsequent scans. The overhead required is that one has to keep track of the transactions that probably would contain frequent item-sets. But the advantage of saving the transaction scan time would outperform the overhead as the database size increases.

To maximize the efficiency of the parallel algorithm the data is divided equally among the number of processes in the system so that each process can be utilized to its maximum.



IV. EXPERIMENTAL RESULTS To conduct experiments, a pseudo-random data generator  has been developed that generates transactions of random length and elements are also randomly generated within a given range. Data set containing 10 million transactions generated using the above software has been used for the experiments. Each transaction contained data items of variable length from 1 to 15 from an item-set {1?25}. The parallel versions were implemented using the MPICH2 library functions under windows environment. A commodity cluster was created with 4 machines with varying processor speed, random access memory and cache. Table 2 lists the configuration of the system in the commodity cluster.

Figure 2 shows the experimental result of the time taken to generate k-frequent item-sets on homogeneous nodes running 1 and 2 processes in parallel. The results show the scalability of the parallel Apriori Algorithm on homogeneous nodes. In Figure 3 shows the time taken by 4 processes on homogeneous system consisting of 2 dual core machines with the same configuration and compares it with the time taken by the algorithm on 4 processes in the commodity cluster of heterogeneous nodes. All the experiments were performed with a support of 15%. As the data was generated using a pseudo-random data generator, the support was taken to be low so that there could be at-least 3 to 4 frequent-item sets generated.

It is observed from the results of Figure 3 that with heterogeneous nodes the performance of the algorithm is dictated by the slowest node on the system. The load needs to be balanced taking into consideration the processing capacity of the system.

TABLE2. CLUSTER SPECIFICATION  System Processor Speed RAM Cache CORE 2 Duo  E4800 2997 MHz 2037 MB 6144 K  CORE 2 Duo E4800 2997 MHz 2037 MB 6144 K  CORE 2 CPU 4300 1794 MHz 1014 MB 2048K  Pentium D CPU 2793 MHz 502 MB 1024K        Figure 2: Parallel Apriori on Homogeneous nodes     Figure 3: Parallel Apriori on Commodity Cluster with 4 processes

V. CONCLUSION The parallel Apriori algorithm works efficiently with homogeneous nodes by making sure that load is equally  balanced among nodes. When the algorithm runs on a commodity cluster with each node having different processor speed, memory and cache, the performance of the algorithm is affected. The processor with slowest processing capacity in the cluster dictates the speed of the algorithm. Performance of the algorithm can be improved by re-distributing the load in such a way that the utilization of every processor is maximized as per its capability. We are working on a model that will note the processing capability of every node in the commodity cluster and then balance out the workload for better performance.

