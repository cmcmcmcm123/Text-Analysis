Discovery of Association Rules from Data including Missing Values

Abstract?This paper proposes a method that deals with missing values in the discovery of association rules. The method deals with items composed of attributes and attribute values. The method calculates two kinds of support. One is characteristic support and the other is possible support. The former is based on the number of examples that do not include missing values in attributes composing target items. The latter is based on the number of examples that do not include missing values in all attributes. The method extracts all item sets whose characteristic supports are larger than or equal to the predefined threshold. The paper evaluates the proposed method by comparing it with the previous method and verifies the effect of the proposed method.



I. INTRODUCTION  The discovery method of frequent patterns starts from basket analysis of receipts collected from retail businesses. It is possible for the method to efficiently generate candidates of frequent patterns by using the monotonic property of the patterns [1] [2]. The property is called the Apriori property.

Also, it is possible for the method to speedily discover the patterns by devising a storage method for the data [3] [11].

However, the method deals with transactions composed of different items. Here, each transaction corresponds to a receipt.

Each item has two values. One value shows that the item is included in the receipt and the other value shows that the item is not included. On the other hand, in the case of examples composed of some attributes, each attribute can be larger than or equal to three values. If the discovery method discovers frequent patterns from the data, the method requires transfer of the original values to the values composed of two values. That is, the preprocessing generates items composed of attributes and their attribute values. The number of generated items corresponds to the number of attribute values. The preprocessing tends to generate many items. As the number of items is large, the discovery method requires generation of huge candidate patterns. The number of candidate patterns increases exponentially because the method requires checking the combination of items. Therefore, if the discovery method deals with the examples, it is important to decrease the number of candidates.

Also, in the case of the examples, the examples do not always have all attribute values. That is, some attribute values are missing. The values are called missing values. The values are usually preprocessed by the three kinds of methods. The first one gets rid of examples including the missing values.

The second one completes missing values by referring to  distribution of attribute values related to the missing values [5]. The third one regards missing values as specific attribute values. However, the first method cannot use the information of remaining attribute values in examples including the miss- ing values. The second method cannot always infer appropriate attribute values and assigns wrong attribute values to the missing values. The wrong attribute values may discover wrong patterns. The third method may discover patterns whose meaning cannot be interpreted. This is because the missing values are equally dealt with even if the missing values can originally have different meanings. Therefore, it is necessary to efficiently deal with the missing values.

These methods are common preprocessing methods in the field of machine learning and do not always aim at the discovery of frequent patterns. Some methods have been proposed for the discovery of association rules. Ragel [6] completes missing values by using association rules discovered from the data including missing values. Ragel [7] divides a database composed of examples into the valid databases.

Here, the database that does not include missing values is valid database. In the valid database, Ragel [7] redefines the support and the confidence, and discovers association rules based on the support and the confidence. Shen and Chen [9] propose a method that composes association rules and reuses them in order to improve the validity of both association rules and completed missing values. Shintani [10] divides a database including missing values into some databases not including missing values and discovers association rules from the divided databases. Here, the number of examples including the rules, the support of the rules, and the confidence of the rules are larger than or equal to use-defined thresholds.

Othman and Yahia [4] introduce the concept of robustness for association rules and aim to select robust association rules for the completion of the missing values. However, these methods based on the completion have the problem corresponding to the second common preprocessing method. Also, the other methods do not always sufficiently use the property between the attributes and their attribute values in order to efficiently discover the association rules.

Thus, this paper proposes a new method that decreases the number of candidates by using the property between attributes and attribute values. Also, the method sufficiently uses the information of examples including missing values. We can anticipate that the method discovers more valid frequent   DOI 10.1109/CISIS.2009.92    DOI 10.1109/CISIS.2009.92     patterns. We note that we discover the frequent patterns and do not discover association rules in this paper, but we can easily discover the association rules from the frequent patterns by evaluating the confidence of the frequent patterns. Lastly, this paper verifies the effectiveness of the proposed method by comparing it with the previous preprocessing.



II. DISCOVERY OF FREQUENT PATTERNS  A. Expression of items  The original discovery method of frequent patterns deals with transactions such as receipts of the retail fields. The transactions are composed of items that have two values. Here, one value shows that the item is included in a transaction and the other value shows that the item is not included in it.

Therefore, the method cannot deal directly with data composed of some attributes and their attribute values. This is because some attributes are larger than or equal to three values. The data is called tabular structured data hereafter. If we try to discover frequent patterns from the data, it is necessary to divide each attribute into items composed of two values in the preprocessing. For example, an attribute is given ?blood pressure? and it has three attribute values: ?high?, ?normal?, and ?low?. The combinations of the attribute and its attribute values: ?blood pressure: high?, ?blood pressure: normal?, and ?blood pressure: low? are regarded as items composed of two values. In general, if each example of the data t is given by Formula (1), the example is interpreted as items defined by Formula (2). Here, Ai, (i = 1, 2, ? ? ? ,m) is an attribute, aixi is an attribute value of the attribute Ai, m is the number of attribute values composing examples, and the combination of an attribute and an attribute value Ai : aixi is an item.

(a1x1 , a2x2 , ? ? ? , amxm) (1) (A1 : a1x1 , A2 : a2x2 , ? ? ? , Am : amxm) (2)  B. Missing values  First, we note the data as shown in Table I. Here, a specific attribute of each example has at most an attribute value. Also, in this table, ?-? shows a missing value. This table includes missing values corresponding to respective attributes A1 and A2.

TABLE I EXAMPLES INCLUDING MISSING VALUES  A1 A2 t1 a11 a21 t2 a11 - t3 a12 a21 t4 - a22 t5 a12 a22 t6 a13 a23  If two examples t2 and t4 including a missing value are deleted from the data, the frequencies of attribute values a11, a12 and a13 of the attribute A1 are 1, 2, and 1, and the frequencies of attribute values a21, a22, and a23 of the attribute A2 are 2, 1, and 1. Also, the number of examples where t2 and  t4 are deleted is 4. In the case of the attribute values a11, a12, a13, a21, a22, and a23, the supports defined by Formula (3) are 0.25, 0.5, 0.25, 0.5, 0.25, and 0.25, respectively. Therefore, when the minimum support is 0.3, the patterns A1 : a12 and A2 : a21 are extracted as frequent patterns.

supp(p) = f(p) N  (3)  Here, N is the number of examples and f(p) is the number of examples including the pattern p.

On the other hand, we note individual attributes. In the case of the attribute A1, the data includes only a missing value in the example t4. In the case of the attribute A2, the data includes only a missing value in the example t2. In addition, in each case, the number of examples where the missing value is deleted is 5. Therefore, the supports for the attribute values a11, a12, and a13 are 0.4, 0.4, and 0.2. The supports for the attribute values a21, a22, and a23 are 0.4, 0.4, and 0.2. When the minimum support is 0.3, the patterns A1 : a11 and A1 : a12 are extracted as frequent patterns for the attribute A1, and A2 : a21 and A2 : a22 are extracted as frequent patterns for the attribute A2.

The aforementioned examples show that the complete dele- tion of examples including missing values and the partial deletion of examples based on each attribute lead to different results. The latter one uses the information discarded by the former one. We can anticipate that the latter one discovers more valid patterns than the former one does.

Frequent patterns are usually composed of some items.

It is necessary to decide which examples are deleted from the attribute set corresponding to the items. The attribute set is called the attribute pattern hereafter. In the case that the number of attributes is small, it is possible to decide which examples are deleted for all combinations of attributes in advance. However, the number of the combinations increases exponentially as the number of attributes increases. In the case that the number of attributes is large, it is not possible to decide the example subsets corresponding to all the combinations in advance. Therefore, it is necessary to decide which examples are deleted when the attribute pattern is given.

We note the case that an attribute pattern includes all the attributes. The number of the remaining examples in the case is smaller than or equal to the numbers in the case of other at- tribute patterns. This is because the attribute patterns based on all the attributes are a super subset of other attribute patterns.

Therefore, if two supports are defined by Formula (4) and Formula (5), the relationship suppchar(p, P ) ? supppos(p) is satisfied for a pattern p. In the following, suppchar and supppos are called characteristic support and possible support, respectively. Here, P is an attribute pattern, Pall is the attribute pattern composed of all attributes, and n() is a function that calculates the number of examples not including missing values in the attribute pattern P .

suppchar(p, P ) = f(p) n(P )  (4)     supppos(p) = f(p)  n(Pall) (5)  The characteristic support uses more examples than the possible support does in order to evaluate the support. The characteristic support is a more valid criterion. Thus, this paper tries to discover patterns whose characteristic supports are larger than or equal to the minimum characteristic support.

The patterns are a kind of frequent patterns and are called characteristic patterns hereafter. Also, this paper calls the patterns whose possible supports are larger than or equal to the minimum characteristic support possible patterns.

If an attribute is added to an attribute pattern, examples including new missing values may be added. That is, the value of the denominator in the characteristic support may decrease.

The monotonic property is not always satisfied in the case of the characteristic support. For example, we note the following example. In the case of the attribute pattern (A1, A2), 100 examples are remaining and 27 examples include the attribute value set (a11, a21). Also, the former examples include 10 missing values in the attribute A3 and the latter examples have the attribute value a31 in the attribute A3. Then, in the case of the attribute pattern (A1, A2), the characteristic support in the pattern (A1 : a11, A2 : a21) is 0.27 (= 27100 ). In the case of the attribute pattern (A1, A2, A3), the characteristic support in the pattern (A1 : a11, A2 : a21, A3 : a31) is 0.3 (= 2790 ). The monotonic property is unsatisfied in this example.

On the other hand, the possible support satisfies the mono- tonic property, because the denominator in the possible support is fixed. The possible support of a pattern gives the upper bound of characteristic supports given by super patterns of the pattern. Therefore, if the possible support is smaller than the minimum characteristic support, the characteristic supports of the super patterns are smaller than the minimum characteristic support. That is, the super patterns do not become character- istic patterns. It is not necessary to expand the pattern whose possible support is smaller than the minimum characteristic support. To the contrary, if the possible support of the pattern is larger than or equal to the minimum characteristic support, its supper patterns may become characteristic patterns. It is necessary to keep the pattern even if the pattern is not extracted as a characteristic pattern. In the following, the pattern whose possible support is larger than or equal to the minimum characteristic support is called a possible pattern.

According to these discussions, in the evaluation of the patterns, it is necessary to evaluate the patterns by both the characteristic support and possible support. That is, if the characteristic support of a pattern is larger than or equal to the minimum characteristic support, the pattern is extracted as a characteristic pattern. Otherwise, it is necessary to evaluate the possible support of the pattern. If its possible support is larger than or equal to the minimum characteristic support, the pattern is kept as the seed of the super patterns. We can efficiently discover all characteristic patterns by the two-step evaluations.

C. Generation of candidate patterns  The discovery method of frequent patterns can discover frequent patterns from the tabular structured data by using the expression of the items as shown in Section II-B. The expression requires the generation of items corresponding to each attribute value. The number of items tends to be huge and the amount of calculation tends to be huge. On the other hand, the discovery method repeats the generation of candidate patterns and the calculation of their frequencies. It is important to decrease the number of the candidate patterns. We note the property of the tabular structured data. In the data, each attribute has at most an attribute value. Each item set does not include items whose attributes are equal to each other. It is not necessary for the discovery method to evaluate a candidate pattern such that attributes of items composing the pattern are equal to each other. The discovery method can judge that the pattern is not a characteristic pattern without calculating its frequency.

Sakurai et al. [8] proposed the attribute constraint based on the property of the tabular structured data in the sequential pattern discovery and proposed the discovery method incor- porating the constraint. Also, Sakurai et al. [8] verified the effectiveness of the method by using medical examination data. However, the method judges whether items composing a candidate pattern have the same attribute when the candidate pattern is generated, and it deletes the candidate pattern including the same attribute. The method requires judging the attribute constraint, even if the judgment can be performed much more speedily than the calculation of the frequency can. Thus, this paper proposes a method without judging the attribute constraint.

First, this section explains the outline of the method. The method generates attribute patterns by combining with at- tributes corresponding to items. Also, the method generates attribute value sets by combining with attribute values corre- sponding to attributes. Here, the combinations are performed for different attributes. This two-step generation method can generate all candidate patterns composed of different attributes without judging the attribute constraint. The attribute value set is called the attribute value pattern hereafter.

Next, a more concrete method is explained in the following.

We can assume that the attribute and the attribute values are arranged in a criterion such as the alphabetic order without losing generality. Thus, we assume Ai < Aj , aik1 < aik2 , and aix1 < ajx2 for i < j and k1 < k2. The method can compose all attribute patterns including two attributes by combining the attribute Ai with the other attribute Aj . Figure 1 shows an example of the combinations. In this figure, the method generates the attribute patterns (A1, A2), (A1, A3), ? ? ?, (A1, Am), (A2, A3) ? ? ?, and (Am?1, Am) in order. Also, the method can compose all attribute patterns including three attributes by combining the attribute pattern (Ai, Ap) with the other attribute pattern (Ai, Aq), where i < p < q. The attribute patterns (Ai, Ap) and (Ai, Aq) have the common attribute Ai. For example, the method generates an attribute pattern     (A1, A2, A3) by combining (A1, A2) with (A1, A3). Similarly, the method generates the attribute patterns (A1, A2, A4), ? ? ?, (A1, A2, Am), (A2, A3, A4), ? ? ?, and (Am?2, Am?1, Am) in order.

Fig. 1. Generation of candidate patterns  In general, the method generates all attribute patterns includ- ing r attributes by combining two attribute patterns including (r ? 1) attributes. These attribute patterns including (r ? 1) attributes include the common (r ? 2) attributes and the common attributes are arranged before remaining attributes.

Figure 2 shows an outline of the generation of an attribute pattern. In this figure, each circle shows an attribute and designs on the circle identify the kind of attributes.

Fig. 2. Generation of an attribute pattern  We note the generation of attribute value patterns. The generation is based on processes similar to the aforemen- tioned generation of attribute patterns. That is, the method generates all attribute value patterns including r attribute values by combining two attribute value patterns including (r?1) attribute values. These attribute value patterns including (r ? 1) attribute values have the (r ? 2) common attribute values and the common attribute values are arranged before remaining attribute values. For example, the method gener- ates the attribute value pattern (a11, a21) by combining the attribute value a11 with the other attribute value a21. Similarly, the method generates the attribute value patterns (a11, a22), ? ? ?, (a11, a2m2), (a12, a21), ? ? ? in order as shown in Figure 1. Also, the method generates (a11, a21, a31) by combining (a11, a21) with (a11, a31). It generates the attribute value  patterns (a11, a21, a31), (a11, a21, a32), ? ? ? (a11, a21, a3m3), (a11, a22, a31) in order.

Lastly, we note the method that increases the number of items. The method has two kinds of strategy. The first strategy is the width-based method and generates candidate patterns whose numbers of items are small in order. The other strategy is the depth-based method and generates candidate patterns related to specific items or item sets in order. In order to decide the strategy, we note the attribute value patterns included in the same attribute pattern. If the tabular structured data is scanned for the attribute pattern, the frequencies of the attribute value patterns are calculated and the frequency of examples not including missing values for the attribute pattern is calculated simultaneously. Also, if possible patterns are given for all attribute patterns composed of the common attributes and a different attribute, the candidate patterns are generated by combining two possible patterns without generating redundant candidate patterns. Here, the number of items composing the candidate patterns is equal to the number that adds 1 to the number of the possible patterns. We can anticipate that the width-based method easily discovers all characteristic patterns.

On the other hand, in the depth-based strategy, if all characteristic patterns related to specific items or item sets are discovered, the patterns can be discarded. The strategy tends to require smaller memory space than the width-based method.

In the case of the tabular structured data, the number of items tends to be huge and the number of candidate patterns tends to be huge. In order to save the memory space, it is reasonable to use the depth-based strategy to some extent. Therefore, the strategy used in this paper is a mixture of the width- based strategy and the depth-based strategy. First, the strategy checks first candidate patterns including attribute patterns A1, A2, ? ? ?, and Am in order based on the width-based strategy.

Next, the strategy checks higher candidate patterns related to the attribute patterns in order based on the depth-based strategy. Lastly, in the higher candidate patterns, candidate patterns related to a specific attribute pattern are checked in order based on the width-based strategy. That is, the strategy checks candidate patterns including attribute patterns A1, A2, ? ? ?, Am, (A1, A2), (A1, A3), ? ? ?, (A1, Am), (A1, A2, A3), ? ? ?, (A1, A2, Am), ? ? ? in order.

In this section, we do not pay attention to the division of the data. Each attribute value can keep a corresponding subset of the data. It is possible for the division to decrease the amount of calculation. On the other hand, the division requires additional memory space to store the subsets. The division does not always lead to the decrease of the calculation time because of the swapping process of the space. In future work, we will try to verify the effectiveness of the division.

D. Discovery method  According to the discussion in subsection II-B and subsec- tion II-C, the discovery method of characteristic patterns is described as shown in Figure 3. The method inputs both the tabular structured data (DB) composed of N examples and the minimum characteristic support (MinChSp). It outputs all     characteristic patterns. First, the method calculates the number (Mvl) of examples that do not include missing values by using calcMissingV l(). Mvl is used to calculate possible supports. The method discovers first characteristic patterns whose numbers of items are 1. After that, it discovers higher characteristic patterns whose numbers of items are larger than 1.

In the discovery of the first characteristic patterns, the function createStruct() creates the structure that stores the information related to each attribute Ai and the structure is referred to as St. The method stores the attribute Ai as an attribute pattern to St.PtAry and stores each attribute value aij to St.VlPtAry. Here, PtAry and VlPtAry are members of St. Next, the function calcFq() calculates frequencies of both aij and the missing value of Ai by referring to the row of Ai in DB. This function stores the frequency of aij to FqAry and stores the frequency of the missing value to mvl. The method calculates the characteristic support of aij by using the formula: FqAry[j]N?mvl . If the characteristic support is larger than or equal to MinChSp, the function output() outputs the pattern (Ai : aij) corresponding to aij as a characteristic pattern. Otherwise, the method calculates the possible support of aij by using the formula:  FqAry[j] N?Mvl . Then, if the possible  support is larger than or equal to MinChSp, the pattern is kept to generate candidate patterns whose numbers of items are larger. Otherwise, the pattern is deleted from St.V lP tAry.

The calculation of supports and the judgment are repeated for each attribute value pattern. Lastly, the function judgeP t() checks whether the number of remaining attribute patterns is larger than or equal to 1. If at least one attribute pattern remains, the function addQueue() stores the structure to Queue1. Otherwise, the structure is deleted. Owing to these processes, the method can output all first characteristic patterns and can keep all possible patterns in order to generate higher candidate patterns.

In the discovery of the higher characteristic patterns, the function pickQueue() picks up one of the structures from Queuei. Here, the structures include the information related to attribute patterns composed of i attributes. The picked-up structure is referred to as tSt1. The method picks up other structures by referring to Queuei. The picked-up structure is referred to as tSt2. The function createSt() creates the structure that stores the information related to attribute patterns composed of (i + 1) attributes and the structure is referred to as St. The function genAtPt() generates an attribute pattern composed of (i + 1) attributes by combining the attribute pattern of tSt1 with the attribute pattern of tSt2. This function stores the generated attribute pattern to St. We note that Queuei stores the structures related to attribute patterns such that each attribute pattern includes the (i ? 1) common attributes from the top of the pattern. Therefore, this function generates the pattern by adding the last attribute in the pattern of tSt2 to the attribute pattern of tSt1. Next, the function genV lP t() generates attribute value patterns corresponding to the generated attribute pattern by combining the attribute  value patterns of tSt1 with the attribute value patterns of tSt2. The generated attribute value patterns are composed of (i + 1) attribute values. They are stored to St. Here, we note that this function picks up one of the attribute value patterns of tSt1 and picks up all attribute value patterns of tSt2 in order. After that, this function picks up the next attribute value pattern of tSt1. It repeats the pick-up until all attribute value patterns of tSt1 are picked up. Owing to these processes, this function can generate all attribute value patterns. Next, the function calcFq() calculates frequencies of both the generated attribute value patterns and the missing value of the generated attribute pattern by referring to rows of the generated attribute patterns in DB. This function stores the frequencies of the generated attribute patterns to FqAry and stores the frequency of the missing value to mvl. In a process similar to the discovery of first characteristic patterns, the method calculates characteristic supports and possible supports of the generated attribute patterns. The method also outputs characteristic patterns and keeps possible patterns in St. In addition, if at least one attribute patterns remains, St is stored to Queuei+1. Otherwise, St is deleted.

The method starts the growth corresponding to the top structure of Queuei+1 when the growth corresponding to the top structure of Queuei terminates. Therefore, the method can investigate higher characteristic patterns in the order of A1, A2, ? ? ?, and Am?1. We note the attribute Am is not processed.

This is because Am is the last attribute and higher charac- teristic patterns based on Am are still being investigated for other attributes. Also, we note the algorithm does not include the deletion process of discovered patterns. If it is necessary to delete them, the method requires administrating the link relations among higher attribute patterns in the attribute Ai.

The method can delete the discovered patterns in Ai when the method starts the discovery of higher characteristic patterns in the attribute Ai+1.



III. NUMERICAL EXPERIMENTS  A. Data  This paper performs numerical experiments by using syn- thetic data. The data is generated based on the algorithm as shown in Figure 4. That is, the data is composed of examples whose number (trnum) is defined by the user. Each example is composed of attributes whose number (atnum) is user- defined. Also, each attribute is composed of attribute values whose number (atvnum) is user-defined. In addition, missing values are inserted in the data based on the user-defined missing rate (mrate). Here, the missing rate is defined by the formula: mvnumtrnum?atvnum , where mvnum is the number of missing values. trnum, atnum, atvnum, and mrate are input to the algorithm, and the algorithm outputs the tabular structured data as shown in Figure 5.

In these experiments, trnum is 1,000, atnum is 2, 3, 5, and 10, atvnum is 2, 3, 5, and 10, and mrate is 0.1, 0.2, 0.3, 0.4, and 0.5. For each combination of trnum, atnum, atvnum, and mrate, 10 example sets are generated based on the random numbers using different initial value.

//Initialization A = ?m1 Ai; calcMissingV l(A, &Mvl); //Discovery of first characteristic patterns For each attribute Ai ? A  createStruct(&St); St.AtP tAry = Ai; St.V lP tAry = ?; For each attribute value aij ? Ai  add aij to St.V lP tAry; calcFq(St, DB, &FqAry, &mvl); j = 0; For each attribute value pattern V lP tj ? St.V lP tAry  csup = FqAry[j] N?mvl ;  If csup ? MinChSp; Then output(St, j); Else psup = FqAry[j]  N?Mvl ; If psup < MinChSp; Then delete V lP tj from St.V lP tAry;  j = j + 1; If judgeP t(St) == true; Then addQueue(St, &Queue1); Else delStruct(St);  //Discovery of higher characteristic patterns i = 1; while(true){  while((tSt1 = pickQueue(Queuei))! = NULL){ For each attribute pattern tSt2 ? Queuei  createStruct(&St); genAtPt(tSt1.AtP tAry, tSt2.AtP tAry, &St.AtP tAry); genV lP t(tSt1.V lP tAry, tSt2.V lP tAry, &St.V lP tAry); calcFq(St, DB, &FqAry, &mvl); j = 0; For each attribute value pattern V lP tj ? St.V lP tAry  csup = FqAry[j] N?mvl ;  If csup ? MinChSp; Then output(St, j); Else psup = FqAry[j]  N?Mvl ; If psup < MinChSp; Then delete V lP tj from St.V lP tAry;  j = j + 1; If judgeP t(St) == true; Then addQueue(St, &Queuei+1); Else delStruct(St);  i = i + 1; } i = i ? 1; If i == 0; Then break;  } end;  Fig. 3. A discovery method of characteristic patterns  B. Method  This paper discovers characteristic patterns from generated example sets based on the proposed method. Also, it discovers frequent patterns based on a previous method. That is, the patterns are discovered from generated example sets where examples including missing values are excluded. They corre- spond to characteristic patterns. This paper uses 0.01, 0.05,  1. Read trnum, atnum, atvnum, and mrate.

2. Set trcnt to 0 and set atcnt to 0.

3. If trcnt is larger than or equal to trnum, terminate this  algorithm.

4. Add trcnt to 1.

5. If atcnt is larger than or equal to atnum, go to step 10.

6. Add atcnt to 1.

7. Generate a random number in the range [0, 1).

8. If the random number is smaller than or equal to mrate,  set a missing value to the attribute value corresponding to trcnt and atcnt. Otherwise, generate another random number, select a value from {1, 2, ? ? ? , atvnum} based on the random number with equal probability, and set the value to the attribute value.

9. Return to step 5.

10. Set atcnt to 0 and return to step 3.

Fig. 4. A generation method of synthetic data  Fig. 5. An example of the synthetic data  0.1, and 0.2 as the minimum characteristic support and the minimum support (MinSp). Also, it verifies the effectiveness of the proposed method by evaluating the method from three viewpoints. First, the number of examples used in order to discover patterns is evaluated. Second, the difference of discovered patterns between the proposed method and the pre- vious method is evaluated. Third, the number of redundantly stored possible patterns is evaluated.

C. Results  Figure 6 shows parts of the numerical experiments. In each figure, the horizontal axis shows the number of items. In Fig- ure 6(a) and Figure 6(b), the vertical axis shows the number of examples. In Figure 6(c), Figure 6(d), Figure 6(e), and Figure 6(f), the vertical axis shows the number of patterns. Figure 6(a), Figure 6(c), and Figure 6(e) show the result in the case that trnum=1,000, atnum=10, atvnum=2, mrate=0.1, and MinChSp/MinSp=0.01. Figure 6(b), Figure 6(d), and Figure 6(f) show the result in the case that trnum=1,000, atnum=10, atvnum=10, mrate=0.4, and MinChSp/MinSp=0.01. In addition, Figure 6(a) and Figure 6(b) show the relationship between the number of examples that do not include missing values in the attribute patterns of discovered characteristic patterns and the number of items in the discovered patterns.

In these figures, ?Proposed? shows the results of the proposed method and ?Previous? shows the results of the previous method. Figure 6(c) and Figure 6(d) show the difference of both the discovered characteristic patterns and the discovered frequent patterns. In these figures, ?Common? shows the     (a) The number of evaluated examples (the case: t1000a10v2m0.1sup0.01)  (b) The number of evaluated examples (the case: t1000a10v10m0.4sup0.01)  (c) The difference of discovered patterns (the case: t1000a10v2m0.1sup0.01)  (d) The difference of discovered patterns (the case: t1000a10v10m0.4sup0.01)  (e) The number of characteristic patterns and possible patterns (the case: t1000a10v2m0.1sup0.01)  (f) The number of characteristic patterns and possible patterns (the case: t1000a10v10m0.4sup0.01)  Fig. 6. Experimental results  results in the case that both the proposed method and the previous method discover the patterns. ?Proposed only? shows the results in the case that the proposed method discovers them and the previous method does not discover them. ?Previous only? shows the results in the case that the proposed method does not discover them and the previous method discovers them. Figure 6(e) and Figure 6(f) show the relationships of both the number of characteristic patterns and the number of possible patterns that do not include characteristic patterns.

In these figure, ?Char? shows the results in the case of the characteristic patterns and ?Pos? shows the results in the case of the possible patterns.

D. Discussions  1) Number of evaluated examples: We note the results of Figure 6(a) and Figure 6(b). The results show that the previous  method uses much smaller examples than the proposed one in order to discover characteristic patterns. In particular, in the case that the number of items is small and the rate of missing values is high, this trend tends to be apparent. The proposed method can discover the characteristic patterns based on many examples. We can anticipate that the proposed method more validly discovers the patterns. Therefore, the proposed method is more efficient than the previous ones.

2) The difference of discovered patterns: We note Figure 6(c) and Figure 6(d). In the case that the number of attributes is small, the difference of the discovered patterns is slight.

This is because the variation of characteristic patterns is small. On the other hand, in the case that the number of attributes is large, we can confirm that the difference become huge. The results show that the previous method may fail     to discover important characteristic patterns. Therefore, the proposed method is particularly important in the case that the number of attributes is larger.

3) Characteristic patterns and possible patterns: We note the results of Figure 6(e) and Figure 6(f). The results show many possible patterns are discovered in the case that the number of attributes is large and the rate of missing values is high. The proposed method uses the number of examples that do not include missing values in all attributes in order to calculate possible supports. In this case, many examples tend to include missing values in some attributes. The number tends to be huge. Therefore, the possible supports tend to be evaluated as larger than the characteristic supports. We think this is the reason many possible patterns are discovered.

If it is necessary to discover all characteristic patterns, the proposed method needs to keep all possible supports. However, it is important to decrease the number of possible patterns because many possible patterns require much memory space.

The space may give large impacts to the calculation speed.

On the other hand, as the number of attributes is large, the possibility that characteristic patterns including all attributes are discovered tends to be small and the characteristic patterns tend to be composed of only parts of attributes. So, if we aim at the discovery of characteristic patterns whose numbers are smaller than or equal to the user-defined number of attributes, we may be able to estimate smaller possible supports. For example, we arrange attributes in descending order of the fre- quencies of examples including missing values and accumulate the frequencies in the order until the number of attributes arrives at the user-defined maximum number of attributes. In the case that the number of examples that include multiple missing values is small, this method can appropriately estimate the number (n(P?)) of examples that do not include missing values in attribute patterns (P?). Here, P? is composed of attributes whose number is user-defined maximum number. On the other hand, in the case that the number of the examples is large, this method may not be able to estimate it appropriately.

This is because many examples may be doubly counted. The number of examples that does not include the missing values in P? may be smaller than n(Pall). Therefore, it is necessary to apply other methods to evaluate the number in the latter case. We may be able to use sampling methods of attribute patterns. In future work, we will try to consider estimation methods for a smaller number.

4) Generality of the proposed method: Real data sets include many missing values. For example, in the case of a questionnaire, respondents do not always answer all the questions. The unanswered questions become missing values.

Also, in the case of medical field, doctors do not always per- form all tests on all patients and ask them the same questions owing to the restrictions of time and cost. The unperformed tests and the unanswered questions become missing values. In addition, parts of the collected data may be destroyed owing to human errors, breakdown of machines, and poor transmission environments. The destroyed data becomes missing values.

On the other hand, the proposed method deals with missing  values without depending on the application tasks. Therefore, the method can be applied to many application tasks.

In this paper, we showed only the experimental results based on the synthetic data. However, numerical experiments based on real medical data are performed and the results are similar to the results shown in this section. In future work, we are planning to consider whether the characteristic patterns have larger impact. We believe the proposed method efficiently acquires characteristic patterns composed of items from the tabular structured data.



IV. SUMMARY AND FUTURE WORK  In this paper, we proposed a method that discovers charac- teristic patterns from tabular structured data including missing values. The method efficiently uses the information included in the data without deleting examples including missing values or completing missing values. Also, the method generates both attribute patterns and attribute value patterns step-wisely. This paper applies the proposed method to synthetic numerical data and verifies the effectiveness of the proposed method.

In future work, we will try to apply the proposed method to many application fields and to verify the effectiveness of the method. For example, we are planning to apply the method to medical data, financial data, and so on. Also, we will try to consider a method that estimates possible supports more appropriately in order to decrease the number of generated possible patterns. Lastly, we will try to expand the method to cover the sequential data including missing values.

