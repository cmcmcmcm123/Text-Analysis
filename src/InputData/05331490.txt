Mining Generalized Actionable Rules Using Concept Hierarchies

Abstract  A series of mining actionable rule methods have been proposed from various aspects, but the existing models do not incorporate the concept of hierarchy/taxonomy into the mining process and restrict the terms used to build actionable rules to atomic concepts.  In order to resolve this problem, an integrated framework for extracting multiple-level actionable rules with ontology support is proposed so more generalized knowledge from data can be extracted.  This type of generalized rules will contain not only the attribute values contained in data, but also some concepts encoded in a given taxonomy.

Obtaining generalized actionable rules are a necessity since they provide a more general view of the domain.

The proposed framework is based on a breadth-first top-downward model to be developed by extending the existing single-level actionable rule discovery methods.

This framework can improve the quality of the extracted actionable rules in terms of their interestingness and understandability.

1. Introduction  Discovering actionable patterns in a data set has been a focused topic in recent data mining research [6- 10][13-23].  This type of research moves further than the classical data mining methods [1][3][14][26] by not only summarizing the data but also comparing the context of segmentations. Given a historical database, where each record is described by a set of values, an actionable rule is an expression (?X ?Y), ?X and ?Y are a set of attributes with value variation. The intuitive meaning of such a rule (what changes within attribute X), is needed to re-classify some objects from a lower profitability class to a higher one (in attribute Y). An example of such a rule might be that 94% of special need students will learn better and improve his/her grade by one letter grade if their teacher?s office hours are increased to 6 hours. In this case, 94% is called the confidence of the rule.  The left support of the rule ?X ?Y is the percentage of the records that  contain the initial values on both X and Y and its right support is the percentage of records in X and Y after changing the values of both.   The problem of mining actionable rules is to find all rules that satisfy user- specified minimum left support, right support, and confidence thresholds.

Over the years many techniques have been proposed for discovering actionable rules from large collections, such as a domain-dependent way [13], a post-process analyzing rules manner [6] [9-10] [16-19] [23-25], and a domain-independent way [20-21].

Unfortunately, those approaches do not incorporate the concept of hierarchy/taxonomy into the mining process and restrict the terms used to build actionable patterns to a single concept level.  The first issue is that the rules discovered at the atomic concepts level may not provide informative knowledge of the dataset due to specificity.  The second issue is that the rules at the atomic concepts level may not have minimum supports, and thus some significant actionable rules may not be discovered.  To increase the chance of finding significant actionable rules at primitive concept level, the minimum left and right support thresholds must be reduced substantially. By doing this, it may generate too many rules.  To reduce the number of valid rules, one may place higher confidence in the pattern extraction procedure.  Pursuing high confidence usually results in a complex model and overly detailed rules.

In practice, the end users often prefer a simpler and more understandable model with slightly less accuracy.

Therefore, there is a need to extract actionable rules using the information of pre-defined taxonomy over the attribute values.

The concept hierarchies as background knowledge have long been successfully used to help generate a simpler and more meaningful result in information retrieval, association rule mining, etc. [2][5][15]. In many applications, the taxonomy information is either stored implicitly in the database or provided by domain experts [4] and can be used to generalize primitive level concepts to higher level ones. More initiative and informative regulations can be found by mining data at higher abstract concepts. By exploring multiple-level concepts, rules can be generated across different levels  2009 Fifth International Joint Conference on INC, IMS and IDC  DOI 10.1109/NCM.2009.410   2009 Fifth International Joint Conference on INC, IMS and IDC  DOI 10.1109/NCM.2009.410     of concept hierarchy to derive more interesting, concise, simple, and understandable rules.

The most intuitive approach to mine generalized actionable rules is to apply the existing single-level actionable rule mining methods to exam attribute value-pairs at multiple levels of abstraction under the same minimum left support, right support and confidence thresholds.  This approach is simple but it may lead to some undesired results. Since the size of granules at each level of the hierarchy is different, the higher level concepts tend to have higher support and the lower level concepts tend to have lower support.

One is likely to find many strong rules at higher level concepts and those rules have high potential to echo to prior knowledge and expectations. This side effect exists in most of the generalized association rule methods [2][15]. To avoid this drawback, we use the same approach as in [5] -- a different set of thresholds will be used for different levels of abstraction.

In this study, we assume that domain experts will provide a concept hierarchy for each nominal attribute and the Rosetta-type algorithm will be utilized to automatically discretize for continuous attributes. An integrated framework for extracting generalized actionable rules with ontology support is proposed so more generalized knowledge from data can be extracted.  This type of generalized rules will contain not only the attribute values in data, but also some concepts encoded in a given taxonomy. Obtaining such rules is a necessity since they provide a more general view of the domain and the discovered rules are also simpler and more understandable than classical actionable rules. The proposed framework is based on a breadth-first top-downward model to be developed by extending the existing single-level actionable rule discovery method, StrategyGeneratorIII [22].  It is a two-part process: identifying a set of frequent-closed- factor-sets within the different level concepts at different thresholds and generating reclassification rules from these factors. The anti-monotonic property is applied, in order to avoid construction of meaningless factor-sets, by considering if a factor is not frequent then its descendants will not be frequent as well.

This paper is organized as follows. In section 2, the concept of hierarchy is introduced.  An actionable rule and a non-redundant actionable rule are reviewed in sections 3 and 4, respectively.  The generalized actionable rules and the new algorithm are proposed in section 5. This study is concluded in section 6 along with future work.

2. Concept Hierarchy  Let V = {v1, v2, ? , vm} be a set of possible values for attribute A.  Let H be a concept hierarchy for the attribute A, which organizes relationships of values in a tree structure, shown in Figure 1.  In such tree, each level represents a different degree of abstraction. In [2], authors define the concept hierarchy as a partial order set and represent the special-general relationship between concepts as a partial order relation. Given two concepts v and w in a partial order relation P, i.e., (v, w) ?P and v?w where v is a descendant of w and w is an ancestor of v.  For example, the relationship of concepts middleAge  and Adult can be described as middleAge ? Adult, where the concept of Adult is a more general than the concept of middleAge, or that middleAge is a more specific concept than Adult. The value sits at root node representing the most general concept and the leaf node representing the primitive concept. Note that only the leaf values of concept hierarchy are represented in the original database and the attribute name is represented in the root node.

Intuitively, an attribute can be extended to have taxonomy by moving up from bottom to top. By grouping the similar primitive values into a general concept, the new-formed concept will be represented as a parent node of those primitive values.  The domain of the new-formed concept will be identified as the union of their children?s domain.  For example, the domain of Adult can be formed by adding the record ID that contains its children? values: Young, MiddleAge, and Senior.

3. Actionable Rules  Reclassification Rule Mining (RRM) [21-23] and Action Rule Mining (ARM) [7-8], [16-20] both aim to build an actionable model that allows users to perform actions directly.  The structure of reclassification rules is similar to action rules; however, it differs in the following ways.  For RRM, the target of discovery is not pre-determined, while for ARM there is one and only one predetermined target.  RRM aims to discover  Figure 1. Concept hierarchy for age  ?..

MiddleAge ?..

age  Children  Young  18      19 ?30  Adult  Senior     all the patterns Z from a given data set, while ARM discovers only one specific subset of Z.  In addition, attributes are not required to be classified as stable, flexible, or decision attributes in reclassification rule mining.  RRM is more general than ARM.  It is important that the space of all actionable patterns is known.  If practical implementation of discovered actionable rules is too expensive to achieve, an alternative way is needed for searching the space of all patterns to obtain the same reclassification effect.    In this paper, we will focus on reclassification rule type of actionable rules.

3.1. Information System  An information system [12] is used for representing knowledge and is defined as a pair S = (U, A), where U is a nonempty, finite set of objects, and A is a nonempty, finite set of attributes, i.e. a : U?Va is a function for any a ? A, where Va is called the domain of a.   Elements of U are called objects. In this paper, for the purpose of clarity, objects are interpreted as customers.

3.2. Factor-Set  Any set of factors is called a factor-set F.  It is partitioned into constant factors FS and flexible factors FF.   Let us assume that (v1, v2 ? Va ). Then, the expressions (a, v1 v2) are elements of FS? FF.  If v1? v2, then (a, v1 v2) refers to a flexible factor and it denotes the fact that the value of attribute a can be changed from v1 to v2 for a number of objects in S.

Similarly, the term (a, v1 ? v2)(o) means that the attribute value a(o) = v1 can be changed to a(o) = v2 for the object o.  If v1= v2, (a, v1 v2) refers to a constant factor and its value does not change.  A factor-set is defined as any conjunction of factors where each factor is associated with a different attribute, for example, F=(a1, v1 v2), (a2, v1 v2), ?, (an, v1 v2) (o1, o2, ?, om), where (?i ? n)(?j ? n)[(i  j) ?  (ai  aj)] for m objects [20].

3.3. Frequent Factor-Set  A factor-set ? which supports the user specified minimum left support threshold s1 ? 0 and minimum right support threshold s2 ? 0 is said to be frequent.

The collection of all frequent factor-sets supporting these two thresholds is denoted by (s1, s2)= ?1, ?2, ?, ?n .  supL(? ) denotes left support and supR(? )  denotes right support of ?.  A frequent factor-set ? in S is defined as a term  ?= [(a1, ?1? ?1) ? (a2, ?2 ? ?2) ?? ? (ap, ?p? ?p)],  where (?i ? p)(?j ? p)[(i j) ?  (ai  aj)], supL(? ) ? s1, and supR(? ) ? s2 .

The Left Support defines the domain of a factor- set which identifies objects in U on which the factor- set can be applied.  The larger its value, the more interesting the factor-set will be for a user. The left hand side of the factor-set ?,  is defined as PL  = {?1, ?2,?, ?p}.  The domain DomS(PL) is a set of objects in S that exactly match PL.  The left support supL of the factor-set ? is defined as  supL(?) = Card[DomS(PL)] /Card[U].

The Right Support shows how well the factor-set is supported by objects in S. The higher its value, the stronger its effect will be.  The right hand side of the factor-set ?  is defined as  PR = {?1, ?2,?, ?p}.   The domain DomS(PR) is a set of objects in S that exactly match PR. The right support supR of the factor-set ? is defined as  supR(?) = Card[DomS(PR)]/Card[U].

3.4. Reclassification Rule  The set of all candidate rules is constructed from frequent factor-sets.  The number of factors in a factor- set will be called the length of the factor-set.  A factor- set of the length k is referred to as a k-factor-set.  Each rule is extracted through k-factor-set having at least two flexible factors fF, when k ? 2.  A candidate rule whose confidence is greater than the user specified minimum confidence c is called a strong rule and it belongs to (s1, s2, c).

A reclassification rule  in S is defined as a statement  = where , ? , ? , and ?  =?.

The factor-set is called the action premise (antecedent) and  is called the effect (consequent) of the reclassification rule.  Let us assume that  = [[(a1, ?1? ?1) ? (a2, ?2 ? ?2) ?? ? (ap, ?p? ?p)] and  = [(b1, ?1 ? ?1) ? (b2,     ?2 ? ?2) ? ? ?(bq, ?q ? ?q)].  If we say that objects o1, o2 ? U  support the reclassification rule in S, if and only if:  ? (?i ? p) [[ai (o1) = ?i]  ? [ai (o2) = ?i]], ? (?j ? q) [[bj (o1) = ?i] ? [bj(o2) = ?i]].

Statistical significance of a rule  is expressed in terms of left support and right support, denoted as supL( ) and supR( ), respectively.  The left support measure defines the range of the rule, i.e., the proportion of objects which are applicable. The right support measure defines the feasibility of the rule, i.e., the proportion of objects which are role models for others.  Because supports indicate the frequency of the factor-set occurring in the rule, then supL( ) and supR( ) are basically the same as the supports for the factor-set ?, i.e., supL( )=supL(?) and supR( )= supR(?).

The strength of a reclassification rule is characterized by confidence, which is denoted by conf( ).  Statistical significance of the antecedent of a rule  is called antecedent-right-support and antecedent-left-support which are denoted as a-supL( ) and a-supR( ), respectively. a-supL( )  is defined as the number of objects in S which have property ?i, for i = 1, 2, ?, p.  a-supR( ) is defined as the number of objects in S that have property ?j, for j = 1, 2, ?, q.

The confidence of a reclassification rule  in S is defined as conf( ) = [supL( )?supR( )]?[a-supL( )?a- supR( )].

4. Non-redundant Reclassification rules  A set of non-redundant reclassification rules is based on the concept of closed frequent factor-set, which are sufficient to drastically reduce the rule set without information loss [20].

By a closed factor-set in S we mean a term  = [(a1, ?1? ?1) ? (a2, ?2 ? ?2) ?? ? (ap, ?p? ?p)] iff none of its supersets  ? satisfies supL( ) = supL( ?), supR( ) = supR( ?) .

It means that  is not closed if at least one of its immediate supersets has both supports the same as .

By a frequent closed factor-set ?, we mean that   ? = [(a1, ?1? ?1) ? (a2, ?2 ? ?2) ?? ? (ap, ?p? ?p)] is a frequent closed factor-set iff  ? ? , supL(?) ? s1 and supR(?)? s2.

A closed factor-set ? which supports the user specified minimum left support threshold s1 ? 0 and minimum right support threshold s2 ? 0 is said to be a frequent and closed factor-set. supL(? ) denotes left support and supR(? ) denotes right support of ?.  If ? = ?, supL(? )= supL(?) and supR(? )= supR(?).  The collection of all such factor-sets supporting these two thresholds is denoted by (s1, s2)= ?1, ?2, ?, ?n .

Let us first formally define what we mean by a redundant reclassification rule.  Assume i denotes the rule [(X,???)i  (Y,???)i].  1 is more general than rule 2 if 2 is valid, 1 and 2 have the same left/right support and confidence, (X,???)1 ?(X,???)2, and (Y,???)1?(Y,???)2, and it is denoted as 1 ? Let  = { 1,  2, ?  n} be a set of rules that have the same left and right support counts and confidence, i.e., supL( 1) = supL( 2)= ? = supL( n), supR( 1) = supR( 2)= ? = supR( n), and conf( 1) = conf( 2)= ? = conf( n).  m is a redundant rule if n is valid and  n ? m (Y,???)n =(Y,???)m and (X,???)n  (X,???)m, but not (X,???)m (X,???)n, or  (X,???)n =(X,???)m and (Y,???)n (Y,???)m.

Therefore, the non-redundant rules are those that have the shortest antecedent and the longest consequent. A non-redundant reclassification rule  is defined as  : [(X,???)  (Y,???)] is  not redundant iff none of the existed rules  ?: [(X?,?????)  (Y?,?????)] with supL( ) = supL( ?),  supR( ) = supR( ?), conf( ) = conf( ?), satisfies (X?,?????) ? (X, ???) and (Y,???) ? (Y?,?????).

As mentioned in the introduction, some shortcomings in the existing methods for actionable rule mining are due to not incorporating the concept of hierarchy/taxonomy into mining procedure.

GAR( H, S, sl1, sl2,  cl) Input: H ?  a hierarchy concept for every attribute S = (U, A, V) ? an information system, where U are objects, A are  attributes, and V are their values.

sl1 ?  threshold for a minimum Right Support at level l sl2 ?  threshold for a minimum Left Support at level l cl ?  threshold for a minimum Confidence at level l  //Step 1 Find the frequent generalized closed factor-sets: the sets of  factors that have minimum left and right supports at highest level  //Step 1.1 Find the domain of each concept at level l and its support count above the predefined sl1 or sl2.

AV:= Get all distinct attribute-concept-value pairs for all attribute-concept-value pair(a,v) do begin HD(A, CV, L, D):=Get domain of each concept value //CV ? concept value, L ? Level value, 1, 2, 3,?., D ? domain While L ?  -1 do For each concept value If Card(HD) >= min (sl1 , sl2) //qualified attribute-value pairs (a, v) distinctValueQueue:= ? (a,v) end if end for end while end for  //Step 1.2 Find the frequent generalized closed factor-set //Step 1.2.1 1-factor-sets  N:= Get all elements at level  1 from distinctValueQueue M:= Get all elements at level  1 from distinctValueQueue while subLevelDistinctValueQueue changed do for each attribute-value pair (an,vn) do  For each attribute-value pair (am,vm)  do If (an = am) AtomicFactor:= (al, vn?vm) If ( supL(AtomicFactor)? sl1 & supR(AtomicFactor)?sl2 ) atomicFactorQueue:= ?( AtomicFactor)  subLevelDistinctValueQueue:= ?(subLevelDistinctValue) end if end if end for  N:= Get all elements from subLevelDistinctValueQueue M:= Get all elements from subLevelDistinctValueQueue end for end while  //Step 1.2.2 2-factor-sets  F:= Get all atomic factors  from atomicFactorQueue // every factor denotes as f=(a, v?v) n:= F.size         // the total number of primitive factors in F for (i:=0; i++; i>n) do for (j:=(i+1); j++; j>n) do if (fi.a? fj.a ) newCandidate:= {fi ?fj} if ( supL(newCandidate) >= s11 & supR(newCandidate) >=s12 ) //add this new factor-set into the frequent factor-set // queue and mark its length with 2 frequentFactorQueue:= ?( newCandidate)  end if end if end for end for  //Step 1.2.3 Find the frequent k-factor-sets, where k >2.  This step is iteratively generating new k-factor-sets using the frequent (k- 1) factor-sets found in the previous iteration until there are no new frequent factor-sets generated.

P:=Get all frequent (k-1)-factor-sets from frequentFactorQueue n:= the total number of (k-1)-factor-sets  in P For (i:=0; i++; i>n) do If the first (k-2) factors are identical in fi and fj newCandidate:= {fi ?fj} If ( supL(newCandidate) >= s1 & supR(newCandidate) >=s2 ) If all subsets of newCandidate ? frequentFactorQueue //add this new factor-set into the frequent factor-set // queue and mark its length with k frequentFactorQueue:= ?( newCandidate) end if end if end if end for  //Step 1.2.4 Fmax:= get all frequent factor-sets f with max Length from frequentFactorQueue maxLength:= the length of a factor-set in Fmax CompactFactorQueue := ? Fk for (i := (maxLength-1), i- -, i:=1) do Fi:= get all frequent factor-sets f? with length i from frequentFactorQueue for each f? in Fi do if f?.supL > max(? f.supL in Fi+1 ) if f?.supR > max(? f.supR in Fi+1 ) CompactFactorQueue := ? f? end if end if end for end for  //Step 2. Generate strong Rules having the confidence above c.

For each frequent factor-set f in frequentFactorQueue do n := f.length //the length of the frequent factor-set // EQ for elementQueque EQ:= Parse the factor-set into factors and store each factor with level of 999 for the length of  the premise i from 1 to n-1 get next available premise for all factors in EQ Consequence := { f- premise.factor} end for ruleCandidate := [ premise  Consequence] if (conf(ruleCandidate)? c) // add rule the output and place a positive mark results:= ? ruleCandidate mark EQ.level with level i //place a Positive mark end if end for end for print all Reclassification rules from result.

Figure 2. Pseudo-code of algorithm GAR     5. Generalized Reclassification Rules  Authors in [15][5] introduce hierarchy/taxonomy into association rule mining to identify more interesting rules and their results show promise. In this paper, we also studied generalization in mining, but used a different mining paradigm, actionable rule mining.  We developed a similar approach, called Generalized Actionable Rules (GAR), in order to find a concise set of reclassification rules.  The algorithm GAR is a breadth-first approach to mining a set of all frequent generalized closed factor-sets within concept hierarchies from top to lower level abstraction and then constructs reclassification rules straightforward by partitioning the valid factor-sets into a IF-THEN representation.

A generalized closed factor-set G in S is defined as:  G = [(a1, ?1? ?1) ? (a2, ?2 ? ?2) ?? ? (ap, ?p? ?p)] iff G ?  and none of its factors is the ancestor factor of others  A frequent generalized closed factor-set ?G is defined as:  ?G = [(a1, ?1? ?1) ? (a2, ?2 ? ?2) ?? ? (ap, ?p? ?p)] is a frequent generalized closed factor-set at level l iff  ?G ? G, supL(?G) ? sl1 and supR(?G)? sl2, where sl1 and sl2 are the user specified minimum left support  and minimum right support thresholds at level l, respectively.

If a factor occurs rarely, its descendants will be even less frequent. For finding generalized actionable rules, different minimum left support, right support, and confidence thresholds can be specified at different levels.   By doing this, if users want to find actionable rules at relatively lower levels of abstraction, the minimum left and right supports can be set relatively low without compromise, generating many valid uninteresting rules at higher or intermediate levels.

A generalized reclassification rule G in S is defined as a statement  G  = where , ??G, ? , ? =?, and no factor-set in  is  an ancestor of any factor-set in  in the concept hierarchy.

Algorithm GAR consists of two main steps: (1) generate all frequent generalized closed factor-sets, (2) generate strong generalized reclassification rules.

Below is the short description of the algorithm and the pseudo-code of GAR is presented in Figure 2.

5.1. Generate all Frequent Generalized Closed Factor-Sets.

This step computes the frequent factor-sets in the data set through several iterations and the breadth-first top-downward approach is utilized. In each iteration, we generate new candidates from frequent generalized closed factor-sets found in the previous iteration; the left support and right support for each candidate is then computed and tested against the user-specified thresholds for that level.  It examines the highest top level of abstraction to generate frequent closed factor- sets and then progresses deeper into their frequent descendants at lower concept levels. The lower level concepts will be evaluated only when its ancestor has large left and right supports at the corresponding upper level.  End users have the power to gradually decrease the minimum left and right support thresholds at lower levels of abstraction in order to discover rules at different levels.

5.2. Generate Generalized Rules.

A reclassification rule is extracted by partitioning the factor-set W into two non-empty subsets X and Y and represented as X  Y, where (X, Y? W), (X, Y? FS), and X=W-Y.  A level-wise approach is utilized for generating rules, where each level corresponds to the number of factors belonging to the rule premise.  The rule-premise is extended one factor at a time.  It starts with one factor on the rule-premise, such as X.  If the rule confidence is above the predetermined threshold, it is called a strong rule and marked with a positive mark.

The principle of minimum description length is also adopted to identify the general rules.  Therefore, in the next iteration, it generates new candidates from only unmarked rules found in the previous iteration by moving one of the consequence-type factors to the rule premise, now X is a 2-factor-set.  Repeat the previous step until there is only one flexible factor on the rule consequent, such as Y.   By doing this, the resulting set of rules is comprised of those with the shortest length of premise, not all the lengths.

6.   Conclusion  This paper addressed the problem of discovering generalized actionable rules in a dataset.   We investigated the properties of Reclassification Rules, presented the notions of generalized closed factor-sets, generalized reclassification rules, and introduced algorithm GAR.  This new algorithm is based on the concept of frequent generalized closed factor-sets to generate a very concise set of rules. The proposed framework provides more generalized knowledge from data than previous existing methods by incorporating concept hierarchy into mining procedure.  In addition, the quality of the extracted rules in terms of their interestingness and understandability is improved.

Therefore, it would be easier for the user to comprehend the rule result in a timely manner.   In the future, we intend to apply this approach to a large variety of databases and application domains.

7. References  [1] Agrawal, R., Imielinski, T., and Swami, A.: Mining association rules between sets of items in large databases. In: on the Management of Data. P. Buneman and S. Jajodia, Eds.

ACM Press, Washington DC, 1993, pp. 207?216.

[2] Beneditto, M. E. M. D. and Barros, L. N. d.: Using Concept Hierarchies in Knowledge Discovery. A.L.C.

Bazzan and S. Labidi (Eds.): SBIA 2004, LNAI 3171, pp.

255?265, 2004. Springer-Verlag Berlin Heidelberg 2004.

[3] Grzymala-Busse, J.: A new version of the rule induction system LERS. In: Fundamenta Informaticae, 31(1), 1997, pp.

27-39  [4] Han, J., Cai, Y., and Cercone, N.: Data-Driven Discovery of Quantitative Rules in Relational Databases. In: IEEE Trans. Knowledge and Data Eng., vol. 5, pp. 29-40, 1993.

[5] Han, J., and Fu, Y.: Discovery of multiple-level association rules from large databases. In: Proc. of the 21st Int'l Conference on Very Large Databases, Zurich, Switzerland, September 1995.

[6] He, Z.., Xu, X., and Deng, S.: Mining Cluster-Defining Actionable Rules. In: Proceedings of NDBC?04, 2004.

[7] He, Z., Xu, X., Deng, S., and Ma, R.: Mining Action Rules From Scratch. Expert Systems with Applications. Vol.

29, No. 3, 691--699 (2005)  [8] Im, S. and Ra , Z.W.: Action Rule Extraction from a Decision Table: ARED.  In: 17th International Symposium on Methodologies for Intelligent Systems (ISMIS), pp.

160?168. Springer,  Toronto, Canada (2008)  [9] Ling, C. X., Chen, T., Yang, Q., and Chen, J.: Mining Optimal Actions for Intelligent CRM. In: 2002 IEEE IEEE Computer Society, Maebashi City, Japan (2002)  [9] Liu, B., Hsu, W., and Ma, Y.: Identifying Non-actionable Association Rules. In: Proceedings of KDD 2001, pp. 329-- 334. San Francisco, CA, USA (2001)  [10] Pasquier, N., Bastide, Y., Taouil, R., and Lakhal, L.: Discovering Frequent Closed Itemsets for Association Rules.

(ICDT?99), pp 398--416, Jerusalem, Israel (1999)  [11] Pawlak, Z.: Information Systems - Theoretical Foundations. Information Systems Journal. Vol. 6, pp. 205-- 218 (1981)  [12] Piatesky-Shapiro, G. and Matheus, C. J.:  The interestingness of deviations. In: Proceedings of AAA Workshop on Knowledge Discovery in Database, pp. 25--36.

AAAI Press, Menlo Park, CA (1994)  [13] Quinlan, J. R.: C4.5: program for machine learning.

Morgan Kaufmann, 1992.

[14] Ramakrishnsn, S. and Rakesh, A.: Mining Generalized Association Rules. In: Proceedings of ICDM?03, pp. 685- 688. IEEE Computer Society, Florida, USA (2003)  [15] Ra , Z.W. and Tsay, L.-S.: Discovering Extended Action-Rules (System DEAR). In: Proceedings of the IIS'2003 Symposium, Advances in Soft Computing, pp. 293- 300. Springer, Zakopane, Poland (2003)  [16] Ra , Z.W. and Wieczorkowska, A.: Action Rules: How to Increase Profit of a Company. In: Principles of Data Mining and Knowledge Discovery, Proceedings of PKDD'00, LNAI, pp. 587-592. Springer, Lyon, France (2000)  [17] Tsay, L.-S. and Ra , Z.W.: Action Rules Discovery: System DEAR2, Method and Experiments. Journal of Experimental and Theoretical Artificial Intelligence, Vol. 17, No. 1-2, pp. 119-128. Taylor and Francis (2005)  [18] Tsay, L.-S. and Ra , Z.W.: E-Action Rules. In: Lin, T.Y., Xie, Y., Wasilewska, A., and Liau, C.-J. (eds) Foundations of Data Mining, Studies in Computational Intelligence, pp. 261--272. Springer, Berlin / Heidelberg (2007)  [19] Tsay, L.-S. and Ra , Z.W.: Discovering the Concise Set of Actionable Patterns. In: 17th International Symposium on Methodologies for Intelligent Systems (ISMIS), LNAI, Vol.

4994, pp. 169--178. Springer (2008)  [20] Tsay, L.-S., Ras, Z.W., and Im, S.: Reclassification Rules. In: IEEE/ICDM Workshop on Foundations of Data     Mining (FDM 2008), pp. 619--627. IEEE Computer Society, Pisa, Italy (2008)  [21] Tsay, L.-S., and Im, S.: Mining Non-Redundant Reclassification Rules.  In: Proceedings of the Twenty & Other Applications of Applied Intelligent Systems (IEA/AIE'09), Tainan, Taiwan, June 24-27, 2009, 806-815  [22] Wong, R. C.-W. and Fu, A. W.-C.: ISM: Item Selection for Marketing with Cross-selling Considerations. In: the Eighth Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), pp. 431--440. Sydney, Australia (2004)  [23] Yang, Q., Yin, J., Lin, C. X., and Chen, T.: Postprocessing Decision Trees to Extract Actionable Knowledge. In: Proceedings of ICDM?03, pp. 685-688.

IEEE Computer Society, Florida, USA (2003)  [24] Zhang, H., Zhao, Y., Cao, L., and Zhang, C.:  Combined Association Rule Mining. In: Advances in Knowledge Discovery and Data Mining, Proceedings of the PAKDD Conference, Lecture Notes in Computer Science, 5012, pp.

1069-1074. Springer, Antwerp, Belgium (2008)  [25] Zhang, T., Ramakrishnan, R., and Livny, M.: BIRCH: An efficient data clustering method for very large databases.

In: Proceedings of ACM SIGMOD Conference, Montreal, Canada, pp. 103?114.

