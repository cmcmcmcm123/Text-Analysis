201O 2nd International Conforence on Education Technology and Computer (ICETC)

Abstract- In this paper, we present a technique to analyze and correlate the different types of computer log files. Log files are generated from servers and network devices to record operations that occur in the computers and networks. As log files are too enormous to manualize, we develop a tool to maximize accuracy as well as efficiency while high speed processing is the goal. Firstly, we must improve the accuracy by using learning algorithms to classify the normal operations from the abnormal ones such algorithms include tf-idf,  association rules, k-means clustering, and decision tree.

Secondly, we may adapt for less accuracy in order to gain speed for both with and/or without parallel processing techniques. We also construct an adaptive learning algorithm  to update the model. Then we flush out out-of-date model while the logs are being captured and processed. The result can achieve the goal as they can reach about 30-404Yo in real-time processing with nearly zero false positive results.

Keywords-component; Data Mining; Data Stream; Learning  Algorithm; Computer Log Analysis

I. INTRODUCTION  Presently, computer network is the global network. Other than generally uses, there are numerous computer attackers trying to scan ports and finding weak spots to attack the computers. Therefore, most systems will have event recorders called Log which records the events regularly occur or sequency. The example of purposes of log such as many programmers can fmd out bugs, and many administrators can audit the invalid operations etc. Logs can be kept in either human readable format or machine language.

To achieve highest utilization, we must convert them into one common format, analyze them and then come to the conclusion.

Most commercial intrusion analyzer products are currently just signature-based detection. The advantages of this method are high accuracy and high speed. However, it does not work well to handle attack patterns that do not match patterns or zero day attacks.

We adapt the leaming algorithms and join to the signature-based technique which we gain the satisfactory result. We needed the result that had small number of false positives and known zero days attacks.

A. Background

II. THEORETICAL ISSUE  Recently, many analyzers are still based on the use of signature-based detection that uses local database to keep track the anomaly patterns and reduce false alarm like SNORT [1]. Some analyzers use behavior-based technique to detect the new attack but it leads to high false alarm rate [2].

Those who applied leaming algorithm technique to achieve higher accuracy, however most of them focus on network traffic detector [3, 4, and 5]. On the other hand, leaming algorithm technique requires too much time and has many parameters.

B. Related Works  Paul D. et al. [2], introduced new leaming algorithm for network intrusion detection. Their new algorithm builds rare class prediction models for identifying known intrusion and their variations. The algorithm also created anomaly detection schemes for detecting novel attack whose nature is unknown. They use 2 algorithms to work together. First, the PN rule which is a two stage leaming algorithm which based on computing the rules. The first stage tried to discover P? rules that cover most of the intrusion example, then the second stage discover N-rules which eliminate false alarms generated in the fIrst P step. Next, CREDOS which is a novel algorithm that use ripple down rule. It uses over fItting technique with the training data then prunes them to improve generalization capability. The results showed that their algorithm can discover more new intrusions than standard leaming algorithm but there are too many false positives. As a result, the owners suggest using of this system in conjunction with rule-based intrusion detection like SNORT for better result in real environment.

Pingchuan M. [8], used many leaming algorithms to incorporate with log analyzer. The algorithms are divided into 2 parts. The unsupervised learning algorithms like Bayesian clustering and k-means clustering. The supervised learning algorithms, Naive Bayes classifIer, and decision tree. His method started by ftltering the data then converted to dataset. Next, his analyzer sent data to unsupervised algorithms first and take result plug-in to the supervise algorithms. This combination can produce the good results,  978-1-4244-6370-11$26.00 to 2010 IEEE VI-383     but it consumes too much time and can not guarantee for the best result.

C. Theoretical Aspects  To complete the main algorithm, we use many learning algorithms in this paper. We start with Syslog protocol which use collect and centralize log data. Next, we discuss about association rule, tf-idf, k-means clustering, and decision tree.

The Syslog protocol [13], is a standard for forwarding log messages in a computer network. The term Syslog is often used for both the actual Syslog protocol as well as the applications that sending Syslog messages. Syslog is a client/server protocol; the Syslog sender sends a small (less than IKB) textual message to the Syslog receiver. The receiver is commonly called Syslog daemon or Syslog server.

As Syslog can integrate log data from many different types of systems into a central repository.

The association rule learning [14] is a popular and well researched method for discovering interesting relations during variables in large databases. Piatetsky-Shapiro describes analyzing and presenting strong rules discovered in databases using different measures of interestingness. Based on the concept of strong rules, Agrawal et al. introduced association rules for discovering regularities while products in large scale transaction data recorded by point-of-sale (POS) systems in supermarkets, web usage mining, intrusion detection and bioinformatics.

The tf-idf weight [15] or term frequency-inverse document frequency is a weight often used in information retrieval and text mining. This weight a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document. However, it is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. The term count in the given document simply the number of times a given term appears in that document. This count is usually normalized to prevent a bias towards longer documents. That is because they may have a higher term count regardless of the actual importance of that term in the document. We need to give a measure of the importance of the term ti within the particular document 4. Thus we have the term frequency defmed in Equation (1).

containing the term, and then taking the logarithm of that quotient as defined in Equation (2).

Where ?  ?  idJ: = log IDI , I{d : t, E d} 1  IDI is the total number of documents in the corpus  (2)  I{d: I, E d} 1 is number of documents where the term ti appears (that is ni,j :t:. 0). If the term is not in the corpus, this will lead to a division-by-zero. It is common to use 1+ I{d:t E d} 1 then (tf-idif) =tl' xidij, , '.1 tI'.1 ,  A high weight in tf-idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms. The tf-idf value for a term will always be greater than or equal to zero.

In statistics and machine learning, k-means clustering [16] is a method of cluster analysis which aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean. It is similar to the expectation-maximization algorithm for mixtures of Gaussians in that they both attempt to fmd the centers of natural clusters in the data.

Given a set of observations (x},x2, ... ,xn), where each  observation is a d-dimensional real vector, then k-means clustering aims to partition the n observations into k sets (k<n) s={S},S2"",Sk} so as to minimize the within-cluster  sum of squares (WCSS) as you see in Equation (3):  (3)  where i is the mean of Si'  Given an initial set of k means ml (i), . . .  ,mk(l) which may  be specified randomly or by some heuristic, the algorithm proceeds by alternating between two steps:  Assignment step: Assign each observation to the cluster with the closest mean (i.e. partition the observations according to the Voronoi diagram generated by the means).

II" n?.

'.1'.1 =  , .1 "" n .

(1) Update step: Calculate the new means to be the centroid  L-k k.1  Where ni,j is the number of occurrences of the  considered term (ti) in document 4, and the denominator is the sum of number of occurrences of all terms in document 4.

The inverse document frequency is a measure of the general importance of the term. We obtained it by dividing the number of all documents by the number of documents  of the observations in the cluster.

The algorithm is deemed to have converged when the assignments no longer change.

Decision tree learning [16] in data mining and machine learning uses a decision tree as a predictive model which maps observations about an item to conclusions the item's target value. More descriptive names for such tree models are classification trees or regression trees. In these structures,  Vl-384    2010 2nd International Conforence on Education Technology and Computer (ICETC)  leaves represent classifications and branches represent conjunctions of features lead to those classifications.

Decision tree learning is a common method used in data mining. The goal is create a model that predicts the value of a target variable based on several input variables.



III. METHODOLOGY  In computer network, there will be some detect anomaly or suspicious actions which we propose the analyzing a correlating log files. The universal format which we suggest was to centralize logs data, converted log to common log format. After processing through learning algorithms technique combination like association rules, tf-idf, k-means clustering and decision tree; we will be able to classify the abnormal from the normal operations. Then we did translate to easy to understand format for administrators like graph that is potential solution to the problem.

We started correlate logs after collecting log files, and then we setup a table to separate each source, user cache table, result cache table and common table which consist of common field like date/time, source IP Address, source name, destination IP Address, destination name, source port, destination port, etc. We listed the important filed in Table 1 below. We found the first fmal mapped-data in this common table by matching logs from each source with cache user table which contained IP-port sequence or MAC Address within short time. We also put the latest database in cache table for speedy calculation. We got the ready to process data after different sources of input have been processed.

Field Name  Conn_id  Updated  Usemame  Src_ip  Src_mac  DesUp  Service  14:33:34 __ apaome: tie _u U.)li) 10.?2.7.1i9 - - (01 ApE/2010:1t:a pdf/cu7 lliD.pdf HTTP/t.O? _  14:22:2.

-  __ apaclu!: .... l.a!." as:. * 202.146 .214 - - (Ol/Apr/2010:U: i.o=l-.lK.E:n.g._W.3c:.UO{E:n.91roa-=n,aU20T.cllno y).? 8TTP/l.0?  .t.u.c,.ac:.,h/c:d/th/<:>uicU..-_php? _  Figure I. The correlation process  TABLE!. COMMON TABLE FIELD  description  Logs from same connection was group to same conn_id  Date/time from service logs  Usemame of user that authenticate from server  User IP address  User MAC address  Service server IP address  Service name  Msg Message from logs  Score_based _id Score of selected match class  Score Score generated from customized tf-idf algorithm  Result The result which can be good, bad or unknown  reason The reason of result  A. Class Rank Algorithm  The process of learning algorithm, which we called Class Rank algorithm, was started from grouped the logs from same connection into single unit called detection unit. Then, we used signature-based technique with rule classifier to fmd out known good class (or white class) and bad class (or black class). Both classes had set as based class for calculation.

The left over was assigned as unknown class which had to re-calculate score one-by-one using tf-idf algorithm. Next, we did individually compare the qualification whether it was close to the black class. For black class, the score will be range from -1 to o. We also had to calculate score for the qualification of the same unknown class. Then we compared whether it was more close to the white class or not. For this class range, score will be between 0 up to 1. K-means and decision tree were done the class-assigned decision job. If the score was uncertain, k-means and decision tree might decide to put it back to the unknown class. The process was repeatedly done till the rest of unknown class could be all classified. Please refer to Figure 2.

Black 4 - I I ? White  Group A ?

I Nov 10 00:00 ....

I Nov 10 01:10 ....

I I  ?? 3 r::: .:i L Good base + l TF-IDF "'.erolt 7/+/- r;. 1.

JI\ '\,.' I Nov 10 02:00 ... . I  Gmup of event D.o,ion tm  ? (Auociation rule) l Bad base - Bas? on rule A  Figure 2. Analysis algorithm  The cache result was regularly flushed out to keep the resource free and reduced time to process. The system can be summarize as shown in Figure 3.

< Convert to common format > cleaning I  Remove outliner I I Preprocessing module  r log files correlation I I Processing module  ? Move Known Activity to base I Analyze module  L -tf-idf classifier -k-mean dustering -decision tree  rl Cleanup data I I Post processing module Y Convert to selected format I  ( ?  Report module )  VI-385     Figure 3. The flow chart of system  B. Experimental Benchmark  We started the test of algorithm by collected the log files from servers of King Mongkut's University of Technology Thonburi and compared with human verify dataset.

Supposedly, we performed the test 10 times independently. The first 5 tests were constrained by time limited, while the last 5 did not have such limitation. The test could give the accurate percentage as well as false positive rate.

For efficiency, we measured the system load by tested the program and could fmd consumption percentage by using 4 main factors including CPU load, Memory usage, Disk usage and Network usage of host machine.

We balanced between 2 factors: accuracy and efficiency to make reasonable cost system. The learning algorithm combination algorithms were use to increase accuracy. Then, we tuned the parameters of algorithms to get high efficiency at the lowest overhead possible. We also suggest distributed architecture form of this system to increase performance.

The additional usability could increase by construct an adaptive learning algorithm to update the model on the fly while the logs are being captured. The old data was flushed out every interval of time. The new data flow which is high flow rate of data, was processed depend on windows time to make model fresh.



IV. RESULTS  We prepared the prototype with PHP programming language along with MySQL database. We selected PHP because it was easy to write with many support functions.

For database, we selected MySQL because it is one of best free database with high performance. We needed to examine the algorithm. Then we decided to select some type of log to process, which reduce implementation time. The select logs to process were only web server log, DHCP log and radius log. However, the algorithm could support many type of computer logs with filtering support options. We also had written the code with parallel technique option. Then, it could achieve high performance by increase number of processing threads and could run with many servers. We had use only one high performance database server to protect the consistency.

After we ran the prototype of class rank algorithm, we could easily see that it could get satisfaction. It could receive up to 70-80% correct results if we gave enough time to process. Besides, we must think about the computation time.

Therefore, we needed to ignore some target results to reduce time consumption. The result dropped to around 30-40% target results, with nearly real-time situation. The advantage of an algorithm is that it has nearly zero false positive (about <5%). Then, we could rely on most result they provided.

However, results had depended on some initial data because  some parts in process algorithm use k-means cluster technique.

80 - I - 70 -+- Real-time 60 Detection  ? 50- ___ Unlimited time  ? ??- Detection  ..... .....

20 - 10 -  0 RunlD 1 2 3 4 5  Figure 4. Detection Results  The detection results depend on time usage as you see from Figure 4. The result can reach about 70-80% as describe above if we leave system run in no limited time mode. We should think efficiency in real-time environment.

Then, we adjusted to filter some results out to reduce time consumption. Finally, we got target results around 30-40%, with parallel mode using acceptable time which is delay about 5 minutes. We think of this result can acceptable in real-time environment. Although, percent results seem to be low but we can install this system as plus-in to normally signature-based program in real environment. Therefore, some results were captured by the primary detection software and our system is the next filter to capture the missing targets.

2.5 ... ? 2 ? .......... ? ? 1.5  ? -+-Real-time ? ?  Detection If 1 ___ Unlimited time 0.5 Detection  0 RunlD 1 2 3 4 5  Figure 5. False Positive Results  The false positive rate shown in Figure 5 of this algorithm was surprising low. This is the strong point of this algorithm. It could take around only 1-2% of all found targets. We could take some parameters to make the detection results more than above value and time consumption was reduce. Nevertheless, it took too much false positive. Therefore, we adjusted the parameters to keep the false positive low while receive the highest detection possible. Finally, it showed the results as you can see in Figure 4.

VI-386     80 ... - I 70 -- I 60 -+- Real-time Parallel  ? 50 ___ Unlimited time Parallel ? 40 0.. 30 -+- Real-time Single  20 ?Unlimited time Single  10 ? 0 lI( ? )I( ? I RunlD 1 2 3 4 5  Figure 6. System Load  The system load was very high as Figure 6. This load came from parallel technique. We ran multiple threads of process algorithm to accelerate the speed but we had limit resource to test prototype. Therefore, the time consumption was too much than we plan. We use only around 5-8% system load if we ran only one thread.



V. DISCUSSION  Computer networks are dangerous environment today.

Many computer systems integrate operation recorders called Logs or Event Logs. Administrators need a solution for centralizing logs from every computer. In addition, central log system should generate alerts or take some actions the abnormal operations occurring in real-time or near real-time to make the whole system more secure.

A. Achievements  This paper discuses how to collect logs data from devices in the network into central server. Convert them into one common format Then, analyze and interpret data to get the important information from many garbage data. Finally, convert again into the graphical formats for easy understand.

Therefore, administrators can understand easily. The system use the adaptive learning algorithm to process the data stream and data model can change along the time. Therefore, our system flush out the old model when it thinks the model is too old.

This system is target to detect the abnormal activities in computers logs using combination between signatures-based and learning algorithms based technique. The learning algorithms used in this paper were associate rule, tf-idf, k? means clustering, and decision tree. We started to collect the logs to central, removed garbage data and make correlation between them into one common table. After that we applied customized learning algorithms to receive the target results.

We received good response about the results after implementation_ It showed the algorithm could achieve high performance, good accuracy and high efficiency with parallel processing. We got low false positive results but it stilled misses some targets when we needed to use it in real-time processing because we filtered out some target results to reduce time consumption. The overall result was still acceptable when it work in real-time environment We performed the test for 10 times and logically compared the  results with three benchmarks which is detection result, false positive result, and system load.

B. Summary  We ran system on real data environment which divided into training data and test data_ We got expected results. The algorithm could process in limited time processing at an acceptable accuracy. On the other hand, we could increase the accuracy to more than 70% with unlimited time processing. The false positive rate was under 5% in both cases.

Finally, the main target of our system which is making an automatic system could detect problem automatically without human interaction. In conclusion, our proposed prototype system could give you the incredible low work load for administrators about manual logs examination.

