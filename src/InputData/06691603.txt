An NML-based Model Selection Criterion for General Relational Data Modeling

Abstract?Whereas the main interest in most existing data mining approaches has been sequence data on a single type of object, namely attribute data, real-world databases store infor- mation about multiple relationships between various classes of objects. The modeling of these general relational data (GRD) plays an important role in eliciting knowledge across multiple relations. It is not reasonable to directly apply existing modeling methods to GRD, because GRD have statistical properties that distinguish them from attribute data. In this paper, we address the issue of statistical model selection in GRD modeling. From the viewpoint of the minimum description length principle, we propose a new model selection criterion by considering the statistical properties of GRD. We employ the normalized maximum likelihood code-length as a model selection criterion, and provide an asymptotic expansion theorem for its applica- tion to GRD modeling. To demonstrate its use in a critical application, we apply our proposed criterion to the issue of model selection in relational data clustering. An experiment using artificial datasets demonstrates the effectiveness of our technique compared to other criteria, and we also present a brand analysis using real beer-purchase data.

Keywords-relational data; model selection; normalized max- imum likelihood code-length; stochastic block model

I. INTRODUCTION  We are concerned with the modeling of general relational data. In the fields of statistics, machine learning, and data mining, interest is typically focused on sequence data for a single type of object, known as attribute data. How- ever, real-world databases store information about multiple relationships between various classes of objects. Let us consider the example of a purchase records database (Fig.

1). We are interested in two classes of objects: customers and items. The database may not only store the profiles of customers or attributes of items, but also data about the purchase relationship between customers and items. This may sometimes include information regarding friendships among customers. Data that consist of multiple tables on multiple classes of objects are called general relational data (GRD) [1]. A certain customer profile may be strongly correlated with certain attributes of the items they buy.

Customers or items could be divided into a number of clusters to help us understand the purchase behavior of the global customer base. The mining of GRD aims to reveal such hidden patterns, eliciting knowledge across multiple tables.

? ? ? ???  ?  ?????? ???? ??	??? ??	??? ??? ????  ??? ? ? ?? ??? ?  ? ?? ?? ??? ??? ??? ???  ? ??? ???? ??? ?????? ??? ????  ????? ?? ?? ? ??? ??  ?	??? ? ? ? ??? ?  ? ? ?? ???  ? ? ? ???  ?  ?  ?  ?  ?  ?  ?  ? ? ? ?  ? ? ? ?  ? ? ? ?  ???  ?  ?  ?  ? ? ? ???  ?  ?  ?  ?  ?  ?  ?  ? ? ? ? ?  ? ? ? ??? ?  ? ? ? ?  ? ? ? ?  ?  ?  ?  Figure 1. Example of general relational data  Statistical modeling plays an important role in GRD min- ing. The central issue in GRD modeling is the selection of an appropriate statistical model. Statistical model selection determines the optimal probabilistic model for representing the given data from a set of candidate models. Simple models with few degrees of freedom may fail to capture the nature of the data, whereas complex models with many de- grees of freedom may overfit the data, losing their structural significance. To justify the statistical meaning of the selected probabilistic model, it is important to balance this trade-off.

The problem of model selection arises frequently in several practical data mining tasks, such as structure learning in Bayesian networks (e.g., [2]), determination of the number of clusters in clustering (e.g., [3]), and rank determination in matrix factorization (e.g., [4]).

The purpose of this paper is to propose a new model selection method that considers the statistical properties of GRD models, and to empirically demonstrate its advantages over other possible criteria.

In attribute data modeling, many criteria for model selec- tion have been established. Akaike [5] proposed Akaike?s in- formation criterion (AIC), which is founded on information entropy; later, Schwarz [6] stated the Bayesian information criterion (BIC) on the basis of Bayesian statistics. Following these, a number of other criteria have been proposed. Some criteria, such as BIC, have been common ways to deal with the model selection of relational data (e.g., [7]). However,      few model selection criteria suitable for GRD modeling have been studied. Most existing criteria have been derived under the condition that the central limit theorem (CLT) holds uniformly over the attribute data. Such criteria cannot be directly applied to GRD models, because GRD are a mixture of attribute and relational data of various sizes, meaning that the CLT does not hold uniformly over them. It is therefore important to develop criteria that account for the variously sized data and asymptotic properties of GRD.

In this paper, we consider model selection on the basis of the minimum description length (MDL) principle [8], which offers a general model selection framework from an information-theoretical approach. The underlying idea of the MDL is that learning can be equated with data compression. MDL theory asserts that the best model is that which minimizes the total code-length required to encode the given data and the model itself.

In applying MDL-based model selection, the normal- ized maximum likelihood (NML) code-length, which is also known as the stochastic complexity, can be the most ef- fective criterion [9]. NML is justified in the sense that it achieves Shtarkov?s minimax criterion [10], and it has been reported that NML-based model selection is effective in a wide range of learning issues, such as clustering [3], [11], denoising [12], and Bayesian network structure learning [2], [13]. The drawback of NML is that its na??ve computation is generally hard, requiring a number of summations over the whole domain of all variables. This can increase the computational complexity of GRD modeling explosively.

In terms of attribute data modeling, Rissanen [9] provided an asymptotic expansion theorem for NML, and this enables us to calculate the approximate NML. However, Rissanen?s theorem does not apply to GRD modeling, and its asymptotic expression is not appropriate for GRD models. Thus, we propose a new theorem, based on that of Rissanen, to approximate NML for GRD models.

The significance of this paper can be summarized as follows:  1) A new NML-based criterion suitable for GRD model- ing: We propose a new criterion suitable for GRD modeling on the basis of NML. As GRD are a mixture of various-sized attribute and relational datasets, we cannot apply ordinary one-way asymptotic evaluation. In this paper, we deal with the various sizes of GRD, and derive an asymptotic form of NML for GRD models. This enables NML-based model selection in GRD modeling.

2) Application to relational data clustering: As a critical application, we address the task of determining the num- ber of clusters in relational data clustering. We apply our proposed criterion to the selection of the stochastic block model (SBM) [14], and derive a new efficient method for calculating the NML code-length for SBM to determine the optimal number of clusters. Using artificial datasets, we experimentally demonstrate that our criterion is more  effective than other possible criteria. In addition, we demon- strate the validity of our model selection method through its applications to real beer-purchase data.

The rest of this paper is organized as follows: We first introduce GRD in Section II, and explain MDL-based model selection with the NML code-length in Section III. In Sec- tion IV, we present a new theorem for NML of GRD models.

We describe an application to relational data clustering in Section V, and show experimental results in Section VI. We finally give our conclusions in Section VII.



II. GENERAL RELATIONAL DATA MODELING  In this section, we formulate, and describe the modeling and statistical model selection of GRD. We also present some examples of GRD model selection tasks.

Suppose we have S types of object classes, such as ?customers? or ?items,? and let the s-th class contain Ns objects. Let N = (N1, . . . , NS) denote an object size vector.

We define attribute data as a sequence of vectors attached to each object. We denote the attribute data of the s-th object class as xNss = xs1 ? ? ?xsNs ? XNss (s ? {1, . . . , S}).

We also consider relational data, which consist of infor- mation about the relationship between objects. Relational data can be expressed by a network, matrix, or tensor. We denote relational data among the s-th and t-th objects as a matrix xNs?Ntst = {xst,ij} ? XNs?Ntst (s, t ? {1, . . . , S}).

Let us now consider a mixture of B attribute or relational datasets D = (xN1 , . . . , x  N B ), where x  N b (b ? {1, . . . , B})  denotes attribute or relational data. We refer to the data D as general relational data. Note that we distinguish ?general relational data? from ?relational data.?  We now formulate the statistical modeling of GRD and model selection. We consider variables in the given data D to be instantiations of random variables D? = (XN1 , . . . , X  N B ).

Statistical modeling aims to grasp the characteristics of the joint distribution of variables D? through a probabilistic model P = {P (D?)}. In this paper, we are especially interested in a parametric class of probability distributions P = {P (D?; ?) | ? ? ?}, where ? is a parameter space and P (D?; ?) denotes a probability distribution of D? specified by a parameter ? ? ?. Suppose that we are also given a finite countable set of candidate models M. A model M ? M is an indicator of the corresponding probabilistic model PM = {P (D?; ?) | ? ? ?M}, where ?M is a parameter space specified by the indicator M . Statistical model selection then aims to choose the optimal model M? from M to represent the joint distribution of D.

As mentioned in Section I, model selection in GRD modeling concerns several practical tasks. In Bayesian net- work structure learning, we seek to determine the optimal factorization structure of the joint distribution of data D.

Suppose we have the following classes of probabilistic     models:  P (XN1 , . . . , X N B ) =  B? i=1  P (XNi |Pai), (1)  where Pai ? {XN1 , . . . , XNB } denotes a set of parent variables of XNi . Bayesian network structure learning aims to compose the optimal sets of parent variables {Pai}.

Friedman et al. proposed Probabilistic Relational Models (PRMs), which can be thought of as an extension of Bayesian networks for relational database mining [15]. The structural search of PRMs is also a challenging model selection task.

Relational data clustering is the task of partitioning ob- jects into clusters according to observable relational data, which may include network clustering and co-clustering.

Suppose that we are given unipartite network data about a single type of N objects, denoted by xN?N . Let the latent attribute variables ZN ? {1, . . . ,K}N indicate the cluster to which each object belongs. We can see that relational data clustering is a kind of GRD modeling, as we model observ- able relational variables XN?N and unobservable attribute variables ZN simultaneously, i.e., D? = (XN?N , ZN ). In relational data clustering, model selection determines the optimal number of clusters K. We consider an example of SBM in Section V [14].

The task of rank determination in matrix factorization is similar to that of cluster number determination in relational data clustering, except that the domain spaces of latent variables are continuous rather than discrete.

One crucial characteristic of GRD is the variety of data sizes. The size of attribute data increases linearly with respect to the number of objects Ns; the size of relational data is polynomial with respect to the number of objects, i.e., O(N2s ), O(N3t ), or O(NsNt). Thus, statistics derived from GRD models also have various convergence or divergence properties: The sample distributions of MLEs in GRD mod- els converge to normal distributions at different rates with respect to the object size vector N. Therefore, we have to develop criteria that account for the variety of data sizes and asymptotic properties of GRD.

If the probabilistic model can be factorized as (1), we could also consider applying existing information criteria to each factor P (XNi |Pai) individually and summing them. In this paper, we rather consider the NML code-length to de- velop a criterion that is independent of such a factorization.

We experimentally demonstrate that the proposed criterion is more effective than the methods described above.



III. NORMALIZED MAXIMUM LIKELIHOOD CODE-LENGTH  In this paper, we consider model selection based on the MDL principle. This section introduces its mathematical formulation, and describes the NML code-length.

A. MDL Principle and NML Code-length  The MDL principle asserts that the best model is that which minimizes the total code-length required for encoding the given data L(D|M) as well as the model itself L(M):  M? = arg min M?M  {L(D|M) + L(M)}.

In this paper, we assume that the code-length of L(M) can be uniformly calculated as ln |M|. With regard to the code- length of the given data L(D|M), the normalized maximum likelihood (NML) code-length is employed. The NML code- length is defined as follows:  LNML(D;M) def= ? lnP (D; ??(D)) + ln ? D?  P (D?; ??(D?)).

(2) ??(D) is the maximum likelihood estimate of ? from D.

The NML code-length is justified in the sense that it achieves Shtarkov?s minimax regret [10]. Let us consider all possible prefix code-length assignments over the domain of D. When a model M is given, it is reasonable to employ the code-length assignment that minimizes the following ?max regret? criterion:  max D?  {L(D?;M)? min ???M  {? lnP (D?; ?)}}. (3) Shtarkov showed that the NML code-length (2) minimizes the max regret (3). According to the MDL principle, we select the model that minimizes the NML code-length (2) as the best model [9].

B. An Asymptotic Property of NML  NML-based model selection suffers from two computa- tional problems. First, when the domain space of the input variables D? or the parameter space ?M is unbounded, the second term of NML (2) may diverge to infinity.

Rissanen [12] and Hirai and Yamanishi [11] addressed this problem by restricting the domain range of the given data. Second, computation of the second term in (2) is generally expensive. This is because we must sum over the whole domain of all variables. As the number of random variables increases, this combinatorial computation increases explosively. This problem is especially remarkable in GRD modeling, because we consider multiple attributes and rela- tional variables.

In this paper, we are concerned with the latter computa- tional complexity problem, provided the former divergence problem can be avoided.

One approach to the reformulation of this problem is to use an approximation of (2). In terms of attribute data modeling, Rissanen [9] provided the following asymptotic expansion theorem for NML. Assume the attribute data xN are given, and that a probabilistic model PM = {P (XN ; ?) | ? ? ?M}, where the parameter space ?M is compact, satisfies the following five conditions.

i) There exists the Fisher information matrix, i.e.,  IN (?) def = ? 1  N E?  [?2 lnP (XN ; ?) ????T  ] ? I(?),  and ?c1, c2, 0 < c1 ? |I(?)| ? c2 < ? for all ? ? ?M .

ii) The elements of I(?) are continuous in ?M .

iii) ?  ?M  ? |I(?)| <?.

iv) Let ??(XN ) denote the MLEs of ?. Then, ? N(??(XN )? ?) d? N (0, I?1(?)).

v) There is a finite positive-definite matrix C0 that, for all D?, satisfies  ? 1 N  ?2 lnP (XN ; ?)  ????T  ??? ?=??(XN )  < C0 <?.

Then, the following theorem holds.

Theorem 1. (Rissanen?s asymptotic expansion theorem [9]) Assume that a probabilistic model PM satisfies the condi- tions i)?v). Then, for all data XN such that ??(XN ) ? ?M ,  LNML(xN |M) = ? lnP (xN ; ??(xN )) + dM  ln N  2?  + ln  ? ?M  ? |I(?)|d? + o(1), (4)  where limN?? o(1) = 0 uniformly over all xn.

According to (4), we obtain an analytical approximation of NML that is guaranteed to converge to the exact NML with respect to the object size N .



IV. ASYMPTOTIC PROPERTY OF NML FOR GRD MODELS  In this section, we give a theorem on asymptotic expan- sion of the NML code-length. This is derived by extending Rissanen?s theorem [9] into GRD modeling. The obtained formula leads to a novel criterion for GRD model selection.

Recall that, because of the variety of data sizes in GRD, we cannot apply one-way asymptotic evaluations to GRD modeling. In order to deal with the variety of data sizes, we introduce the following functional matrix.

Suppose we have dM functions f1(N), . . . , fdM (N), where fi : NS ? [0,?), and a diagonal matrix function F (N) = diag{f1(N), . . . , fdM (N)}. For two vectors a and b of the same length, a > b or a ? b denotes compo- nentwise inequality. Each function fi is a nondecreasing unbounded function of each element of N. In this paper, N ? ? denotes that each component of N becomes sufficiently large. With respect to F (N), we consider a probabilistic model PM = {P (D?; ?) | ? ? ?M} that satisfies the following conditions:  i?) There exists the following matrix:  JN(?) def = ?F?2(N)E?  [?2 lnP (D?; ?) ????T  ] ? J(?)  as N ? ?, and ?c1, c2, 0 < c1 ? |J(?)| ? c2 < ? for all ? ? ?M .

ii?) The elements of J(?) are continuous in ?M .

iii?) ?  ?M  ? |J(?)| <?.

iv?) Let ??(D?) denote the MLEs of ?. Then,  F (N)(??(D?)? ?) d? N (0, J?1(?)) as N ? ?, which allows MLEs to converge to normal variables at different rates.

v?) There is a finite positive-definite matrix C0 that, for all D?, satisfies  ?F?2(N)? 2 lnP (D?; ?)  ????T  ??? ?=??(D?)  < C0 <?.

Under the above conditions, we give a new asymptotic  expansion theorem of NML for GRD modeling.

Theorem 2. (Main theorem) Let a probabilistic model PM satisfy conditions i?)?v?). Then, for all data D such that ??(D) ? ?M , LNML(D|M) = ? lnP (D; ??(D)) + ln |F (N)| (5)  ?dM  ln 2? + ln  ? ?M  ? |J(?)|d? + o(1),  where limN?? o(1) = 0 uniformly over all data D.

In the special case where the given data D are attribute data xN , N = N and fi(N) =  ? N for all i, and Theorem  2 is equivalent to Theorem 1. From Theorem 2, we obtain an approximation of NML for GRD modeling. We propose this approximation as a suitable criterion for model selection in GRD modeling.

Proof sketch: The proof of Theorem 2 closely follows that given in [9] for Theorem 1. Hence, we give only a sketch of the proof. Let us discretize the parameter space ?M into rectangles whose side-length along each axis is r/fi(N).

Rd(??) denotes the rectangle whose center is located at ??.

We then prepare two functions,  Pd(??) def =  ? ??Rd(??)  |F 2(N)J(??)| 12 (2?)dM/2  ? exp ( ?1 (? ? ??)TF 2(N)J(??)(? ? ??)  ) d?,  Qd(??) def =  ? D?:??(D?)?Rd(??)  P (D?; ?)dD?.

The proof has three steps. For sufficiently large N, a) Pd and Qd become arbitrarily close. b) ln  ? ?? Qd(??) converges to  the normalized term of NML (2). c) ln ?  ?? Pd(??) approaches the approximation of the normalized term of NML (5).



V. NML-BASED MODEL SELECTION FOR THE STOCHASTIC BLOCK MODEL  In this section, we apply our proposed criterion (5) to model selection in relational data clustering.

Relational data clustering is a practically important issue.

It can be applied to a wide range of data mining tech- nologies, including social network analysis, recommendation systems, and market analysis. We introduce the stochastic block model (SBM) [14], which is a latent-variable model of the community structure within a graph, providing a model- based clustering of the graph?s nodes.

At this point, we are concerned with a unipartite graph containing N nodes (objects) of one class and a series of undirected relations between two objects. The affinity matrix of the graph, denoted as xN?N , is square and symmetric.

We consider the simplest case of X = {0, 1}: xij = 1 if objects i and j are connected, otherwise xij = 0. The diagonal elements of xN?N are fixed to 0.

Assume that the graph consists of K clusters, i.e., each of the N nodes belongs to one of K clusters. Let us introduce a latent variable zN = z1 ? ? ? zN as a tuple of the indicator vectors zi ? {1, . . . ,K}, which denote that the i-th object belongs to the zi-th cluster. Assume that the random variables Z1, . . . , ZN are generated independently according to the following probability distribution:  Zi ? Categorical(p) (i = 1, . . . , N),  where Categorical(?) denotes the categorical distribution, p denotes a parameter vector of it and p = (p1, . . . , pK?1) ? [0, 1]K?1 such that  ?K?1 k=1 pk ? 1, and pK = 1?  ?K?1 k=1 pk.

Conditioned by Zi and Zj , the edges between objects i and j, Xij , are assumed to be generated from a Bernoulli distribution:  Xij |Zi, Zj ? Bernoulli(?ZiZj ).

Let ? = {?kl} denote a symmetric parameter matrix of Bernoulli distributions and ? = (p,?). Fig. 2 illustrates generative process of SBM.

Assuming the number K of clusters, SBM is a parametric probabilistic model of D? = (ZN , XN?N ) that can be written as follows:  PK = { P (D?; ?) = P (ZN ;p)P (XN?N |ZN ; ?) | ? ? ?K  } .

Estimating the latent variable ZN from the observable relational variables xN?N is equivalent to relational data clustering. For example, to learn the parameters ?, Daudin et al. [16] developed the variational EM algorithm.

An important issue in the inference of SBM is the selection of K, the number of clusters, from candidates K.

Let us derive the NML code-length for SBM. When all data D = (zN , xN?N ) are given, the MLEs of p and ? are  ? ? ? ? ? ? ? ?  ? ? ? ? ? ? ? ? ?  ? ? ? ? ? ? ? ? ?  ? ? ? ? ? ? ? ? ?  ? ? ? ? ? ? ? ? ?  ? ? ? ? ? ? ? ? ?  ? ? ? ? ? ? ? ? ?  ? ? ? ? ? ? ? ? ?  ? ? ? ? ? ? ? ? ?  ? ? ? ? ? ? ? ?   ?  ? ?  ?  ?  ?  ?      1=Z 2=Z  3=Z  ??? ??? ???  ??? ??? ???  ??? ??? ???  =?  = ?NNX  Figure 2. An illustration of generative process of SBM  p?k = ak/N , and ??kl = a?kl/akl, where ak def =  ? i 1zi=k,  a?kl def =  ? i,j ?=i xij1zi=k,zj=l, and  akl def =  { ak(ak ? 1)/2 (k = l), akal (k = l).

Following the derivation of NML for na??ve Bayes models in [3], the normalization term of NML for SBM is written as  CSBM(N,K) (6)  = ?  ? ak=N  { N !

a1! ? ? ? aK !

K? k=1  (ak N  )ak ?? k?l  Ccat(akl, 2) } ,  where Ccat(n, k) is the normalization term of NML for the categorical distribution with k states, which coincides with the Bernoulli distribution in the case of k = 2.

Because Ccat(n, 2) can be calculated in O(n) time, the computation of (6) requires O(K2 ?NK+1) time: Fig. 3 plots the real computation time of CSBM(N,K) on a double- logarithmic scale. We can see that the computation time increases explosively with respect to N and K. Indeed, the computation of CSBM(N = 500,K = 3) takes almost one week (? 6 ? 105 s) in our experimental setting. This is clearly expensive.

We now derive an approximation of CSBM(N,K) accord- ing to Theorem 2. We order the parameters as  ? = (p1, . . . , pK?1, ?11, . . . , ?KK , ?12, ?13, . . . , ?K?1K).

The dimensionality of the parameter space ?K of SBM is dK  def = K ? 1 + K(K + 1)/2. Let us set dK functions of  {fi(N)} as  fi(N) =  {? N (i < K),  N (i ? K).

The first K ? 1 functions correspond to the parameter p, and the others to ?. Using a functional matrix F (N) =           N  C P  U ti  m e  (s ec  .) Computation time of NML      K=1 K=2 K=3 K=4  Figure 3. Real computation time of NML for SBM (6)  diag{f1(N), . . . , fdK (N)}, we can calculate J(?) of SBM as:  J(?) = lim N??  ?F?2(N)E? [?2 lnP (D?; ?)  ????T  ] =  ( M(p) 0 0 {pklB(?kl)}  ) ,  where M(p) denotes the Fisher information matrix of the categorical distribution, B(?kl) denotes that of the Bernoulli distribution, and  pkl def =  { p2k/2 (k = l),  pkpl (k = l).

We also obtain the limit of the sample distribution of ??(D?) as  F (N)(??(D?)? ?)?d N (0, J?1(?)).

The integral of  ?|J(?)| can be computed as ln  ? |J(?)| 12 d? = ?K  ln 2 +K ln ?  ( K + 2   )  ? ln ? ( K(K + 2)   ) +  K(K + 1)  ln?.

Hence, from Theorem 2, the following theorem is derived:  Theorem 3. For SBM, the logarithm of the normalization term in the NML code-length is asymptotically expanded as follows:  lnCSBM(N,K) (7)  = K ? 1 +K(K + 1)  lnN ? dK  ln 2?  ?K ln 2 +K ln ?  ( K + 2   ) ? ln ?  ( K(K + 2)   )  + K(K + 1)  ln? + o(1).

0 50 100  ?4  ?2    N  Lo g?  re gr  et d  iff er  en ce  ( bi  ts )      K=2 K=3  Figure 4. Asymptotic behavior of the approximate NML (7) (blue and red plot lines) to the exact NML (6) (black dots baseline)  Because we can calculate (7) analytically, its computation time (less than 10?3 s in our experiment) is negligible.

Fig. 4 illustrates the asymptotic behavior of the approx- imate NML (7) against the exact NML (6). It is clear that the approximate NML (7) converges to the exact NML (6) as N increases.



VI. EMPIRICAL RESULTS  A. Evaluation Using Artificial Datasets  We conducted an experiment using artificial datasets to evaluate the performance of the approximate NML (7) in comparison with other criteria.

1) Experimental settings: We generated 30 networks ac- cording to the generative model of SBM for each object size N and the true number of clusters Ktrue. Parameters ? = (p,?) were sampled according to the following distributions:  p ? Dirichlet(?), ?kl ? Beta(?, ?), ?lk ? ?kl (k ? l ? {1, . . . ,Ktrue}).

The hyper-parameters were set to ? = (100, . . . , 100), which has length Ktrue, and ? = 1.

The model selection criteria were evaluated using the number of networks, out of 30, that accurately selected Ktrue as the optimal model. We refer to this metric as the recovery rate (RR).

For the clustering, we employed the collapsed Gibbs sampling algorithm with a nonparametric Bayes model of SBM, known as the infinite relational model (IRM) [17].

The algorithm generates candidate cluster assignments zN  for various numbers K of clusters. We selected the cluster assignment and the number of clusters that minimized a certain criterion. The IRM employed two kinds of priors: the Chinese restaurant process (CRP) and the beta distribution.

The hyper-parameter of the CRP was fixed to ? = lnN , and that of the beta distribution was set to ? = 1/2. The clustering and model selection procedure is summarized in Fig. 5.

Input: Observed Network xN?N , Object size N , hyper- parameters for clustering ?, ?  Output: Selected number of clusters Kbest, selected cluster assignment zNbest  Lbest ?? for i = 1? 15 do  Initialize zN randomly for j = 1? 100 do zN ? CollapsedGibbsSampler(xN?N , zN , ?, ?) K ? # of clusters of zN , ?? ? ??(xN , zN ) % MLE L? ? lnP (xN?N , zN ; ??) + lnCSBM(N,K) if L < Lbest then Lbest ? L, zNbest ? zN Kbest ? # of clusters of zNbest  end if end for  end for  Figure 5. Pseudocode for the experimental model selection procedure  2) Criteria for comparison: In our experiment, we com- pared the model selection performance of our proposed NML criterion with seven other criteria, namely AIC [5], two types of BIC [6], the minimum message length (MML) [18], integrated classification likelihood (ICL) [19], [16], and Bayesian marginal likelihoods (BMLs) of SBM and IRM.

AIC and BIC1 were defined according to their na??ve definitions in attribute data modeling:  AIC(D;K) = ? lnP (D; ??(D)) + dK , BIC1(D;K) = ? lnP (D; ??(D)) + dK  lnN.

We also applied BIC to the models P (Zi;p) and P (Xij ;?kl) individually, and obtained BIC2 by summing them:  BIC2(D;K) = ? lnP (D; ??(D))+K ? 1  lnN+ ? k?l  ln akl.

The general definition of MML was proposed by [18], from which we derived MML for SBM as follows:  MML(D;K) = ? lnP (D; ??(D)) + lnCMML(N,K), (8) where  lnCMML(N,K)  = K ? 1 +K(K + 1)  lnN +  dK  (1? ln 12)  ?K ln 2 +K ln ?  ( K + 2   ) ? ln ?  ( K(K + 2)   )  + K(K + 1)  ln?.

Because MML requires prior distributions for its param- eters, we employed the Jeffreys prior [20] in (8), i.e., Dirichlet(?) for the categorical distribution, where ? = (K+ 2)/2(1, . . . , 1), and Beta(?, ?) for the Bernoulli distribution, where ? = 1/2.

Daudin et al. [16] proposed that a model could be selected for SBM on the basis of the ICL criterion [19], which is also known as the integrated completed likelihood. ICL is based on an approximation of BML of SBM.

If we employ conjugate priors, we can analytically calcu- late BML itself. To calculate BML of SBM, we employed the Jeffreys prior described above. For the IRM, the hyper- parameter of the CRP was set to ? = lnN , and that of the beta distribution was set to ? = 1/2.

3) Results: Fig. 6 shows the RR obtained by each of the tested criteria for object sizes ranging from 40 to 2000 with Ktrue = 10 and 50 on semilogarithmic scales. We only show the RRs of AIC, BIC1, BIC2, ICL, BML of IRM, and NML, as the plots for MML and BML of SBM are very close to that of NML. In Table I, we evaluate the overall performance of each criterion quantitatively using the area under the curve (AUC). This is the ratio of the area under the curve to the corresponding rectangle. The closer the AUC is to 1, the faster the RR curve increases. The best two AUC values in each column are shown in bold.

From Fig. 6, we can see that, with the exception of AIC and BIC2, the criteria selected the true number of clusters Ktrue with a high RR for large object sizes N . In terms of AUC, NML outperforms the other criteria, particularly when Ktrue is small. When Ktrue is large, NML is not markedly superior to that of the other criteria. Because of its consistently high performance, however, we may conclude that the approximate NML gave the best overall results in our experiment.

Table I AREA UNDER THE CURVE (AUC)  Ktrue 5 10 20 50 100 200 AIC 0.774 0.703 0.754 0.577 0.344 0.120 BIC1 0.917 0.823 0.694 0.495 0.328 0.120 BIC2 0.026 0.087 0.013 0.005 0.000 0.000 MML 0.917 0.848 0.737 0.569 0.344 0.120 ICL 0.895 0.784 0.643 0.423 0.269 0.086 BMLSBM 0.919 0.860 0.768 0.577 0.344 0.120 BMLIRM 0.880 0.841 0.788 0.561 0.344 0.120 NML 0.921 0.868 0.784 0.573 0.344 0.120  B. Evaluation Using Market Datasets  We applied the relational data clustering and model se- lection methods described above to brand analysis. This is an important issue that allows enterprises to objectively evaluate the brand image of their own and rival products. By analyzing customer purchase data, we can expect to obtain some insight into the relationship between different brands in a market.

0.2  0.4  0.6  0.8   N  R ec  ov er  y R  at e  K true  =10      AIC BIC1 BIC2 ICL BML_IRM NML     0.2  0.4  0.6  0.8   N  R ec  ov er  y R  at e  K true  =50      AIC BIC1 BIC2 ICL BML_IRM NML  Figure 6. Results of artificial model selection experiment  We use a dataset of beer purchases in Japan. This is QPR data provided by MACROMILL, Inc., and HAKUHODO, Inc.

The dataset has the following specifications: Purchase data relate to the period November 1st 2010 to January 31st 2011. The data relate to 3,185 customers and 65 brands of beer. The dataset consists of more than 20,000 beer purchase transactions. Brands of beer include three types: regular beer, ?low-malt? beer, and a third type of beer. Low-malt beer is a beer-type alcoholic drink with a relatively low malt ratio, and the third beer is a beer-tasting alcoholic drink with no malt.

To commence our analysis, we processed the raw purchase records into a 0/1-valued matrix. We set the matrix entries xij = 1 if the j-th customer purchased more than two cans of the i-th beer during the data acquisition period, and set xij = 0 otherwise. Next, we excluded customers whose column vector included fewer than five positive values. Beer brands whose row vector included no positive values were also excluded. Finally, we obtained a 0/1-valued matrix of 51 beer brands and 101 customers.

We conducted co-clustering analysis on the purchase re- lational matrix of customers and beer brands. Co-clustering is a framework that simultaneously partitions the rows and columns of a matrix (see, e.g., [21]). This enables us to  ?????????  ? ? ? ? ? ? ? ? ? ?   ?  ?  ?  ?  ?  ?  Figure 7. Result of co-clustering the beer purchase matrix  Table II NUMBER OF BEER BRANDS IN EACH CLUSTER  Regular Low-malt 3rd Total A 0 0 7 7 B 0 4 6 10 C 3 0 0 3 D 0 0 4 4 E 7 1 0 8 F 10 6 3 19  Total 20 11 20 51  extract customer clusters and beer-brand clusters, as well as their global relationship. In this example, we applied SBM-based co-clustering, and employed the variational EM algorithm developed in [16] for SBM. The optimal numbers of customer and beer clusters were selected according to the NML code-length for SBM, which was derived in the same way as (7).

Fig. 7 shows the results of co-clustering and model selection of the purchase relational matrix. We obtained six beer-brand clusters (A?F) and two customer clusters. Table II summarizes the types of beer brands in each cluster.

From Table II, we can see that clusters C and E are regular beer-brand clusters, whereas clusters A and D consist of the third type of beer-brands. Customers included in the left cluster in Fig. 7 mainly buy regular beer, and customers belonging to the right cluster prefer the third type. We can characterize the brand clusters as follows: Cluster C is the top-selling cluster of regular beer, and cluster D is the top- selling cluster of the third type of beer. Clusters E and A are the middle-standing clusters of regular and third-type beers, respectively. Note that Table II shows one low-malt beer brand in cluster E. We can infer that this beer?s brand image is more similar to that of regular beer than low-malt beer, as the people who prefer regular beer brands also drink this low-malt beer.

This analysis shows that the co-clustering of a beer purchase matrix can lead to the understanding of the rela-     tionship between beer brands and customers. Our proposed criterion played an important role in determining the number of clusters in this analysis.

We finally compare our proposed criterion with other ones. For example, AIC discovered four customer clusters, and the customer behavior suggested by each cluster is rather different from that given by NML. ICL selected only five brand clusters, whereas results of NML also distinguished brand clusters B and E. Each of these results shows certain aspects of the data, but we emphasize that the model selec- tion results given by NML are justified from the perspective of data compression.



VII. CONCLUSION In this paper, we have addressed the issue of statistical  model selection in GRD modeling. From the viewpoint of the MDL principle, we have clarified the asymptotic property of NML in GRD modeling, and have proposed a new model selection criterion. We have applied our proposed criterion to the issue of model selection in SBM learning, and have derived a new and efficient method of calculating the NML code-length for SBM. The effectiveness of our NML-based criterion has been demonstrated experimentally using artificial data and real beer-purchase data. Thus, for the first time, we have realized a model selection technique that considers the statistical characteristics of GRD models.

We will extend our discussion to other problems and criteria.

