Efficient Identification of Coupled Entities in Document Collections

Abstract? The relentless pace at which textual data are gener- ated on-line necessitates novel paradigms for their understanding and exploration. To this end, we introduce a methodology for discovering strong entity associations in all the slices (meta- data value restrictions) of a document collection. Since related documents mention approximately the same group of core entities (people, locations, etc.), the groups of coupled entities discovered can be used to expose themes in the document collection.

We devise and evaluate algorithms capable of addressing two flavors of our core problem: algorithm THR-ENT for computing all sufficiently strong entity associations and algorithm TOP-ENT for computing the top-k strongest entity associations, for each slice of the document collection.



I. INTRODUCTION  Millions of bloggers author in excess of 5 million blog posts daily, where they comment upon a wide variety of private mat- ters and public on-going events. This collective ?discussion? takes place in a semi-anonymous manner, as bloggers typically reveal their demographic profile (age, gender, occupation, lo- cation). Valuable knowledge can be extracted from such user- generated data by identifying the stories and events currently capturing the attention of different demographics.

Consider an emerging crisis situation like the fatal 2008 Listeriosis outbreak in Canada. It would be invaluable for both the authorities and citizens to identify active discussion about such an event, before it reaches its climax. As another exam- ple, a new TV series targeted at all US teenagers is launched.

An advertiser could compare the stories discussed by different groups of teenagers and identify surprising divergences, e.g., that the series is surging in popularity among female teenagers, but failing to capture the attention of males.

In order to provide such intuitive knowledge discovery fa- cilities, we leverage mentions of entities extracted [3] from the documents? textual component and document meta-data (e.g., blogger demographics). The key observation is that related documents, capturing the same story, mention approximately the same group of core entities. E.g., blog posts discussing the Canadian ?Listeriosis? outbreak mention the disease in conjunction with ?Public Health Agency of Canada? and lo- cations like ?Toronto?. By identifying such groups of strongly associated entities, i.e., groups of entities that are recurrently mentioned together, we implicitly detect the underlying event of which they are the main actors. Strong entity associations can be computed for each demographic segment and presented on demand (along with additional information such as relevant  blog posts) as we ?drill-down? to the demographic of interest, or compared for different demographics [2].

To support such interactive browsing and exploration of a document collection, we need to pre-compute and materialize strong entity associations for all meta-data value combinations that can be possibly requested. We refer to the fraction of documents matching a certain meta-data restriction as a slice of the collection (e.g., a demographic segment such as ?US teenagers? in the case of blogs). Two variations of our core problem are of interest: computing all sufficiently strong entity associations (Threshold Variation) and computing the top- k strongest entity associations (Top-k Variation), for all the different slices of the document collection.

While there exist a significant number of set-similarity and statistical correlation measures for assessing association strength between two entities, we focus on the Likelihood Ratio statistical test whose unique properties and intuitive behavior render it ideal for exposing interesting and mean- ingful entity associations in user-generated data ([6] and Section IV). Existing techniques in association rule mining and set-similarity search focus on measures inadequate for our application (e.g., support/confidence [1], X 2 test [5], set- similarity measures [4]) and cannot support the complex, non- linear Likelihood Ratio test.

The relentless pace at which user-generated content is accumulated necessitates highly efficient solutions. In order to address this challenge, we make the following contributions.

? We develop algorithm THR-ENT (Thr. Variation), which eliminates from consideration provably weak associations.

? We develop algorithm TOP-ENT (Top-k Variation), which terminates as soon as it can guarantee that the top-k associ- ations computed so far constitute the final result.

? We demonstrate the efficiency and applicability of the proposed techniques using both real and synthetic data.



II. FORMAL PROBLEM STATEMENT  Consider a collection C of n documents d1, . . . , dn, anno- tated with l categorical meta-data attributes A1, . . . , Al. We denote with di(A1, . . . , Al) the meta-data values annotating di. Let A ? Powerset(A1, . . . , Al) be a subset of the l meta- data attributes and A? be the cartesian product of their corre- sponding domains. Each element a of set A? defines a slice sC(a) of the collection: sC(a) = {d ? C|a ? d(A1, . . . , Al)}.

We have at our disposal an entity extraction algorithm whose application on the document collection reveals the     mentions of m entities e1, . . . , em. We define the document- list ei = {dj |dj mentions ei} of entity ei and the entity-list dj = {ei|ei mentioned in dj} of document dj . We denote with ci = |ei| the number of documents that mention entity ei and with cij = |ei ? ej | the number of documents where entities ei and ej are mentioned together. We will also refer to cij as the co-occurrence of ei and ej .

The Likelihood Ratio measure quantifies the likelihood that, for the pair of entities examined, the assumption of independent occurrence in documents does not hold. Higher values indicate a higher likelihood that the assumption is violated and therefore signify stronger association. Consider two entities ei and ej . We denote with N11 the number of documents containing both. Similarly, let N10 (N01) be the number of documents containing entity ei (ej) but not entity ej (ei) and N00 the number of documents containing neither entity. We also denote with E11, E10, E01, E00 the expected values of these quantities under the independence assumption.

These quantities can be easily expressed as a function of n, ci, cj and cij . Then:  ? Likelihood Ratio: L(ei, ej) = ?  x?{0,1}  ?  y?{0,1} 2Nxy ln  Nxy Exy  Our goal is to compute associated entities, for all the slices of the document collection. We identify two interesting variations of this problem.

? Problem 1 (Threshold Variation): For each slice s, compute pairs (ei, ej) such that L(ei, ej) ? T .

? Problem 2 (Top-k Variation): For each slice s, compute the k pairs (ei, ej) with the highest L(ei, ej) values.



III. ALGORITHMS  A. ALL-PAIRS: Evaluating All Pairs  Let us initially focus on a single slice. We assume that we have in main memory the document-lists ei associated with each entity ei. In order to compute value L(ei, ej) for pair (ei, ej), we need four pieces of information: n, ci, cj and cij .

The main challenge in evaluating L(ei, ej) for every entity pair is computing co-occurrences cij . This information can be represented as a symmetric co-occurrence matrix Cm?m with cij as its elements. Due to its symmetry, we are only interested in elements that lie below its diagonal (Figure 1(a)).

Matrix C can be efficiently computed one row-at-a-time.

The row-at-a-time computation of C begins from the row corresponding to entity e1 (Figure 1(a)) and progresses to- wards the row of entity em. Consider the row of entity ei.

Its elements correspond to the co-occurrences of entity ei with all preceding entities e1, . . . , ei?1. The entire row can be computed by merging the sorted by entity identifier entity-lists d corresponding to the documents where entity ei appears.

Example 1: Suppose that the document-list for entity e9 is e9 = [1, 5, 8] (e9 appears in documents d1, d5 and d8).

Respectively, the entity-lists for documents d1,d5 and d8 are d1 = [2, 3, 6,9], d5 = [3, 7,9] and d8 = [2, 3, 7,9] (e.g., document d1 contains entities e2, e3, e6, in addition to e9).

The element sequence resulting from the merge of the three  e1 e2 e3 e4 e5 e6 e7 e8  e1 e2 e3 e4 e5 e6 e7 e8 (a)  e1 e2 e3 e4 e5 e6 e7 e8  e1 e2 e3 e4 e5 e6 e7 e8 (b)  Forward Merge  Reverse Merge  Fig. 1. The co-occurrences matrix C.

lists is [2, 2, 3, 3, 3, 6, 7, 7,9,9,9]. This implies that entity e2 appears twice in the documents where entity e9 occurs and therefore c92 = 2. Similarly, we compute that c93 = 3, c96 = 1 and c97 = 2. We are certain that all other elements of row e9 are equal to zero.

Intuitively, by computing C in a row-at-a-time fashion we search the entity-lists of all documents in which ei appears and focus only on the entities that actually co-occur with ei. The expected low number of entities per document and heavy-tailed distribution of entity frequencies (most entities mentioned in a few documents) benefit this approach.

The entity-lists d essentially correspond to an inverted index of the entity document-lists e. This inverted index is incrementally populated as entities (rows of matrix C) are processed. We denote with Ii such an inverted index, populated with entities e1, . . . , ei. Inserting an entity ei entails identifying the appropriate entity-lists where ei should be placed, and inserting it at the end of those lists. This index growing process, in conjunction with the fact that entities are processed in ascending identifier order e1, . . . , em, implies that the index?s entity-lists are also maintained sorted in ascending entity identifier order.

This observation has an interesting consequence that we will utilize. Consider inverted index Ii?1 used to compute C?s row corresponding to entity ei. The merging process produces co- occurrences in a well-defined order, i.e., ci1, ci2, . . . , ci(i?1) (left to right). We refer to this operation as forward merge (Fig- ure 1(b)). However, this is not the only possibility. Although the index?s inverted lists contain entities in ascending identifier order, we can process the lists by scanning them backwards.

This reverse merge operation (Figure 1(b)) produces co- occurrences in the order ci(i?1), ci(i?2), . . . , ci1 (right to left).

We note that in either case only non-zero co-occurrences are produced. These techniques constitute algorithm ALL-PAIRS which can be used to address both Problems 1 and 2.

B. THR-ENT: Problem 1 (Threshold Variation)  Given our interest only in pairs such that L(ei, ej) ? T , computing the entire matrix C is wasteful. We need to focus our attention on computing the part of C that corresponds to entity pairs whose association can exceed T .

Lemma 1: L(ei, ej) ? P (ei, ej), where P (ei, ej) is the Likelihood Ratio value obtained by setting cij = min(ci, cj).

We refer to P (ei, ej) as the potential of entity pair (ei, ej), since the association strength of an entity pair is maximized when the less frequent entity always co-occurs with the most frequent one. The lemma instructs us to focus only on elements cij such that P (ei, ej) ? T . Let us concentrate on a single row of matrix C, corresponding to entity ei. One could compute the     potential of every pair and identify the ones with P (ei, ej) ? T . However, the row elements that need to be evaluated reveal a ?fragmented? image.

Consider Figure 2(a). Elements corresponding to pairs that have the potential to exceed the association threshold are highlighted in grey. This immensely complicates matters, since the efficient row-at-a-time approach of computing row elements generates those elements in a well specified order (either left to right or right to left, Figure 1(b)). Consequently, we need to compute all row elements sequentially, until we reach the last element that is needed. In the worst case, the entire row will be processed (e.g., row e5 in Figure 2(a)). The following lemma helps alleviate this limitation.

Lemma 2: Suppose that entities e1, . . . , em are sorted in descending order of their frequencies, i.e., ci ? cj for i < j.

Then, for any entity ei and its preceding entities e1, . . . , ei?1 it holds that P (ei, e1) ? P (ei, e2) ? ? ? ? ? P (ei, ei?1).

Lemma 2 implies that if entities e1, . . . , em are sorted in descending order of their frequencies, the potential of pairs in a row is monotonically non-decreasing from left to right.

Consequently, a threshold T divides a row into two sections: pairs on the left that do not have the potential to exceed the threshold and pairs on the right that have the potential to exceed it (Figure 2(b)).

Now, pairs that need to be computed are concentrated on the right side of the row and can be accessed using a reverse merge (Figure 2(b)). It should be noted that even among the elements whose corresponding pair potentials exceed threshold T , only those with cij > 0 will actually be computed. The techniques described above constitute algorithm THR-ENT.

C. TOP-ENT: Problem 2 (Top-k Variation)  The potential of entity pairs can be used to devise a ?Threshold algorithm? for Problem 2. We can incrementally generate candidate entity pairs in descending order of their potential and evaluate their association in that order. As soon as the potential of the next candidate entity pair is less than the top-k ?threshold?, i.e., the minimum association among the top-k entity pairs, we can be certain that the top-k pairs en- countered so far are the overall top-k pairs. The challenge lies in reconciling the incremental, descending-potential candidate generation process and the efficient row-at-a-time approach to computing entity co-occurrences.

e1 e2 e3 e4 e5 e6 e7 e8  e1 e2 e3 e4 e5 e6 e7 e8 (c)  e1 e2 e3 e4 e5 e6 e7 e8  e1 e2 e3 e4 e5 e6 e7 e8 (d)  Frontier 1 Frontier 2  Frontier 3  e1 e2 e3 e4 e5 e6 e7 e8  e1 e2 e3 e4 e5 e6 e7 e8 (a)  e1 e2 e3 e4 e5 e6 e7 e8  e1 e2 e3 e4 e5 e6 e7 e8 (b)  Fig. 2. Principles of Algorithms THR-ENT & TOP-ENT.

The reconciliation can be performed by pre-sorting, as be- fore, entities in descending order of their frequencies. Lemma 2 guarantees that for each row of matrix C, the corresponding pair potential is monotonically non-decreasing from left to right. Figure 2(c) illustrates: darker shades of gray signify  higher pair potential. The direction of the arrow is the direction of decreasing pair potential. Note that high-potential elements are concentrated towards the matrix diagonal.

Lemma 2 provides a guaranteed potential ordering within each matrix row, but not for the entire matrix. Given two matrix elements (entity pairs) in different rows, we can draw no conclusions about their relative potential. Still, a total pair ordering can be derived by progressively merging the rows of matrix C. This operation results in a stream of candidate pairs ordered in descending potential order.

Let us discuss this process in more detail. Each of the m rows of matrix C is associated with an ?iterator?. The iterator ITi associated with row i incrementally performs a reverse-merge and provides row pairs and their co-occurrences in descending potential order. At any time, an iterator is associated with the potential of the pair that it currently points. The iterators are inserted in a heap H which maintains them in descending order of their corresponding potential.

Then, the top of heap H provides the iterator pointing to the pair with the maximum overall potential: it is the pair with the maximum potential among all m matrix rows. After the maximum potential pair is retrieved and evaluated, the iterator that provided the pair is advanced, and heap H is updated.

Intuitively, heap H dictates how to alternate between the m on-going reverse merges after each element computation.

The proposed algorithm ?sweeps? the elements of co- occurrence matrix C using a frontier. This is depicted in Figure 2(d). The frontier connects the elements from each row currently in heap H. All the matrix elements on the left of the frontier have lower potential than those computed so far, i.e., elements on the right of the frontier. Figure 2(d) presents this frontier at three phases during the algorithm?s execution.

The threshold process described above is implemented by algorithm TOP-ENT. Note that the algorithm has a starting phase where the entire inverted index is populated and iterators ITi and heap H are initialized.

D. Data-assembly Infrastructure  Algorithms THR-ENT and TOP-ENT need to be applied on every slice of the document collection. Assembling the relevant document-lists ei required for their application on each of the numerous slices is a formidable challenge.

Our solution exploits the considerable overlap among slices: except for the most ?fine-grained? slices, all remaining ones can be expressed as the union of other disjoint slices. E.g., consider a slice s={blogs by US bloggers} that is the union of two disjoint slices s1={blogs by US males} and s2={blogs by US females}. Then, the document-list esi of entity ei for slice s is simply the concatenation of the corresponding document- lists es1i , e  s2 i for slices s1 and s2. We refer to the process of  concatenating the document-lists from disjoint slices in order to generate the lists of a new slice as slice merging.

Our data-assembly infrastructure is based on the MEMORY- CUBE algorithm [7]. It first generates the document-lists for the most ?fine-grained? slices and then progressively merges already processed slices to generate new ones.



IV. EXPERIMENTAL EVALUATION  All the techniques were implemented using 64-bit Java 6.

Our test platform was a 2.4Ghz AMD Opteron 850 processor.

The Java VM was constrained to use up to 10GB of memory.

A. Benefits of Likelihood Ratio Test  Set-similarity based measures (such as Jaccard) and the X 2 test disproportionately ?reward? high relative overlap between the document-lists of two entities, without accounting for entity frequencies [6]. As a result, the most associated pairs identified by such measures involve infrequent entities which occur (almost) always together. Such associations are unrepresentative of interesting and prevalent stories.

The Likelihood Ratio (LR) test ?rewards? in an intuitive manner both considerable overlap and high entity frequency.

This behavior cannot be replicated by simply enforcing a threshold on the minimum frequency of entities considered and using a different association measure. As an example, the following table presents the top-5 entity pairs under the LR measure and their (extremely low) Jaccard values, discovered in 290k blog posts collected on Feb 12, 2009.

ei ej ci cj cij Jaccard Barack Obama White House 2006 9352 1304 0.13 Charles Darwin The Origin of Species 2927 848 664 0.21  Israel Gaza 2161 947 586 0.23 Microsoft MS Windows 2415 2549 736 0.17  David Letterman Joaquin Phoenix 1280 686 442 0.29  B. Performance on Real Data  Our real-data collection is comprised of 1.4M blog posts.

Each post is associated with author information (Age, Gender, Country, Profession, Influence). Influence is quantified by the number of links to the author blog during the previous year.

The domain size of attribute Age is 7 (age ranges), of Gender is 2, of Country is 224, of Profession is 42 and of Influence is 5 (ranges of in-link values). The posts were pre-processed by the entity extractor of [2]. The extraction process revealed the mentions of 280K unique entities.

Figure 3 presents the total time required by the different techniques to address Problems 1 (Figure 3(a)) and 2 (Figure 3(b)). The time figures include the overhead of slice merging, which was only 33 seconds. THR-ENT offers a substantial performance benefit that becomes more pronounced as the threshold value increases. Higher threshold values allow al- gorithm THR-ENT to process only a small number of entity pairs per row of matrix C (Section III-B) and even ignore rows corresponding to less frequent entities entirely, since the LR measure assigns small association values to such entities. TOP- ENT clearly outperforms ALL-PAIRS, although expectedly the performance gap decreases as the number of requested associations k increases.

C. Performance on Synthetic Data  Algorithms ALL-PAIRS and THR-ENT (Problem 1) were also applied on a single, synthetically generated slice. The comparison of algorithms ALL-PAIRS and TOP-ENT (Problem 2) yielded similar results.

0 200 400 600 800 1000        (a): Threshold  T im  e (s  )      All-Pairs Thr-Ent  0 200 400 600 800 1000        (b): Top-k  T im  e (s  )      All-Pairs Top-Ent  Problem 2Problem 1  Fig. 3. Real data, Problems 1 and 2.

The parameters influencing performance are (a) the number of documents n, (b) the number of entities m (c) the number of entities per document p and (d) the specified association threshold T . We set the default values of these parameters to n = 500k, m = 500k, p = 10 and T = 200. The entity frequency distribution followed Zipf?s Law with exponent 0.8, the value witnessed in our real data set.

For each document we independently selected p entities ac- cording to their occurrence probability. Populating documents with independent entities fails to introduce strong associations.

However, this does not affect our results: algorithm THR-ENT performs pruning based on the assumption that all entity pairs are as strongly associated as possible (Section III-B).

Figure 4 presents a sample of our experimental results. All parameters (n, m, p, T ), other than the one varied in each plot, are set to the default values stated before. The figures verify the performance improvement offered by algorithm THR-ENT under a wide range of experimental parameters.

10k 100k 1M 10M      Number of posts (n)  T im  e (m  s)      0 5 10 15 20 25     x 10   Entities per document (p)  T im  e (m  s)      All-Pairs Thr-Ent  All-Pairs Thr-Ent  Fig. 4. Synthetic data, Problem 1.



V. CONCLUSIONS  We developed techniques for the discovery of entity asso- ciations in all the slices of a meta-data annotated document collection. Such associations can be used to expose interesting underlying themes in the collection. Efficient association dis- covery is performed by algorithms THR-ENT and TOP-ENT.

