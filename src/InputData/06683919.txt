An Algorithm for Cost-Effectively Storing Scientific  Datasets with Multiple Service Providers in the Cloud

Abstract?The proliferation of cloud computing allows scientists to deploy computation and data intensive applications without infrastructure investment, where large generated datasets can be flexibly stored with multiple cloud service providers. Due to the pay-as-you-go model, the total application cost largely depends on the usage of computation, storage and bandwidth resources, and cutting the cost of cloud-based data storage becomes a big concern for deploying scientific applications in the cloud. In this paper, we propose a novel algorithm that can automatically decide whether a generated dataset should be 1) stored in the current cloud, 2) deleted and re-generated whenever reused or 3) transferred to cheaper cloud service for storage. The algorithm finds the trade-off among computation, storage and bandwidth costs in the cloud, which are three key factors for the cost of storing generated application datasets with multiple cloud service providers. Simulations conducted with popular cloud service providers? pricing models show that the proposed algorithm is highly cost-effective to be utilised in the cloud. 1  Keywords-cloud computing; scientific application; datasets storage

I.  INTRODUCTION With the rapid growth of e-science, domain scientists  increasingly rely on computer systems to conduct their research [5] [16] [23] [26], e.g. cluster, grid and HPC (High Performance Computing) systems. In recent years, cloud computing is emerging as the latest parallel and distributed computing paradigm which provides redundant, inexpensive and scalable resources on demand to user requirements [13].

The emergence of cloud computing offers a new way for deploying scientific applications. IaaS (Infrastructure as a Service) is a very popular way to deliver services in the cloud [1], where the heterogeneity of computing systems [36] of one service provider can be well shielded by virtualisation technology. Hence, scientists can deploy their applications in unified cloud resources such as computing, storage and network services without any infrastructure investment, and only pay for their usage according to the pay-as-you-go model.

However, along with the convenience brought by using on- demand cloud services, users have to pay for the resources used, which can be substantial. Especially, nowadays scientific applications are getting more and more data intensive [11] [21]   *Yun Yang is the corresponding author of this paper.

[28], where generated datasets are often gigabytes, terabytes, or even petabytes in size. As reported by Szalay et al. in [27], science is in an exponential world and the amount of application data will double every year over the next decade and future. These generated data contain important intermediate or final results of computation, which may need to be stored for reuse [7] and sharing [8].  Hence, cutting the cost of cloud-based data storage in a pay-as-you-go fashion becomes a big concern for deploying scientific applications in the cloud.

In the cloud, users have multiple options to cope with the large generated application data. As excessive storage and processing power can be obtained on-demand from commercial service providers, users can either store all data in the cloud and pay for the storage cost, or delete them and pay for the computation cost to regenerate them whenever they are reused.

Furthermore, as cloud computing is such a fast growing market, more and more different cloud service providers with cost- effective storage solutions appear [3]. This phenomenon allows users to transfer the generated application data to cheaper services for storage with paying for the incurred bandwidth cost. Hence, in the cloud, users can flexibly store their data with different storage strategies which also lead to different total costs correspondingly. In light of this, a good storage strategy should be able to balance the usage of computation, storage and bandwidth resources in the cloud, which are three key factors for the cost of storing generated application data.

Existing work [33] only investigates the trade-off between computation and storage within one cloud service provider, where bandwidth cost has not been considered.

In this paper, by investigating the trade-off among computation, storage and bandwidth, we propose a novel cost- effective algorithm for storing the generated application datasets in the cloud. We utilise a Data Dependency Graph (DDG) to represent generated application data in the cloud [33] and design the novel T-CSB algorithm which can calculate the Trade-off among Computation, Storage and Bandwidth (T- CSB) in the cloud. The T-CSB algorithm can be utilised to cost-effectively store the generated application data with multiple service providers in the cloud.

The remainder of this paper is organised as follows. Section II presents a motivating example of scientific application and   DOI 10.1109/eScience.2013.34     analyses the research problems. Section III introduces some preliminaries and data storage cost model in the cloud. Section IV presents our novel T-CSB algorithm in detail. Section V describes our experimental results for evaluation. Section VI discusses the related work. Section VII summarises our conclusions and points out future work.



II. MOTIVATING EXAMPLES AND PROBLEMS ANALYSIS In this Section, we introduce a real world application in  Structural Mechanics which generates large intermediate data with various sizes, and analyse the problems of storing them in the cloud.

A. Motivating Examples Finite Element Modelling (FEM) is an important and  widely used method for impact test of objects, where classic applications are split Hopkinson pressure bar test, gas gun impact test, drop hammer test, etc. In the Faculty of Engineering and Industrial Sciences, Swinburne University of Technology, researchers of the Structural Mechanics Research Group conduct FEM simulations of Aluminium Honeycombs under dynamic out-of-plane compression to analyse the impact behaviour of the material and structure. In their research, numerical simulations of the dynamic out-of-plane compression are conducted with ANSYS/LS-DYNA software which is a powerful FEM tool for modelling non-linear mechanics of solids, fluids, gases and their interaction. The FEM application has four major steps as shown in Figure 1.

Figure 1. Overview of FEM application  From Figure 1, at beginning, based on the researchers? design, the object with special structure (i.e. the honeycombs structure in this example) for FEM analysis is generated in the Object Modelling step. Then, researchers specify more detailed parameters of the object model in the FEM Initiation step, e.g.

material of the object and elements for modelling. Based on the well-defined model, researchers can run different FEM simulations according to requirements of the experiment, e.g.

speed of the compression and time interval for recording data.

This is the most time consuming and important step in the FEM application, which also generates the largest volume of data as simulation results. Depending on the speed of the compression, the computation time of this step varies from several hours to around one hundred hours, while depending on the time interval for recording data, the size of generated data varies from gigabytes to hundreds of gigabytes. These data are very  important for researchers, based on which the simulation results can be demonstrated in various ways for analysis.

As researchers often need to run different simulations, large volume of the generated results data are accumulated as time goes on. However, due to the capacity limit of the local storage system, researchers can only store the recently generated results. Whenever they want reuse or re-analyse the results of pervious simulations, they have to re-run the simulation from beginning to regenerate the data, which is not efficient. Hence researchers consider of migrating the FEM application to the cloud where the storage bottleneck can be avoided in a cost- effective way.

B. Problems Analysis The storage limitation would not be the case in the cloud,  because the commercial cloud service providers can offer virtually unlimited storage resources. But, due to the pay-as- you-go model in the cloud, cost is one of the most important factors that users would care about. In order to make good use of the redundant cloud resources from different service providers, we need to design a smart algorithm to greatly reduce the cost of storing large generated application data in the cloud. However, designing this algorithm is not an easy job, where the following two issues need to be carefully investigated.

1) All the resources in the cloud carry certain costs. No matter how we dealt with the generated data (e.g. storing, re- generating or transferring); we have to pay for the corresponding resources used. Different data vary in size, and have different re-generation costs and usage frequencies, e.g.

data generated in the FEM application in Figure 1; therefore, it is most likely not cost effective to store all the generated data in the cloud. Intuitively, some heuristics can be applied for reducing the cost of storing the generated data. For example, we can delete the less frequently used data which have large size but small re-generation cost, and re-generate them whenever reused. Also, for the less frequently used data which have large size and huge re-generation cost, we can transfer them to cheaper places for storage, e.g. to other cloud storage systems, or even out of cloud to users? own spare storage devices. Hence, there is a trade-off among computation, storage and bandwidth in the cloud which can minimise the cost of storing the generated application data. However, finding this trade-off is not easy, as data in the cloud have dependencies (i.e.

complex generation relationships) and this is also the key issue for cost-effective data storage in the cloud.

2) The best trade-off among computation, storage and bandwidth may not be the best strategy for storing the generated application data. When the deleted data are needed, the regeneration not only imposes computation cost, but also causes a time delay, e.g. Step 3: FEM Simulation in Figure 1 sometimes takes several days to finish. It is also the same for data being transferred to other places are needed to be transferred back. Depending on the different time constraints of applications [20], users? tolerance of this delay may differ dramatically. Therefore, for some applications, users? preferences on storage are needed to be investigated. However, for some application, users do not concern about waiting for them to become available, hence they may delete or transfer the     rarely used data to reduce the overall application cost.

Therefore, this issue is not the focus of this paper.

In this paper, we focus on the first research issue only.

We design an algorithm which can find the trade-off among computation, storage and bandwidth in the cloud, and thus be utilised for cost-effective data storage with multiple cloud service providers..



III. SCIENTIFIC DATASETS STORAGE IN CLOUDS In this section, we first present some preliminaries  including a classification of application data in the cloud and the important concept of DDG (Data Dependency Graph).

Then we present the data storage cost model which represents the trade-off among computation, storage and bandwidth in the cloud.

A. Application Data and DDG In general, there are two types of data stored in the cloud,  original data and generated data.

1) Original data are the data uploaded by users, for example, in scientific applications they are usually the raw data collected from the devices in the experiments. For these data, users need to decide whether they should be stored or deleted since they cannot be regenerated by the system once deleted.

As cost of storing original data is fixed, they are not considered in the scope of this paper.

2) Generated data are the data newly produced in the cloud while the applications run. They are the intermediate or final computation results of the applications, which can be reused in the future. For these data, their storage can be decided by the system since they can be regenerated if their provenance is known. Hence, our work is only applied to the generated data in the cloud that can automatically decide the storage status of generated datasets in applications. In this paper, we refer generated data as dataset(s).

DDG (Data Dependency Graph) [33] is a directed acyclic graph (DAG) which is based on data provenance in scientific applications. All the datasets once generated in the cloud, whether stored or deleted, their references are recorded in DDG.

In other words, it depicts the generation relationships of datasets, with which the deleted datasets can be regenerated from their nearest existing preceding datasets. Figure 2 depicts a simple DDG, where every node in the graph denotes a dataset. We denote dataset di in DDG as DDGdi ? . Furthermore, d1 pointing to d2 means that d1 is used to generate d2; d2 pointing to d3 and d5 means that d2 is used to generate d3 and d5 based on different operations; d4 and d6 pointing to d7 means that d4 and d6 are used together to generate d7.

Figure 2. A simple Data Dependency Graph (DDG)  To better describe the relationships of datasets in DDG, we define a symbol: ? , which denotes that two datasets have a  generation relationship, where di ? dj means that di is a predecessor dataset of dj in DDG. For example, in Figure 2?s DDG, we have d1 ? d2, d1 ? d4, d5 ? d7, d1 ? d7, etc.

Furthermore, ? is transitive, i.e.

kikjjikji ddddddddd ???????? .

B. Datasets Storage Cost Model In a commercial cloud computing environment, service  providers have their cost models to charge users. In general, there are three basic types of resources in the cloud: computation, storage and bandwidth. Popular cloud services providers? cost models are based on these types of resources.

For example, Amazon cloud services? prices are as follows2:  $0.10 per CPU instance hour for the computation resources;  $0.15 per Gigabyte per month for the storage resources; $0.12 per Gigabyte bandwidth resources for data  downloaded from Amazon via Internet.

In this paper, we facilitate our datasets storage cost model in the cloud as follows:  Cost = Computation + Storage + Bandwidth  where the total cost of the datasets storage, Cost, is the sum of Computation, which is the total cost of computation resources used to regenerate datasets, Storage, which is the total cost of storage resources used to store the datasets, and Bandwidth, which is the total cost of bandwidth resources used for transferring datasets.

To utilise the datasets storage cost model, we assume that the application be deployed in one cloud service3, denoted as c1, and there be m different cloud services, denoted as {c1, c2, ? cm}, for storing the generated datasets in the cloud. For a dataset in DDG {d1, d2, ? dn}, denoted as DDGdi ? , we define its attributes as follows: <xi, yi,s , zi,s , fi, vi, provSeti, CostRi>4, where  ? xi denotes the generation cost of dataset di from its direct predecessors in the cloud.

? yi,s denotes the cost per time unit (i.e. storage cost rate) of storing dataset di in cloud service cs. Especially, yi,1 denotes the cost rate of storing di in the cloud service where the application is deployed.

2 The prices may fluctuate from time to time according to market factors.  As this paper?s focus is cost-effectiveness, to simplify the problem, we assume that the same types of computation resources are used for generatio and regeneration of datasets, and the same types of storage resources are used for storing datasets.

3 We assume that the application only run with one cloud service due to the following two reasons: 1) Some applications contain dedicate commercial software, e.g. the ANSYS/LS-DYNA software in the FEM application introduced in Section II. Due to the license restriction, these kinds of software cannot be freely installed in different service providers? resources in the cloud.

2) Migrating applications, especially scientific applications to a cloud service is a complex process. In order to take advantage of on-demand cloud services, software in the applications usually need second development to facilitate the dynamic scale up and down in the cloud.

4 These atrributes were introduced in our prior work [33], based on which we incorperate bandwidth cost of data transfer into the original definitions. If needed, please refer to our prior work [33] for more detailed description of these attributes.

? zi,s denotes the transfer cost of dataset di from service provider cs to c1 , especially, zi,1 =0.

? fi is a flag which denotes the storage status of dataset di.

Specifically, , {1,2,... }if s s m= ?  represents that dataset di is stored in cloud service cs, and fi=0 represents that dataset di is deleted.

? vi denotes the usage frequency, which indicates how often di is used.

? provSeti denotes the set of stored provenance that are needed when regenerating dataset di. If we want to regenerate di, we have to find its direct predecessors, which may also be deleted or stored in other cloud services. provSeti is the set of the nearest stored predecessors of di in the DDG.  Hence the generation cost of di is  ,{ }  { }  ( ) j i  j i j k i  i j sj d provSet  k ik d provSet d d d  genCost d z  x x ?  ? ? ? ?  =  + +  ?  ? (1)  As we can see from formula (1), the regeneration cost of di is two folds: 1) the bandwidth cost of transferring di's stored provenance datasets to c1 which is the cloud service that the application is deployed, and 2) the computation cost of regenerating di in c1.

? CostRi is di?s cost rate, which means the average cost per time unit of dataset di in the cloud. The value of CostRi depends on the storage status of di, where  , ,  ( ) , 0 / / , / / i i i i  i i s i i s i i s  genCost d v f d is deleted CostR  z v y f s d is stored in c ? =?  = ? ? + =? (2)  Hence, the total cost rate of storing a DDG is the sum of CostR of all the datasets in it, which is ? ?DDGd ii RCost . We further define the storage strategy of a DDG as F, which denotes the storage status of datasets in the DDG. Formally,  { }i iF f d DDG= ? , which is the set of every dataset's attribute fi indicating the cloud service in which di is stored. We denote the cost rate of storing a DDG with the storage strategy F as SCR (Sum of Cost Rate), where  ( )FDDGd ii RCostSCR ? ?=                                        (3) Based on the definition above, different storage strategies  lead to different cost rates for the application. This cost rate, i.e.

cost per time unit, represents the cost-effectiveness of storage strategies, which incorporates the trade-off among computation, storage and bandwidth costs in the cloud. In next section, we will present the design of our T-CSB algorithm for cost- effective datasets storage based on this trade-off model.



IV. COST-EFFECTIVE DATASETS STORAGE ALGORITHM IN MULTIPLE CLOUD SERVICES  In the section, we first briefly introduce the philosophy of the novel T-CSB algorithm, and then describe the detailed steps of the algorithm in order to find the trade-off among computation, storage and bandwidth costs for storing generated datasets with multiple cloud services.

A. Overview of T-CSB (Trade-off among Computation, Storage and Bandwidth) Algorithm In this paper, we design the T-CSB algorithm that can find  the minimum cost storage strategy for storing datasets of linear DDG with multiple cloud storage services. Linear DDG means a DDG with no branches, where each dataset in the DDG only has one direct predecessor and successor except the first and last datasets. The minimum cost storage strategy found by the algorithm represents the best trade-off among computation, storage and bandwidth costs in the cloud. Given a general DDG, the T-CSB algorithm can be utilised in every linear segment of the DDG respectively and thus find the trade-off for storing datasets with multiple cloud services.

The basic idea of the T-CSB algorithm is to construct a Cost Transitive Graph (CTG) based on the linear DDG. First, for every dataset in the DDG, we create a set of vertices in the CTG representing different storage services where the dataset can be stored. Next, we design smart rules for adding edges to the CTG and setting weights to them. Based on rules, we guarantee that in the CTG, the paths from the start vertex to the end vertex have a one-to-one mapping to the storage strategies of the DDG, and the length of every path equals to the cost rate of the corresponding storage strategy in the cloud. Then we can use the well-known Dijkstra shortest path algorithm (or Dijkstra algorithm for short) to find the shortest path in the CTG, which in fact represents both the minimum cost storage strategy for datasets of the DDG with multiple storage services, and the best trade-off among computation, storage and bandwidth costs in the cloud.

B. Detailed Steps in the T-CSB Algorithm Given a linear DDG with datasets {d1, d2 ? dn} and m  cloud services {c1, c2 ? cm} for storage. The T-CSB algorithm has the following four steps:  Step 1: Create vertices for the CTG. First, we create the start and end vertices, denoted as verstart and verend. Then, for every DDGdi ? , we create a vertex set { }miiii verververV ,2,1, ...,= , where m is the number of cloud services in which di can be stored. Hence veri,s represents dataset di storing in cloud service cs.

Step 2: Add directed edges to the CTG. For every CTGver si ?, , we add out-edges to all vertices in the set of  { }, , ',i s i s i i i iver ver CTG d d DDG d d? ? ? ? ?? ? ? ? ? . In other words, for any two vertices CTGverver sisi ???,, ,  belonging to different datasets? vertex sets (i.e. ii VV ?? ), we create an edge between them. Formally,  , , , ,, , ,i s i s i i i i i s i sver ver CTG d d DDG d d e ver ver? ? ? ? ? ?? ? ? ? ? ? ? < > .

Especially, for startver , we add out-edges to all other vertices in the CTG, and for endver , we add in-edges from all other vertices in the CTG.

Step 3: Set weights to edges in the CTG. The reason we call the graph Cost Transitive Graph is because the weights of its edges are composed of the cost rates of datasets. For an edge  , ,,i s i se ver ver ? ?< > , we denote its weight as , ,,i s i sver ver? ? ?< > , which is defined as the sum of cost rates of di' and the datasets     between di and di', supposing that only di and d i' are stored with corresponding cloud services and the rest of datasets between di and d i' are all deleted. Formally:  ( ) ( )  , ,  { }  , , { }  ,  ( ) k k i k i  k k i k i  i s i s  i kd d DDG d d d  i s i i s k kd d DDG d d d  ver ver CostR CostR  z v y genCost d v  ?  ?  ?  ? ?  ? ? ? ? ?  ? ? ? ? ? ? ? ? ?  < >  = +  = ? + + ?  ?  ? (4)  Since we are discussing linear DDG, for the datasets between di and di', di is the only dataset in their provSets. Hence we can further derive:  ( ) ( )( )  , , , ,  ,{ } { }  ,  * k k i k i h h i h k  i s i s i s i i s  i s k h kd d DDG d d d d d DDG d d d  ver ver z v y  z x x v  ?  ?  ? ? ? ? ? ? ?  ? ? ? ? ? ? ? ?  < > = ? +  + + +? ?   Step 4: Find the shortest path of the CTG. From the above construction steps, we can clearly see that the CTG is an acyclic oriented graph. Hence we can use the Dijkstra algorithm to find the shortest path from startver  to endver . The Dijkstra algorithm is a classic greedy algorithm to find the shortest path in graph theory. We denote the shortest path from  startver  to endver as >< endstart ververP ,min .

For the given linear DDG with datasets {d1, d2 ? dn} and m cloud storage services {c1, c2 ? cm}, the length of  >< endstart ververP ,min  of its CTG is the minimum cost rate for storing the datasets in the DDG, and the corresponding storage strategy is represented by the vertices that >< endstart ververP ,min traverses.

In Figure 3, we demonstrate a simple example of constructing CTG for a DDG with two datasets {d1, d2} and m different cloud services for the storage.

Figure 3. Example of constructing CTG for DDG  Next, we analyses the efficiency of our T-CSB algorithm.

As introduced above, for a linear DDG with n datasets and m cloud services for storage, we need to create mn vertices in the CTG. Hence the number of edges in the CTG is in the magnitude of m2n2. Since the time complexity of calculating the longest edge?s weight is O(n2), the worst case time complexity for creating the CTG is O(m2n4). Next, the time complexity of the Dijkstra algorithm is O(m2n2). Hence the total time complexity of the T-CSB algorithm is O(m2n4).



V. EVALUATION In this section, we demonstrate simulation results  conducted on Amazon cloud. First, we introduce our simulation setup and evaluation method. Then, we present general simulation results and evaluate the cost effectiveness of our algorithm.

A. Simulation Setup and Evaluation Method As Amazon is a well-known and widely recognised cloud  service provider, we conduct experiments on Amazon cloud using on-demand services for simulation. We implement our algorithm in Java programming language and run the algorithm on the virtualised EC2 instance with the Amazon Linux Image to evaluate its cost effectiveness and efficiency. We choose the standard small instance (m1.small) to conduct the experiments, because it is the basic type of EC2 CPU instances, which has a stable performance of one ECU5.

To evaluate the cost effectiveness of our T-CSB algorithm for multiple cloud storage services, we compare it with different representative storage strategies for one cloud service provider, which are as follows:  ? Store all datasets strategy, in which all generated datasets of the application are stored in the cloud.

? Store none datasets strategy, in which all generated datasets of the application are deleted after being used.

? Cost rate based strategy reported in [32] [35], in which we store datasets in the cloud by comparing their own generation cost rate and storage cost rate.

? Local-optimisation based strategy reported in [34], in which we only achieve the localised optimum of the trade- off between computation and storage in the cloud.

Next, we assume that the scientific application be deployed in Amazon cloud using EC2 service6 ($0.1 per CPU instance hour) for computation and S3 service ($0.15 per gigabyte per month) for storage. To utilise our T-CSB algorithm, we assume that generated datasets can be transferred to another two cloud services for storage with the prices: Storage Service One: $0.1 per gigabyte per month for storage and $0.01 per gigabyte for outbound7 data transfer and Storage Service Two: $0.05 per gigabyte per month for storage and $0.06 per gigabyte for outbound data transfer. We only use the above prices as representatives, as many cloud service providers (e.g. GoGrid8, Rackspace9, Haylix10, and Amazon Glacier11 etc.) have similar pricing models.

5 ECU (EC2 Computing Unit) is the basic unit defined by Amazon to measure the compute resources. Please refer to the following address for details.

http://aws.amazon.com/ec2/instance-types/ 6 Amazon cloud service offers different CPU instances with different prices, where using expensive CPU instances with higher performance would reduce computation time. There exists a trade-off of time and cost [14] which is different to the trade-off of computation and storage described in this paper, hence is out of this paper?s scope.

7 At present, most cloud storage services only charge on the outbound data transfer, while inbound data transfer is usually free.

8 GoGrid: http://www.gogrid.com/ 9 Rackspace: http://www.rackspace.com/ 10 Haylix: http://www.haylix.com/ 11 Amazon Glacier: http://aws.amazon.com/glacier/     Figure 4. C  TABLE I.  DETA  Strategies  DDGs  Cost Rate based Strategy  Local- Optimisation  based Strategy  Deleted Stored (S3) Deleted Stored (S3)  100 datasets 64 36 57 43 200 datasets 133 67 118 82 300 datasets 203 97 176 124 500 datasets 334 166 286 214 700 datasets 466 234 406 294  1000 datasets 644 356 577 423     To further demonstrate the practicality algorithm, we adapt real cloud service pr models and use them as the additional cloud respectively in the simulation. Specifically,  (1) Amazon Glacier. Glacier is an ext storage service that provides secure and dur data archiving and backup. The pricing model is: $0.01 per gigabyte per month for storage, $0 for outbound data transfer from Glacier.

(2) Haylix cloud storage. Haylix is a le IaaS cloud service provider, who provide storage with fast access for local Australian transfer over the Internet is often expensive an in general, some cloud service providers cooperate with network Infrastructure provide to provider dedicate connection service (e Connect) for boosting the data transfer speed cloud. Hence, we use the pricing models of H Direct Connect in our simulation, i.e. $0.12 month for storage in Haylix, $0.046 per gigab data transfer from Haylix.

Cost-effectiveness comparison of different storage strategies  AILED DATASETS STORAGE STATUS OF DIFFERENT STORAGE STRATEGIES  T-CSB algorithm with Two Additional Storage Services  T-CSB algorithm with Additional  Haylix Storage  Deleted Stored (S3) Stored  (Service1) Stored  (Service2) Deleted Stored (S3)  Stored (Haylix  43 0 29 28 57 38 5 93 0 38 69 118 72 10  149 0 69 82 173 110 17 223 0 98 179 286 187 27 324 0 150 226 404 262 34 428 0 182 390 573 379 48  of our T-CSB roviders? pricing d storage service  tremely low-cost rable storage for for using Glacier 0.02 per gigabyte  eading Australian s reliable cloud n users. As data nd relatively slow  (e.g. Amazon) ers (e.g. Equinix) .g. AWS Direct in and out of the  Haylix and AWS per gigabyte per  byte for outbound  B. Simulation Results The simulations are conducte  DDG with datasets of random si usage frequencies. In the experime large DDGs with different numbe random size from 1GB to 100GB.

random, from 10 hours to 100 hou again random, from once per month to run our T-CSB algorithm, we pa linear DDG segments with 50 data the T-CSB algorithm.

Based on the above settings, we DDGs with different number of dat rates (i.e. average daily cost) of sto shows the increases of the daily co the number of datasets grows in illustrates detailed datasets storage different storage strategies.

From Figure 4, we can see that t ?store all datasets? strategies are investigating the trade-off between   12 The impact of DDG partition on cost-effec strategy has been investigated in our prior wo    T-CSB algorithm with Additional  Glacier Storage d x) Deleted  Stored (S3)  Stored (Glacier)  5 0 95 29 0 171 29 0 271 50 0 450 67 0 633  103 0 897  d on randomly generated izes, generation times and ents, we randomly generate er of datasets, each with a The generation time is also  urs. The usage frequency is h to once per year. In order artition the large DDGs into asets12, on which we apply  run evaluation strategies on tasets and calculate the cost oring the datasets. Figure 4 ost of different strategies as n the DDG, and Table I  status of the DDGs under  the ?store none dataset? and very cost ineffective. By  n computation and storage,   ctiveness and efficiency of storage ork [34].

the ?cost rate based strategy? and ?local-optimisation based strategy? can smartly choose to store or delete the datasets in one cloud storage service (as shown in Table I), thereby largely reducing the cost rate for storing datasets with one cloud service provider. If more cloud storage services are available, as shown in Figure 4, the simulation of ?T-CSB algorithm with two additional storage services? demonstrates further reduction of the cost rate by taking bandwidth cost into account. Table I shows the number of datasets transferred and smartly stored in two representative cloud storage services with our T-CSB algorithm. Furthermore, how much cost can be reduced depends on the price of available storage services. In the simulation of ?T-CSB algorithm with additional Haylix storage?, although some datasets are transferred to Haylix for storage (as shown in Table I), the cost rate only drops slightly comparing to the ?local-optimisation based strategy? (as shown in Figure 4). This is because the price of Haylix is not much cheaper than Amazon S3 cloud. In contrast, in the simulation of ?T-CSB algorithm with additional Glacier storage?, our T-CSB algorithm significantly reduces the cost rate (as shown in Figure 4) by transferring datasets to Glacier13 for storage (as shown in Table I).

From the above simulation, we can see that for different price models of cloud storage services, our T-CSB algorithm can always store the datasets accordingly, even in the situation that the price difference is minor (e.g. the simulation of ?T- CSB algorithm with additional Haylix storage?). Hence our T- CSB algorithm is very effective in reducing the cost (i.e. cost- effective) for storing generated application datasets with multiple service providers in the cloud.



VI. RELATED WORK Today, research on scientific applications in the cloud  becomes popular [17] [18] [25] [30]. Comparing to the traditional computing systems, e.g. cluster, grid and HPC systems, a cloud computing system has cost benefits in various aspects [4]. With Amazon clouds? cost model and BOINC volunteer computing middleware, the work in [19] analyses the cost benefits of cloud computing versus grid computing. The work by Deelman et al. [11] also applies Amazon clouds? cost model and demonstrates that cloud computing offers a cost- effective way to deploy scientific applications. The work mentioned above mainly focuses on the comparison of cloud computing systems and the traditional distributed computing paradigms, which shows that applications running in the cloud have cost benefits. However, our work focuses on reducing cost for running application in the cloud.

This paper is mainly inspired by the research in the area of scheduling, in which much work focuses on reducing various ?costs? for applications [29], systems [31] or data centre networks [10]. The difference is that scheduling aims at improving resource utilisation whilst our work investigates the trade-off among computation, storage and bandwidth costs, which is a unique issue in cloud computing due to the pay-as- you-go model. Another important foundation for our work is the research on data provenance. Due to the importance of data   13 Data stored in Glacier usually need 3 to 5 hours to become available when users retrieve them. As analysed in Section II.B, users? delay tolerance is out of the scope of this paper. Hence we only focus on the cost in the simulation.

provenance in scientific applications, many works about recording data provenance of the system have been done [9].

Recently, research on data provenance in cloud computing systems has also appeared [22]. More specifically, Osterweil et al. [24] present how to generate a data derivation graph for execution of a scientific workflow. Foster et al. [12] propose the concept of virtual data in the Chimera system, which enables the automatic regeneration of datasets when needed.

Our DDG is based on data provenance, which depicts the dependency relationships of all the generated datasets in the cloud. With DDG, we can manage where the datasets are stored or how to regenerate them.

As the trade-off among computation, storage and bandwidth is an important issue in the cloud, much research has already embarked on this issue to a certain extent. First, plenty of research has been done with regard to the trade-off between computation and storage. The Nectar system [15] is designed for automatic management of data and computation in data centres, where obsolete datasets are deleted and regenerated whenever reused in order to improve resource utilisation. In [11], Deelman et al. present that storing some popular intermediate data can save the cost in comparison to always regenerating them from the input data. In [2], Adams et al. propose a model to represent the trade-off of computation cost and storage cost. In [33], the authors propose the CTT-SP algorithm that can find the best trade-off between computation and storage in the cloud, based on which a highly cost-effective and practical strategy is developed for storing datasets with one cloud service provider [34]. However, the above work did not consider bandwidth cost into the trade-off model. In [6], Baliga et al. investigate the trade-off among computation, storage and bandwidth in the infrastructure level of cloud systems, where reducing energy consumption is the main research goal. In [3], Agarwala et al. transform application data to certain formats and store them with different cloud services in order to reduce storage cost in the cloud, but data dependency and the option of data regeneration are not considered in their work. In this paper, we propose the T-CSB algorithm which can find the best trade- off among computation, storage and bandwidth costs for storing datasets of linear DDG in the cloud. This algorithm can be utilised for cost-effectively storing generated application datasets with multiple service providers in the cloud.



VII. CONCLUSIONS AND FUTURE WORK In this paper, we have investigated the unique features of  storing large volume of generated scientific datasets with multiple cloud service providers in the cloud. Towards achieving the cost-effectiveness, we have proposed a T-CSB (Trade-off among Computation, Storage and Bandwidth) algorithm to find the minimum cost storage strategy for datasets of linear DDG, which also represents the best trade-off among three key factors (computation, storage and bandwidth) for the cost of data storage in the cloud.  This algorithm can be utilised for cost-effectively storing generated application datasets with multiple service providers in the cloud. General simulations indicate that our T-CSB algorithm is very effective in reducing cost for cloud storage.

In our current work, we assume that the storage of one cloud service provider have a unified price. However, in the     real world, the price of cloud storage is different according to different usages. In the future, we will incorporate more complex pricing models in our datasets storage cost model.

Furthermore, methods for forecasting dataset usage frequency can be further studied, with which our T-CSB algorithm can be adapted to different types of applications more easily.

