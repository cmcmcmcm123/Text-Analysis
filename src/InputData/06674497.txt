A New Dimension of Parallelism in Ultra High Throughput LDPC Decoding

Abstract?In modern communication systems the required data rates are continuously increasing. High speed transmissions can easily generate throughputs far beyond 1 Tbit/s. To ensure error free communication, channel codes like Low-Density Parity Check (LDPC) codes are utilized. However state-of-the-art LDPC decoders can process only data rates in the range of 10 to 50 Gbit/s. This results in a gap in decoder performance which has to be closed. Therefore we propose a new ultra high speed LDPC decoder architecture. We show that our architecture significantly reduces the routing congestion which poses a big problem for fully parallel, high speed LDPC decoders. The presented 65nm ASIC implementation runs at 257 MHz and consumes an area of 12 mm2. The resulting system throughput is 160 Gbit/s, it is the fastest LDPC decoder which has been published up to now. At the same time we show that extremely parallel architectures do not only increase the maximum throughput but also increase area and power efficiency in comparison to state-of-the-art decoders.



I. INTRODUCTION In today?s digitally connected society everybody is ?online?  all the time. Due to the extensive use of bandwidth-hungry multimedia applications like YouTube, HDTV and voice-over- IP, the bandwidth demand on wireless and wired commu- nication channels is rapidly growing. IEEE has completed the ratification of the IEEE 802.3ba standard, setting the target Ethernet speed as 100 Gbit/s [1]. For fiber channel applications the upper limit is even higher, early results show throughputs in the order of 100 Tbit/s [2]. At the same time efficient forward error correction is required to ensure error free transmission. To approach the channel capacity while serving high data rates, LDPC codes are excellent candidates.

Today LDPC codes are used in a wide range of applications like 10 Gigabit Ethernet (10 GBASE-T, IEEE802.3an) [3], broadband wireless communication (UWB, WiGig) [4] [5] and storage in hard disc drives [6]. Even though LDPC codes belong to the best channel coding schemes known today, their implementation poses big challenges. This is due to the iterative, computationally expensive decoding procedure, requiring massive parallel architectures to achieve the targeted throughput.

LDPC codes have been introduced by Gallager in 1962 [7] but the high decoding complexity made the application at that time impossible. When LDPC codes have been rediscovered in the late 90s, the throughput demands have been moderate.

Serial decoder architectures have been sufficient to fulfill the  Fig. 1: H Matrix and Tanner Graph Hardware Mapping  requirements. As demands on the throughput rose, partially parallel architectures became necessary. Today fully parallel decoder architectures are required to satisfy applications like 10 GBASE-T. As future standards emerge, current architec- tures will not be able to facilitate the demanded throughputs of 100 Gbit/s and more. However these high throughputs can only be achieved at the cost of flexibility, e.g. IEEE 802.3an defines only one block length and code rate. This makes new architectural approaches necessary.

In this paper we explore a new dimension of parallelism for LDPC channel decoders. We propose to unroll the iterative decoding loop and instantiate a dedicated hardware for each it- eration which is pipelined. This allows for decoders processing one block per clock cycle. The novel architecture reduces the routing congestion and latency. This allows for a significantly higher area and energy efficiency in comparison to state-of- the-art architectures which make decoders with ultra high throughput of 100 Gbit/s and more feasible.



II. LDPC DECODING ALGORITHM  LDPC codes [7] are linear block codes defined by a sparse parity check matrix H of dimension M ? N , see Fig. 1a.

A valid code word ?x has to satisfy H?xT = ?0 in modulo-2 arithmetic. A descriptive graphical representation of the whole code is given by a Tanner graph. Each row of the parity check matrix is represented by a check node (CN) and corresponds to one of the M parity checks. Respectively each column corresponds to a variable node (VN) corresponding to one of the N code bits. The Tanner graph shown in Fig. 1b is the alternative representation for the parity check matrix of Fig. 1a.

Edges in the Tanner graph reflect the ?1?s in the H matrix.

There is an edge between VN n and CN m if and only if Hmn = 1. LDPC codes can be decoded by the use of different algorithms. Which one fits best has to be chosen dependant on   2013 IEEE Workshop on Signal Processing Systems     the required communications performance. For example the ?- min algorithm [8] performs better than the min-sum algorithm [9] but has a significantly higher implementation complexity.

In the following discussion we focus on the min-sum algorithm as it is used for our implementation. All algorithms have in common, that probabilistic messages are iteratively exchanged between variable and check nodes. First the variable nodes are initialized with the channel values ?chn . According to the connections in the parity check matrix these values are passed from each variable node n with n ? {0, ..., N ? 1} to the connected check nodes M(n) with  M(n) = {m|m ? {0, ...,M ? 1} ?Hmn ?= 0}. (1)  The set of all variable nodes connected to check node m with m ? {0, ...,M ? 1} is defined as  N (m) = {n|n ? {0, ..., N ? 1} ?Hmn ?= 0}. (2)  The check node computation can be split in two parts, the actual parity check, represented by the sign sgn(?) and the result?s probability represented by the magnitude |?|. These extrinsic messages are computed as follows:  sgn ( ?(i)m?n  ) =  ? n??N (m)\n  sgn ( z (i) n??m  ) (3)  ????(i)m?n ??? = ? ? min  n??N (m)\n  ( |z(i)n??m|  ) , (4)  with the extrinsic scaling factor (ESF) ?. Finally the messages are returned to the variable nodes which generate new intrinsic messages z(i) for iteration i.

z(i)n?m = ? ch n +  ? m??N (n)\m  ? (i?1) m??n = ?  (i?1) n ? ?(i?1)m?n (5)  with  ?(i)n = ? ch n +  ? m??M(n)  ? (i) m??n (6)  The sign of the a posteriori probability ? can be interpreted as the hard decision bit. Message exchange between variable and check node continues until either a valid code word is found or a maximum number of iterations is exceeded.

What we described till now reflects a two-phase scheduling.

This schedule processes all check nodes before the variable nodes produce new messages. However it is also possible to apply a so called layered scheduling [10] [11] which is only applicable to partial parallel decoder architectures. In the layered scheduling the check nodes are processed in a way, that the variable nodes are intermediately updated. Thus the remaining CNs can take results into account which have been generated in the same iteration. The layered schedule can achieve the same communications performance as the two- phase schedule while reducing the needed iterations by almost 50%.

Fig. 2: LDPC Decoder Design Space

III. STATE-OF-THE-ART LDPC DECODER ARCHITECTURES  The LDPC decoder design space comprises a multitude of parameters which have to be tuned to the specific re- quirements. Each standard has different needs in means of error correction performance, number of supported code rates, codeword lengths and throughput. There are numerous design decisions which have to be made for the hardware to satisfy the requirements. We have presented an in depth investigation of the design space in [12]. In this paper we focus on the design decisions regarding parallelism. The design space extension we present is exploring a new dimension of parallelism in LDPC channel decoding.

There are multiple dimensions in which the degree of parallelism can be chosen, see Fig. 2. The lowest level of parallelism is on the message level. Each message can be transmitted in tuples of bits or fully parallel. Today fully parallel message transfer can be found in the vast majority of architectures. The second degree of parallelism can be found on processing node level. The node?s in- and outgoing edges can be processed one after an other, partially parallel or fully parallel. However the choice of the node?s edge parallelism is directly linked to the so called ?hardware mapping?. The hardware mapping is the highest level of parallelism which is used today. It describes how many check and variable nodes are instantiated. When we are talking of a fully parallel decoder, an architecture instantiating all processing nodes is meant. In contrast partially parallel decoders have a lower number of nodes than the Tanner graph. They process the parity check matrix in a time multiplex fashion and thus have a very high decoding latency. This allows for easy adaption of the architecture to new standards but limits the achievable throughput. The edge parallelism is chosen to fit the needs of the hardware mapping, for example if only one submatrix is     Channel values  Decoded bits  Variable Nodes  Check Nodes  Chv Reg  NW -1  NW  ...

...

...

...

Fig. 3: Fully Parallel LDPC Decoder Architecture  processed at a time, an edge parallelism of one is sufficient.

For applications like 10 GBASE-T Ethernet only fully parallel architectures can achieve the required throughput. Fig. 3 depicts the high-level structure of this architecture. However, there are some problems with the fully parallel architecture. At first we have to consider the two networks between VNs and CNs. Dependent on the code length and quantization, each of them comprises between several thousands and hundred thousands of wires which have to be routed according to the parity check matrix. To achieve a good communications performance the parity check matrices have long cycles and thus no locality, resulting in massive routing congestions. It has been shown in earlier publications [13] [14] that the area utilization is heavily impaired by this fact and only 50% of the chip is used by logic. The second drawback of the fully parallel architecture is the high latency which is unavoidable due to the iterative nature of the algorithm. The latency issue holds also for partially parallel decoders. In these architectures only after one block is completed, the next one can be accepted. Thus the decoder?s latency directly impacts the achievable throughput.

Fully parallel, inflexible decoders can still satisfy today?s standards requirements in means of throughput. However, for future standards the throughput demands will further increase but in addition flexibility in means of the supported code rates will be required which cannot be implemented by state-of-the- art decoders.



IV. EXPLORING A NEW DIMENSION IN THE LDPC DECODER DESIGN SPACE  In the following section we discuss the possibilities to overcome the limitations in LDPC decoder throughput.

A. Core Duplication  One solution to achieve the throughputs required by future standards is to instantiate several LDPC decoder cores in parallel. There are two possible starting points for the core duplications. First we can use partially parallel architectures which allow for flexibility but suffer from high latencies as explained earlier. Furthermore they cannot cope with unstruc- tured parity check matrices which gain better communications performance. The second option is to instantiate several fully parallel decoder cores allowing for reduced latency and high throughput. However due to routing congestions they cannot achieve a satisfying area efficiency and flexibility. Summarized  Fig. 4: Unrolled LDPC Decoder Architecture  Architecture Flexibility Low Latency Area/Energy Eff.

Parallel inst., partially parallel architecture  + - o  Parallel inst., fully parallel architecture  - o -  Unrolled, fully paral- lel architecture  - + +  TABLE I: Parallel LDPC Decoder Architectures  simple decoder duplication is not an efficient architectural approach for very high throughput decoders.

B. Unrolling Iterations  We propose a new architecture, shown in Fig. 4 to overcome the highlighted drawbacks. So far, the iterations were not considered as a degree of parallelism for LDPC decoders. In [15] and [16] the unrolling of decoding iterations for turbo codes has already been considered. However no hardware implementation has been published. Thus an in depth inves- tigation especially of the influence on routing congestion and implementation efficiency is not available.

We unroll the iterative decoding loop and instantiate an unrolled, fully parallel LDPC decoder which is pipelined.

The number of pipeline stages determines the latency, but the throughput is fixed by the cycle duration. Hence this architecture has a throughput of one codeword per clock cycle.

The approach allows for ultra high throughput LDPC decoder cores.

There is an essential change in the resulting data flow.

Where before data have iteratively been exchanged between VNs and CNs, now all data flows in one direction. The result is a unidirectional wiring avoiding the overlap of opposed net- works. This is a big benefit for the routing. Table I summarizes the benefits and drawbacks of the different approaches.

Fig. 5: Bit-Error Rate for IEEE802.11ad, code rate 13/16 and 64-QAM modulation

V. THE UNROLLED LDPC DECODER IMPLEMENTATION  In the following section we present the architectural details of the unrolled LDPC decoder. The proposed idea of unrolling the iterations can be combined with most other design de- cisions. Due to space limitations we can only present one point in the huge design space which is spanned by this new approach. The implemented decoder is designed for one fix code rate of 13/16 complying to the IEEE 802.11ad standard with a quantization of four bit. We are applying the min- sum algorithm for the decoding process. Fig. 5 shows the communications performance for the implemented decoder. In comparison to a six bit reference implementation significant losses can be observed. The hardware unrolls nine iterations as the decoding performance hardly increases after that. The focus of this paper is clearly on the presented architecture and can easily be adapted to other codes or quantizations if required by the target application.

A. Top Level Architecture  The top level architecture has already been shown in Fig. 4.

Each stage consists of all the check and variable nodes defined in the parity check matrix and a pipeline register. The decoding process is consecutively executed in the pipeline. The number of the stages defines the number of decoding iterations. The inputs for each stage is the result of the previous stage and the channel value. This means the channel value has to be available for all iterations and is therefore stored in the pipeline register. Due to the fully parallel node instantiation the com- putation for one iteration is processed within one clock cycle.

A new block can thus be accepted in every cycle and when the pipeline is filled also a block result is output every cycle. The check and variable nodes can be further pipelined to increase the maximum achievable frequency. In difference to state-of- the-art architectures for the presented decoder the latency has no impact on the achievable throughput. Independent of the number of pipeline stages a new block can be accepted in every clock cycle. For the presented design there are pipeline  in 0  in 1  + 2C2SM  2C2SM  +  +  -  -  +  + 2C2SM  2C2SM  +  +  -  -  in 2  in 3  chv  out 0  out 1  out 2  out 3  Fig. 6: Fully Parallel Variable Node Architecture  sort  sort CS4  sort  sort CS4  CS4  sort  sort CS4  sort  sort CS4  CS4  CS4 esf  esf  SM2TC  SM2TC  neg.

Min0 pos.

Min0  neg.

Min1 pos.

Min1  in 0 in 1 in 2 in 3 in 4 in 5 in 6 in 7  in 8 in 9 in 10 in 11 in 12 in 13 in 14 in 15  Fig. 7: Fully Parallel Check Node Minimum Search Stage  registers at the decoder?s in- and outputs, after each check and variable node and inside the check nodes. These additional registers allow for much higher overall clock frequency and thus a higher throughput and area efficiency.

B. Variable Node  The variable nodes implement equation 5 which is basically an accumulation of all values and a subtraction for each output.

Fig. 6 shows the according hardware architecture. It processes all edges in parallel, for the used code the maximum variable node degree is four. We maximize the degree of parallelism by an adder tree summing up all extrinsic messages and the channel value. The incoming messages have already a two?s complement representation which results in very little hardware cost for the variable nodes. All outgoing messages are computed in parallel and are converted to a sign-magnitude representation (2C2SM) which is required for efficient check node processing.

C. Check Node  The basic operation in the check node is given in equations 3 and 4. In comparison to the variable node, the check node is way more complex. We can separate the check node in two main parts. One minimum search stage and the output stages. Fig. 7 depicts the minimum search stage, locating the two smallest magnitudes of the 16 inputs. In the presented architecture this is done with a tree like structure. Each two values are sorted and forwarded to compare select (CS) units with four inputs. Due to the sorted inputs the CS units can neglect some comparisons and save hardware resources. After     sign  min  out 0  neg.

Min0 pos.

Min0  neg.

Min1 pos.

Min1  Fig. 8: Fully Parallel Check Node Output Stage  three CS stages the two smallest magnitudes are found and get prepared for the output stages. Therefore the messages are scaled with the ESF and converted from sign magni- tude representation to the two?s complement number format (SM2TC). Fig. 8 shows one of the 16 output stages. Each is responsible to generate the message for one outgoing edge. As we have pulled all the expensive calculations into the minimum search part, the output stages are very simple. They consist of three multiplexers selecting one of the four generated outputs of the minimum search stage. Two multiplexers select the correct sign and the last one chooses between first and second minimum. The control signals for the output stages are generated by an xor of the message signs and a comparison of the minimum with the respective input.



VI. IMPLEMENTATION RESULTS The LDPC decoder is implemented on a 65nm low power  bulk CMOS library from ST Microelectronics. We consid- ered the following PVT parameters: Worst Case (WC, 1.1V, 125?C), Nominal Case (NOM, 1.2V, 25?C) and Best Case (BC, 1.3V, -40?C). Synthesis was performed with Synopsys Design Compiler in topographical mode, Placement & Routing (P&R) with Synopsys IC Compiler. Synthesis as well as P&R were performed with the Worst Case PVT settings of the 65nm library.

The final design reaches a frequency of 257 MHz and thus is capable to process 257 ?106 blocks per second. For the Nominal Case 420 MHz can be achieved. The architecture processes one block per clock cycle, resulting in a maximum throughput of 160 Gbit/s for the Worst Case scenario. After P&R 12 mm2 are required for the core. An area utilization (cell/core area ratio) of 76 % is achieved. We also evaluated the power consumption of the physical design using Synopsys PrimeTime-PX. The average power consumption at a clock frequency of 257 MHz is 5.36 W for the Nominal Case.

The physical layout of the unrolled LDPC decoder can be seen in Fig. 9. Each iteration can be found as one of the vertical areas in the corresponding layout. Channel messages ripple from left to right through the chip while decoding.

All routing is very structured and pointing from one iteration to the next. This confirms the expectation of a simplified routing. Table II compares some recently published high throughput LDPC decoder designs. Partially as well as fully parallel decoders are presented, supporting different standards and algorithms. Obviously a fair comparison is difficult since the implementation efficiency must always be linked to the  Fig. 9: Unrolled fully parallel LDPC decoder layout. Each decoder iteration is represented by a different column.

communications performance. But all the decoders where designed for high throughput.

The reduced routing complexity of the unrolled decoder im- pacts the energy consumption. The presented architecture can compete in means of energy per decoded bit in comparison to other implementations. Regarding further energy optimizations the proposed architecture is an excellent candidate for a Near- Threshold circuit [21]. E.g. the throughput of 10 Gbit/s can already be fulfilled by the presented decoder running at less than 20 MHz. Thus aggressive voltage scaling to 0.5-0.6V can be applied. This increases the energy efficiency by at least a factor of three, allowing for the same or even better energy efficiency than [19].

Even though we have to use a two-phase scheduling we can achieve a very good area efficiency, even in comparison with layered decoders like [17]. Layered decoders in general achieve a higher area efficiency as they need less iterations for the same communications performance. However as layered scheduling can only be applied to partially parallel decoders it is prohibitive for ultra high speed architectures. The increase in area efficiency of factor 1.3-8 of our design in comparison to other decoders underlines that the unrolling simplifies the networks and allows for efficient routing.

Finally the presented architecture is more than four times faster than the fastest comparable decoder. Thus it is to the best of our knowledge the first channel decoding system unrolling the iterative decoding loop and the fastest LDPC decoder ever published.



VII. CONCLUSION Future communication standards will pose big challenges in  means of throughput requirements and flexibility for decoder designs which cannot be solved by today?s architectures. We     Decoder [17] [18] [19] [20] Proposed  CMOS Technology 65nm LVT 65nm 65nm GP 65nm 65nm SVT Supply Voltage [V] 1.2 0.9 1.15 1.3 1.2 Frequency [MHz] 400 400 540 195 257 Standard IEEE 802.15.3c IEEE 802.3an IEEE 802.11ad IEEE 802.3an IEEE 802.11ad Block Size 672 2048 672 2048 672 Code Rate 1/2, 5/8, 3/4, 7/8 1723/2048 1/2 1723/2048 13/16 Level of Parallelism partially parallel partially parallel partially parallel fully parallel fully parallel, unrolled Scheduling layered two-phase n/a two-phase two-phase Algorithm min-sum min-sum min-sum threshold min-sum min-sum Iterations 10 8 10 11 9 Quantization 6 4 5 5 4 Decoding Latency [ns] 100 260 148 n/a 105 Post P&R Area [mm2] 1.30 5.05 1.6 4.84 12.09 Throughput [Gbit/s] 6.7 8.5 9 36.3 160.8 Energy Eff. [pJ/bit/It.] 8 11.76 0.9 3.36 3.61 Area Eff. [Gbit/s/mm2] 5.2 1.7 5.6 7.5 13.6  TABLE II: State-of-the-Art High Throughput LDPC Decoder Comparison  explored a new dimension in the LDPC decoder design space and implemented the first LDPC decoder unrolling the iterative decoding loop. The resulting 65nm ASIC design consumes 12 mm2, runs at a clock frequency of 257 MHz and generates a throughput of 160 Gbit/s. To the best of our knowledge, this is the fastest LDPC decoder ever published. Finally it is not only the outstanding throughput but also the very high power and area efficiency that makes our approach suitable for future applications. Compared to the best competitors we increased the area efficiency by at least 80%.

