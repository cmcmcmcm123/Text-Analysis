Multi-Label Classification by ART-based Neural Networks and

Abstract This paper presents a data mining system for  multi-label classification and hierarchy extraction from the  predictions provided by a multi-label classifier. Classes in multi-  label classification tasks are often hierarchically organized and  the hierarchy is assumed to be known. A much less investigated  approach and a more challenging task, however, is to suppose  that the underlying class taxonomy is unknown and that a  data mining system can infer it automatically. In our setting,  the proposed system is trained with multi-label data and is  subsequently able to produce multi-label predictions along  with hierarchical relationships between classes. The hierarchy  extraction algorithm is based on building association rules  from label co-occurrences. Within the framework we exam-  ine the performance of two recently introduced multi-label  extensions of Adaptive Resonance Theory (ART)-based neural  networks: Multi-Label Fuzzy ARTMAP (ML-FAM) and Multi-  Label Fuzzy Adaptive Resonance Associative Map (ML-ARAM)  in comparison with two state-of-the-art classifiers Multi-Label  k-Nearest Neighbors (ML-kNN) and BoosTexter, taking into  account the quality of hierarchy extraction. We also develop a  novel distance measure for the quantitative evaluation of the  derived class hierarchies and compare it with two other distance  measures. To demonstrate the effectiveness of the proposed  approach, experiments on several benchmark datasets have  been performed.



I. INTRODUCTION  Searching for information in an ample database of doc-  uments classified in multiple categories can be an exhaus-  tive task. Automatic hierarchical classification of documents  using techniques from data mining and machine learning  fields is now becoming a key technology in managing  great amounts of information available from the Internet  and large databases. This classification task is considered  as Hierarchical Multi-label Classification (HMC) because  instances are labeled by multiple classes rather than just one,  and the classes are hierarchically organized. In recent years,  hierarchical multi-label classifiers have been developed that  mirror the structure of the class taxonomy [1][4] or methods  incorporating the class hierarchy information into a classifier    [5], [6].

HMC algorithms work with known class taxonomies,  which should usually be provided by domain experts. How-  ever, it is often difficult to obtain such a hierarchy, for  Fernando Benites is with the Department of Computer and Informa-  tion Science, University Konstanz, Box 712, 78457 Konstanz, Germany  (email:fernando.benites@uni-konstanz.de).

Florian Brucker is with the Department of Computer and Informa-  tion Science, University Konstanz, Box 712, 78457 Konstanz,Germany  (email:florian.brucker@uni-konstanz.de).

Elena Sapozhnikova is with the Department of Computer and Infor-  mation Science, University Konstanz, Box 712, 78457 Konstanz,Germany  (email:elena.sapozhnikova@uni-konstanz.de).

example, when the data are changing frequently. For this  reason, we are interested in solving a more challenging  Multi-label Classification (MC) task when the hierarchy itself  is unknown and should be found automatically. To solve this  task, techniques for data-driven Hierarchy Extraction (HE)  are needed. The proposed data mining system consists of a  multi-label ART-based neural classifier (ML-FAM or ML-  ARAM recently introduced in [7]) and an HE algorithm. It  produces class hierarchies taking into account relationships  between the labels assigned by the classifier to multi-labeled  objects.

The key difference of our approach in comparison to  learning hierarchies from data directly is the use of a  multi-label classifier, which serves as a data mining system  and enables HE even when available multi-label data are  sparse or many instances remain single-labeled. One of the  advantages of this approach is that a relatively small training  set can be used to learn the classifier or it can be trained  online, subsequently producing a large number of multi-label  predictions for HE. A potential application of the proposed  system is, for example, information fusion when the same  data exist, but are labeled differently either by independent  experts or by alternative class taxonomies. In such cases, the  system would be able to integrate available knowledge by  creating a common class hierarchy.

We examined the impact of using different multi-label  classifiers on our system. Two well-known state-of-the-art  multi-label classifiers ML-kNN [8] and BoosTexter [9] were  chosen for comparison with ML-FAM and ML-ARAM. We    empirically evaluated the system with these four classifiers  taking into account not only their classification performance  but also the quality of hierarchies obtained from their pre-  dictions. The latter can be quantitatively assessed by using  datasets with known class taxonomies and calculating special  hierarchy proximity measures between extracted and real  hierarchies. In addition to the existing Constrained Tree Edit  Distance (CTED) [10] and Taxonomic Overlap (TO) [11],  we developed a new distance measure for hierarchies named  Lowest Common Ancestor Path Distance (LCAPD), which is  better suited to our needs.

The paper is organized as follows. After introducing a  general description of HE in Section II, the multi-label clas-  sifiers are presented in III. Hierarchy proximity measures are  discussed in IV. Experimental results evaluating the proposed  system on multi-label datasets with known hierarchies are  described and discussed in Section V. Finally, Section VI  concludes the paper.

978-1-4244-8126-2/10/$26.00 2010 IEEE

II. HIERARCHY EXTRACTION FROM PREDICTED  MULTI-LABELS  A. Hierarchical Multi-label Classification  HMC extends MC by imposing a hierarchy on multi-  labels: A hierarchy H on L is considered as a rooted labeled  directed tree, i.e. an arrangement of L in the tree form. We  require that H contains each label i ? L exactly once and  that the root of the tree has label 0 6? L. Since there is a  one-to-one mapping between hierarchy nodes and labels we  usually do not distinguish between them. Thus H can be  interpreted as a set of edges H ? (L ? {0})  L where  (i, j) ? H iff i is the parent of j. We denote with pH(i) the  parent of i in H and with AH(i) all its ancestors (excluding  i).

The connection between multi-labels and a hierarchy H  is as follows: If a multi-label m contains a certain label  i ? L we require that its parent and thus all its ancestors are  also in m: AH(i) ? m for all i ? m. If this holds true, we  can say that m is consistent with H . A family M of multi-  labels is consistent with H if every m ? M is consistent  with H . In turn, H is then also termed as being consistent  with m and M , respectively. It is important to note that  the prediction task is still the same as in traditional MC,    and that the hierarchy itself is not part of the training data.

Thus, while the original multi-labels in an HMC dataset are  assumed to be consistent with the corresponding hierarchy,  this cannot, in general, be true for predicted multi-labels.

How well the predicted multi-labels adhere to the hierarchy  depends heavily on the used classification algorithm.

B. Hierarchy Extraction  HE is the process of extracting an adequate hierarchy from  a given family of multi-labels M . This problem does not  have a unique solution: For example, the trivial hierarchy  {(0, i) | i ? L} is consistent with any family of multi-labels.

It is also important to note that predicted multi-labels usually  contain errors which make them inconsistent with the true  hierarchy. A good HE algorithm therefore has to trade off  consistency for level of detail: The more trivial edges the  hierarchy contains (i.e. edges linking a label to 0) , the  higher its consistency usually is, but the less information  is contained in it and vice versa.

1) Basic Algorithm: Similar to [12], an association rule  generator was used for building a hierarchy from multi-label  predictions. The algorithm works as follows:  a) Confidence Matrix: The relationships between indi-  vidual labels are estimated using a confidence matrix  C = (cij)i,j . For two labels i, j the confidence of j  being an ancestor of i, cij , is defined as cij := nij/ni,  where nij is the number of multi-labels containing both  i and j and ni the number of multi-labels containing  (at least) i. Since a label is never an ancestor of itself  we set cii := 0 for all labels i.

b) Thresholding: For any two labels i, j we check which  of the confidences cij and cji is smaller, and that  confidence is set to 0. In addition for a fixed label  i we expect that the values in the i-th row Ci :=  (cij)j fall into two groups: Ancestors j of i should  have a confidence cij that is close to 1, while non-  ancestors should have confidences close to 0. To detect  and remove non-ancestors from Ci we therefore sort  Ci descendingly and find the largest difference in  consecutive sorted values. All confidences except the  ones before that difference are set to 0. This procedure  is applied to each label i.

c) Level Assignment: From each pair of labels i, j with    a confidence cij > 0 a rule i ? j is created. To each  label i ? L a level li is assigned as follows: Assign  level 1 to all labels not appearing as consequents in  any of the obtained rules. Remove all rules j ? k  where lj = 1. The next level is assigned to all labels  not appearing as consequents in any remaining rule.

Remove all rules j ? k where j is in that level. Repeat  this procedure until a level has been assigned to each  label or no rules remain. The next level is then assigned  to any labels to which no level has yet been assigned.

d) Hierarchy Construction: To construct the tree we as-  sign each label i to a unique parent. The idea is that  the parent p(i) of i should have a high confidence  value ci,p(i) and a level that is as low as possible  while still being higher than the level of i. The set  of candidates Qi therefore contains all labels j for  which cij > 0, and whose level is higher than the  level of i: Qi := {j ? L : cij > 0 ? lj > li}.

If Qi = ?, then 0 6? L is chosen as the parent of i,  representing the virtual root of the hierarchy. Otherwise  we select among the candidates those with the lowest  level: Q?i := {j ? Qi : lj = mink?Qi lk}. The j ? Q?i  for which cij is maximal is then chosen as the parent  of i.

The main problem of the adaptive filtering method used  in the Thresholding step of the basic algorithm is that it fails  if a label i is a root label and therefore does not have any  ancestors. In this case Ci does only contain noise, and only  part of that noise is removed using the adaptive filtering.

Therefore, additional filtering of the confidence matrix C  is needed. We propose two following approaches: Global  Thresholding and Voting.

2) Global Thresholding: This algorithm tries to find an  appropriate global threshold value hc for the confidence  matrix. It analyzes the curve formed by the values cij > 0  when sorted in ascending order, looking for jumps, or, if  there is no jump high enough, by checking if there is a point  where this curve has slower growth, finding stable regions  in the process. It is similar to the Thresholding step of the  basic algorithm but uses the whole matrix and handles noise  better.

First apply the Confidence Matrix and Thresholding steps    of the basic algorithm. Then:  1) Sort in ascending order all values cij > 0 in  the sequence x = {xn}, n = 1, . . . , N and cal-  culate the greatest jump for the sequence K =  argmaxn(?xn),?xn = xn+1 ? xn.

2) If  P  n  xn  2?N ??xK < 0.1, we select a tolerance of 10% of  the whole range, set hc = xK and continue to 3), since  the jump is high enough. Otherwise x is smoothed by  a low pass filter with a rectangular window of size  s = 2 ? log(|L|): f(x). Then hc = xc+1 is the entry of  x with the lowest index plus one, satisfying: ?f(xc) <P  n  ?f(xn)  N  .

3) By setting all values from the confidence matrix cij <  hc to 0 we filter the matrix globally. The Level Assign-  ment and Hierarchy Construction steps of the basic  algorithm are then performed to construct the final  hierarchy.

3) Voting: In this method, a fixed thresholding with a  threshold c ? [0, 1] is performed after the Thresholding step  by setting all confidences cij < c to 0. Afterwards, the  algorithm continues as specified above. Since the optimal  threshold value depends on the set of multi-labels, we extract  hierarchies as described for different thresholds (in our case  for 200 values equally distributed in the range of [0, 1]). To  construct the final hierarchy we attach the parent that was  chosen most frequently to each label.



III. MULTI-LABEL CLASSIFIERS  A. FAM and ARAM  Due to space constraints only the basic steps of the algo-  rithms are presented below, for more details see [13], [14]. A  Fuzzy ARTMAP (FAM) neural network is built of two self-  organizing Fuzzy ART modules, ARTa and ARTb (Figure  1), which process inputs A and targets B respectively by  adapting existing or creating new prototype nodes in their  second layers F2. The F2 fields are linked by the Map    Field  an associative memory, which contains associations  between ARTa and ARTb prototypes. This enables FAM  to learn mappings between input and target pairs during  training. In classification tasks, the target vectors usually  represent class labels, for example in the binary form.

Initially, the ART modules have only one uncommitted  prototype node, where the weight vector is set to unity  and the Map Field weights are also set to unity. In the  complement coded form [14], vectors A and B represent  an input pattern and its class label vector. After presentation  of A to the F1 field of ARTa, the activation function Tk  given by (1) is calculated for each of the N nodes in F2 and  the winner is then chosen by the Winner-Take-All (WTA)  rule (2):  Tk(A) =  |A ?W ak|  ?+ |W ak|  (1)  where ? denotes the fuzzy AND, element-wise min operator,  and ? > 0 is called the choice parameter;  TK = max  {  Tk : k = 1, . . . , N  } (2)  The choice of K must be confirmed by checking the match  criterion:  |A ?W aK |  |A|  ? ?a (3)  Map Field  Fig. 1. FAM neural network.

where ?a ? [0, 1] is the vigilance parameter. If (3) fails,  the system inhibits the winner K and enables another node  to be selected. When no existing prototype provides the  satisfactory similarity, the network adds a new uncommitted  neuron and adapts it. Thus, N grows during learning.

The described operations are identical for both modules  ARTa and ARTb, but at ARTa the Map Field vigilance  criterion that tests the correctness of the learned target vector  B corresponding to the ARTa winner K should be checked  additionally:  |U b ?W abK |    |U b|  ? ?ab (4)  where U b denotes the ARTb output vector and W abK denotes  the weights connecting the Kth ARTa node to the Map Field.

The choice of the Map Field vigilance parameter ?ab is not  critical since the left-side term of the inequality (3) can be  equal to either 1 or 0. If the inequality fails, then Match  Tracking initiates the choice of a new node by increasing  the ARTa vigilance parameter ?a to a value slightly greater  than the left-side term of (3). This search process continues  until the input is either assigned to a committed node that  satisfies both the ARTa match criterion (3) and the Map  Field vigilance criterion (4) or to a previously uncommitted  neuron. A successful end of search starts the learning process  at ARTa and ARTb (similarly):  W  a(new)  K = ?a  (  A ?W  a(old)  K  )  + (1? ?a)W  a(old)  K (5)  where ?a ? [0, 1] is the learning rate. The fast-learning mode  corresponds to setting ?a,b = 1. With fast learning, the Map  Field weights are set to U b?W abK once the node K becomes  committed and this association is permanent.

Although fuzzy ARAM may be tuned to be functionally  equivalent to FAM, the architecture differs significantly (Fig-  ure 2). ARAM contains no Map Field because its F2 nodes  are linked directly to the F1 fields of both ARTa and ARTb  modules. In its simplified version, ARAM first computes  the activation function (1) and selects the winner node K  according to (2). Then the winner choice should be confirmed  by both match criteria simultaneously:  |A ?W aK |  |A|  ? ?a,  |B ?W bK |    |B|  ? ?b (6)  Fig. 2. ARAM neural network.

If any of the inequalities of (6) is violated, the node K is  inhibited, if all nodes are inhibited, a new uncommitted node  is added. The Match Tracking process is started if needed.

When each match criterion is satisfied in the respective  module, resonance occurs and the learning process follows.

During learning the selected node K learns to encode the  input and target vectors by adjusting its weight vectors W aK  and W bK similarly to (5).

B. Multi-label Extensions  It has been shown in [7] that standard algorithms do not  perform well on the muti-label datasets due to the use of  the WTA rule (2), therefore the following modification has  been proposed: A set of Nb best categories with the largest  activation values is chosen according to the rule: a category k  is included in the set, if the relative difference (TK?Tk)/TK  is below a predefined fraction t of the activation range  r = TK ? Tmin, where Tmin is the minimum activation  of committed neurons. Activations of the Nb categories  are subsequently normalized, uk = TkPN  b  s=1  Ts  . The resulting  distributed output pattern P is calculated as follows:  P =  Nb?

s=1  usps. (7)  where for FAM ps = W br with r such that W  ab  sr = 1 which  means that between neurons s of ARTa and r of ARTb  an association in the Map field exists, and ps = W bs for  ARAM. Thus, P contains a score for each label that is  proportional to the number of neurons among the Nb best  categories predicting this label. Since the fast learning of  ART networks leads to varying performance with the input  presentation order, voting across several networks trained  with different orderings of a training set improves classi-    fication performance. The sum over the output of Equation  (7) for each voter produces a collective distributed output  pattern P , which is used to determine the predicted classes.

Finally, a postprocessing filter method [12] is applied: the  signals P are sorted descendingly, then all corresponding  output classes are included in the multi-label up to the point  of maximum decrease in the signal size from one class to  the next.

A small modification is made in the ARTb weights, which  now count label occurrences during learning by increasing  their values by 1 each time the corresponding multi-label is  presented. This modification leads to changes in the ARTb  activation calculation of ML-FAM which is Tk = |B?W bk|.

Another difference between the multi-label and the standard  algorithms is that the former ones do not use Match Tracking  with raising vigilance: The winner is simply inhibited.

C. Comparison of Classifiers  To compare the two ART-based networks with other ML-  classifiers, we chosen ML-kNN described in [8] and BoosTex-  ter [9] which is an implementation of Boosting for the field  of text categorization, based on the well-known AdaBoost  algorithm [15]. ML-kNN and BoosTexter were chosen since  these popular MC algorithms are nearest-neighbor- and rule-  based, respectively, and therefore allow a comparison of the  results of ML-FAM and ML-ARAM with algorithms based  on other approaches than neural networks.



IV. PROXIMITY MEASURES FOR HIERARCHIES  Approaches for quantitative comparison of graphs and  trees are either defined on an abstract level, taking into  account only the pure graph structure, or make use of the field  specific semantics encoded in the graph. From the former  category, we utilized the Constrained Tree Edit Distance  (CTED) [10] as a representative of the popular edit distances,  whereas Taxonomic Overlap (TO) [11] was chosen from the  field of concept representation, which also deals with hierar-  chical structures encoded in graphs and trees. Additionally,  the Lowest Common Ancestor Path Distance (LCAPD) is a  new tree distance measure, developed especially for the type  of hierarchies common in HMC. A description of the mutual  differences between these measures and the motivation for  the development of a new metric are given at the end of this  section.

A. Constrained Tree Edit Distance  CTED [10] is based upon the idea of transforming one  hierarchy into another via certain local editing operations.

The distance between two hierarchies is the minimum total  number of operations needed for transforming the one into  the other. Three modification operations are considered: A  node i is deleted by transmitting its children to its parent and  removing i from the hierarchy. Inserting is the complement  of deleting: Inserting i as a child of j makes i the parent  of some subset of the children of j. Changing a node  means changing its label. In order to make computation in  polynomial time possible, the constraint that disjoint subtrees  have to be mapped to disjoint subtrees is included in the  definition [10], [16].

The original definition does not include any normalization.

For two hierarchies with q labels each, 2q is the highest  possible CTED  corresponding to deleting all labels of the  first hierarchy and inserting all labels of the second hierarchy.

We normalize the CTED by 12q which results in distances in  [0, 1].

B. Taxonomic Overlap  TO [11] compares the nodes of two hierarchies according  to how many ancestors and descendants they share. We de-  scribe a version adapted to our experiments. Define semantic  cotopy as  SC(i,H) := AH(i) ? {i} ? {j ? H | i ? AH(j)}.

Given two hierarchies H1 and H2, the corresponding overlap  for i ? L is defined as  O(i,H1, H2) :=  |SC(i,H1) ? SC(i,H2)|  |SC(i,H1) ? SC(i,H2)|  Note that this is a symmetric expression in H1 and H2 with  values in [0, 1]. The TO of H1 and H2 is the average of all  overlaps:  TO(H1, H2) :=   q  ?

i?L  O(i,H1, H2)  For two identical hierarchies, TO is 1, while it is 0 if H1  and H2 are totally unrelated. Since the two other hierarchy    proximity measures used in the experiments are distances  (with smaller values indicating higher similarity), we use  TO? := 1? TO to simplify comparisons.

C. LCA-Path Tree Distance  Since in a hierarchy a parent is a more general concept  than any of its children, the locations of the same label i in  two hierarchies H1 and H2 can be compared by how far one  has to travel up the chain of ancestors in both hierarchies  until a common ancestor is found. For a label i ? L the  depth dH(i) of i is defined recursively by dH(0) := 0 and  dH(i) := dH(pH(i)) + 1 for i 6= 0. Further denote with  a1, . . . , av the ancestors of i in H1, such that a1 = pH1(i),  a2 = pH1(a1), etc., v = dH1 (i) and av = 0. The lowest  common ancestor of i in H1 and H2, lca(i,H1, H2), is the  first ancestor of i in H1 which is also an ancestor of i in  H2:  j? := min  {  j | j ? {1, . . . , v} such that aj ? AH2(i)  }  lca(i,H1, H2) := aj? .

Since 0 is an ancestor of all labels in all hierarchies, the  LCA always exists. Note that usually lca(i,H1, H2) 6=  lca(i,H2, H1).

We use the LCA to measure how far apart the positions  of a label in both hierarchies are: If a label i has the same  parent in both hierarchies it is regarded as being in the same  position, otherwise the distances of i to its LCA in both  hierarchies are used to measure how far apart both locations  are. The cost for i is defined as  d(i,H1, H2) :=  2?

k=1  dHk(i)? dHk(lca(i,H1, H2))  D(i,H1, H2) :=  {  0, if pH1(i) = pH2(i)  d(i,H1, H2), otherwise  (8)  This can be seen as the length of the path from i to  lca(i,H1, H2) in H1 and from lca(i,H1, H2) to i in H2.

As D is not symmetric, the complete LCA-path distance    between H1 and H2 is the mean value  D(H1, H2) :=   W  ?

i?L    (  D(i,H1, H2) + D(i,H2, H1)  )    2 3  (a)    1 3  (b)  Fig. 3. Sample hierarchies.

where W is a normalization factor: The worst case for a  misplaced label i is that lca(i,H1, H2) = 0, since then  (8) reduces to D(i,H1, H2) = dH1(i) + dH2(i). The same  holds accordingly for D(i,H2, H1). Thus choosing W =?

i?L dH1 (i) + dH2(i) ensures that D(H1, H2) ? [0, 1].

D. Comparison  All three hierarchy proximity measures presented above  reflect different concepts of tree similarity. It is thus not  surprising that CTED and TO*, although they perform well  in the domains they were designed for, may produce counter-  intuitive results when used in the field of HE. This is  demonstrated by the hierarchies shown in Figure 3: Despite  the fact that the relationships between the nodes in both  hierarchies are completely different, their CTED and TO*  are 0.33 and 0.44, respectively. Their LCAPD, on the other  hand, is 1.00. Thus only LCAPD marks both hierarchies with  the highest possible distance.

Some hierarchies used in our experiments contain nodes  with exactly one child (single-child labels). It can be shown  that TO* does not distinguish between such labels and their  children, in the following sense: Given a hierarchy H and  a single-child node i, we denote with H ? the hierarchy    that is obtained from H by swapping i and its child. Then  TO?(H,H ?) = 0. This is, in general, not the case for either  CTED or LCAPD.



V. EXPERIMENTS  We tested the proposed data mining system with four  multi-label classifiers: ML-FAM, ML-ARAM, ML-kNN and  BoosTexter on three real-world datasets with increasing  complexity from the text-mining field. In our experiments  we compared extracted hierarchies to the original hierarchy  by means of three hierarchy proximity measures: LCAPD,  CTED, and TO*.

The experimental setup of ML-FAM and ML-ARAM  had the following parameter values: choice parameter ? =  0.0001; learning rates ?a,b = 1.0 for fast learning; vigilance  parameters ?a = 0.9 and ?b = 1.0. The parameter t was  chosen to be 0.05 for the 20 newsgroups dataset and 0.02  for the other datasets. The number of voters V was set to 9.

Following [8], we used 10 nearest neighbors (k = 10) and  Laplace smoothing (s = 1) for ML-kNN.

BoosTexter was trained using 500 boosting rounds as in  [8] and the threshold for converting rankings into multi-labels  was set to 0 [9].

A. Multi-label Classification Performance Measures  We used a large set of performance measures for eval-  uation of the MC experiments: First, two example-based  measures for multi-label predictions: Accuracy (A) and F-  measure (F) [17] were taken. Accuracy measures how many  of the predicted labels are actually present while F-measure  is the harmonic mean of precision and recall calculated on  the per-instance basis. The larger the A and F values, the  better the MC performance.

Then two label-based measures were calculated on the  basis of binary counts for each label: the numbers of true  positives, true negatives, false positives, and false negatives.

We used the micro-averaged version of F1 with the binary  measures counted on the whole test set. The perfect perfor-  mance corresponds to F1 = 1. Additionally, micro-averaged  precision and recall were used for computing the Area Un-  der a Precision-Recall Curve (AUPRC) [6]. Micro-averaged  precision corresponds to the proportion of predicted labels  in the test set that are correct and recall to the proportion of  labels that are correctly predicted. AUPRC has been claimed    to be a well-suited performance measure for MC tasks  where the number of negative instances significantly exceeds  the number of positive instances [6]. Another advantage of  AUPRC is its global nature and independence of a certain  threshold value. The closer the AUPRC value is to 1, the  better the performance.

Since these measures are based on the comparison of  multi-labels, they depend on a transformation from rank-  ings to classes. As a contrast we also used four well-  known ranking-based performance measures: One-Error  (OE), Ranking Loss (RL), Coverage (C), and Average Pre-  cision (AP) [8]. Smaller values indicate better performance  for all ranking-based performance measures except AP. One-  Error evaluates how many times the top-ranked label is not  in the set of proper labels of the instance. Ranking Loss is  defined as the average fraction of pairs of labels that are  ordered incorrectly. Coverage evaluates how far we need, on  average, to go down the list of labels in order to cover all  the proper labels of the instance. Average Precision evaluates  the average fraction of labels ranked above a particular label  i ? mt which actually are in mt.

And finally, the special hierarchical loss function H-loss  [2] were utilized. Following [3], normalized costs were  calculated: ci := 1/|c(p(i))|, (i ? L), where c(i) is the set  of all direct children of i. Hierarchical loss (H-loss) does not  consider mistakes made in subtrees of incorrectly predicted  labels and penalizes only the first mistake along the path  from the root to a node. The smaller the H-loss value, the  better the performance.

B. 20 Newsgroups Dataset  We modified the popular single-label dataset 20 News-  groups [18], [19] by considering eight additional labels  corresponding to the intermediate levels of the hierarchy:  faith, recreation, recreation/sport, recreation/vehicles, poli-  tics, computer, computer/hardware, science. This dataset is  a collection of almost 20,000 postings from 20 newsgroups  sorted by date into training (60%) and test (40%) sets. The  data were preprocessed by discarding all words appearing  only in the test documents and all words found in the stop  word list [20]. Afterwards, all but the 2%-most-frequent  words were eliminated to reduce the dimensionality. Docu-  ments were represented using the well-known TF-IDF (Term    Frequency  Inverse Document Frequency) word weighting  scheme [19]. The TF-IDF weights were then normalized to  the range of [0, 1]. Conversion to TF-IDF and normalization  were performed separately for training and test data. This re-  sulted in the 1,070-dimensional dataset with 11,256 training  instances, 7,493 test ones and 28 labels.

To test the performance of two HE algorithms, we first ex-  tracted hierarchies from the True Test Multi-Labels (TTML)  and calculated the corresponding proximity measures. Both  algorithms successfully extracted the original hierarchy.

We studied the performance of the multi-label classifiers  and their ability to infer the class hierarchies in the presence  of only partly available hierarchical information. We per-  formed a series of HE experiments with multi-labels having a  decreasing number of inserted non-leaf labels describing the  levels in the hierarchy. We randomly removed such labels  from 20%, 30%, and 40% of the training instances leaving  them single-labeled. The results for predicted test multi-  labels are shown in Table I where the bold face marks the  best classifier, and the first column (left) for distances is the  result of HE by Voting and the second (right) by Global  Thresholding (referred as GT).

Comparing classification performance, one can see that  the ART-based networks are superior to both the other  classifiers in terms of most performance measures and that  ML-FAM slightly outperforms ML-ARAM. Taken together  they win on at least 6 and at most 8 out of 9 evaluation  measures. BoosTexter has the second best performance, but  its predictive power degrades more quickly with the increase  in the number of single-label instances. The poorest MC  results were shown by ML-kNN, its performance decreased  very fast with any reduction in the number of multi-labels:  For example, F1 decreased by 15% while removing 40% of  labels instead of 30%. It is also interesting to note that when  trained on the dataset with 40% removed labels, ML-FAM  and ML-ARAM significantly outperformed ML-kNN trained  on the original dataset with all labels.

The hierarchy proximity measures confirm the good qual-  ity of predictions produced by the ART-based networks: The  hierarchies were correct extracted by both HE algorithms of  Section II-B even with 40% removed labels. The predictions  of ML-kNN were the worst: The Voting variant of the HE    algorithm could not extract the correct hierarchy with 30%,  assigning five labels incorrectly to the root label. None of  the HE algorithms could extract the correct hierarchy in  the absence of 40% multi-labels. With 40% and Voting, the  number of labels falsely assigned to the root was 13, while  with GT it was only three. For BoosTexter, Voting assigned  two labels wrongly to the root label in the experiment with  TABLE I  20 NEWSGROUPS ALL, -20%, -30% AND -40% RESULTS  Measure all 20% 30% 40%ARAM FAM kNN BoosT. ARAM FAM kNN BoosT. ARAM FAM kNN  BoosT. ARAM FAM kNN BoosT.

A 0.635 0.638 0.429 0.549 0.613 0.633 0.383 0.456 0.596 0.619 0.322 0.412 0.563 0.591 0.255  0.387  F1 0.694 0.696 0.565 0.677 0.675 0.688 0.528 0.604 0.662 0.677 0.469 0.566 0.640 0.657 0.392  0.542  F 0.691 0.692 0.480 0.605 0.671 0.688 0.429 0.507 0.658 0.676 0.364 0.465 0.630 0.652 0.296  0.441  OE 0.221 0.220 0.336 0.222 0.259 0.236 0.387 0.275 0.273 0.259 0.415 0.301 0.301 0.291 0.434  0.316  RL 0.100 0.098 0.124 0.073 0.108 0.110 0.128 0.077 0.098 0.103 0.132 0.079 0.101 0.106 0.135  0.082  C 4.188 4.168 6.080 4.164 4.397 4.446 6.184 4.286 4.246 4.340 6.326 4.328 4.334 4.463 6.397  4.379  AP 0.789 0.790 0.677 0.778 0.774 0.782 0.657 0.758 0.769 0.774 0.645 0.747 0.759 0.764 0.638  0.740  AUPRC 0.775 0.772 0.618 0.749 0.743 0.727 0.581 0.691 0.733 0.722 0.555 0.671 0.715 0.708  0.535 0.660  H-loss 0.103 0.123 0.121 0.094 0.102 0.098 0.124 0.108 0.106 0.103 0.132 0.117 0.115 0.111  0.145 0.122  Wins 1 5 0 3 1 6 0 2 2 6 0 1 2 6 0 1  LCAPD 0 0 0 0 0 0 0 0 0 0 0.12 0 0.05 0 0 0 0.51 0.26 0.17 0  CTED 0 0 0 0 0 0 0 0 0 0 0.14 0 0.07 0 0 0 0.50 0.21 0.21 0  TO* 0 0 0 0 0 0 0 0 0 0 0.11 0 0.05 0 0 0 0.39 0.18 0.15 0  30% removed labels and and six labels in the experiment  with 40% removed labels. GT resulted in zero distances in  the both cases. Assigning more labels to the root creates more  shallow and wider hierarchies (trivial case as stated before).

The good hierarchy extraction with ART networks demon-  strates the system robustness  even with strongly damaged  data the system can rebuild the original hierarchy.

C. RCV1-v2 Dataset  The next experiment was based on the tokenized version of    the RCV1-v2 dataset introduced in [21]. Only the topics label  set consisting of 103 labels arranged in a hierarchy of depth  four is examined here. Documents of the original training set  of 23,149 were converted to TF-IDF weights and normalized.

Afterwards the set was splitted in 15,000 randomly selected  documents as training and the remaining as test samples.

In this case, the Voting variant of HE applied to the TTML  resulted in the LCAPD, CTED and TO* values 0.12, 0.15  and 0.13, respectively. The corresponding values of the GT  variant are 0.05, 0.07 and 0.05. The poor performance of the  Voting method is due to the fact that for the TTML only very  high threshold values succeed in removing enough noise.

The Voting results are thus dominated by bad hierarchies  extracted for all but the highest thresholds.

The classification and HE results for this dataset are shown  in Table II. ML-ARAM has better performance results on  this data set in all points than ML-FAM except for RL,  being the best of all classifiers in terms of the multi-label  performance measures. BoosTexter is the best in terms of all  ranking measures.

For both HE algorithms the distances of BoosTexter are the  best, those of ML-FAM second, followed ML-ARAM and  ML-kNN. All three distance measures correlate. Interesting  is also that for ML-kNN the distance values obtained by both  HE methods are almost the same.

The hierarchy extracted by GT from the TTML has much  lower distances values as compared with the hierarchies  extracted by both methods from predicted multi-labels. This  reflects a specific problem of HE, since only a small fraction  of the incorrectly classified multi-labels can prevent building  of a proper hierarchy. For example, 16.5% of misassigned  labels in the extracted hierarchy are responsible for about  80% of LCAPD calculated from the predictions of ML-  ARAM. This large part of the HE error is caused by only  4% of the test data. Under these circumstances the other  distances behave analogically. Most labels were not assigned,  making them trivial edges, but six labels were assigned to  a false branch. This can happen when labels have a strong  correlation and in the step Hierarchy Construction of the  basic algorithm the parent is not unique in the confidence  matrix. BoosTexters results suffer less from this problem  because it generally sets more labels for each test sample.

Both HE algorithms behaved similarly on the predictions  of the ART networks. They constructed a deeper hierarchy  than the original one and wrongly assigned the same 11  labels to the root node. The higher distances come from  Voting assigning more labels to the wrong branch. For ML-  kNN both algorithms again create very similar hierarchy  trees, both misassigned 28 labels to the root label. For  BoosTexter it was seven with Voting and eight with GT.

Voting produced a deeper hierarchy here.

D. WIPO-alpha Dataset  The WIPO-alpha dataset1 comprises patent documents  collected by the World Intellectual Property Organization  (WIPO). The dataset is split into training and test docu-  ments. Preprocessing was performed as follows: From each  document, the title, abstract and claims texts were extracted,  stop words were removed using the list from [20] and words  were stemmed using the Snowball stemmer [22]. All but  the 1%-most-frequent stems were removed, the remaining  stems were converted to TF-IDF weights and these were  normalized to the range of [0, 1]. Again, TF-IDF conversion  and normalization were done independently for the training  and the test set. The original hierarchy consists, from top  to bottom, of 8 sections, 120 classes, 630 subclasses and  about 69,000 groups. In our experiment, only records from  the sections A (5802 training and 5169 test samples) and  H (5703 training and 5926 test samples) were used. Each  1http://www.wipo.int/classifications/ipc/en/ITsupport/Categorization/  dataset/wipo-alpha-readme.html August 2009  TABLE II  RCV1-V2 RESULTS  Measure ARAM FAM kNN BoosT.

A 0.748 0.731 0.651 0.695  F1 0.795 0.777 0.735 0.769  F 0.805 0.787 0.719 0.771  OE 0.077 0.089 0.104 0.063  RL 0.087 0.086 0.026 0.015  C 11.598 11.692 8.563 5.977  AP 0.868 0.860 0.839 0.873  AUPRC 0.830 0.794 0.807 0.838  H-loss 0.068 0.077 0.097 0.081  Wins 4 0 0 5  LCAPD 0.29 0.22 0.25 0.20 0.34 0.34 0.21 0.18    CTED 0.32 0.23 0.28 0.22 0.38 0.37 0.24 0.20  TO* 0.27 0.18 0.22 0.17 0.31 0.30 0.21 0.17  document in the collection has one so-called main code and  any number of secondary codes, where each code describes  a group the document belongs to. Both main and secondary  codes were used in the experiment, although codes pointing  to groups outside of sections A and H were ignored. We  also removed groups that did not contain at least 30 training  and 30 test records (and any documents that only belonged  to such small groups). This resulted in 4,688 training and  7,364 test records with 924 attributes each and a label set of  size 131.

In this case, the Voting variant of the HE algorithm applied  to the TTML resulted in the LCAPD, CTED and TO* values  of 0.13, 0.12 and 0, respectively. GT showed the same  values. Remarkable are the TO* distances, which are equal  to 0. This is due to the fact that the WIPO-alpha hierarchy  contains 16 single-child labels that are not partitioned by  the true multi-labels: whenever a single-child label j is  contained in a multi-label, so is its child, and vice versa. It is  therefore theoretically impossible to deduce from the multi-  labels which of them is the parent of the other. As a result the  HE algorithms often choose the wrong parent, resulting in  higher LCAPD and CTED values. TO*, as described above,  is invariant under such choices.

The results obtained on the WIPO-alpha dataset are shown  in Table III. The classification performance of the ART-based  networks on this dataset is slightly worse than that of Boos-  Texter. Mostly in the terms of OE, RL, C, AP, AUPRC, and  H-loss measures BoosTexter is better because its rankings are  better and it assigned more labels to each sample. But the  ART networks have better HE results because their predicted  labels are more consistent with the original hierarchy. ML-  kNN has the worst classification results and distance values  again. The reason for the high relative difference between  LCAPD as well as CTED and TO* obtained for the ART  networks or BoosTexter as compared to the results of the  other datasets is because most of the labels were assigned in  the right branch but not exactly where they belong.

Both HE algorithms extracted the same hierarchy from the  predictions of ML-ARAM and a very similar hierarchy for  ML-FAM. About 5% labels were assigned wrongly to the    root label in the hierarchies of the ART networks. For ML-  TABLE III  WIPO-ALPHA(AH) RESULTS  Measure ARAM FAM kNN BoosT.

A 0.588 0.590 0.478 0.564  F1 0.694 0.691 0.614 0.693  F 0.682 0.682 0.593 0.679  OE 0.052 0.057 0.110 0.042  RL 0.135 0.136 0.056 0.025  C 25.135 25.269 22.380 11.742  AP 0.790 0.785 0.724 0.802  AUPRC 0.720 0.684 0.688 0.762  H-loss 0.090 0.093 0.149 0.079  Wins 1 2 0 6  LCAPD 0.16 0.16 0.17 0.17 0.32 0.38 0.21 0.21  CTED 0.18 0.18 0.19 0.19 0.38 0.53 0.27 0.27  TO* 0.05 0.05 0.07 0.07 0.24 0.32 0.08 0.08  kNN both HE methods wrongly assigned about the half of  the labels and about 20% of total labels were assgined to the  root label. Here, GT extracted a much worse hierarchy as  shown by CTED being 0.15 higher for GT than for Voting.

For BoosTexter both HE methods built the same hierarchy  and no label was wrongly assigned to the root. All extracted  hierarchies were one level deeper than the original one.

Although Voting produced worse hierarchies than GT on  two previous datasets, this time its distance values were  comparable or even better. In comparison to Voting, GT has  higher values for all distances on the multi-labels of ML-  kNN. Voting has the advantage of being a much simpler  method and of being more dataset independent. Still the tree  distances have the same ranking order for all classifiers for  both HE methods.



VI. CONCLUSION  In this paper we studied Hierarchical Multi-label Classifi-  cation (HMC) with unknown class hierarchies. The objec-  tive was to derive hierarchical relationships between out-  put classes from predicted multi-labels automatically. We  have developed a data-mining-system based on two recently  proposed multi-label extensions of the FAM and ARAM  neural networks: ML-FAM and ML-ARAM as well as on  a Hierarchy Extraction (HE) algorithm. The proposed HE  algorithm builds association rules from label co-occurrences    and has two modifications. The presented approach is general  enough to be used with any other multi-label classifier or  HE algorithm. We have also developed a new tree dis-  tance measure for quantitative comparison of hierarchies.

In extensive experiments made on three text-mining real-  world datasets, ML-FAM and ML-ARAM were compared  against two state-of-the-art multi-label classifiers: ML-kNN  and BoosTexter. The experimental results confirm that the  proposed approach is suitable for extracting middle and  large-scale class hierarchies from predicted multi-labels. In  future work we intend to examine approaches for measuring  the quality of hierarchical multi-label classifications.

