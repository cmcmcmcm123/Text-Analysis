Distributed Classification using Class-Association  Rules Mining Algorithm

Abstract? Associative classification algorithms have been successfully used to construct classification systems. The major strength of such techniques is that they are able to use the most accurate rules among an exhaustive list of class-association rules.

This explains their good performance in general, but to the detriment of an expensive computing cost, inherited from association rules discovery algorithms. We address this issue by proposing a distributed methodology based on  FP-growth algorithm. In a shared nothing architecture, subsets of classification rules are generated in parallel from several data partitions. An inter-processor communication is established in order to make global decisions. This exchange is made only in the first level of recursion,  allowing each machine to subsequently process all its assigned tasks independently. The final classifier is built by a majority vote. This approach is illustrated by a detailed example, and an analysis of communication cost.

Keywords- Distributed data mining; Association rule mining; Class-association rules; FP-growth algorithm

I. INTRODUCTION Many of data mining algorithms are suitable for distributed  computing for two reasons: scaling up this algorithms, or mining inherently distributed data. This paper focuses on classification algorithms based on association rules, a.k.a ?Associative Classification  algorithms? (AC)  which is a promising  approach in data mining that utilizes the association rule mining techniques to construct classification systems.

Several studies [1] [2] [3] have provided evidence that such algorithms are able to extract classifiers competitive with those produced by many of traditional classifiers like decision trees, rule induction and probabilistic approaches. The advantage of this approach lies in  (1) its simplicity compared to other mathematical classification methods, and its competitive results in term of accuracy (2) the  exhaustive quest for all rules allows to find many interesting and useful rules not being discovered  by the state-of-the-art classification systems like C4.5 [4] (3) It produces an understand model based on user- specified constraints. A typical associative classification system is constructed in two stages: (1) generating the complete set of class association rules (CARs) that satisfy the user-specified minimum support (called minSup) and minimum confidence (called minConf); (2) selecting the most accurate rules to build the classifier (pruning phase). The major strength of such systems is that they are able to use the most  accurate rules for classification because their rule generators aim to find all rules. However, when datasets contain a large number of rows and/or columns, both rule generation and rule selection in such systems are time consuming. To address this issue, we  propose a distributed algorithm for  associative classification based on FP-growth algorithm [5]. The remainder of this paper is organized as follows: the next section gives an outline on AC  algorithms, the section 3 provides a review of related works. In section 4, a detailed framework of the proposed distributed algorithm is described, following by a conclusion and discussion  on the tracks which always deserve to be explored.



II. ASSOCIATIVE CLASSIFICATION ALGORITHMS Classification is one task of data mining which allows  predicting if a data instance is member of a predefined class. In a more general objective, the task of association rule discovery focus on finding rules of the form A ? B relating disjoint set of database attributes, which is interpreted to mean ?if the set of attribute-values, or ?itemset? A is found together in a database record, then it is likely that the itemset B will be present also?.

The discovering of ?rules? in these two tasks is a strong common point which has inspired several researchers  to explore the possibility of using association rule discovery methods in  classification. The basic idea is to view classification rules as a special case of association rules, in which only the class attribute is considered in the rule?s consequent.   In order to build classifier, an AC algorithm uses a training data set D to produce class association rules (CARs) in the form of  X ? y,  where X ? I, the set of ?items?, and y ? Y , the set of class labels. An item is described as an attribute name Xi and a value xi, denoted < X?, x? >. The rule R: X ? y has support s, denoted as supp(R), if  s% of the cases in D contain X and are labeled with class y. A rule R: X ? y holds in D with confidence c, denoted as conf(R), if c% of cases in D that contain X are labeled with class y. The main goal is to construct a set of CARs that satisfies the minimum support minSup and minimum confidence minConf constraints, and  that is able to predict the classes of previously unseen data (the test data set), as accurately as possible. Other measures can be used like the conviction which take into account  the class frequency, it is defined as: conv(X ? y) = ((1 ? supp(y))) ? ((1 ? conf(X ? y) )), where  supp(y) is the frequency of the class label  y in D.

A. Generating rules for classification To find the complete set of classification rules passing  certain support and confidence thresholds, AC systems mine the training data set using a variant association rule discovery method. Several AC approaches, e.g. CBA [1], have been adopted from Apriori algorithm [6] which is based on an iterative process:  generate candidates, and generate frequent itemsets. This method is computationally expensive because it needs  repetitive scans of the data base.  Others, like CMAR [2], are based on FP-growth algorithm [5],  one of the most efficient serial algorithms to mine frequent item sets. The effectiveness of FP-growth algorithm comes from its FP-tree structure which   compresses a large  database into a compact form, and its divide-and-conquer methodology.  Furthermore, it avoids candidate generation, and needs only two scans of the entire data base.

B. Pruning Rules Pruning techniques rely on the elimination of rules that are  either redundant or misleading from taking any role in the prediction process of test data objects. The removal of such rules can make the classification process more effective and accurate. A survey of pruning methods is documented by Thabtha in [3].



III. RELATED WORK The design of distributed associative classification systems  does not differ too much from the design of distributed association rule mining methods, at least in the first step (generation of CARs). For this reason, a set of recent works on distributed FP-growth algorithm will be presented. These approaches proposed  for association rules discovery in distributed computing environment, may be adapted to generate the class-association rules in a distributed classification context.

A. Class-Association Rule mining based on serial FP-growth In AC algorithms using  FP-growth, classification rules are  generated by adapting this algorithm to take into account the class label. The key idea used in CMAR system [2] for example is to store the class label in the FP-tree structure. We illustrate this by the following example (fig. 1), where minSupp = 2 (66%). First, the training data set is scanned to find the list of frequent items F-list sorted by descending order of supports. A second scan is done to construct the FP-tree, where  each frequent item in each record is inserted in the tree according to the order of  F-list. This operation allows to represent a shared prefix of several records only once, like the prefix ?a1a2? shared by the first and second records. Support counts are summed in each node, and class label is attached to the last node in the path. In the phase of frequent CARs mining, frequent items are recursively mined as follows: construct  conditional pattern base (CPB) for each item in F- list, construct conditional FP-tree (CFP-tree) from CPB. When the conditional FP-tree contains a single path, the recursion is stopped and frequent patterns are enumerated using a combining method. This process is more detailed in [5]. Once a frequent item is found, rules containing this item can be generated immediately, by merging class values in the lower branches of the considered item. For example, to find rules  having  b3,  CPB is built by enumerating prefixes of b3 in the initial FP-tree:{a1a2(y1:1); a1(y1:1)}.

Figure 1. An example of bulding FP-tree taking into account the class labels.

Then CFP-tree is obtained considering only frequent items in  CPB. In this case, CFP-tree contains only one node  a1:2, and the CARs generated are: b3? y1,  and  b3a1? y1, obtained by combining the node a1 of CFP-tree, and the considered item b3. The two rules have support 2 and confidence 100%. The remaining rules can be mined similarly.

B. Distributed  association rule mining based on fp-growth algorithm In very large databases and with low value of minimum  support, the used FP-tree structure may not fit into the main memory. Several algorithms have been proposed in the literature to address the problem of frequent itemset mining in parallel and distributed environment. Many of them are based on apriori algorithm, e.g. CD and DD strategies [7], but they still suffer from sequential a priori limitations. Inspired by its intrinsic divide-and-conquer nature, and its performance gain over a priori-based methods, FP-growth has been the subject of several studies in parallel and distributed frequent itemsets mining. The main goal is to reduce the time spent in computation, with a minimum of interaction between data sites.

A compromise between memory constraint and inter-processor communication cost is difficult to obtain in distributed frequent itemset mining. The ideal state is a model that allows a ?total? independent processing between data sites. This may be possible generally if a certain form of data replication is assumed like in [8]. Other works are based on an information exchange in different forms: local conditional pattern bases [9] or sub-trees [10]. Clearly there are tradeoffs between these two different approaches. Algorithm designers must compromise between approaches that copy local data globally and approaches which retrieve information from remote machines as needed.

C. Distributed associative classification To our knowledge, and up to the writing of these lines,  there are been  few published studies that have focused on distributed associative classification. Thakur [11] proposed a parallel model for CBA system [1]. The parallel CAR generation phase is an adaptation of CD approach [7] for mining CARs in associative classification.  The training data set is partitioned among P processors, and in each iteration, each site calculates local counts of the same set of candidates and broadcasts these to all other processors. After a synchronization step, CARs are generated from frequent itemsets. This strategy inherits two major limitations of CD approach : a   tight synchronization at the end of each step, and the duplication of the entire set of candidates at each site.



IV. THE PROPOSED MODEL The goal of this work is to propose a distributed model for  the associative classification technique. We start by presenting a simple sequential algorithm based on FP-growth approach.

A. The sequential associative classification algorithm In the sequential version (Algorithm 1), the list of CARs is  generated by FP-growth algorithm like in CMAR. To evaluate the rank of a rule, a total order is defined as follows: Given two rules r1 and r2, r1 is said having higher rank than r2, denoted as r1? r2, if and only if (1) conv(r1)>conv(r2); (2) conv(r1) = conv(r2) but conf(r1)>conf(r2); (3) conv(r1)=conv(r2), conf(r1)=conf(r2) but supp(r1)>supp(r2); or (4) The convictions, confidences and supports of  r1 and r2 are the same, but r1 has fewer attribute values in its left hand side than r2 does. The use of conviction measure might give more information on the rule rank,  because the class frequency is taken into account (cf. 2).

Algorithm 1 Sequential AC Input: training data set D, minSup, minConf Output: the list of CARs 1. Scan D and build F-list 2. Scan D and build FP-tree structure, storing also  the class  labels of each record; 3. For each item in F-list, construct CPB and CFP-tree 4. Mine recursively each CFP-tree, by extracting CARs.

5. Sort the CARs.

Seeing that this algorithm will be parallelized, we propose to build the classifier by all the rules without any pruning operation. The classification  of a new record will be made as follows:  go through the list of all sorted rules until the first rule that covers the example is found (a rule r covers an instance d if d satisfies all conditions of the rule body of r);  classify the example according to the class at the right -hand side of the rule. If no rule covers this example, mark the example as unclassified.

B. Associative Classification in a distributed environment In this scheme (Algorithm 2), ? processors work in parallel  on a shared-nothing architecture. The processors begin with counting local support for each item. This counts are then exchanged across the group in order to each processor calculates their sum. After discarding globally infrequent items, each processor  constructs the F-list structure sorted by descending order of frequent item supports (lines 2-4). In the next phase, local  CFP-trees are built by scanning local data partitions and considering only local items belonging to F-list.

The class label is attached to the last node in the path (line 5).

Thereafter, the local CFP-trees are used in parallel to generate local conditional pattern bases CPBs in each processor, for each item in  F-list (line 6).

This partial information will be communicated between the processors and merged to generate the initial global CPB for each item. A trivial method to assign tasks to different processors is to perform a bloc-cyclic item distribution, so that the item i will be assigned to the processor number (? ? 1)%? . Thereby, each processor will send its local CPBs to  corresponding processors avoiding to send ?all? to ?all? (line 7- 9).

Algorithm 2 Distributed AC Input: P data partitions a training data set D; minSup, minConf Output: A classifier Clmaj representing a majority vote between P classifiers, 1. For ? = 0 to ? ? 1 do in parallel, in processors ?	. . ? ?? 2. Scan local data partition Dj and count  local support for  each item ?, ????(?); 3. Broadcast ????(?) for each item ?; 4. Build ?-???? sorted in support descending order, by a global  reduction ???(?) = ? ????(?)? , let ? the size of  ?-????.

5. Scan local data partition and build local ??-???? with local  items, according to the order of ?-???; 6. For each item ?  in  ?-????  which belongs to the local  partition, build local conditional pattern base ????(?); 7. The  ?  items of  F-list  are equitably assigned to each  processor according a block-cyclic data distribution; 8. Send ????(?)  to the corresponding processors; 9. Merge received and local ???(?) for assigned items ? 10.Delete local ??-????? and local ???(?) for not assigned items i; 11.Apply Sequential AC algorithm serially and independently  on assigned items; 12.Sort locally the subset of rules CARj; 13. End do in parallel  This data distribution strategy  could ?contribute? to balance the load between the  processors, because generally the amount of work needed to process an item of F-list increases for the items with low supports, thus those having longer prefixes.  After this communication, each machine independently mines recursively its assigned items, without any  need of synchronization. At this level, several data structures can be deleted from distributed main memories: local CFP-trees and local CPBs for the items assigned to other processors (line 10). At the end, if each site products a subset of CARs, the union of all subsets must be exactly the total rule set obtained in the serial version of the algorithm. To classify a new record d, a simple and intuitive technique consists in performing a majority vote between the P components of the final composite model. This strategy was used successfully in ensemble learning methods, with e.g. decision trees as base classifier [12]. The instance to classify is presented to each classifier Clj, (j= 1..P), like in the sequential version, thus it will be classified according to the prediction of the majority, denoted by  Cl???(d) = argmax???!? I!Cl? = c"#?$? ", where c is a class label, Clj is the classifier obtained in the processor j, and I(A) is an indicator function that returns 1 if A is true and 0 otherwise. The example in (fig.2, a) illustrates the parallel construction of  F-list structure in three processors. After communicating local counts, processors compute the sum and discard non global frequent items. Instead of building a global FP-tree, which may not fit in main memory, local FP-trees are constructed in each processor (fig.2, b), with the same method     presented in figure 1, but considering only global frequent items, i.e. belonging to F-list.

Figure 2. An example of generating CARs in a shared-nothing architecture.

Thereafter, local CPBs are built in each processor, for each item in F-list structure, and communicated to corresponding processors using a bloc-cyclic distribution strategy. So, the items {b3, b2} will be assigned to processor P0; {a1, c3} to P1; and  {a2, b1} to P2. The process of local CARs mining will be executed independently in the three processors resulting on the classifier Cl1?Cl2?Cl3. For example (fig.2, c), in the first recursion level, processor P0 generates the CAR ?b3? y1? for the assigned item b3. On the other hand, processor P2 repeats  the recursion on the built CFP-tree for the item a2,  until obtaining one branch.

C. Communication cost analysis  In the proposed algorithm, the information exchange allows to build the initial global F-list, and the initial CPBs.

The  first communication phase is not very expensive because each processor broadcasts only support counts for all items (q items) to all the processors (P processors). So in this stage there are P!P-1"q  messages exchanged.  All this information exchanged is integer valued and its volume is very small. On the other hand, the major communication cost comes from the exchange of the initial CPBs across all processors. This can be optimized by avoiding an all-to-all strategy. Each processor has information on what it must send to each  other processor, such as the CPB for the item i  (i= 1..k) is sent to the processor number (i-1)%P . In this phase, each processor send (k/P) CPBs  to corresponding processors, so the total number of  messages is k. The larger of these messages depends on the maximum size of CPB. In the worst case, the item is present in all transactions of the data partition, with the largest prefix of size (m ? 1) (m is the number of attributes). So the maximum message size in this communication phase is O &(m-1)(n/P)', where n is the total number of records in the training data set D.



V. CONCLUSION  Many distributed algorithms have been proposed for classification algorithms like decision trees, but so far, there are few  works   in associative classification. In this paper, we have presented a distributed model which allows the  class- association rules discovery in  a shared-nothing architecture.

Our solution embraces one of the fastest known sequential algorithms (FP-growth), and extends it to generate classification rules  in a parallel setting. The divide-and- conquer nature of this latter facilitates the parallelization process, however, since the data set is distributed, global decision making becomes a difficult task. To avoid the replication of data in the sites, we have chosen to communicate the needed information. This exchange is made only in the first level of recursion,  allowing each machine to subsequently process all its assigned tasks independently. At the end, a global classifier is built by all discovered rules,  and applying a majority vote strategy. In order to evaluate this choices, it is imperative to carry out an experimental evaluation which permits us in the future to analyze several important costs: accuracy, scalability, speedup, memory usage, communication, synchronization, and also the load balancing.

