Hiding Sensitive XML Association Rules via Bayesian Network

Abstract? Privacy Preserving Data Mining (PPDM) is receiving a lot of attention recently by researchers from multiple domains, especially in Association Rule Mining. The outputs of Association Rule Mining often involve values of attributes that can be used to characterize the identities of the users. The relations between antecedents and consequents are also explicitly displayed. The purpose of preserving association rules is to minimize the risk of disclosing sensitive information to external parties. In this paper, we proposed a PPDM model for XML Association Rules (XARs). The proposed model identifies the most probable items called ?sensitive items?, and to modify their original data sources, so that the resultant XARs can have higher accuracy and stronger reliability. Such reliability is not addressed before in the literature in any kind of methodology used in PPDM domain and especially in XML association rules mining. Thus, the significance of the suggested model sets to open a new research dimension to the academia in order to control the sensitive information in a more unyielding line of attack.

Keywords-component; formatting; style; styling; insert (key words)

I.  INTRODUCTION  In data mining, trends and patterns are identified on a huge set of data to discover knowledge. In such analysis, varieties of algorithms exist for extracting knowledge such as clustering, classification and association rule mining. Thus, association rules mining is one domain for delivering knowledge on complex data. Moreover, the basis of the discovered association rules is usually determined by the minimum support s% and minimum confidence c% to represent the transactional items in database D. Thus, it has the implication of the form A?B, where A is the antecedent and B is the consequent. The problem with such display of rules is the disclosure of sensitive information to the external part when data is shared. Hence Privacy Preserving in Data Mining (PPDM) related to Association Rules emerges.

In PPDM, Sensitive information is controlled with the help of identification of sensitive item(s) or sensitive rules.

The question is how to select or identify the sensitive item(s)? In literature, various methodologies, such as in [2, 3, 4, 5, 6], are proposed by the researchers. The problem with  these techniques is generation of sensitive rules with the use of antecedent and consequent while a number of techniques use the assumption based on sensitive item(s) identification after transformation of database. Furthermore, this act raises another question that whether the assumed or the identified item is estimated with reliability to modify the original data source? Thus, all these questions need to be answered for the more accurate results for such NP-hard problem [7].

In order to tackle these research questions, Bayesian network [8, 10] is adopted here in our method, because of its reliability. The problem again stood up because in most areas, a lot of works have been proposed especially in rule mining. This question has been solved through XML which is used for interchange of information over the web and may have information disclosure in the form of association rules.

In such a case, XML association rules can be found in literature as in [1] but the security issue of XML Association Rules (XARs) has not been studied. Thus we focus to generate controlled XARs with high accuracy and reliability.

In this scenario, we use K2 algorithm [8] to generate Bayesian Network (BN) based on XML document with reliability and accuracy. From the same XML documents, XARs are generated using the apriori algorithm [9] based on the original and modified XML transactional itemset. As K2 algorithm [8] generates BN probabilistically using Bayesian approach, so we prepare a Conditional Probability Table (CPT) for the occurrence of each item. This table is later used for the identification of sensitive item that has the maximum conditional probability which shows its highest occurrence among relation measurement of transaction items. Thus this item is used to modify only the more than one largest item based transactions which is not dominant to the others. Such modified database is D?, from which XARs are generated.

The remainder of the paper is structured as follows.

Section II provides a literature review on PPDM, XARs and BN. Section III presents the proposed PPDM model while Experimental Results are shown in section IV. Finally, section V concludes and mentions about the future work       - 466 -

II. LITERATURE REVIEW  With the widespread and escalating increases in the digital data globally, privacy issues got the attention of the researchers in different domains. Therefore, we focus the privacy issues of association rule mining when sharing information between parties. The objective of privacy preservation of association rules is to discover valuable, non- obvious information from the large databases efficiently. As a result, the literature review is given on privacy risks minimization in association rules in a reliable way.

The reality of information sharing involves disclosing risks; thus, Dasseni et al. [2] investigated confidentiality issues regarding association rules. The reason to investigate this issue is the disclosure risk involved in some of the association rules which are above the privacy threshold. This disclosure risk needs to be resolved and needs to be hidden the sensitive information revealed through association rules.

Therefore, a problem is formulated using association rules to address and hide the disclosed information. Such formulation considers a database D from which the relevant rules are mined and Ri is the subset of R; where Ri represent the sensitive information. In such a case, database D needs to be transformed into modified database called D?. This transformed database is used to generate R with an exception of Ri.

To provide a solution, two approaches are presented to hide Ri such as hiding frequent sets at source to prevent the generation of Ri and bring down the threshold values such as support and confidence.

As an association rule A?B has a set of antecedent and consequent represented by A and B respectively. Moreover, the confidence of A?B can be termed as Conf(A?B)=Supp(AUB)/Supp(A). This relation shows the dependency of confidence on Support. Therefore, three strategies with five assumptions are proposed in [2].

? Decrease the support of the rule based on antecedent or consequent.

? This strategy has two cases.

o Partially supported transactions which  cause to increase the support using the antecedent.

o Fully supported transactions that maintain both the antecedent and consequent causes to decrease the support of rule using the consequent.

Hence, the purpose of all these strategies is to preserve the sensitive information from being disclosed by modifying database based on single element modification of either an antecedent or consequent. The proposed work is simple and can be generalized with an importance of reducing the disclosed information with the help of support and confidence. Also, this approach does not control the side effects after the modification of database. Moreover, it does not provide a criterion for the identification of sensitive item(s) in the antecedent or consequent existing in a rule.

Furthermore, Guo proposed a framework by reconstructing the data for hiding the sensitive rules [3].

They formulated a problem based on transactional database  D from R rules are generated with MST (Minimum Support Threshold) and MCT (Minimum Confidence Threshold). In R, Rh sensitive rules exists such that Rh ? R. Therefore, Rh represents the sensitive association rules disclosing private information to public and need to be secured. In such situation, the original databaseD is changed to the released database D?.

From D?, R?Rh rules are mined under the same MST and MCT. Therefore to stop exposing the sensitive information through rules, the framework is divided into three phases as Frequent Set Mining (FS), Perform Sanitation Algorithm and FP-tree based inverse frequent set mining [3].

Hence, the proposed framework considers the frequent itemset (FS) generated from the original database with the MST and MCT. With the use of sanitation algorithm, non- sensitive rules (FS?) are extracted and converted back into the FP-tree. From FP-tree, the released database is obtained including the infrequent items. The main strength of the framework is hiding the sensitive association rules inversely without reducing the MST and MCT. Besides, the selection of FP-tree in forming a release database creates complexity especially if the multiple transactions may have a same itemset and with the huge depth in the tree. Thus, the framework cannot control the generation of ghost rules, unable to hide the entire sensitive rules and loses the rules after modification in the original database due to the unavailability of unyielding basis to identify and select sensitive item(s).

In addition to minimal effect in the transformed database, Rajalaxmi et al. [4] proposed data sanitization algorithm based on hybrid conflict ratio approach in order to reduce the side effects caused in the original database. This approach picked the victimized transactions and items in the sanitization process and minimizes the legitimate loss. In this process, the original database is modified based on the assumption of sensitive itemsets listed already known as an input. The purpose of this modification is to keep minimum, the effect to the non-sensitive itemsets. For this purpose, partial list of transactions is selected for the database modification in the sanitization process and victim items are deleted from the transactions and as a result victimized transactions are as side effects. Thus, to measure the conflict ratio in transactions and ith item conflict ratio in a transaction, n(P) and n(P?), and n(PSi) and n(P?Si) are used as given in the following equations (1) and (2) [4].

'  '  ( ) ( ) / ( ) (1)  ( ) ( ) / ( ) (2) i i  s s  i s s  TCR t n P n P  ICR t n P n P  = ?  = ?    As informative rules can be a subset of generated rules, so Wang et al. [5] focused on rule body and rule head to present their algorithms with the predicted itemset. For this purpose, ISL (Increased Support on LHS) and DSR (Decrease Support on RHS) algorithm are suggested. In such case, let R be the rule set such as C?A (66%, 100%) and C?B (50%, 75%) with their support and confidence. So the predicted itemset P={C} is assumed based on the given association rule body (LHS). The other predicted itemset Q= {A, B} can be generated on the given association rules head  - 467 -    (RHS). The sequence of Q can be generated from the sorted rules on confidence in descending order. This generated sequence of Q is matched with P and added to the Q (if matched). Finally, association rule head is matched with the Q and removed those rules from rule set R. Thus, the database D is modified using P in order to increase the support of item in the non-existing transactions.

Furthermore, the database D is also modified based on Q with the decrease of the support of item existence in the transactions. The suggested approach is based on the assumption of already known predicted itemset which leads to manual data mining. Hence the proposed method cannot become practical in case of the best case or the worst case.

With the new metric introduction for not to disclose sensitive information, Saygin et al. [6] presented algorithms for privacy preserving by demonstration of security issues related to association rules. They extended their approach by making some modifications in the original dataset. This modification is carried out with an introduction of new symbol ??? mark. The question mark in the transactions neither represents the presence nor the absence of an item.

With this modification in the original dataset, support and confidence are also affected and modified definitions are presented. For this purpose, the support for an itemset is taken in interval form such as [minSupport(A), maxSupport(A)] [6]. Moreover, the confidence interval is presented as [minConfidence(antecedent? consequent), maxConfidence(antecedent? consequent)]. The following equations (6) and (7) represents the minConfidence(antecedent?consequent), maxConfidence(antecedent?consequent).

The reliability estimation is carried by Doguc et al. [10] who came up in literature with a generic approach to estimate the trustworthiness of a system. Such estimation of an approach in terms of reliability is presented by BN (Bayesian Network) Model. In such estimation, historical dataset is used based on the edges and nodes. Edges represent the relationship among nodes which is uncertain.

For this purpose, Bayesian theorem is used through which an ith node occurrence can be measured with a priori occurrences of jth nodes. Thus K2 algorithm [10, 8] can be used to measure the reliability of occurrences of nodes at any position in an incremental fashion. Moreover, K2 algorithm [8] quantify associations and rank the parent set with a reduction of search space heuristically with the use of scoring function. Additionally, the limitation of K2 algorithm [8] is the fixed order including the first node in order as parent which can benefit in mining process of preserving association rule privacy. This may reduce the n attributes set to (n-1)th attributes set.

To sum up the discussion on PPDM in the light of the reviewed literature, we come to know that no single technique is perfect because of the NP-hardness of the problem [7]. Also each methodology suggests its own parameters. These parameters may be common in one aspect and different in other aspect while preserving the sensitive rules in data mining. In such case, sensitive item selection and sensitive rules are assumed to delete from database or hide rules respectively. Therefore, the problem of  modification in database using the sensitive item is based on the assumption in association rule mining domain. Thus the question rises that ?which domain has yet got so much attention in disclosing the information through association rules?? To the bottom of the answer, supplementary literature is reviewed on XML association rules which focus only on the generation of XML Association Rules rather than their sensitivity.



III. PROPOSED PPDM MODEL  As XML is emerged as a standard data source to interchange information over web due to its flexible structure, the problem with such exchange is the outflow of secrecy of information while sharing data publically.

Therefore, we consider a XML document that contains the transactional information of items according to their transaction IDs. To focus on the XML domain and address security issues in the context of XML association rules, we proposed a PPDM model as shown in Figure 1.

Figure 1.  Proposed Model for Privacy Preservation of XARs.

This model reads XML document automatically and converts it into transactional itemset for apriori [9] and binary table for K2 algorithm [8]. Apriori generates XARs and K2 generates BN with CPT. Thus, a sensitive item is identified based on the maximum probability in CPT and deleted from the original data source for the not dominant largest transaction as considered an assumption with constant support. As a result, the experimental results are presented in the next section which is evident to minimize the disclosure risk of information while shared. Thus, to avoid such disclosure, the main steps involved in the process of hiding sensitive XML association rules using the proposed model as  ?  ???? ??? ???  ?????????????  ?? ???  ????????????  ????????????????????  ??????????? ?????????????????  ??????????????!???? ? ???????"#????????????  ???????????????????? ?? ??????  $????????????!?? ????%??????????  &	????  %???????%?????? ?  ?  ?  ?????????'?(??)	????? ???  ?	??*?? +, - ???	?????  ?????? (??)	??? ?? ???  ???? ???????????????????????  ? ??  ??  ??  ??  ??  ????%?????????? &	??????? ???    - 468 -    in Figure 2. This algorithm detects the sensitive item(s) identified from XML document in order to delete it from the transaction which has the maximum conditional probability.

Such deletion of item is helpful in hiding sensitive XARs.

Figure 2.  Proposed Algorithm for hiding Sensitive XARs.



IV. EXPERIMENTAL RESULTS  The algorithm suggested, as in section 3, is implemented in Matlab 7.6 because of the complexity of reduction in development including the built-in functions. Before showing experimental results on a dataset, we selected XARs based on the maximum conditional probabilistic item as sensitive. Based on this sensitive item, 131 XARs can be declared as sensitive from 254 XARs with support of 4%.

Thus, the following XML association rules are sensitive as shown in Figure 3.

Figure 3.  Sample of Sensitive XML Association Rules  Moreover, the sensitive item is identified based on the maximum conditional probability of item dislocation_of' symbolized as E in Bayesian Network which is shown as in Table 1.

TABLE I.  ITEMS CONDITIONAL PROBABILITY IN BN                     We used XML document containing transactional items  which are symbolized as shown in Table 2.

TABLE II.  SYMBOLIZED ITEMS ALPHABETICALLY               Moreover, the BN is generated using K2 algorithm [8] as  shown in Figure 4.1 based on the same transactional items in alphabetical order as shown in Figure 4.

Figure 4.  Bayesian Network using K2 Algorithm [8].

Input: XML document Output: Hidden XML SARs 1.  max  2. Repeat steps 3-8 two times  3. max { };  1 / / arg  4.

5.        { } max  6.

7.

i i  i  i i  Identify imum probability item  L T i L est Transaction  each T do  IF L T THEN  Delete Item from Transaction  =  ?  =  ?    8.

9.    '  END IF  Next  Generate XARs on D    A?E, B?E, C?E, D?E, E?F, E?G, E?H, E?I, A,B?E, A,C?E ,A,D?E, A,E?F ,A,E?G, A,E?H, A,E?I, B,C?E, B,D?E, B,E?G, B,E?H, C,D?E, C,E?F, C,E?G, C,E?H, C,E?I,D,E?F,D,E?G, D,E?H, D,E?I, E,F?G, E,F?H, E,F?I, E,G?H, E,G?I, E,H?I, A,B,C?E, A,B,D?E, A,B,E?G, A,B,E?H, A,C,D?E, A,C,E?F, A,C,E?G, A,C,E?H, A,C,E?I, A,D,E?F, A,D,E?G, A,D,E?H, A,D,E?I, A,E,F?G, A,E,F?H, A,E,G?H, A,E,G?I, A,E,H?I, B,C,D?E, B,C,E?G, B,C,E?H, B,D,E?G, B,D,E?H, B,E,G?H, C,D,E?F, C,D,E?G, C,D,E?H, C,D,E?I, C,E,F?G, C,E,F?H, C,E,G?H, C,E,G?I, C,E,H?I, D,E,F?G, D,E,F?H, D,E,F?I, D,E,G?H, D,E,G?I, D,E,H?I, E,F,G?H, E,F,G?I, E,G,H?I, A,B,C,D?E, A,B,C,E?G, A,B,C,E?H, A,B,D,E?G, A,B,D,E?H, A,B,E,G?H, A,C,D,E?F, A,C,D,E?G, A,C,D,E?H, A,C,D,E?I, A,C,E,F?G, A,C,E,F?H, A,C,E,G?H, A,C,E,G?I, A,C,E,H?I, A,D,E,F?G, A,D,E,F?H, A,D,E,G?H, A,D,E,G?I, A,D,E,H?I, A,E,F,G?H, A,E,G,H?I, B,C,D,E?G, B,C,D,E?H, B,C,E,G?H, B,D,E,G?H, C,D,E,F?G, C,D,E,F?H, C,D,E,G?H, C,D,E,G?I, C,D,E,H?I, C,E,F,G?H, C,E,G,H?I, D,E,F,G?H, D,E,F,G?I, D,E,G,H?I, A,B,C,D,E?G, A,B,C,D,E?H, A,B,C,E,G?H, A,B,D,E,G?H, A,C,D,E,F?G, A,C,D,E,F?H, A,C,D,E,G?H, A,C,D,E,G?I, A,C,D,E,H?I, A,C,E,F,G?H, A,C,E,G,H?I, A,D,E,F,G?H, A,D,E,G,H?I, B,C,D,E,G?H, C,D,E,F,G?H, C,D,E,G,H?I, A,B,C,D,E,G?H, A,C,D,E,F,G?H, A,C,D,E,G,H?I  Item Order Item Symbol BN Probability  2 B 0.210  3 C 0.874  4 D 0.640  4 D 0.636  5 E 0.936  6 F 0.872  7 G 0.689  8 H 0.890  8 I 0.875  9 J 0.306  9 K 0.294  Lymph dataset Items Item Symbol 'bl_of_transaction_c' A 'bl_of_transaction_s' B  'block_of_affere' C 'by_pass' D  'dislocation_of' E 'early_uptake_in' F 'exclusion_of_' G 'extravasates' H  'regeneration_of' I  - 469 -     Figure 5.  Dependency of Items via Bayesian Network  Additionally, K2 algorithm [8] is used this alphabetical order to calculate the conditional probabilities of the occurrence of the items. This occurrence of an item is in terms of their maximum probability which is recorded in MS-Excel File as shown in Figure 5. Furthermore in parallel to BN, we generated association rules using the same transactional items with the help of apriori algorithm [9].

These association rules are generated on the original data source as shown in Table 3.

TABLE III.  XML ASSOCIATION RULES FROM D    Now the transactional items used in this overall privacy preservation of association rule mining is based on lymph dataset available at [11]. In this dataset, there are 9 attributes out of 18 which represent the presence or absence of an item in a transaction. Thus, we used only 9 attributes as shown in Table 2 with 151 transactions. With the use of this dataset, a sensitive item ?E? is selected based on the maximum probability. The reason for choosing the maximum probability item is the frequent occurrence of this item among different transactions. Therefore, this item is used to modify the original data source having maximum length transactions. This restriction limits to affect the original data source minimally for the maximum useful output without disclosing the sensitive information to external parties. After modification of the data source, the apriori algorithm [9] is again used to generate the XML association rules which hide the sensitive XML association rules as shown in Table 4.

TABLE IV.  ASSOCIATION RULES BASED ON D?    The overall summary of the hiding process of XML  association rules is shown in Table 5 which shows the effectiveness of the proposed technique  TABLE V.  SUMMARY OF XML ASSOCIATION RULES ON D?    In Table 5, the results from the proposed model as in  Figure 1 are summarized for a  compact view. From this table, SARs, hidden Sensitive XARs and Side effects of the model can be examined for future proposed techniques.

Rule # Association Rules Support Confidence Lift Ratio 1 A==>B 4.7 26.9 3.8 2 A==>C 17.6 100.0 1.2 3 A==>D 13.5 76.9 2.1 4 A==>E 13.5 76.9 0.8 5 A==>F 12.8 73.1 0.7 6 A==>G 16.2 92.3 0.8 7 A==>H 15.5 88.5 1.2 8 A==>I 4.1 23.1 2.3 9 B==>C 4.7 100.0 1.2  10 B==>D 4.7 100.0 2.8  11 B==>E 4.7 100.0 1.0 12 B==>G 4.7 100.0 0.9 13 B==>H 4.7 100.0 1.3 14 C==>D 20.9 37.8 1.1 15 C==>E 35.8 64.6 0.7  ??? ?!!"#? $%&?? '?&'? ?&'? ?'? ?!!"@? KK&?? X?&Y? ?&'?  ?X? ?!!"Z? $?&Y? ?Y&%? ?&%?  ?%? ?!!"[? K&?? '&$? ?&'?  ??? ?!!"?? ?%&?? X?&?? ?&X?  ??? ?!!"#? ?'&?? '?&?? ?&'? ??? ?!!"@? ??&?? XX&%? ?&X? ?$? ?!!"Z? ??&?? XX&%? ?&??  ?K? ?!!"[? ?&?? ?Y&?? ?&Y?  ?Y? ?!!"#? Y?&?? 'X&?? ?&X? ??? ?!!"@? Y%&Y? X%&X? ?&X?  ?'? ?!!"Z? $Y&X? YK&?? ?&'?  ?X? ?!!"[? K&'? '&?? ?&'?  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \\?  248 A,D,E,G,H==>I 4.1 37.5 3.8 249 B,C,D,E,G==>H 4.7 100 1.3 250 C,D,E,F,G==>H 12.8 95 1.3 251 C,D,E,G,H==>I 4.1 24 2.4 252 A,B,C,D,E,G==>H 4.7 100 1.3 253 A,C,D,E,F,G==>H 8.8 100 1.3 254 A,C,D,E,G,H==>I 4.1 37.5 3.8  Rule # Association Rules Support Confidence Lift Ratio ?? ?!!"?? K&'? ??&%? $&X? ?? ?!!"?? ?'&?? ???&?? ?&?? $? ?!!"?? ?$&Y? '?&%? ?&?? K? ?!!"?? ??&Y? ?Y&K? ?&'? Y? ?!!"#? ??&X? '$&?? ?&'? ?? ?!!"@? ??&?? %?&$? ?&X? '? ?!!"Z? ?Y&Y? XX&Y? ?&?? X? ?!!"[? K&?? ?$&?? ?&$? %? ?!!"?? K&'? ???&?? ?&??  ??? ?!!"?? K&'? ???&?? ?&X? ??? ?!!"@? K&'? ???&?? ?&%? ??? ?!!"Z? K&'? ???&?? ?&$? ?$? ?!!"?? ??&%? $'&X? ?&?? ?K? ?!!"?? $$&X? ??&?? ?&?? ?Y? ?!!"#? $%&?? '?&'? ?&'?  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  \ \  ?''? ????????@!!"Z? K&'? ???&?? ?&$? ?'X? ????????#!!"@? ?&X? '?&K? ?&?? ?'%? ????????#!!"Z? '&K? 'X&?? ?&?? ?X?? ????????@!!"Z? X&X? X?&$? ?&?? ?X?? ??????#?@!!"Z? %&Y? %$&$? ?&?? ?X?? ??????@?Z!!"[? K&?? $$&$? $&$? ?X$? ??????#?@!!"Z? '&K? ?X&X? ?&%? ?XK? ??????#?@!!"Z? ?&X? '?&%? ?&?? ?XY? ??????#?@!!"Z? ??&X? X?&?? ?&?? ?X?? ????????#?@!!"Z? ?&X? '?&%? ?&??  ????? ?%&??  ?%&?? $?????? ?%&??  ????? .?????  /?/? /?/? ?????#? ?????????  ?YK? ?$?? ?X? ?$? ?Y?? %? $?  - 470 -

V. CONCLUSION  In this work, XML document is read automatically with the conversion of transactional itemset and binary table.

Such forms of data are used in order to generate XARs and BN with the help of apriori and K2 algorithm. Moreover, our contribution in this regard is to analyze and use the CPT to identify sensitive XML transactional item in order to modify the itemset. This leads us to hide the XARs. In such process of hiding, reliability and accuracy issue were the main concern which was validated through K2 [8] for the accurate computation of sensitive item. Moreover, the proposed PPDM model is more effective when there is at least one transaction equal in length as an assumption of the model.

Further, the assumption of largest transaction will be analyzed in optimal way.

