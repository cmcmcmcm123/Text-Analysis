All-monotony: a generalization of the all-confidence antimonotony

Abstract?Many studies have shown the limits of sup- port/confidence framework used in Apriori-like algorithms to mine association rules. One solution to cope with this limitation is to get rid of frequent itemset mining and to focus as soon as possible on interesting rules. Many works have focused on the algorithmic properties of the confidence.

In particular, the all-confidence which is a transformation of the confidence, has the antimonotone property. In this paper, we generalize the all-confidence by associating to any measure its corresponding all-measure. We present a formal framework which allows us to make the link between analytic and algorithmic properties of the all-measure. We then propose the notion of all-monotony which corresponds to the monotony property of the all-measures. Our results show that although being very interesting, all-monotony is a demanding property.



I. INTRODUCTION  Rules-based data mining methods are very important approaches in machine learning and datamining. They may provide a concise and potentially useful knowledge that is understandable by the end-users. This last feature, that allows to explain why a particular prediction is made for a particular case or why two items are related, could be of strong importance in various applications like in credit scoring or medicine.

In unsupervised learning paradigm, association rule min- ing is certainly the most popular method [1]. Our work focuses on this method, so we first recall its fundamentals.

An association rule is a rule A ? B, where A and B are two sets of items (also called itemsets) such that A 6= ?, B 6= ? and A ? B = ?, meaning that given a database DB of transactions (where each transaction is a set of items) whenever a transaction T contains A, then T probably contains B also [1]. Within the support/confidence framework, the problem of mining association rules is to generate all association rules that have support and con- fidence greater than an user-specified minimum support threshold and minimum confidence threshold respectively.

The support of A ? B, denoted supp(A ? B), is the proportion of transactions containing A and B in DB, and the confidence, denoted conf(A ? B), is the proportion of transactions containing A and B inside the set of transactions containing A in DB. Association rule discovery is a two-step process. Firstly, the minimum support constraint is applied  to find all frequent itemsets in DB. Secondly, the frequent itemsets and the minimum confidence constraint are used to form rules. This strategy implies two major problems: the first one concerns complexity issue while the second one concerns the quality of extracted rules.

Finding all frequent itemsets in DB is computationally expensive since there are O(2k) possible itemsets where k is the number of items in DB. However the downward-closure property of support (also called antimonotonicity [1]) allows to implement very efficient algorithms to find all frequent itemsets. Interesting surveys may be found in [2] and [3].

It is important to notice that support-based approaches give an important role to the support constraint. Thus mining interesting rules without the support has been identified as an important challenge [4].

It is also well known that finding all and only all interesting rules is a difficult problem. Indeed algorithms produce a huge number of rules, especially within the support/confidence framework, while the percentage of in- teresting rules is often only a very small fraction of the all rules. A first strategy to reduce the number of mined rules consists of increasing the user-specified minimum support threshold. Unfortunately this strategy will miss many inter- esting rules, especially the nuggets (rules with low support and very high confidence). A second strategy consists of increasing the user-specified minimum confidence threshold.

This will favor rules with large consequent which may give many uninteresting rules: for example, in a classical market database, we would extract the rule washing liquid? bread, which is due only to the fact that bread is bought by most of the consumers. The use of the confidence only takes all its sense only when the value of the confidence is compared to supp(B). Thus one can use additional objective measures of interest in order to rank the rules in a post-analysis phase.

Objective measures are said data driven as they could be defined on the basis of the contingency table of A and B.

Interesting surveys and comparisons may be found in [5], [6], [7].

Another efficient approach is to apply additional con- straints [8], [9] on item appearance or to use additional interestingness measures as soon as possible to reduce both the time to mine databases and the number of founded   DOI 10.1109/ICMLA.2009.110     itemsets like in [10]. Indeed, as it was stressed in [11], interestingness measures may play different roles. They can be used to prune the search space (this is the case for the support), to select the interesting rules (this is the case for the confidence), or to quantify the utility of rules (this is the case for a cost/benefit measure).

We are interested in objective measures that can be used for all of these roles. To overcome the previous mentioned problems with the dictatorship of the support constraint different solutions were proposed at the algorithmic level and mainly for the confidence measure. We focus here on the all-confidence [12] that makes the confidence measure antimonotone. This is thus a promising property.

The rest of the paper is organized as follows. In Section II, we present recent works that focus on the algorithmic properties of some measures. In Section III, we present our formal framework which allows us to make the link between analytic and algorithmic properties of the measures.

We apply in Section IV this framework and propose the all- monotony that generalizes the antimonotone property of all- confidence. We then demonstrate a sufficient condition of non-existence of the proposed generalization all-monotony.

The practical application to 37 measures is discussed in Section V and we conclude in Section VI.



II. RELATED WORKS  Many studies have shown the limits of support/confidence framework used in Apriori-like algorithms to mine associ- ation rules. In order to find nuggets, a lot of these works have focused on the algorithmic properties of the confidence measure. These works are mainly divided into two families: the first one tries to reduce the number of considered items and the second one exploits the analytical properties of measures. We here focus on the latter one.

Inspired by the support example, [13] proposed the h- confidence, a transformation of the confidence, that has the antimonotony property. In addition, the h-confidence allows to filter the uninteresting itemsets with the cross-support property. These measures are used in HYPERCLIQUE-MINER algorithm proposed by [13]. The h-confidence measure is mathematically identical to all-confidence measure intro- duced by [12]. So like the support measure they could be used in Apriori-like algorithms. Our proposal, which is an extension of [14], focusses on all-confidence. An itemset is considered as interesting under the all-confidence constraint if the confidence of all rules that can be generated from this itemset is greater than a fixed threshold.

Based on one monotony property of the confidence, [15] introduced the Universal Existential Upward Closure property. This property allows to explore the itemsets lattice with a top down approach for classification rules. It is thus efficient in particular for nuggets discovery. A generalization of this property is provided in [16].

For an associative classification task, following a work by [17], [18] introduced the CORCLASS algorithm based on direct branch-and-bound exploration of candidates and on an antimonotonic property of convexity of the ?2.

In [19] the author introduced the notion of optimal rule set for classification rules. A rule set is optimal if it contains all rules, except those with no greater interestingness than one of its more general rules. Optimal rule sets allow to define an efficient pruning strategy that could be applied with 13 measures as demonstrated case by case by [19].

This work is, from the best of our knowledge, one of the first works that is applied to a large set of measures. In addition, an optimal rule set has similar properties as closed itemsets and nonredundant rule sets as proposed in [20].

A necessary and sufficient condition of applicability of the underlying pruning strategy is proposed in [21].



III. ASSOCIATION RULE?S SPACE  In this section we adapt the formal framework presented in [16], close to the formalism used by [22], which enables an analytic study of objective interestingness measures related to their algorithmic properties. We will propose the all- monotony property that generalizes the monotony property of the all-confidence and use our framework to study the condition of existence of this property in Section IV.

If the size of the database is not taken into account such measures can be expressed using the contingency table of relative frequencies, and consequently with only three variables: a measure can be considered as a function R3 ? R ? {??,+?} (in fact the measures have various restricted domains in R as shown in [6]). The study of their variations, with respect to these variables, links the measures and their algorithmic properties. It implies to describe a domain of definition in order to study only real cases. The final goal is then to derive, from the algorithmic properties founded, some efficient pruning strategies.

A. Association rules & database  A boolean database DB is completely described by a triplet (A, R, T ), where A is a set of items, T is a set of attributes, and R is a binary relation on A ? T . An association rule is defined by a database DB, a non-empty set A ? A (A is an itemset) called antecedent, and a non- empty set B ? A called consequent such that A ? B = ?.

We denote a rule by A DB?? B, or simply A??B when there is no possible confusion. The support of an itemset A is the frequency of appearance of this itemset in the database.

We denote it by suppDB(A), or, if there is no ambiguity, supp(A). The support of the rule A??B is the support of itemset AB.

B. Contingency tables  Let A and B be two itemsets on the database DB. The contingency table (left part of Figure 1) in relative joint     frequencies of A and B gives information about the simul- taneous presence of these itemsets (right part of Figure 1).

B B?  AB? A?BAB  A  BA?B?  A supp(AB) supp(AB?) supp(A) A? supp(A?B) supp(A?B?) supp(A?)  supp(B) supp(B?) 1  Figure 1. Contingency table of A and B.

The contingency table has 3 degrees of freedom: one needs at least 3 of its values to describe it, and 3 values are enough to find all the other values. For example, the contingency table is fully described by the two marginal frequencies supp(A) et supp(B), and the joint frequency supp(AB). An association rule on a given database is de- scribed by two itemsets and we naturally can speak about the contingency table of an association rule. This leads us to propose the notion of descriptor system.

Definition 1. We call descriptor system of the contingency table a triplet of real functions (f, g, h) over the association rules which fully describes the contingency table of associ- ation rules.

Example 1. Let consider the functions ant(A ? B) = supp(A), cons(A ? B) = supp(B), conf(A ? B), ex(A ? B) = supp(AB) and c-ex(A ? B) = supp(AB?).

Then the triplets (conf, ant, cons), (ex, ant, cons) and (c-ex, ant, cons) are descriptor systems of the contingency table.

In other words, an objective interestingness measure is a function from the space of association rules to the space of extended real numbers R ? {??,+?} and thus can be expressed with a descriptor system (with 3 real functions).

C. Minimal joint domain  A descriptor system d of the contingency table is then a triplet of variables over the space of association rules, and an interestingness measure m can be written with the help of a function ?m of this triplet. If we want to make an analytic study of this function, we only need to restrict the analysis to the joint variation domain of the triplet. Indeed, the elements that are out of this domain do not correspond to a real situation and therefore the analysis has no sense.

Definition 2. We call the couple (?m,Dd), made from this function and the joint variation domain (associated to a specific descriptor system), the d-adapted function of measure of the measure m.

It is important to notice that the form of the functional part of this function of measure depends on the descriptor system. However, when this system is fixed, the adapted function of measure is uniquely defined. In the following,  we voluntarily omit to mention the chosen descriptor system if there is no possible ambiguity.

If d is a descriptor system of the contingency table, the joint variation domain associated to this system is defined by the constraints laid down by the values of d between themselves.

Example 2. Let ex : (A DB?? B) 7? suppDB(AB) be a function and let dex = (ex, ant, cons) be a descriptor system.

For this system, we have the set of constraints:  ??? 0 < ant < 10 <cons< 1max{0, ant+ cons? 1}? ex ?min{ant, cons} (1) Let us define the following domain: D = (x, y, z) ? Q3  such that D satisfies the constraints (1), i. e.??? 0 < y < 10 < z < 1max{0, y + z ? 1} ? x ? min{y, z} (2) The domain Ddex associated with dex matches this defi-  nition and thus we know that Ddex ? D (D is complete).

To prove the inverse inclusion (i.e. D is minimal), we have to prove that each element of D has a corresponding association rule (and then a database).

Proof: Considering an element (x, y, z) of D we need to construct a database DB containing an association rule A ? B, such that m(A ? B) equals to ?m(x, y, z). Since x, y and z are rational numbers, we can define n ? N such that (x? n, y? n, z ? n) ? N3. Our database should verify the following equalities:  supp(AB) = x, supp(A) = y, supp(B) = z (3)  The constraints (2) of the domain assure that 0 < x < y < y + z ? x ? 1 holds. We can thus construct the database of Table I, satisfying the equalities (3). Then, the second inclusion is verified.

1 x ? n y ? n (y + z ? x) ? n n A 1 ? ? ? 1 1 ? ? ? 1 0 ? ? ? 0 0 ? ? ? 0 B 1 ? ? ? 1 0 ? ? ? 0 1 ? ? ? 1 0 ? ? ? 0? ?? ?  AB ? ?? ?  A?B ? ?? ?  B?A  Table I DATABASE FOR THE DOMAIN Ddex  Finally, we have identified the joint variation domain of the descriptor system dex: it is exactly D. The study of the ?m function can be restricted to domain D.



IV. TOWARDS all-measure AND all-monotony  We will now focus on the descriptor system dex, and write it d. The variation of a function f on the first variable in     the domain D will refer to the study, for all (y, z), of the variations of x 7? f(x, y, z) in {x|(x, y, z) ? D}.

Definition 3. (all-measure) Let m be an interestingness measure for association rules. We define the all-m measure on pattern I as:  all-m(I) = min AB=I,A 6=?,B6=?  {m(A? B)}. (4)  The all-measure of pattern I is the smallest value of m measure over all I generated rules.

Definition 4. (all-monotony) We say that the measure m is all-monotone if the all-m measure is antimonotone.

We first show that all increasing monotone transforma- tions of confidence are all-monotone.

Property 1. The confidence and all increasing functions of confidence are all-monotone.

Proof: The case of confidence is easy since the all- measure of confidence is the same as the all-confidence of [12].

Let m be a such measure, and I ? I ? two itemsets. Since m is an increasing function of confidence:  min AB=I  m(conf(A? B)) = m (  min AB=I  conf(A? B) )  Since the confidence has all-monotony property:  m (  min AB=I  conf(A? B) ) ? m  ( min AB=I?  conf(A? B) )  Finally, m has the all-monotony property:  min AB=I  m(conf(A? B)) = m (  min AB=I  conf(A? B) ) ?  ? m (  min AB=I?  conf(A? B) )  = min AB=I?  m(conf(A? B))  On the other hand, we propose Theorem 1 and Theorem 2 to prove that a large number of measures don?t verify this property.

Theorem 1. Let m be an interestingness measure for association rules and (?m,Dex) an adapted function of measure of m. If ?m is strictly decreasing on the second (the antecedent) and the third (the consequent) variables then the measure m does not have the all-monotony property.

Proof: Suppose that ?m is strictly decreasing on the second (the antecedent) and the third (the consequent) vari- ables. We will prove that, for this measure, it is possible to build a simple database which will be a counterexample for the all-monotony property. We choose the minimal number of items necessary to study the all-monotony property, i.e.

3. Let A, B and C be these items. For a database, we will compare the association rules generated by AB and ABC  A 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 B 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 C 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0  Table II EXAMPLE OF DATABASE THAT VERIFIES THE (11-16) CONSTRAINTS.

itemsets and for this database let be {x, y, z, t, y?, z?} ? Q as  supp(AB) = x, supp(A) = y, supp(B) = z supp(ABC) = x, supp(C) = t supp(AC) = y?, supp(BC) = z?  Using the support definition, we have:  0 < supp(A) < 1 =? 0 < y < 1 (5) 0 < supp(B) < 1 =? 0 < z < 1 (6)  0 < supp(ABC) ? min(supp(AC), supp(BC)) =? 0 < x ? min(y?, z?) (7)  y? = supp(AC) < supp(A) = y (8) z? = supp(BC) < supp(B) = z (9) t = supp(C) ? supp(AC) + supp(BC)  ?supp(ABC) ? y? + z? ? x (10)  Inequalities are strict in order to fully exploit the fact that the function ?m is strictly decreasing. In addition, without loss of generality, we suppose that the item C is less frequent that B, i.e. t ? z. The constraints (5-10), that allow to construct our rational numbers, can be summarized by:  (5) =? y ?]0, 1[ (11) (6) =? z ?]0, 1[ (12)  (8) =? 0 < y? < y (13) (9) =? 0 < z? < z (14)  (7) =? 0 < x ? min{z?, y?} (15) (10) =? y? + z? ? x < t (16)  t ? z (17) t+ (y ? y?) + (z ? z?) ? 1 (18)  Remark 1. The values y = 416 , z = 16 , y  ? = 216 , z ? =  16 , x =  16 , t =  16 verify these constraints and we can  build the database of Table II.

Remark 2. Generally, for all n ? N, any database with only 3 items {A,B,C} and n+ 3 transactions T1, ? ? ? ,Tn+3 with T1 = {A},T2 = {B},T3 = {C},T4 = T5 = ? ? ? = Tn+3 = {A,B,C}, verify the (11)-(16) constraints.

Considering a data base verifying the (11-18) constraints, we can write1, under the variations hypothesis:  1The constraint number used is on the top of associated inequality.

m(A? B) = ?m(x, y, z) < ?m(x, y?, z) = m(AC? B) < ?m(x, y, z?) = m(A? BC)  15+13 < ?m(x, x, z)  ? ?m(x, x, t)  ? ?m(x, x, t) = m(AB? C) (19)  m(B? A) = ?m(x, z, y) < ?m(x, z?, y) = m(BC? A) < ?m(x, z, y?) = m(B? AC)  15+14 < ?m(x, z, x)  ? ?m(x, t, x)  ? ?m(x, t, x) = m(C? AB) (20)  (19) and (20) =? all-m({A,B}) < all-m({A,B,C}), which contradicts the anti-monotony property.

Example 3. The adapted function of Bayes factor is  ?BF (x, y, z) = x  y ? x ? 1? z  z .

It is strictly decreasing on the second (the antecedent) and the third (the consequent) variable and using Theorem 1, this measure does not have the all-monotony property.

Let?s us compute BF on Table II: BF (A ? B) = 13 , BF (AC? B) = 1, BF (B? AC) = 1, BF (BC? A) = 32 , BF (B? A) = 37 , BF (A? BC) =  9 , BF (AB? C) =?  and BF (C? AB) =?.

It shows that BF does not have the all-monotony property  (all-BF ({A,B}) < all-BF ({A,B,C})).

Remark 3. The strict monotony is a strong hypothesis. Let see the Loevinger measure:  L(A? B) = 1? supp(AB?) supp(A)supp(B?)  Its adapted functions of measure on D is  ?L(x, y, z) = 1? y ? x  y ? (1? z)  On x ? y = 0 plane, its partial derivative with respect to the third variable is zero so it is not strictly monotone with respect to this variable. But if we compute the Loevinger measure on Table II, we obtain: L(A? B) = ? 12 , L(AC? B) = 0, L(B ? AC) = 0, L(BC ? A) = 19 , L(B ? A) = ? 16 , L(A? BC) =  13 , L(AB? C) = 1, and L(C? AB) =  35 .

The Loevinger measure does not have the all-monotony property. There are other measures that have the same property on the second variable on the plane defined by x? z = 0. We then propose the theorem 2  Theorem 2. Let m be an interestingness measure for association rules and (?m,Dex) an adapted function of measure of m. If ?m (x, y, z) is strictly decreasing on the second and the third variable, elsewhere that on x? z = 0 and x ? y = 0 planes and if ?m (x, y, z) is constant on x? z = 0 and x? y = 0 planes, then the m measure does not have the all-monotony property.

Proof: The proof is the same as for Theorem 1 where the inequality (17) is substituted by an equality.

Remark 4. The converse of Theorem 2 is not true and then we don?t have a necessary and sufficient condition. The contramin measure is a counter-example. Using the database introduced on Remark 2, we obtain:  contr(A? B) = supp(AB)? supp(AB) supp(B)  all-contr({A,B}) = n? 1 n+ 1  all-contr({A,B,C}) = n? 1 n  i.e. ?n > 1, all-contr({A,B}) < all-contr({A,B,C}) and thus the contramin is not all-monotone. Its adapted function of measure is 2x?yz and the sign of its variation with respect of z depends on 2x ? y and contramin is not decreasing with respect of its third variable.



V. PRACTICAL APPLICATIONS AND DISCUSSIONS  The main purpose of our work is to provide a common framework to answer if a measure is or is not all-monotone and thus if it can be used or not in an Apriori-like algorithm instead of support to efficiently prune the search space.

The first positive result of our study is that a set of classical interestingness measures are all-monotone. This is the case of increasing functions of the confidence like confidence, Sebag-Shoenauer, Ganascia and example and counter-example rate. It should be noticed that they rank the rules in the same manner but depending on the user goals one may prefer to use one or the other [6].

The Theorem 2 and specific cases (see some counterex- amples of the remarks 1 and 2) lead to less optimistic results. A large set of measures are not all-monotone: speci- ficity, precision, lift, leverage, centered-confidence, Jaccard, positive confidence, odds ratio, Klosgen, contramin, added value, conviction, one way support, J1-mesure, Piatetsky- Shapiro, cosine, Loevinger, information gain, Bayesian fac- tor, Zhang, implication index, Kappa, Yule?s Q, Yule?s Y, interestingness weighting dependency, collective strength, and Pearson. While being very interesting, the all-monotony is a demanding property.

In [14], we also show that the measure of recall is all- monotone, since all-recall(I) = all-conf(I).

Finally, we observe that Theorem 1 and 2 eliminate every measure verifying the Piatetsky-Shapiro third condition for objective interestingness measures [23]: m monotonically     decreases with supp(A) (or supp(B)) when other param- eters remain the same. Moreover, the search for interesting itemsets using the measure all-m introduces a learning bias towards the itemsets for which each item is strongly associated with all the others according to the measure m.



VI. CONCLUSION  Objective measures do have some interesting algorithmic properties. However, these properties have not been fully exploited, except for the confidence measure. We have pro- posed a formal framework for the analytic study of measures in relation with their algorithmic properties. We here propose all-monotony, an extension of the all-confidence property.

We then apply this framework to the all-monotony and show that although being very interesting, all-monotony is a demanding property. Indeed few measures are all-monotone and thus could be used with the underlying pruning strategy.

However, increasing functions of the confidence are all- monotone.

