A Clustering-based Collaborative Filtering Approach for Mashups  Recommendation over Big Data

Abstract?Spurred by services computing and Web 2.0, more and more mashups are emerging on the Internet. The overwhelming mashups become too large to be effectively recommended by traditional methods. In view of this challenge, we propose a clustering-based collaborative filtering approach for mashup recommendation over big data. This approach mainly divided into two phases: clustering and collaborative filtering. By using clustering techniques, the data size is reduced so that the computation time of collaborative filtering algorithm is decreased significantly. Several experiments are done to verify the efficient of the proposed approach at the end of this paper.

Keywords- clustering, collaborative filtering, mashup, API, tag.



I. INTRODUCTION The Web has undergone a major change from a  primarily publication platform towards a participatory and ?programmable? platform, where numerous heterogeneous Web-delivered services are emerging, resulting in the creation of Web mashups [1]. The term ?mashup? implies easy, fast integration, frequently using open application programming interfaces (API) and data sources to produce enriched results that were not necessarily the original reason for producing the raw source data [2]. Many companies and institutions provide various mashup solutions or re-label existing integration solutions as mashup tools. For example, HousingMaps (http://www.housingmaps.com) combines property listings from Craigslist ( http://www.craigslist.org/) with map data from Google Maps ( http://maps.google.com/) in order to assist people moving from one city to another and searching for housing offers [ 3 ]. More interesting mashups include Zillow (http://www.zillow.com/) and SkiBonk ( http://www.skibonk.com/).

To speed up the mashup development process and to enable end users more easily to mash up their web applications, numerous mashup-specific development tools and frameworks have emerged, e.g., Google mashup Editor (http://code.google.com/gme/) provides a template-based environment for mashup development and Yahoo! Pipes (http://pipes.yahoo.com/pipes/) allows the mixing of popular data feeds to create data mashups via a visual editor.

As the number of mashups increases continuously, how to  create and find values from the mashup-oriented big data become an important research problem.

Recommender systems (RSs) have been proved to be a valuable means for dealing with the information overwhelming problem by pointing a user towards new not- yet-experienced items that may be relevant to the active user?s current task. ?Item? is the general term used to denote what the RSs recommend to users and ?the active user? is the user looking for suggestions. Collaborative filtering (CF) such as item- and user-based methods are the dominant techniques applied in RS algorithms [4]. Given an unknown rating of a target item by the active user to be estimated, user-based CF first measures rating similarities between the active user and other users, then predicts the unknown rating by averaging the (weighted) known ratings of the test item by rating similar users, while item-based CF first computes similarity between the target item and other items, then predicts the unknown rating by averaging the (weighted) known ratings of similar items by the active user. Since the relationships between items are relatively static, item-based algorithms may be able to provide the same quality as the user-based algorithms with less online computation.

However, in terms of item-based CF for mashup recommendation over big data, the main challenges include: 1) how to effectively process large quantities of service within acceptable processing time; and 2) how to improve the accuracy of CF algorithm.

In view of these challenges, we propose a cluster-based collaborative filtering approach (CbCF), which consists of two phases: clustering and collaborative filtering. In fact, clustering is used as a preprocessing step to separate big data into manageable parts. Since the number of services in a cluster is much less than the total number of services, the computation time of CF algorithm can be reduced significantly. Besides, the ratings of similar services within a cluster are more relevant to each other than those of dissimilar services, therefore the recommendation accuracy based on users? ratings may be enhanced.

The rest of this paper is organized as follows. In Section II, CbCF approach is described in detail step by step. Then, a case is studied in Section III. In Section IV, several experiments are conducted on real world data which is extracted from Programmableweb.com. Related work is   DOI 10.1109/CSE.2013.123    DOI 10.1109/CSE.2013.123     analyzed in Section V. At last, we draw some conclusions in Section VI.



II. CBCF APPROACH CbCF approach operates on a user-cluster matrix instead  of on the whole user-item matrix. It can be divided into four steps as follows.

Step 1: Compute tag-based similarity and APIs-based similarity between mashups, which are then combined into a content similarity.

Step 2: Propagate similarity according to mashups similarity network. Many mashups those are originally dissimilar establish similar relationship via propagation.

Step 3: Cluster mashups based on their similarities. By using K-means cluster algorithm, mashups are partitioned into K clusters.

Step 4: Recommend mashups according to their predicted ratings. A mashup?s predicted rating is measured by the ratings of its neighbors in the same cluster.

These steps are described in detail in the next subsections.

A. Compute Content Similarity Since a mashup is labeled with some tags, the similarity  between every two mashups can be measured according to the number of co-labeled tags [5]. That is to say, while two mashups are labeled with one or more than one same tag, they are considered similar.

Although tags are a flexible way of categorizing data, they are prone to syntactic variations due to the amount of freedom users have, which results in different tags with the similar meanings. And, tagging is related to the use of synonyms and homonyms [ ].

Therefore, more information is required for the similarity computation of mashups. As a mashup often uses several APIs to complete its functions, the co-used APIs also denote the similarity between mashups.

Mathematically, tags-based similarity and APIs-based similarity are both computed by Jaccard similarity coefficient (JSC) which is a statistical measure of similarity between sample sets. For two sets, JSC is defined as the cardinality of their intersection divided by the cardinality of their union [6]. Concretely, tags-based similarity between two mashups is computed by formula (1) [7,8]:  ?????, ?? ? = 	????? 		????? 	        (1)  Here, ??  and ??  are two mashups, ??  is a set of tags labeled to ??  and ??  are a set of tags labeled to ?? .

Similarly, APIs-based similarity between two mashups is computed by formula (2):  ?????, ?? ? = 	????? 		????? 	        (2)  Here, ??  and ??  are two mashups, ?? is a set of APIs used by ??  and ?? is a set of APIs used by ?? .

Content similarity between two mashups is computed by weighted sum of tags-based similarity and APIs-based similarity, which is expressed as follow:  ????, ?? ? = ? ? ?????, ?? ? + (1 ? ?) ? ?????, ?? ? In this formula, ??  and ??  are two mashups, ? ? [0,1]  is the weight of tags-based similarity and (1 ? ?)  is the weight of APIs-based similarity. The weights express relative importance between these two. For definiteness and without loss of generality, the weights are set to the same value in this paper, i.e., ? = ? = 0.5. In this way, direct similarities between mashups are generated.

B. Propagate Content Similarity For those mashups that have no direct content similarity  relationships, a similarity propagation procedure based on structure information is performed to find more mappings.

Similarity propagation can provide additional information and leads to extra information accessible for recommendation purposes and particularly relaxes the sparsity and the cold start problems.

Since the content similarity relationship is symmetrical, we generate an undirected mashup graph ? = (?, ?), with ? represents a set of vertices and ? represents a set of edges.

Let ??? ? ?  denote a vertex labeled by the mashup ?? .

Add an edge from a vertex ??? ? ? to a vertex ??? ? ? if they have a content similarity relationship.

Let ?? ???? , ??? ? (? ? 1, ? ? 1, ? > ?) denote the kth possible path from a vertex ??? ? ? to a vertex ??? ? ? .

Each path is defined as a sequence of vertices ??? , ? ??? +? , ? , ??? , where 0 < ? ? ? ? ? . Let ?? ???? , ??? ? represents the length of ?? ???? , ??? ?. After set a propagation path length threshold L which is an integer greater than 1, the content similarity is propagated according to one of the three rules defined as follows: Rule 1: If there is only one path from vertex ??? to ??? , and the length of this path 1 < ? ???? , ??? ? ? ? , then the propagated content similarity between mashup ??  and ?? is the minimum content similarity of mashups labeled to adjacent vertices on this path multiply a path length decay factor ?, i.e.,  ????? , ?? ? = ??!"???? , ??+1?, ? , ?????1, ?? ?# ? ?  where ? = ???1??? ? ,?? ? ?+1? .

Here, ???  and ??? +1  are the first two adjacent vertices on the path from  ???  to ??? , ???  and ??? +1  are the last two adjacent vertices on the path from ??? to ??? , ???? , ??+1? is the content similarity between  ??  and ??+1 ,     ?????1, ?? ?  is the content similarity between  ???1 and ?? , ?1 ???? , ??? ? is length of the only path from ??? to ??? .

Rule 2: If there are n paths ($ > 1) from vertex ??? to ??? , and for any path ? (1 < ? ? $) ,  1 < ?? ???? , ??? ? ? ? , then the propagated content similarity between mashup ?? and ??  is the average value of the propagated content similarities via different paths, i.e.,  ????? , ?? ? = ? ?? ? ??? , ?? ?$?=1  $ Here, ??? ??? , ?? ?  is the propagated content similarity between two mashups ??  and ??  via the kth path from vertex ??? to ??? .

Rule 3: If there is no path from vertex ??? to ??? , or for any path ? (1 < ? ? $)  from vertex ??? to ??? , ?? ???? , ??? ? > ?, then ????? , ?? ? = 0.

After similarity propagation, indirect similarities between mashups are obtained. A content similarity matrix S is then built. Each entry of S is the corresponding content similarity or propagated content similarity between two mashups.

C. Cluster Mashups The goal of cluster analysis is that the objects within a  group be similar to one another and different from the objects in other groups. The greater the similarity within a group and the greater difference between groups, the better or more distinct the clustering. Clustering methods can be classified into three categories: partitioning methods, density-based methods [9,10], and hierarchical methods [11, 12  ]. A commonly-used partitioning method is K-means [13], which has two main advantages: relative efficiency and easy implementation. Therefore, we use K-means algorithm for mashups clustering. By this algorithm, the set of mashups M is divided into clusters 1, 2, ? , & , where ? ? ? = * and ? =  1 ? 2 ? ? & .

Many clustering algorithms make soft rather than hard assignments. With hard assignments, every mashup is a member of one and only one cluster. Soft assignments allow for degrees of membership and membership in multiple clusters. By k-means algorithm, mashups are clustered into one of k groups by iteratively reassigning each mashup to its nearest cluster. The distance of a mashup to a cluster is defined as the distance of that mashup to the centroid of the mashups currently assigned to that cluster. Choosing the proper initial centroids is the key step of this algorithm. A simple approach is to choose the initial centroids randomly, but the resulting clusters may be poor. Another commonly used technique is to perform multiple runs, each with a different set of randomly chosen initial centroids, and select the set of clusters with the minimum sum of the square error.

We use a basic k-means algorithm as shown in Figure 1 to cluster mashups.

Input: A target number of clusters k, a set of mashups M, a content similarity matrix S.

Output: An assignment of mashups to clusters. The assignment is represented as a mapping from each mashup to a particular cluster ? (? = 1, 2, ? , ?).

1: Select k points as initial centroids.

2: Repeat 3: Form k clusters by assigning each mashup to its closest  centroid.

4:  Re-computes the centroid of each cluster.

5: until Centroid do not change.

Figure 1. k-means algorithm for mashups clustering  To improve the efficiency of CbCF approach, clustering algorithm is implemented offline.

D. Recommend Mashups (1) Compute rating similarity  Common ?similarity? measures include the Pearson correlation coefficient (PCC) [14] and the cosine similarity between ratings vectors. PCC was found to perform better than cosine vector similarity [15]. Therefore, PCC is applied in this paper. Based on a PCC formula proposed in [16], the rating similarity between the target mashup ?/  and a mashup ??  is computed as follow:  ?3??/, ?? ? = ? ?4?,/ ? 4/? ? ?4?,? ? 4? ?6??7/ ?7?  8? ?4?,/ ? 4/?26??7/ ?7? ? 8? ?4?,? ? 4? ?26??7/ ?7? (3)  Here, 7/ is a set of users who rated ?/  while 7? is a set of users who rated ?? , 6?  is a user who both rated ?/  and ?? , 4?,/  is the rating that 6?  gave to ?/ , 4?,?  is the rating that 6?  gave to ?? , 4?  is the average rating of mashup ?? , and 4? is the average rating of mashup ?? . It should be noted that there is a precondition before compute the rating similarity between every two services in CbCF. That is, service :/  and :?  must both belong to a same cluster / , i.e., :/ ? / ? :? ? / . Furthermore, it should be noted that if the denominator of formula (4) is zero, we make 0.

(2) Select neighbors Neighborhood has an impact on the recommendation  quality [ 17 ]. In traditional IbCF approach [ 18 ], the neighbors of a mashup ?? are determined according to constraint formula (4).

!(??)  =  " ??  	 ?3??/, ?? ? > ?, ?? ? ??  # (4) Here, :?????, ?? ? is the rating similarity between mashup ??  and mashup ?? . It means that a mashup whose similarity exceeds a pre-defined threshold can be chosen as a neighbor of a distinct mashup. All the neighbors of the mashup ?? are put into the neighbor set !(??).

(3) Compute Predicted Rating The predicted rating is typically an average of ratings of  training mashups that have been determined to be similar to the test mashup, weighted according to the degree of similarity between the training and test mashups. As proposed by Zheng et al. [19], the predicted rating of a mashup is computed by Eq. (5):  ?(6A , ?/ )  =  4?/ + ?  ?4A,? ? 4?? ? ? ?3??/, ?? ?? ? ?!(? / )  ?  ?3??/, ?? ?? ? ?!(? / ) (5)  Here, 6A  is a active user who is looking for suggestions, ?/  is a target mashup which is relevant to the current task of the active user, 4?/  is the average rating of mashup ?/ , !(?/ ) is the neighbor set of the mashup ?/ , and ?3??/, ?? ? is the rating similarity between ?/ and ??  which is computed by Eq. (3).

If the predicted rating of a mashup exceeds a threshold, it will be recommended to the active user.



III. A CASE STUDY As shown in Table I, seven  mashups (?1, ?2, ?3, ?4, ?5, ?6 and ?7)  and the corresponding APIs and tags are listed. For example, mashup ?1  is labeled by ?news?, ?photo?, ?search? and ?semantic?. Mashup ?2  is labeled by ?auto?, ?cars?, ?search?, and ?shopping?. That is to say, there is one same tag (i.e., ?search?) among the total seven tags. Therefore, the tags-based similarity between ?1 and ?2 is computed as ??(?1, ?2) = |?1??2||?1??2| =  7. And since there is only one API  used by both ?1 and ?2, the APIs-based similarity between ?1 and ?2 is computed as ??(?1, ?2) = |?1??2||?1??2| = 1. If the weight of tags-based similarity ? is set to 0.5, the content similarity between ?1 and ?2is computed as ?(?1, ?2) =? ? ??(?1, ?2) + (1 ? ?) ? ??(?1, ?2) = 0.57.

TABLE I. CASE OF MAHSUPS  No. Mashup Name Used API Labeled Tags  m1 SemanticSearcher.com Google Base news, photo, search, semantic  m2 AllAutoSites Google Base auto, cars, search, shopping  m3 360 Cities Google Maps 3d, mapping, panoramic, photo  m4 Anthems on Google Maps  Google Maps + YouTube  anthems, deadpool, mapping, video  m5 Audio Disco MTV + YouTube deadpool, music, video  m6 Favmvs Google Search + MTV  deadpool, MTV, music, video  m7 MTV Billboard charts 411Sync mobile, MTV, music, WAP  Content similarities between every pair of mashups are computed in the same way and the results are put into Table II.

TABLE II. CONTENT SIMILARITY MATRIX  m1 m2 m3 m4 m5 m6 m7  m1 / 0.571 0.071 0 0 0 0  m2 0.571 / 0 0 0 0 0  m3 0.071 0 / 0.321 0 0 0  m4 0 0 0.321 / 0.367 0.167 0  m5 0 0 0 0.367 / 0.542 0.083  m6 0 0 0 0.167 0.542 / 0.167  m7 0 0 0 0 0.083 0.167 /  Each mashup is represented by a vertex. Every content similarity relationship is represented by an undirected edge.

And the weight of each edge is the related content similarity.

We then get an undirected weighted graph as Figure 2.

Figure 2. Content similarity propagation  Based on the three rules in Section II-B, the propagations of mashups? content similarities are discussed in three cases as follows (L=4):  (1) There are only one path from ??1  to ??4  (i.e., ??1 ???3 ? ??4 ), and ????1 , ??4 ? = 2 , which meets Rule 1.Therefore, the propagated content similarity between ?1 and ?4 is computed as: ??(?1, ?4) = ??!{?(?1, ?3), ?(?3, ?4)} ? ? = 0.053 ,  where ? = ?????? 1 ,?? 4 ?+1? = 34.

Similarly, ??(?2, ?3) = 0.053 ??(?2, ?4) = 0.036??(?2, ?5) = 0.018 ??(?2, ?6) = 0.018.

(2) There are three paths from ??1  to ??5  (i.e., ??1 ? ??3 ???4 ? ??5 , ??1 ? ??3 ? ??4 ? ??6 ? ??5 and ??1 ???3 ? ??4 ? ??6 ? ??7 ? ??5 ). Their lengths are  ?1???1 , ??5 ? = 3 , ?2???1 , ??5 ? = 4  and ?3???1 , ??5 ? = 5 , respectively. According to Rule 2, the propagated content similarity betweem ?1 and ?5 is computed as:  m3  m1  m4 m5  m2 m6  m7  0.571  0.071  0.321 0.367 0.083  0.1670.167 0.542     ??(?1, ?5) = ??1(? 1,? 5)+??2(? 1,? 5)2 = 0.027, where ??1(?1, ?5) = ??!{?(?1, ?3), ?(?3, ?4), ?(?4, ?5)} ? ? , ? = ???1??? 1 ,?? 5 ?+1? = 24, and ??2(?1, ?5) =??!{?(?1, ?3), ?(?3, ?4), ?(?4, ?6), ?(?6, ?5)} ? ?,  ? = ???2??? 1 ,?? 5 ?+1? = 14.

Similarly, ??(?1, ?6) = 0.027 ??(?1, ?7) = 0.018??(?3, ?5) = 0.162 ??(?3, ?6) = 0.143??(?3, ?7) = 0.056 ??(?4, ?7) = 0.090.

(3) There are three paths from ??2 to ??7  (i.e., ??2 ? ??1 ???3 ? ??4 ? ??5 ? ??7 ??2 ? ??1 ? ??3 ? ??4 ???6 ? ??7  and ??2 ? ??1 ? ??3 ? ??4 ? ??6 ? ??5 ???5 ), but ?1???2 , ??7 ? = 5 , ?2???2 , ??7 ? = 5 and ?3???2 , ??7 ? = 6. That is to say, the lengths of these paths are all greater than the propagation path length threshold ?.

Therefore, according to Rule 3, ??(?2, ?7) = 0.

The content similarity or propagated content similarity between ??  and ??  is put into Table III.

TABLE III. CONTENT SIMILARITY MATRIX  m1 m2 m3 m4 m5 m6 m7  m1 / 0.571 0.071 0.053 0.027 0.027 0.018  m2 0.571 / 0.053 0.036 0.018 0.018 0  m3 0.071 0.053 / 0.321 0.162 0.143 0.056  m4 0.053 0.036 0.321 / 0.367 0.167 0.090  m5 0.027 0.018 0.162 0.367 / 0.542 0.083  m6 0.027 0.018 0.143 0.167 0.542 / 0.167  m7 0.018 0 0.056 0.090 0.083 0.167 /  Based on k-means algorithm (k=2), these mashups are partitioned into two clusters, i.e., ?1  and ?2  are clustered into a group (C1) and ?3, ?4, ?5, ?6 A$H ?7 are clustered into a group (C2).

Suppose there are four users have rated these mashups (as shown in Table IV). The ratings are on 5-point scales and 0 means the user did not rate the mashup. As 63 does not rate ?5 , 63 is regarded as an active user and ?5  is looked as a target mashup in this case. Furthermore, in order to verify this method, ?3  is selected as a reference target mashup for 63. The target cluster is 2.

TABLE IV. USER-MASHUP RATING MATRIX  C1 C2  m1 m2 m3 m4 m5 m6 m7  u1 5 4 4 3 3 1 0  u2 1 1 4 5 4 4 2  u3 4 4 2 2 0 2 3  u4 5 4 5 5 5 1 5  Using formula (3), rating similarity between ?5 and every other mashup in the target cluster are calculated and listed in Table V. And rating similarity and enhanced rating similarity between ?3 and every other mashup in the target cluster are calculated and listed in Table VI.

TABLE V. RATING SIMILARITIES WITH ?5 Mashup pair Rating similarity  (?3, ?5) 0.544 (?4, ?5) 0.736 (?6, ?5) 0 (?7, ?5) 0.781  TABLE VI. RATING SIMILARITIES AND ENHANCED RATING SIMILARITIES WITH ?3  Mashup pair Rating similarity  (?3, ?4) 0.839 (?3, ?5) 0.544 (?3, ?6) -0.187 (?3, ?7) 0.499  The Pearson correlation coefficient ranges in value from -1 to +1, indicating perfect negative correlation at -1, absence of correlation at zero, and perfect positive correlation at +1.

If set the rating similarity threshold I to 0.4, !(?5) ={?3, ?4, ?7} and !(?3) = {?4, ?5, ?7}.

Then, ?? (463,?5 )  = 2.25 and ?? (463,?3 )  = 2 As for ?5, if the recommending threshold is set to 3, it  will not be recommended to 63 . In addition, as the real rating of ?3 by 63 is 2 (see Table IV), it can be inferred that CbCF has good prediction accuracy.



IV. EXPERIMENTS AND ANALYSIS  A. Experiment Context 1) Experiment Platform  All Experiments are implemented using Visual C++ 6.0 running on an HP desktop with an Intel Core i3 3.20GHz CPU, 2GB RAM, and Windows 7 (32bit) operating system.

2) Experiment Dataset     Numerous of mashups, APIs and user profiles are listed on ProgrammableWeb.com and the number is still on the increase [20]. Up to November 2012, 6,226 mashups and related information are crawled from this site. The mashups are labeled with 20,936 tags among which 3,520 tags are different. And, 12,200 APIs are used by these mashups among which 4,506 APIs are different.

Since rating is a new feature appearing on ProgrammableWeb.com, there are very few ratings available. Therefore, we generate pseudorandom integer integers in the range 0 to 5 as the ratings of mashups. Each rating represents an entry in a $ ? ? user-mashup matrix.

Assume there are 500 users that have rated some mashups published on the website. Then the matrix consists of 500 rows and 6226 columns. In total, 50,000 non-zero ratings are generated. The sparsity level of the dataset is 98.39% (sparsity level =1-50000/500*6226=0.9839).

We add an empirical evaluation based on a well known statistical test, namely the k-fold cross validation [21]. The ratings records are split into k mutually exclusive subsets (the folds) M1, M2, ?, Mk of equal size. During each fold ? ? {1, 2, ? , ?}, it is tested on Mi and trained on the rest. The cross-validation process is then repeated k times, with each of the k subsets used exactly once as the validation data. We generate five distinct splits of training and test data from the rating dataset, i.e., k=5. For each data split, 80% of the original set was included in the training data and 20% of it was included in the test data. The test sets in all cases were disjoint [22].

3) Cluster of Dataset Using K-means algorithm, the mashups are partitioned  into K clusters. Set the values of K to 3 and 4 respectively, the centroids and the number of mashups in each cluster are shown in Table VII and Table VIII.

TABLE VII. THE CENTROID AND THE NUMBER OF MASHUPS OF EACH CLUSTER (K=3)  Direct path Centroid1 Centroid2 Centroid30.886 0.752 0.701 Number of mashups 857 2482 1799  2-path Centroid1 Centroid2 Centroid30.756 0.529 0.567 Number of mashups 5179 9425 9912  3-path Centroid1 Centroid2 Centroid30.758 0.361 0.546 Number of mashups 5253 748521 20289  TABLE VIII. THE CENTROID AND THE NUMBER OF MASHUPS OF EACH CLUSTER (K=4)  Direct path Centroid1 Centroid2 Centroid3 Centroid40.886 0.752 0.713 0.700 Number of mashups 857 2482 100 1699  2-path Centroid1 Centroid2 Centroid3 Centroid40.756 0.614 0.563 0.529 Number of mashups 5177 776 9239 9324  3-path Centroid1 Centroid2 Centroid3 Centroid40.756 0.526 0.567 0.361 Number of mashups 5180 10371 9992 748520  The value of k in k-means algorithm has effect on the computation time. A big value of k may reduce the scale of cluster and thus decrease the computation time. However, if k is set to a big value optionally, the value of centroids is very close, which would exclude the possibility of some mashups to be the neighbors of the target mashup. As a trade-off, we set the value of k to 3.

B. Experiment Evaluation For each test mashup in each fold, its predicted rating is  calculated based on IbCF and CbCF approach separately.

We then use a widely popular statistical accuracy metric named Mean Absolute Error (MAE), which is a measure of the deviation of recommendations from their true user- specified ratings [23].

The threshold ?  in formula (4) affects the number of neighbors. We create a threshold vector with values ranging from 0 to 0.1 (with a step size of 0.01) and compute the predicted rating of each mashup in the test data. The average MAE of the test mashups in each fold is calculated and listed in the Figure 3. It can be observed that CbCF yields lower MAE values than IbCF, which means CbCF may generate better recommendation to users than IbCF.

Figure 3. Comparison of MAE  0 0.02 0.04 0.06 0.08 0.1 0.65  0.7  0.75  Alpha  M AE Fold 1  0 0.02 0.04 0.06 0.08 0.1 0.65  0.7  0.75  Alpha  M AE  Fold 2  0 0.02 0.04 0.06 0.08 0.1 0.65  0.7  0.75  Alpha  M AE  Fold 3  0 0.02 0.04 0.06 0.08 0.1 0.65  0.7  0.75  Alpha  M AE  Fold 4  0 0.02 0.04 0.06 0.08 0.1 0.65  0.7  0.75  Alpha  M AE  Fold 5  IbCF  CbCF     To verify the efficiency of CbCF, the computation time of predicted ratings is compared with the computation time of IbCF. It can be seen from Figure 4, CbCF spends less time than IbCF because the number of neighbors of a mashup is smaller in most cases.

Figure 4. Comparison of run time

V. RELATED WORK AND COMPARATIVE ANALYSIS Clustering methods for CF have been extensively  studied by some researchers. Ungar et al. [24] clustered users and items separately using variations of k-means and Gibbs sampling. Users are clustered based on the items they rated and items are clustered based on the users that rated them. Users can be re-clustered based on the number of items they rated, and items can be re-clustered based on the number of users rated them. Each user is assigned to a class with a degree of membership proportional to the similarity between the user and the mean of the class. The performance of CF on synthetic data is good, but not good on real data. O?Connor et al. [25] and Sarwar et al. [23] used clustering techniques to partition the data into clusters and use a memory-based CF algorithm to make predictions for  CF tasks within each cluster. Xue et al. [26] combined the strengths of memory-based CF approaches and model-based CF approaches in order to enable recommendation by groups of closely related individuals. By using the rating information from a group of closely related users, unrated items of the individual user in a group can be predicted.

Quan et al. [27] proposed a CF algorithm that adds item clusters to a user-based approach. When predicting an item?s rating for a user, it looks at similar users with respect to ratings within that item's cluster. There are also other studies on clustering techniques for collaborative filtering [28,29,30,31]. In the above research work, users and items are clustered using the rating data while the additional information, for example the relationship between items, is ignored. Additional information has been proved to be very useful in certain application domains [ 32 ]. In proposed CbCF approach, mashups are first clustered based on the synthetic similarity distance, which is the weighted sum of tags-based similarity and APIs-based similarity. All mashups are partitioned into k clusters and a traditional CF algorithm is operated on a user-cluster matrix instead of on the whole user-item matrix. By reducing the dimensions of user-item rating matrix and excluding some irrelevant mashups, this approach can improve the quality of recommendation and the online performance of CF algorithms.



VI. CONCLUSION Big data require new forms of processing to enable  enhanced decision making, insight discovery and process optimization. We present a cluster-based collaborative filtering approach for mashups recommendation over big data. Besides using ratings data, similar relationship between mashups is proposed to identify their neighborhoods. A clustering technique is applied to find the groups of similar mashups. After that, a traditional CF algorithm is used to generate the recommendations. The results of experiments with real-world data set show that the CbCF approach outperforms traditional item-based collaborative filtering algorithms in terms of MAE and the computation time of predicted ratings.

