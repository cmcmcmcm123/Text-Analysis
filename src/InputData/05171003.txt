Confidence-based Concept Discovery in Relational Databases

Abstract  Multi-relational data mining has become popular due to the limitations of propositional problem definition in struc- tured domains and the tendency of storing data in relational databases. Several relational knowledge discovery sys- tems have been developed employing various search strate- gies, heuristics, language pattern limitations and hypoth- esis evaluation criteria, in order to cope with intractably large search space and to be able to generate high-quality patterns. In this work, we improve an ILP-based concept discovery method, namely Confidence-based Concept Dis- covery (C2D) by removing the dependence on order of tar- get instances in the relational database. In this method, the generalization step of the basic algorithm of C2D is modi- fied so that all possible frequent rules in Apriori lattice can be searched in an efficient manner. Moreover, this improved version directly finds transitive rules in the search space. A set of experiments is conducted to compare the performance of proposed method with the basic version in terms of sup- port and confidence.

1 Introduction  Real life data intensive applications require the use of databases in order to store complex data in relational form.

The need for applying data mining and learning tasks on such data has led to the development of multi-relational learning systems that can be directly applied to relational data [3]. Relational upgrades of data mining and con- cept learning systems generally employ first-order predicate logic as representation language for background knowl- edge and data structures. The learning systems, which in- duce logical patterns valid for given background knowl- edge, have been investigated under a research area, called Inductive Logic Programming (ILP), a subfield of Machine Learning and Logic Programming [8].

?Middle East Technical University Computer Eng. Dept.

?Contact author. Middle East Technical University Computer Engi-  neering Department 06531 Ankara, Turkey ?Middle East Technical University Computer Eng. Dept.

Confidence-based Concept Discovery (C2D) [5, 4] is a predictive concept learning ILP system that employs rela- tional association rule mining concepts and techniques to find frequent and strong concept definitions according to given target relation and background knowledge. C2D uti- lizes absorption operator of inverse resolution for general- ization of concept instances in the presence of background knowledge and refines these general patterns into frequent and strong concept definitions with an Apriori-based spe- cialization operator based on confidence.

In this work, C2D is improved by modifying the gener- alization step of the main algorithm to overcome the draw- backs of dependence on target instance order and limita- tions on constructing certain types of transitive rules.

This paper is organized as follows: Section 2 presents the related work. Section 3 gives an overview of C2D al- gorithm. Section 4 describes the modification in the algo- rithm. Section 5 presents the experiments to discuss the per- formance of improved version according to the basic C2D algorithm. Finally, Section 6 includes concluding remarks.

2 Related Work  FOIL, PROGOL and WARMR are some of the well- known ILP-based systems in the literature. FOIL [9] is one of the earliest concept discovery systems. It is a top-down relational ILP system, which uses refinement graph in the search process. In FOIL, negative examples are not explic- itly given; they are generated on the basis of CWA.

PROGOL [7] is a top-down relational ILP system, which is based on inverse entailment. A bottom clause is a max- imally specific clause, which covers a positive example and is derived using inverse entailment. PROGOL extends clauses by traversing the refinement lattice.

Design of algorithms for frequent pattern discovery has become a popular topic in data mining. Almost all algo- rithms have the same level-wise search technique known as APRIORI algorithm. WARMR [1] is a descriptive ILP system that employs Apriori rule to find frequent queries having the target relation by using support criteria.

C2D is similar to PROGOL as both systems produce con- cept definition from given target. WARMR is another simi-  2009 World Congress on Computer Science and Information Engineering  DOI 10.1109/CSIE.2009.267   2009 World Congress on Computer Science and Information Engineering  DOI 10.1109/CSIE.2009.267     lar work in a sense that, both systems employ Apriori-based searching methods. Unlike PROGOL and WARMR, C2D does not need input/output mode declarations. It only re- quires type specifications of the arguments, which already exist together with relational tables corresponding to pred- icates. Most of the ILP-based systems require negative in- formation, whereas C2D directly works on databases which have only positive data. Similar to FOIL, negative infor- mation is implicitly described according to CWA. Finally, it uses a novel confidence-based hypothesis evaluation crite- rion and search space pruning method.

PROGOL and WARMR can generate transitive rules only by using strict mode declarations. In this improved version of C2D, transitive rules are generated without the guidance of mode declarations.

3 C2D: Confidence-based Concept Discovery Method  The proposed technique is an improved version of the concept discovery system named C2D [5, 4] and is coupled with the rule generation heuristics of the system. Therefore, in this section, we describe the outline and the important features of C2D.

C2D is a concept discovery system that uses first-order logic as the concept definition language and generates a set of definite clauses having the target concept in the head.

When pure first-order logic is used to generate patterns in learning systems, it may also generate unreasonable clauses such as the ones in which predicates having arguments with different types are related. C2D prevents this by disallowing predicate arguments with different types to be unified.

In C2D, three mechanisms are effective for pruning the search space.

1. The first one is a generality ordering on the con- cept clauses based on ?-subsumption and is defined as fol- lows: A definite clause C ?-subsumes a definite clause C?, i.e. at least as general as C?, if and only if ?? such that: head(C) = head(C ?) and body(C ?) ? body(C)?.

2. The second one, which is novel in C2D, is the use of confidence as follows: If the confidence value of a clause is not higher than the confidence values of the two parent clauses in the Apriori search lattice, then it is pruned. A similar approach is used in the Dense-Miner system [10] for traditional association rule mining.

3. The third pruning strategy utilizes primary-foreign key relationship between the head and body relations. If such a relationship exists between the head and the body predicates, the foreign key argument of the body relation can only have the same variable as the primary key argu- ment of the head predicate in the generalization step.

Another new feature is the parametric structure for sup- port, confidence, recursion and f-metric definitions. The  user can set support and confidence thresholds in order to find rules having acceptable support and confidence values.

Similarly, it is possible to allow/disallow recursion in dis- covered concepts. F-metric is the rule evaluation function whose definition is given in [5]  The database given in Figure 1, is used as a running ex- ample in this section. In this example, daughter (d) is the concept to be learned, and two concept instances are given.

Background facts of two relations, namely parent (p) and female (f) are provided. Finally, types of the attributes of relations are listed.

Concept Instances Background Facts Type Declarations d(mary, ann). p(ann, mary). d(person, person).

d(eve, tom). p(ann, tom). p(person, person).

p(tom, eve). f(person).

f(ann).

f(mary).

f(eve).

Figure 1. The daughter database  The algorithm of C2D, given in Figure 2, starts with selecting a positive concept instance. The most general clauses with two literals, one in the head and one in the body, that entail the positive example are generated and then the concept rule space is searched with an Apriori-based specialization operator. In the refinement graph, if support parameter is on and the frequency of a clause is below the support threshold, it is pruned as an infrequent clause. Ad- ditionally, clauses whose confidence values are not higher than their parents? confidence values are also eliminated.

When the maximum depth reached or no more candidate clause can be found, if confidence parameter is on, then the clauses that have less confidence value than the confidence threshold are eliminated for the solution set. Among the produced strong and frequent rules, the best clause (accord- ing to f-metric value) is selected and the rule search is re- peated for the remaining concept instances that are not in the coverage of the selected hypothesis clauses.

Generalization: The generalization step is similar to the approach in [5]. After picking the first uncovered positive example, C2D searches facts related to selected concept in- stance in the database, including the related facts that be- long to the target concept in order for the system to induce recursive rules. Two facts are related if they share the same constant in the predicate argument positions of the same type. Then, the system generalizes the concept instance with all related facts.

In the daughter example, C2D selects the first concept instance, which is daughter(mary, ann), and then finds the related fact set as {parent(ann, mary), parent(ann, tom), fe- male(ann), female(mary)}. The system generalizes daugh- ter(mary, ann) with each related fact. For instance, by ap- plying absorption operator to the daughter(mary, ann) and     - Input: I: Concept Instance set, BF: Background Facts (from DB) - Output: H: Hypothesis set (initially H=?) - Parameters: min-sup, min-conf, B (used in f-metric)  md: maximum depth (body literal count) - Local Variables: Ci: Candidate clauses set at level i, d: level  G: Generalization clauses, FSC: Frequent and Strong clauses set - Repeat until I is covered by H (until I = ?) or  No more possible G can be found according to given parameters: 1. Select p (first uncovered instance) from I.

2. GENERALIZATION: Generate G of p by using BF 3. Initialize C1:=G, FSC:=?, d:=1 4. REFINEMENT OF GENERALIZATION: While Cd ?= ? and d?md  a. FSC = FSC ? FREQ STR CLS(Cd, min-sup) b. Cd+1= CAND GEN(FSC, min-sup)  i.UNION: For each clause pair in level d compute each possible union clause.

ii.For each union clause satisfying min-sup SPECIALIZATION: Unify existential variables FILTERING: Discard infrequent and less-confident clauses  c. d:=d+1.

5. EVALUATION: Eliminate clauses according to min-conf  Select cbest from FSC 6. COVERING: Compute Ic ? I covered by cbest 7. H := H ? cbest, I := I - Ic  -Return H.

Figure 2. C2D Algorithm  parent(ann,mary), the concept descriptions of the form {daughter(mary, ann) : ? parent(ann,mary)}??12  are derived.

Refinement of Generalization: C2D refines the two lit-  eral concept descriptions with an Apriori-based specializa- tion operator that searches the definite clause space in a top- down manner, from general to specific. As in Apriori, the search proceeds level-wise in the hypothesis space and it is mainly composed of two steps: frequent clause set selection from candidate clauses and candidate clause set generation as refinements of the frequent clauses in the previous level.

The standard Apriori search lattice is extended in order to capture first-order logical clauses and the candidate genera- tion and frequent pattern selection tasks are customized for first-order logical clauses.

In the daughter example, for the concept instance daugh- ter(mary, ann), two literal generalizations are generated in the presence of related facts and the first level of the search lattice is populated with these generalizations. With the support threshold value 0.8, the system eliminates the in- frequent clauses. Notice that among 18 clauses generated for daughter(mary, ann) and parent(ann, mary), only 6 of them satisfy the threshold (bold ones below). Other clauses below are generated from other generalizations.

d(A, B) :- p(C, mary). d(A, B) :- p(B, A).

d(A, B) :- p(C, A). d(A, B) :- p(ann, C).

d(A, B) :- p(B, C). d(A, B) :- p(C, D).

d(A, B) :- p(C, tom). d(A, B) :- f(A).

d(A, B) :- f(C).

The candidate clauses for the next level of the search space are generated in three important steps:  1. Frequent clauses of the previous level are joined to generate the candidate clauses via union operator. In order to apply the union operator to two frequent definite clauses, these clauses must have the same head literal, and bodies must have all but one literal in common.

2. For each frequent union clause, a further specializa- tion step is employed that unifies the existential variables of relations having the same type in the body of the clause. By this way, clauses with relations indirectly bound to the head predicate can be captured.

3. Except for the first level, the candidate clauses that have confidence value not higher than parent?s confidence values are eliminated.

Evaluation: For the first instance of the target concept, which has not been covered by the hypothesis yet, the sys- tem constructs the search tree consisting of the frequent and confident candidate clauses that induce the current concept instance. Then it eliminates the clauses having less confi- dence value than the confidence threshold. Finally, the sys- tem decides on which clause in the search tree represents a better concept description than other candidates according to f-metric definition.

Covering: After the best clause is selected, concept in- stances covered by this clause are removed from the concept instances set. The main iteration continues until all concept instances are covered or no more possible candidate clause can be found for the uncovered concept instances.

In the daughter example, the search tree constructed for the instance daughter(mary, ann) is traversed for the best clause. The clause d(A, B) :- p(B, A), f(A) with support value of 1.0 and the confidence value of 1.0 (f-metric=1.0) is se- lected and added to the hypothesis. Since all the concept instances are covered by this rule, the algorithm terminates and outputs the following hypothesis:  daughter(A,B) :- parent(B,A), female(A).

4 Improved C2D: C2D v2  In the basic algorithm of C2D, the experiments show that the selection order of the target instance (the order in the tar- get relation) may change the result hypothesis set. In each coverage set, the induced rules depend on the selected tar- get instance and the covered target instances in each step do not have any effect on the induced rules in the following coverage steps.

As a remedy to this problem, a new mechanism is de- veloped and used in the improved version of C2D, namely C2D v2. For the target relation t(a, b), the induced rules has a head including either a constant or a variable for each argument of t. Each argument can be handled indepen- dently to find the possible head relations for the hypoth-     esis set. As an example, for the first argument a, a con- stant must be repeated at least min sup multiplied by num- ber of uncovered instances in the target relation so that it can exist as a constant in a possible induced rule. To find the possible constants for a, the following SQL statement must be executed in the database:  SELECT a FROM t GROUP BY a HAVING COUNT(*) ? (min sup * num of uncov inst) For example, in the PTE-1 database [11] the target rela-  tion pte active has only one argument (arg1). At the begin- ning, there are 298 uncovered instances in pte active. The min sup parameter is set as 0.05. However, the following SQL statement returns an empty set which means there can- not be a constant for the argument arg1 of pte active:  SELECT arg1 FROM pte active GROUP BY arg1 HAVING COUNT(*) ? 298 * 0.05 The result of the SQL statement (empty set) means that  the argument arg1 of pte active can only be a variable for the head of the solution hypothesis clauses.

In the same manner, for a background relation r(a, b, c), if a constant exists many times for the same argument in r, then it is a frequent value for that argument of r and may exist in a solution clause for the hypothesis set. As an ex- ample, in the PTE-1 database, atom(drug, atom, element, integer, charge) is a background relation and possible con- stants which can exist in the hypothesis set can be found for each argument of atom by executing SQL statements. The possible constants for each argument of atom are as follows:  drug: empty set (only variable) atom: empty set (only variable) element: c, h, o (also variable) integer: 3, 10, 22 (also variable) charge: empty set (only variable) As a result, the following clauses (totally (1*1*4*4*1)*2  = 32 different clause) are created as candidate in the gener- alization step of the improved version.

pte active(A) :- atom(A, B, c, 3, C) pte active(A) :- atom(A, B, c, 10, C) pte active(A) :- atom(A, B, c, 22, C) pte active(A) :- atom(A, B, c, C, D) pte active(A) :- atom(A, B, h, 3, D)  ...

Support and confidence values for each clause are calcu- lated and the infrequent clauses are eliminated. The special- ization and coverage steps are same as in the basic version.

Another drawback of the generalization step of the basic C2D algorithm is about generating transitive rules. In the basic version, only the related facts of the selected target in- stance are used in the algorithm. If a background relation does not have any argument with same type of any argument of the target relation, then that background relation is called  an unrelated relation and has no effect in the result hypoth- esis set. However, as the modified version of C2D handles all the background relations independently, it can find the transitive rules in the search space.

In the daughter example, the target and background re- lations can only have variables for the arguments in the hy- pothesis set. So, the following clauses are produced in the generalization step:  d(A,B):-p(A,B). d(A,B):-p(A,C). d(A,B):-p(C,B).

d(A,B):-p(C,D). d(A,B):-p(B,C). d(A,B):-p(C, D).

d(A,B):-f(A). d(A,B):-f(B). d(A,B):-f(C).

At the end of the first coverage step, the following clause which covers all the target instances is induced:  daughter(A,B) :- parent(B,A), female(A).

5 Experimental Results  5.1 Constructing Transitive Rules  The generation phase of basic C2D algorithm considers only related facts to construct the 2-predicate (one head and one body predicate) generalized rules. However, this ap- proach falls short for the cases where the domain includes many unrelated facts. Michalski?s trains problem [6] is a typical case for this situation. In this data set, the target re- lation eastbound(train) is only related with has car(train, car) relation. The other background relations have an argu- ment of type car and are only related with has car relation.

The basic version of C2D finds the following rules: eastbound(A :-has car(B,car 11). eastbound(A):-has car(east1,B).

eastbound(A):-has car(B,car 12). eastbound(A :-has car(A,B).

eastbound(A):-has car(B,car 13). eastbound(A):-has car(B,C).

eastbound(A):-has car(B,car 14).

As seen above, the generated rules are very general and can not include any information about the properties of the cars of the train. However, C2D v2 takes all of the back- ground relations into consideration and can find the follow- ing transitive rule in the search space:  eastbound(A):-has car(A,B),closed(B). (s=5/5,c=5/7).

5.2 Finite Element Mesh Design  In Mesh Design, the task is to learn the rules to determine the number of elements for a given edge in the presence of the background knowledge. Four different structures called (b-e) in [2] are used for learning in this experiment. Then, the structure a (having 55 examples) is used for testing the accuracy and coverage of the induced clauses. For this ex- periment, recursion is disallowed, support threshold is set as 0.1, confidence threshold is set as 0.1, B is set as 1 and maximum depth is set as 3.

In this experiment, C2D finds rules that cover 31 of the 55 records in the test data set [5]. The improved version, C2D v2, finds rules that cover 29 of the records in the test     Method Type Pred. Acc.

C2D v2 (improved ver.) ILP + DM 0.80  Ashby Chemist 0.77 PROGOL ILP 0.72  RASH Biological Potency An. 0.72 C2D (basic ver.) ILP + DM 0.69  TIPT Propositional ML 0.67 Bakale Chemical Reactivity An. 0.63  DEREK Expert System 0.57  Figure 3. Predictive accuracies for PTE-1  data set. However, the accuracy of the rules found by C2D is 0.29, whereas the accuracy of the rules by C2D v2 is 0.49.

In other words, the improved version finds rules that have nearly same coverage but higher accuracy according to the basic version of C2D.

5.3 Predictive Toxicology Evaluation  The carcinogenecity tests of compounds are vital to pre- vent cancers; however, the standard bioassays of chemicals on rodents are really time-consuming and expensive. In the National Toxicity Program, the tests conducted on the ro- dents results in a database of more than 300 compounds classified as carcinogenic or non-carcinogenic. Among these compounds, 298 of them are separated as training set, 39 of them formed the test set of PTE-1 and the other 30 chemicals constitute the test set of PTE-2 challenge [11].

In this experiment, PTE-1 data set is used, recursion is disallowed, support threshold is set as 0.05, confidence threshold as 0.7, B is set as 1 and max depth is set as 3.

The predictive accuracies of the state-of-art methods and C2D with its improved version for PTE-1 data set are listed in Figure 3. As seen from the figure, C2D v2 has a better predictive accuracy than the basic C2D algorithm. In addi- tion, it finds the best results (having highest accuracy) with respect to other systems.

6 Conclusion  This work presents an improvement for an ILP-based concept discovery system, named C2D, which combines rule extraction methods in ILP and Apriori-based special- ization operator. By this way, strong declarative biases are relaxed; instead, support and confidence values are used for pruning the search space. In addition, C2D does not re- quire user specification of input/output modes of arguments of predicates and negative concept instances. Thus, it pro- vides a suitable data mining framework for non-expert users who are not expected to know much about the semantic de- tails of large relations they would like to mine, which are stored in classical database management systems.

In C2D v2, generalization step of C2D is modified in such a way that most general rules are constructed by con- sidering the number of occurrences of constant arguments in the rules. By this mechanism, the effect of target instance order on rule generation is eliminated and rule quality is im- proved. Another improvement is on constructing transitive rules. Since this new method does not differentiate related and non-related facts, transitive rule generation is not lim- ited to related facts.

The proposed system is tested on several benchmark problems including the Michaslki?s train problem, mesh de- sign and predictive toxicology evaluation test. The experi- ments reveal promising test results that are comparable with the performance of basic C2D method and current state-of- the-art knowledge discovery systems.

