Managing and Analysing Big Audio Data for Environmental Monitoring

Abstract?Environmental monitoring is becoming critical as human activity and climate change place greater pressures on biodiversity, leading to an increasing need for data to make informed decisions. Acoustic sensors can help collect data across large areas for extended periods making them attractive in environmental monitoring.  However, managing and analysing large volumes of environmental acoustic data is a great challenge and is consequently hindering the effective utilization of the big dataset collected.  This paper presents an overview of our current techniques for collecting, storing and analysing large volumes of acoustic data efficiently, accurately, and cost-effectively.

Keywords- acoustic sensing; acoustic data analysis; big data management and processing; environmental monitoring; visualization; citizen science; eScience.



I.  INTRODUCTION Climate change and human activity are subjecting the  environment to unprecedented rates of change. Large scale environmental monitoring to manage environmental change is becoming critical. Acoustic sensing is an effective tool for monitoring species that have regular and predictable vocalisations such as birds, and offers an enduring record to allow for future interpretation of results [1].

In order to provide insights into ecosystem dynamics, the Bioacoustics Research Group collects acoustic data for a range of projects, including investigating rare birds, koala behaviour, and monitoring the environment of Samford Valley's (located 20 kilometres northwest of Brisbane).

The collection of data is very important.  The ability of analyzing and using the data effectively is even more important.   The group aims to provide helpful tools for ecologists, focusing on automated, semi-automated and manual faunal acoustic event detection and identification.

The tools provide information and computational technologies for all aspects of acoustic analysis of data collected through environmental monitoring.

While acoustic sensors are attractive as autonomous data collectors, there are significant issues associated with large- scale data manipulation and analysis.

An acoustic sensor can generate several gigabytes of compressed data per day. Long term recordings to monitor species richness or changes in species composition in large areas can generate large volumes of data. These large  quantities of raw acoustic data must be stored, analysed and summarised. However, the raw audio data recorded from fields directly is difficult to analyse due to various levels of background noise, variation in species vocalisations and overlapping vocalisations. Therefore, the analysis of acoustic sensor data for environmental monitoring is a complex and difficult task.

This research seeks to address the analysis of large volumes of acoustic sensor data by humans or computer programs or a combination of both. The main purpose is to ensure the reliable, efficient and rapid recognition of acoustic events in audio recorded directly from the field.

This paper presents an overview of our current techniques for analysing large volumes of acoustic data.

There are other research projects around the world which are related to automating environmental monitoring using acoustic and other sensors.  Representative research includes Stuart Gage?s Remote Environmental Assessment Laboratory (REAL) 1 .  The REAL project focuses on soundscape interpretation and ecosystem dynamics at high level over a long period.  The AMIBIO project 2 focuses on automatic acoustic biodiversity monitoring and the analysis of acquired data.   Other researchers in the area of ecological research via bioacoustics include Jerome Seuer 3   who studies animal diversity at both species and community levels, and Stuart Parson 4  who focuses on animal behaviour. Our research focuses on terrestrial bioacoustic monitoring. The other projects described here are conducted mainly by ecologists, whereas we focus on developing tools to support ecologists to conduct research.

The remainder of this paper is organised as follows.

Section 2 outlines the data collection approaches employed in our system and their potential applications. Section 3 discusses the nature of the dataset, strategies for managing the large volume of audio data and the framework for data management. Section 4 describes requirements, challenges and techniques for analysing big environmental data.

Section 5 presents different data visualization techniques employed in the system.  Finally Section 6 concludes the paper and discusses future work.

1 http://www.real.msu.edu/ 2 http://www.amibio-project.eu/ 3 http://sueur.jerome.perso.neuf.fr/ 4 http://www.eab.auckland.ac.nz/parsons.html   DOI 10.1109/CSE.2013.146    DOI 10.1109/CSE.2013.146

II. DATA COLLECTION AND UPLOAD Sound is collected by two means: acoustic sensor  networks and recorders. We are also developing a mobile app to allow users to record and upload audio data using their mobile phones [2].  The website (http://sensor.mquter.qut.edu.au/) provides an interface for users to access the acoustic data, annotate sounds of interest, and perform various types of analysis. This portal provides the following core functionality:  ? Data upload and storage.

? Data organisation and structuring  ? Audio recording playback and visualisation  ? Data analysis and annotation.

We describe different data collection approaches below.

A. Networked Sensors for Near Real-Time Sensing These devices provide a near real-time sensing in  locations with 3G connectivity. Due to bandwidth constraints and 3G transmission costs, they are configured to activate at regular intervals (typically two to four minutes at half hourly intervals), upload data to a central repository and then deactivate until the next scheduled recording. These networked sensors consist of a 3G smart phone (HTC), an electret-style external microphone, pre-amplifier, DC-DC converter and a solar panel as an external power supply. The system is capable of operating continuously and autonomously for months at a time with minimal maintenance. Uninterrupted, continuous deployments of 18 months have been achieved to date. Recordings are sampled at 22,050 Hz (or subsequently down-sampled to this value) and at a bit rate of 16.

B. Audio Recorders for Continuous Sensing Audio recorders are designed to provide a short term or  ad hoc high resolution acoustic sensing capability but can be configured to record continuously for extended periods.

Recorded acoustic data is stored internally and the device can record continuously for around 2 weeks. These sensors are highly portable and powered by batteries for ad hoc deployments or externally using a solar power supply for fixed deployments. They are self-contained in an all-weather container. All recordings so far were obtained from various locations around Australia. The recorders returned recordings of up to a day in duration.

C. Handheld Devices for Ad-hoc Sensing We are also developing a mobile application to allow  users to use mobile phones to record audio data of interest and upload them into the central repository. These recordings are typically short (maximum a few minutes) and people can record them whenever they hear interesting things. This mobile app can be used by general citizens to collect audio data for research purposes.  The users will also be able to write brief descriptions about the audio data, view the spectrogram or waveform of the audio and mark and comment on relevant events. For example, if the audio  recorded contains a specific bird call, the user can submit the audio file to our website along with a description and ask the system to suggest the type of bird it is or if there are there any similar calls in the system.



III. STORAGE AND MANAGEMENT Acoustic sensing provides ecologists with the capability  to massively scale ecological observations, both temporally and spatially. At long term scales, even scheduled recordings (e.g. five minute recordings every 30 minutes) provide ecologists with significantly more data than manually collected long term survey data.

A. Characteristics of the dataset Sensors provide an effective means to accumulate data at  large scales and high resolutions. Acoustic sensors have been employed in this capacity for some time, both in marine and terrestrial environments [4]. They can be used to collect data passively, objectively and continuously across large areas for extended periods of time. While these factors make acoustic sensors attractive as autonomous data collectors, large scale data collection presents its own problems:  ? Data volume ? acoustic sensors generate vast quantities of raw audio data which must be stored, analysed and summarized.

? Data variety ? different types of sensors are used in different cases.  Different data formats and different resolutions have to be accommodated.  Even recordings from the same sensor vary a lot depending on the distance between the sound source and the microphone as well as the environment e.g. an open space or a dense bush area.

? Data velocity ? For long term environmental monitoring, data will be collected continuously therefore the data volume will keep increasing.  The speed of increase depends on how many sensors are used and the corresponding recording schedule e.g.

continuous recording or periodical recording.

As an indication of the volume and velocity of data produced by a single acoustic sensor, recording in MP3 format at 44.1 kHz, 128 kbps for 24 hours generates 1.3GB of data [3]. Our repository currently holds over 24TB of acoustic data and keeps growing. To monitor larger areas with more sensors in the future, the volume produced and velocity of production will grow rapidly.

B. Data storage Acoustic data are structured on a hierarchical model of  Projects, Sites and Recordings. A project can represent any logical collection of experiments or studies and specifies users? level of access. Each project consists of a collection of Sites. Sites are physical locations with sensors deployed at each site. Sensors details are stored to ensure retention of experimental design details. Recordings are the raw acoustic data files collected from sensor devices in the field and uploaded to the website.

Data is stored on a central server: raw sound is stored in a file system and sound metadata, tags and other data are stored in a database. In this way, large sound files can be stored efficiently in a regular file system: currently this is hosted by a dedicated file server. The simple file based interface lends itself to more sophisticated hosting for example by portioning across a set of file servers or even a cloud based solution. Long recordings are split into fixed- length segments with configurable lengths. For example, a continuous 24-hour recording can be divided into 240 six- minute segments. This allows the user to start listening at a particular time offset without requiring the entire 24-hour recording.  On the other hand metadata is stored in a relational database, permitting the efficient querying and selection of meta-data. This is important to enable fast and efficient navigation to data on the web server. It also permits complex data sets to be formed through relational joining of metadata.

C. Query support The system currently supports four types of queries:  spatial, temporal, annotation, and project. These query types will be expanded to support more flexible and complex queries in the future. Spatial queries return all recordings gathered from a specific location such as the Samford research facility. Temporal queries return all recordings gathered between time ranges (e.g. 11:00am  and 3:00pm) or date ranges (e.g. 2009-10-01 to 2009-10-14). Annotation based queries return all recordings with certain tags or frequency and duration bounds. Project-based queries return all recordings associated with a project.  We are currently developing techniques for Query-by-example, that is, content-based audio searching which we call Find Event Like This.

D. Data management framework We have designed and partially implemented a multi-  layer data management framework shown in Fig. 1. This system includes four layers: 1) data collection layer for collecting and uploading data, 2) data management layer for storing and accessing data, 3) event processing layer for extracting useful events, and 4) data mining layer for correlation analysis and knowledge discovery.

There is a user interface for each layer.  In the data collection layer, uses can upload data from recorders e.g.

Song Meters or mobile sensors either manually or programmatically.  In the data management layer, users can access the raw audio data directly, listen to them, or search them based on some meta data such as time or location.  In the event processing layer, users can detect events using some automated tools or manually marquee events on the audio display directly or annotate some events with the help of machine recommendations. In the data mining layer, users can use customized tools or commercial software to analyse the events together with related meta data and conduct statistical analysis, correlation analysis, trend analysis etc.

Figure 1.  Multi-layer data management for environmental monitoring via Acoustics

IV. ENVIRONMENTAL AUDIO DATA ANALYSIS Detecting and identifying individual species?  vocalisations against a background of general noise and other vocalisations is a complex and challenging task.

First, we work with raw environmental audio, contaminated by noise and artefacts, and containing calls that vary greatly in volume and clarity depending on the animal?s proximity to the microphone and the surrounding environment. Second, initial experimentation suggested that no single recogniser could deal with the enormous variety of calls. Therefore, we developed a toolbox [5] of generic recognisers to extract invariant features for each call type.

The automated recognition of animal calls has not yet reached a level of reliability that allows ecologists to use the methods without careful verification of results. It makes sense to offer semi-automated tools which allow an adjustable degree of user interaction with the data. The identification of animal calls in arbitrary recordings of the environment remains a difficult task. We believe that it is most unlikely that automated generic recognition of animal vocalisations with high accuracy and consistency will be achieved in the near future. Consequently semi-automated methods involving algorithms and people will be required for accurate and consistent analysis of environmental acoustic data for the foreseeable future. Therefore, our group is researching approaches to audio analysis with both human computation and automatic detection.

This research has led to the development of species specific recognisers that have influenced a number of real- world applications, which include invasive (cane toad and asian house gecko) and endangered (Koala, Lewin's Rail) species monitoring. In addition, species richness studies based on random sampling of acoustic sensor data have been demonstrated to be comparable to traditional survey techniques, such as bird point count surveys [4].

A. Environmental Data Analysis Requirements and Challenges Although we have used sensors to a large collection of  data, analysing the huge dataset has posed a great challenge.

Data Analysis Requirements for environmental  monitoring  Any application that offers analysis tools to ecologists  must necessarily offer graded levels of utility from fully manual to fully automated.  Traditionally acoustics is used by scientists to study a particular species, especially birds, frogs, insects, and some mammals [1]. Bioacoustic monitoring and powerful analysis of big data set have enormous potential to help ecologists scale fauna surveys and enable new form of research. Typical ecological questions to be answered using the results of sound analysis include the followings: ? Species presence/absence e.g. are there any koalas?

? Species abundance e.g. how many koalas there?

? Species richness/diversity e.g. how many different species of birds there?

? Behavioural studies e.g. how is a pest species spreading?

What is the relationship between behaviour and environment?

? Habitat disturbance e.g. how is land use change affecting the habitat of koalas?

? Environmental health measurement e.g. what value does a particular environmental bank have?

Apart from the general short computation time and high accuracy requirements, one more constraint on the data analysis tools is that they must be easy to use and flexible to accommodate scientists? needs.

Challenges in environmental audio data analysis  Two main issues with environmental data are noise and  variability. Other issues include unpredictable, dynamic, and overlapping etc.

? Low Signal-to-Noise-Ratio (SNR). Most approaches on  this study are based on analysis of data collected in the lab, which means they can depress the noise from other sources so that their data has high SNR. Different from their data, our real world data tend to comprise of mixture of sounds from various sources, which has quite low SNR. Environmental acoustics contain a wide variety of non biological noises such as traffic noise having a great range of intensities and a variety of animal sounds that have nothing to do with the task at hand.

? Large variation of calls between different bird species, regions and environments. There are three cases causing variations. The first is distance variation. Birds may fly far away from or close to recorders, resulting in different  appearance of bird calls in the spectrogram. The second is Local Call Variations. The same type of birds can have a broad range of vocalisations and these vocalisations may have significant regional variation.

Even in some special case, some birds mimic sounds of other birds so as to bring misinformation. The third one is environmental variations. The environmental noise including songs from wind, rain and human activity can have great changes over time. Also environmental factors such as wind, rain, vegetation and topography can attenuate, muffle and distort vocalisations considerably.

? Dynamic and unpredictable. In many cases, there is no certain sequential pattern in environmental sound as the presence/absence of certain birds or insects etc can be random. For example, in urban environments traffic noises, air-conditioners, human speech, dogs barking and airplanes can interfere. This brings a challenge in detecting target calls from background noise in an automated way.

? Overlapping. A large number of acoustic events overlap with each other at dawn chorus making it very difficult to recognise individual bird calls.

To sum up, environmental sounds are noisy, unstructured, dynamic and unpredictable.  It is very difficult to build models for environmental sounds, causing problems for environmental sound analysis.

B. Manual Processing Approach (by Experts and Citizen Scientists) The simplest approach of analysing the audio data  collected by sensors is to make use of citizen scientists.

These keen enthusiasts are often interested in listening through the audio data to catalogue the species heard within.

However, this process, is time consuming and inefficient.

The manual approach involves an audio player and some way to take notes.

Additionally, many of the citizen volunteers have considerable expertise in identifying fauna by their vocalisations. However, not all of the participants that contribute are experts and many need help. Even for our expert volunteers, they are often only experts for one ecological region ? outside their geographic area of expertise, their identification capabilities diminish.

C. Automated Approach Automated animal call recognition has the potential to  dramatically improve the ability of ecologists to monitor the environment on large spatial and temporal scales [11, 12].

There are mainly two types of recognisers and they both use lots of parameters: species specific recognisers (e.g. Koala recogniser), and generalised recognisers (e.g. ?Find Event Like This? search in databases). The approach taken by most call recognition methods to date is to construct a detailed representation of each call of interest. This of course requires detailed prior knowledge based on the collection of a large number of representative calls. A general recogniser is much     harder to develop as it is very difficult to define a general feature set that can be used to recognise all bird calls.

The majority of our automated analysis tools are based on the analysis the Time-frequency spectrogram, which is a popular way of representing sound.  This representation suits well for both manual labelling of events on the spectrum and automated analysis.  Chang-Hsing Lee also proposed a new feature descriptor [15] that uses image shape features to identify bird species, which is based on the recognition of gray-level birdsong spectrograms. Another typical representation of sound is waveform, which is in the time domain.

The raw data we collected (sound recordings in waveforms) is first processed by noise reduction using signal processing techniques, which is followed by transforming the waveform into a spectrogram prepared using the Short- Time Fourier Transform (STFT).

The majority of our call recognition algorithms employ features derived from spectrograms [5, 7]. The signal is framed using a window of 512 samples (23.2ms) which offers a reasonable compromise between time and frequency resolution. All the automated, semi-automated recognizing and binary template matching are based on the time- frequency spectrum.

Representative examples of recognition techniques we have implemented include [13, 14]:  MFCC features + HMMs: We have found this technique to be suitable only for high quality single-syllable calls. We used the Hidden Markov Model Tool Kit (HTK) [24] and applied it to the recognition of Pied Currawong (Strepera graculina) calls.

Acoustic Event Detection (AED): We developed AED to extract information about the distribution of acoustic energy in a spectrogram using the minimum of prior knowledge.

AED processes the spectrogram like an image and returns rectangular marquees around isolated acoustic events.

Oscillation Detection (OD): We used a Discrete Cosine Transform to find repeating or oscillating elements of calls within a user specified bandwidth.

Event Pattern Recognition (EPR): This technique extracts acoustic events from the spectrogram, and then detects a 2D pattern of events whose distribution matches a template.

Acoustic Component Detection: We define generic acoustic components in the spectrogram such as lines (usually with angles), oscillations, stacked harmonics, and others. A set of acoustic components which are distinguished from each other are built. Feature vectors of different acoustic components are extracted or defined in order to develop detectors for each component.

Syntactic Pattern Recognition (SPR): this technique models a call as a symbol sequence, each symbol selected from a finite alphabet representing ?primitive? elements of the composite pattern. Seven broad types of ?primitive? elements have been defined to construct various animal calls [7].

Similarity Search (SS): In the process of SS, an arbitrary acoustic event can be provided, and our algorithm will automatically scan through the continuous recording to find the similar events. In this research, we have identified a new  type of features which can be used for similar search in environmental audio dataset.  The initial experimental result with a small dataset is very promising. We are trying to do more experiment on a relatively larger dataset.

The techniques of automated recognition of animal calls are improving over the time.  However, the average recognition accuracy is still very low.  These techniques have not yet reached a level of reliability that allows ecologists to use the methods without careful verification of results.

D. Semi-Automated Approach Manual analysis utilises the sophisticated recognition  capabilities of a human user, but cannot be efficiently scaled to process the volumes of data collected in long-term sensor deployments. Automated techniques are effective for identifying some targeted species in large volumes of data, but they require a high degree of skill to develop and are currently not able to cope with the variability found in animal calls.

Combining manual and automated approaches provides users with the ability to analyse large volumes of acoustic data interactively and systematically. Semi-automated analysis takes advantage of the fact that human analysis capabilities are currently superior to that of automated computational analysis tools.

To deal with these challenges, our research has identified the need to provide participants with flexibility when analysing acoustic data. Using technology to assist a manual process helps increase the efficiency of our citizen scientists.

As more concepts are added, we automate more of the manual process ? this is the spectrum of semi-automated analysis. This spectrum of semi-automated annotation techniques include any form of digital assistance, be it with tools, user interfaces, or collaboration techniques. The automation components may be small initially, but as we add more and more techniques, the system will be less manual and more auto. The semi-automated analysis is a spectrum and heading towards more and more automated.

We continue to investigate the role of citizen scientists in analysing large volumes of environmental acoustic data in order to identify bird species. Specifically, we investigate ways in which the efficiency of a user can be improved through the use of species identification suggestion tools, guided virtual bird walks, the addition of contextual information, and the use of reputation models to predict the accuracy of users with unknown skill levels. The aim is to combine these tools together to allow large volumes of data to be processed effectively by participants with expertise in varying environments and species [8-10].

Further to this goal, we have researched the following forms of assistance that are designed to help our participants when they are analysing acoustic data:  Online collaboration [10]: enables users to scale manual analysis by allowing multiple users to collaborate on identification and annotation issues. Collaboration, through discussion, is also used as a verification mechanism for uncertain identifications.

Online species identification library: To reduce the time taken to identify vocalisations and to improve the     consistency of novice users, the online species identification library can enable comparisons of a call in a spectrogram with spectrograms of previously identified species vocalisation exemplars.

Suggestion Tool: An extension the species identification library is the suggestion tool. Deigned to work while a participant is actively annotating, this tool acts like an auto- complete-box would; it offers real-time suggestions of what acoustic event a participant may be actively annotating [8].

Rapid spectrogram scanning: When analysing acoustic data there are two problems: identification and classification.

Some vocalisations have a unique and characteristic appearance that people can recognise amongst other vocalisations even in complex acoustic environments. Since we use human participants that are shown spectrograms, it is possible take advantage of the advanced recognition capabilities of the human visual cortex. For vocalisations that have a unique or pronounced appearance in a spectrogram, it is possible to significantly increase the speed at which our human participants can process these spectrograms. This is achieved by disabling audio playback, and flashing successive spectrograms past participants at high speeds. We have achieved as much as 12x speedup for time taken to identify a species in a section of audio [7].

Tag validation, correction, and linking: We choose to use a folksonomy tagging system to label our annotations. The tags are descriptive and free form, however, as is the case with most systems that have human components, errors are generated. We have done work to automatically correct, clean, and link our folksonomic tags to taxonomic data structures [6]. Ensuring the tags on our annotations stay clean ensure our core data output for ecologists is not affected by human errors.

Another vein of research includes moving our participant analysis tools to mobile phone. Given that our interactive annotation editor is already web-based and the increasing capability of browsers on mobile devices, this area looks promising.

E. Data Mining for Knowledge Discovery By using human computation approach,  ecologists and  birders have helped us annotate the audio data and get the initial set of event tags, which are textual descriptions of some sound events e.g. a bird call. However, the knowledge embedded in these annotations has not been fully discovered yet. We have attempted to use data mining techniques to analyse these annotated environmental sounds considering species, location and time and try to discover knowledge embedded in the dataset.  For the initial experiment, we have used a commercial data mining software called SAS.

By analysing the large amount of human generated tags and correlate them with time and location, we have found the distribution of animal calls and some useful temporal and spatial relationships in the dataset collected.  Fig. 2 shows the relationship between the number of calls made (Y-axis) and different bird species (X-axis).  It demonstrates that the majority of bird calls recorded are made by a small amount of bird species and some species rarely call (vocally cryptic).

This finding is consistent with the existing ecological  knowledge.  This longtail phenomenon also inspires further research into finding cryptic calls in order to conduct biodiversity analysis through audio analysis. Other findings such as most birds do not call during heavy rain are also consistent with common knowledge.

Figure 2.  Distribution of bird calls and species

V. DATA VISUALISATION Data visualisation is the visual representation of data  while the main goal is to communicate information clearly and effectively through graphical means. Data visualisation provides insights into a rather complex data set by communicating its key-aspects in a more intuitive way.

To convey ideas effectively, we use different data visualisation approaches at different processing stages.

A. Time-frequency spectrogram The raw data we collected (sound recordings in  waveforms) is first processed by noise reduction using signal processing techniques, which is followed by transforming the waveform into a spectrogram prepared using the Short- Time Fourier Transform (STFT) (Fig 3.).

Figure 3.  An example spectrogram with annotations shown  The spectrogram is very useful for manual labelling and fast scanning of acoustic events. With the display of a spectrogram, while listening to the sound the user can marquee the call of interest in a noise reduced spectrogram and enters any textual comments about the marked event and corresponding sound. This forms the basis of our manual acoustic analysis approach. Fig.3 shows an example grey- level spectrogram with annotations of some events. The spectrograms can also have true colours which represent the power of the sound. The spectrogram is very intuitive to show some unique acoustic events such as rain because the sound of rain looks like rain on the spectrogram (many vertical lines) and many bird, insect or mammal calls have their own unique visual structures in the spectrogram.

B. Tag linking When presenting species information e.g. bird types to  users, it is ideal to link together the specific audio file, tags which are normally common names of birds appearing in the audio, scientific names, pictures of the birds, and other useful information.  Fig. 4 shows the concise representation of such linked information in our system.

The initial tags are generated by experienced bird watchers.  With the tags normalized, cleaned, and mapped to species names, the data is ready to be mapped to a formal taxonomic structure.

The Atlas of Living Australia (ALA) is an online resource that provides information on faunal species in Australia. This information can be accessed through their provided web API (Application Programming Interface). We choose to link to the ALA because the majority of our data involves birds (for which the ALA has a wealth of information) and their API is accessible and well documented.

The goal is to use the species name tag on each annotation to search for the associated Life Science Identifier (LSID). Each item in the species name map is searched for using the ALA API. The query is returned containing a list of search results. The taxonomic data returned is then used to provide additional relevant information to a user.

Figure 4.  Example of tag linking  C. 3-D display GeoFlow is a three dimensional (3-D) data visualisation  tool that is developed by Microsoft and integrated with Microsoft Excel 2013. It provides a powerful method for people to look at information in new ways.

As an Excel add-in, GeoFlow provides interactive, 3-D geospatial and temporal data visualisations. It enables users to discover and share new insights from data through rich, 3- D data on a globe. It could help us understand the large-scale geospatial and temporal trends behind the tedious rows and columns. 3-D display is based on Microsoft Bing Map, so it is compulsory to be connected to the internet to use GeoFlow.

We use GeoFlow to show the species distribution according to static geographical locations or dynamic temporal data flow. This can provide us and ecologists a quick view of the collected and recognized data, it is also a preliminary step prior to a complex data mining algorithm.

Below is a snapshot of temporal display created by GeoFlow.

Figure 5.  Snapshot of GeoFlow temporal display  D. Other general visualization tools Our system has also implemented many other popular  visualization tools. Examples include: a map to show all sensor distribution, Pie and Bar charts to demonstrate the status of sensors or users, and word cloud to show the distribution of textual tags.



VI. CONCLUSIONS AND FUTURE WORK The ability to collect data, store and manage them is very  important. The ability to analyse these data, extract useful information to help understanding the data and make informed decisions are even more important.  This paper presents an overview of various approaches of collecting, managing, and analysing large volumes of audio data for environmental monitoring. These tools combined together will allow large volumes of data to be processed efficiently.

Currently our research focuses on terrestrial bioacoustic event analysis in audio data. We have attempted to analyse the large amount of data using three complimentary approaches: manual analysis with the help of participatory citizen scientists, semi-automated analysis which leverages on both human and machine intelligence, and automated analysis based on machine learning and pattern recognition.

All three approaches are aided by data visualization techniques wherever possible.  The research findings so far are promising but there are still many challenging research questions to be addressed.  In the future, we plan to conduct more research in the following areas: ? Engaging citizen scientists to participate in the detection  and identification of animal vocalisations. The current work is focusing on the identification of birds by expert birders.

? The development of interfaces for automated tools that are usable by participants. Currently, our automated analysis are very technical, their parameters are full of jargon, and they take long amounts of time to run. If possible, it would be much more efficient to integrate these automatic tools into our interactive annotation environment, whilst somehow hiding their complexity and allowing our participant scientists to make the most of them.

? Mobile support: We will research ways to allow mobile users to interact with the data more effectively.   For example, we are building mobile applications to allow search and retrieve acoustic data from the central database by providing sample audios.

? Data mining. Although ecologists and birders have helped generate a large amount of tags and we have experimented mining some knowledge based on the textual tags, more extensive correlation analysis between bird calls, bird behaviors, environment, time, location needs to be conducted.

? Flexible user query support.  We will build more flexible user interfaces such as supporting queries by words e.g.

return all afternoon recordings containing a specific bird call.

All the efforts mentioned will allow our system to be more powerful and easier to use thus enabling ecologists, citizens and policy makers to utilize the big dataset to gather useful information more conveniently, in order to better understand the environment and make informed decisions.

