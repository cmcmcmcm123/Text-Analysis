Meaningful Inner Link Objects for Automatic Text Categorization

Abstract   This paper presents a novel approach for automatic  text categorization. The mainstream of the research on rule-based classifier regards document as a container of term, and generates rules by using the term distribution in documents. General speaking, there must be existed some kind of semantic relevance between term and paragraph in a document. We call it Meaningful Inner Link Objects-MILO which must be varied with different semantics of a document itself.

While this paper concentrates on using these MILOs that associate with semantic relevance for text categorization, hence we focus on two problems: (1) finding the best MILOs which associate with semantic relevance; and (2) using these specific MILOs to build a classifier for text categorization. From the experiment results, our proposed classification approach base on MILO has a better accuracy while other state of the art technique without considering the relevance between term and paragraph.

1. Introduction   Text categorization is the task of classifying unlabelled documents into one or several predefined categories. It?s gradually becoming a subject and receives much concern from researchers in recent years.

Because the continuing popularity of Internet technology, user can readily access enormous digital documents. Hence to efficient organize and retrieve these digital documents have become a very important subject for discussion.

Currently, Text categorization is a discipline at the crossroads of information retrieval and machine learning that is mentioned in [1]. The process of text categorization can be simply divided into preprocessing and classifier construction. As shown on Fig.1, there are techniques in preprocessing, such as tf?rf, information gain, chi-square, etc. And these techniques are arranged in [2]. These techniques assign appropriate weights to the terms. And the terms in higher weights will be selected as indices of a document, and other terms in lower weights will be  filtered out. The classifier construction such as Bayes classifier [3] is a probabilistic classifier by applying bayes theory for classification that bases on the hypothesis of independence between terms? relationship. K-nearest neighbor classifier [4] is an example-based classifier which categories an unlabelled document according to the distance information with K nearest examples in the classifier.

In addition, decision tree classifier [5], association rule classifier [6], etc, all of them achieve very significant performance in text categorization.

These techniques mentioned above do not consider the paragraph factor, hence to investigate the relationship about paragraphs further that needs to normalize irregularity paragraphs at first, and then builds a classifier which considers the paragraph factor. Next section will illustrate the way to build a classifier which considers the paragraph factor.

The remainder of this paper is organized as follows: Section 2 introduces our new concept and illustrates the way to build a classifier. Section 3 provides experiment results and Section 4 draws a conclusion.

Figure 1. Concept diagram of text classifier  construction   2. Building a MILO classifier  2.1. Basic concepts   This section will introduce a new concept that is presented in this paper. Firstly, a document is written   DOI 10.1109/IIH-MSP.2009.21    DOI 10.1109/IIH-MSP.2009.21     base on the thought of author. And a thought is fully presented in a document which should be precisely structured and be logically. For example, the head part of a document will introduce the document?s topic, the body part will mainly present the details about this topic, and the tail part will draw conclusions. The semantics of a document will generate certain term structures which are formed across paragraphs for some specific meaning. On the other words, the linkage of terms across different paragraphs in a certain order denotes the author?s specific meaning.

The term?s linkage across different paragraphs is called as Meaningful Inner Link Objects (MILO). Fig.2 illustrates the concept of MILO.

Figure 2. Concept of MILO   As shown on the dotted lines of Fig.2, MILO is a  directional linking relationship which is composed by terms across paragraphs. Such as [A1?B2?A3] is a MILO with length 3 which is composed from a term of P1 to a term of P2 and then ending at a term of P3 respectively. Similarly, [A1?A3] is a MILO with length 2 which is composed from a term of P1 to a term of P3. [A1] is a length 1 MILO in P1. We use the collection of extracted MILOs from all documents in each category to build a classifier.

2.2. The process of building MILO classifier   This section will illustrate the way to use MILO for text categorization. Fig.3 shows the classifier building process. First of all, the selected dataset is divided into training set and testing set in a certain proportion. The classifier is trained by training set and evaluated by testing set.

Figure 3. Construction forMILO classifier   2.2.1. Preprocessing of training set. According to patterns of MILO which is illustrated in Fig.2, they are denoted by the term?s directional linkage across the paragraphs in each document, hence each document will be normalized into three paragraphs first. Scott Piao [7] provides this paragraph normalization tool on the website of The National Centre for Text Mining (NaCTeM). After normalizations, three paragraphs are stored into three databases respectively. For noisy reduction, a stop word list [8] is applied to remove general stop words. In order to define each term?s relevance degree in different paragraphs, the chi-square function is used to calculate the weight of each term?s relevance degree from the three databases respectively.

Eq.1 is the chi-square formula defined base on the term occurrences as follow,  )dc)(ba)(db)(ca( )c*bd*a(*N)C,t( i ++++  ? =  2?  ,          (1)  a: occurrences of term ti in category C, b: occurrences of other terms in category C, c: occurrences of term ti in other categories, d: occurrences of other terms in other categories, N:a+b+c+d, total occurrences of all terms in all  categories.

Finally, each normalized paragraph of every  document in each category is denoted by top T chi- square value terms per category in each database. On the other words, all selected terms are t1, t2, , tT which are used to denote a category.

2.2.2. MILO mining. The selected T terms t1, t2, , tT with relative importance in each document?s normalized paragraphs will be used to find the MILOs as shown Fig.2. Since MILOs are concealed in the document, we present three algorithms for MILO mining. For any document d in training set, let?s define the following notations first,  C : the category of document d, P1: the first normalized paragraph of d denoted by  (o11, o12, , o1?), where o1i, i=1,2, , ?, is the number of occurrences of term ti? {t1, t2, ,     tT}. On the other words, P1 contains ? different terms which belong to {t1, t2, , tT},  P2, P3: the second and third normalized paragraph of d denoted by (o21, o22, , o2?) and (o31, o32, , o3?) respectively with the similar meaning as P1.

CM(l): a set of length l MILOs in category C, OCM(l): a set of number of occurrence with respect  to each MILO in CM(l).

Algorithm 1 Extract MILO with length 1 from a  document d in C Input P1, P2 and P3 of d Output CM(1) and OCM(1) Methods:  (1) for each o1i in vector P1, 1 ? i ? ? (2)  OCM(1)?CM(1) ? o1i (3)  CM(1)?CM(1) ? [t1i] (4) for each o2j in vector P2, 1 ? j ? ? (5)  OCM(1)?OCM(1) ? o2j (6)  CM(1)?CM(1) ? [t2j] (7) for each o3k in vector P3, 1 ? k ? ? (8)  OCM(1)?OCM(1) ? o3k (9)  CM(1)?CM(1) ? [t3k]  Because the MILO extracted by Algorithm 1 has length 1, hence occurrence of each extracted MILO is equal to the occurrence of this MILO?s term.

Algorithm 2 Extract MILO with length 2 from a document d in C  Input P1, P2 and P3 of d Output CM(2) and OCM(2) Methods:  (1) for each o1i in vector P1, 1 ? i ? ? (2)   for each o2j in vector P2, 1 ? j ? ? (3)  OCM(2)?OCM(2) ? min{o1i , o2j} (4)  CM(2)?CM(2) ? [t1i?t2j] (5) for each o2j in vector P2, 1 ? j ? ? (6)   for each o3k in vector P3, 1 ? k ? ? (7)  OCM(2)?OCM(2) ? min{o2j , o3k} (8)  CM(2)?CM(2) ? [t2j?t3k] (9) for each o1i in vector P1, 1 ? i ? ? (10) for each o3k in vector P3, 1 ? k ? ? (11)  OCM(2)?OCM(2) ? min{o1i , o3k} (12)  CM(2)?CM(2) ? [t1i?t3k]  Algorithm 2 extracts MILOs with length 2, such that the linkage of MILOs are composed as P1 to P2, P1 to P3, and P2 to P3 respectively as shown in Fig.2.

Algorithm 3 Extract MILO with length 3 from a document d in C  Input P1, P2 and P3 of d Output CM(3) and OCM(3) Methods:  (1) for each o1i in vector P1, 1 ? i ? ? (2)   for each o2j in vector P2, 1 ? j ? ? (3)     for each o3k in vector P3, 1 ? k ? ? (4)  OCM(3)?OCM(3) ? min{o1i , o2j, o3k} (5)  CM(3)?CM(3) ? [t1i?t2j?t3k]  Algorithm 3 extracts all the MILOs across three normalized paragraphs in an ordered sequence from P1 to P2 and to P3 finally.

The previous designed algorithms will extract all MILOs with lengths 1, 2 and 3 from each document of a category C. By three extracted MILOs, a classifier of training set is built. On the other words, for a category C, the collection of MILOs CM(1), CM(2) and CM(3) are now built as a classifier.

2.3. Prediction with MILOs   When a MILO classifier predicts an unlabelled document?s category, unlabelled document should be normalized into 3 paragraphs at first, then all MILOs of this document are extracted by Algoritms 1, 2 and 3.

In order to define the MILO?s weighting in categorization, the following Algorithm Confidence is designed.

Algorithm Confidence Input D: a set of MILOs from an unlabelled  document, UCM: CM(1) ? CM(2) ? CM(3), and OCM: OCM(1) ? OCM (2) ? OCM (3)  Output the confidence value of D with respect to certain category C  Methods: (1) SC ? ? /* SC : a set of C?s MILOs which match  patterns in D */ (2) weight ?0 /* weight of the length of MILO */ (3) ConfvalueC? 0 /* confidence summation */ (4) for each MILO ??UCM (5) for each MILO ? in D (6)     if(? = ?) (7)     SC ?SC ? ? (8) for each MILO ??SC (9)    if(?.length = 1) (10)     weight ? w1 (11)    if(?.length = 2) (12)      weight ? w2 (13)    if(?.length = 3) (14)      weight ? w3 (15)  ConfvalueC ?ConfvalueC + ?. confC * weight (16)  /* confC is defined as Eq.(2) */  According to any given category?s MILO collection, steps (4)-(7) perform pattern?s matching between the unlabelled document and the given category C. If any MILO is matched, that means not only terms matching but also the direction of MILO is matched. The matched MILOs will be stored into SC.

Steps (8)-(15) add confidences of all matched MILOs in SC. The following Eq.2 is the formula to calculate confidence of a MILO,  categories all in  MILO  of occurences total Ccategory   in  MILO of occurences totalconfC ?  ?= .       (2)  For the weights w1, w2, and w3, if a matched MILO has the longer length, the more important it is (i.e., w3>w2>w1). Algorithm Confidence calculates the     confidence value to estimate the similarity between an unlabelled document and a given category C.

The following steps show how all algorithms are operated as a classifier to classify a given unlabelled document.

1. For each category C in training set, Algorithm 1, 2  and 3 are applied on each document of C to extract all MILOs of C.

2. The MILO?s classifier is composed by all MILO?s set of all categories in training set.

3. For any given unlabelled document D, Algorithm 1, 2 and 3 are applied first to find all MILOs of D, and then Algorithm Confidence is performed to calculate the confidence value of D to each category in training set.

4. Suppose the confidence value of D to a category X have the largest value then we let D belong to X.

3. Experiments results   In this paper, The benchmark in this experiment is 20 Newsgroups [9]. The version of this benchmark in this experiment is ?bydate? version which is already divided into 60 percent training set and 40 percent testing set. And according to [1], the evaluation measures, micro F1 and macro F1 are used for performance evaluation. The experiments are conducted on five subsets of 20 Newsgroups as shown on Table 1 respectively. Table 2 displays the best experiment results which selects 700 terms(i.e. T=700) in highest chi-square value from all categories in each database. We compare our results with the research of Pu et al [10] in 2007. The experiment results of our method have better classification accuracy than the best results of Pu et al?s method.

4. Conclusion   Our new approach also provides the following two  benefits: (1) the MILOs are human readable and allows manual maintenance if necessary. (2) The MILOs in the classifier could be incrementally updated. When the new documents are presented for retraining, Algorithm 1, 2 and 3 can extract MILOs from these new documents and then importing these new MILOs into the classifier directly without reconsidering the past documents. Hence, the cost of retraining is lower than SVM, association rules, , etc. With these advantages of the MILO-based approach for text classification, there are still areas for further improvement. We plan to increase the efficiency of algorithms for reducing the training time and testing time, or to find a way which measures the importance of the MILO to build the classifier with higher quality.

