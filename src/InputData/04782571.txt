Massive Pruning for Building an Operational Set of Association Rules:  Metarules for Eliminating Conflicting and Redundant Rules.

Abstract   Extracting a set of Association Rules (AR) is a  common method for representing knowledge embedded in a database. As long as many authors have aimed at improving the individual quality of these rules, not so many have considered their global quality and cohesiveness: Our objective is to provide the user with a set of rules he/she may combine to reason with, a consistent set as regards to ?common sense logic?. As local quality measures offer no warranty in this respect, we have defined patterns of major incoherencies and have associated metarules to them, resulting in a post-treatment cleaning phase for tracking down incoherencies and proposing corrections. We show that on the artificial Lucas0 database of the Causality Challenge [11], starting from 100 000 rules, we have reduced this rule set by three orders of magnitude, to 69 high-quality condensed rules embedding most of the structure designed by the challenge organizers.

1. Introduction  We will consider a database as an incidence table between objects (or transactions) and properties (or variables).  For example, in the domain of supermarket consumer behavior, objects are shopping carts, variables are articles, and an article is (or not) in a shopping cart. An association rule (AR) is an oriented relation between several variables. In this framework the rule ABC DE for example is fully characterized by four numbers: p (resp. q) is the number of objects simultaneously verifying the properties A, B and C (resp. D and E) amongst the N ones of the collection, r is the number of objects verifying simultaneously all the above-mentioned properties. The quality of a rule may be evaluated with a bunch of measures depending on the sole N, p, q and r. The most notorious ones are support (r or r/N) and confidence (r/p) [9].

As the number of ARs may increase exponentially with the number of variables, pioneers [1] proposed to extract only those with a large support and confidence.

In the sequel, other authors proposed a more drastic pruning along two different directions: 1) keeping the only rules which have large values as regard to some other measures of quality [9], 2) using algebraic properties of itemsets (Galois lattices) to extract a minimal generator set of AR ([2, 9, 15]), also called ?non-redundant ARs? by Zaki [21]. The first way strengthens the individual quality of each rule, and the second way ensures a global quality of the rule set.

Another research line, which is ours, consists in shifting to an upper level: Once a baseline rule set has been extracted, the authors have proposed to look for contradictions, necessarily of non-algebraic kind, as shown in [4] and specified below, either thoroughly inspecting each one for individual discussions and interpretations of these ?inconsistent KD nuggets? [7, 18], or globally eliminating all of them, in an OLAP (On-Line Analytical Process) and data-cube view [6, 8, 22]. Let us consider more thoroughly these two domains.

In the first domain we must mention the problem of exceptions retrieval in databases, where the objective is to bring them to light, for studying, not for cleaning: the reader may refer to the exceptions rules of Suzuki 1998, or to the occurrences of the Simpson?s paradox [7], as well as to the methods for building the sole rules corresponding to a pattern of interest, using a SQL-like request, as Botta [3] does.

The second OLAP domain harbours a bunch of contributions, particularly the works supervised by Jiawei Han at Simon Fraser University (Canada).

Amongst other works, Yongjian Fu [8] is mining the ? multi-level AR?s ?, Hua Zhu [23] is mining the ? intra-dimensional AR?s ?, Qing Chen [6] looks for ? sub-rules ? within ? data cubes ?; these data structures are typical members of the OLAP family, where one can cross variables such as Locations, Products, Quantity and Costs. The complexity of such   DOI 10.1109/eKNOW.2009.12     data mining does not result from the number of variables (e.g. 4), termed dimensions in this context, but from the structure of their values, in case of categorical variables. For example, the Location variable fractionates into a hierarchy with several levels (continents, states, regions, towns), with many values at each level. The inconsistencies between rules that the authors detect and try to correct happen when their components are located in several levels of the hierarchy, or are issued from operations of aggregation within a given level. Depending on the case, they enforce constraints for the extraction of itemsets or AR?s, or cleanse the rule set using meta-rules.

Compared to ours, their meta-rules do not take into account, as far as we know, the problem of negation, more or less specific to binary variables, nor the method for organizing the operation of meta-rules when numerous variables may contradict in many ways.

On the more specific side of statistics and machine  learning, some works are focussing on the special case of classification rules, i.e. AR?s with the same dependent variable in their right sides. Let us point out Toivonen [19] who describes a pruning algorithm (RuleCover) for X Y rules where Y is a target variable. The principle is to reduce iteratively a rule set, in such a way as every individual confirms at least one rule. This kind of a ?subject-oriented? pruning does not consider the components of the rules, nor their support or confidence. Their rule set may be inconsistent, considered with our variable-oriented criteria. Conversely a side effect of our pruning process may be that some subjects confirm no rule.

Moreover, the Jakulin?s approach [14] is to be mentioned also. This author keeps clear of tackling directly the consistency of rules by considering their components; in a pre-processing stage, he avoids inconsistencies rising from the interaction of the descriptive variables with the target variable, by building new variables taking the place of the disputable original ones. This way of seizing the problem is common with the statistical experiment [12]. Unfortunately this method cannot be generalized outside the classification problem.

We propose here a unified approach tackling in the  same mathematical framework both issues of redundancy and contradiction. Our objective is to provide the domain expert with a set of rules he/she may combine in order to reason with. In order to stick to ? common sense logic ?, we define a concept of similarity between rules, instead of the highly restrictive equivalence concept of Zaki [21].

2. ?Common sense logic?.

?Common sense logic? refers to the structuring principles of a form of deduction-based reasoning which make the logic indisputable for anybody gifted with common sense. We propose three principles for individual rules, and a fourth for a rule set.

2.1. Three first principles   Here are some principles which every rule used in  this type of reasoning must verify: - P1 : a valid rule is broken in very few instances :  for example, the rule ?crows are black? is accepted by all, and cannot be doubted because of the spotting of one white crow.  It would require a certain number of crows of a different colour from black to make the rule questionable.

- P2 : a rule must be applicable in a sufficient number of cases : for example, the rule ?three- legged crows are white? would be refused by this type of logic since three-legged crows do not exist or too rarely so.

- P3 : a rule is valid in a given place at a given time : for example, it is possible to imagine that in other worlds, in other times, there could be differently coloured crows.

These first three principles show that this common  sense logic differs from formal logic in its fuzziness (note expressions such as ?a sufficient number ?, ?rarely?, ?a given place ?) and its utility (P2). These three principles which establish the validity of one rule without reference to the other ones are verified by the ARs usually extracted from a database (P1: a confidence threshold is decided, P2: a support threshold too, and P3: the set of rules represents only the database on which it is built).

2.2. The fourth principle   Now it is worth looking into a more complex  example which infringes this common sense logic: ?Simpson?s paradox?.  It was related for the first time in 1903 [20], but it has cropped up regularly since then and each time caused as much surprise [16].

In an exam taken by pupils in various schools girls obtained better results than boys in each of the schools. At the same time the published overall results showed that girls had obtained less good results.  Although there is a perfectly clear mathematical explanation for this phenomenon, which defies common sense (Simpson 1951 [17]), indignation was expressed in the media by supporters of the feminist cause.

To formalize the situation with binary variables, let A be ?being a girl?, B ?being a pupil at school E? and C ?passing?; the 3 rules of this paradox in common sense logic are noted  AB C and AnonB C and A nonC.

The paradox comes from the fact that the user gifted  with common sense naturally summarizes the first two rules, AB C and A nonB C, into the more general rule A C, which conflicts with the third rule A nonC.

Our fourth principle prevents both contradictions (of Simpson?s type, amongst others), as well as redundancies: - P4 : validation of a rule takes place after being  confronted to rules with the same variables, except one, in the same positions, and/or one variable shifted into its negation. In each case a decision of deleting the rule or not follows.

As for the above example of the paradox with three  rules, the pairs of rules which need to be controlled in accordance with P4 are: p1={AB C and A nonB C}, p2={AB C and A nonC} and p3={A nonB C and A nonC}  This principle detects and suppresses not only possible redundancies and contradictions but also interaction effects familiar to statisticians [5]. A detailed description of activating the P4 principle is provided below.

3. The general method for automatic cleansing using P4   P4 covers several elementary problems.  A meta-  rule mri is attributed to each of these problems. Its role is to determine which couples contain two incoherent rules and then to decide whether to reject both, one or none.  When both rules are considered incoherent and of an uneven quality, the poorer one is dropped.

Bearing in mind, however, that the quality measures of AR?s are numerous (currently more than 50 [10]) the user is advised to choose a suitable sub-set of measures.  The choice of several measures means that the better of two rules as determined by one measure may be the worse by another.  When this case occurs the rules are called non-comparable.  Moreover if the quality differences between the two rules are not larger than thresholds defined by the user, they are said to be similar. Users must indicate the required level of cleansing. For a demanding expert, similar and non- comparable rules are both cleansed by the meta-rule rules (this case is termed demanding in the meta-rule definition), whereas for a less demanding expert, the meta-rule will only cleanse the similar rules.

To sum up, the decision taken by a meta-rule mri for a given couple of rules {r1,r2} depends on three elements : - the coherency level of the couple of two rules  determined by two values (coherent/incoherent). Its calculation modality is part of the definition of a meta-rule mri.

- the requirement level of the user determined by himself with two  values, demanding/ undemanding.

- the quality order of the rules of the couple {r1,r2} determined by four values (non-comparable, similar, r1 prevalent over r2, r2 prevalent over r1).

This order is calculated using an AR comparison function parametered by the user and common to all the meta-rules.

Before defining the meta-rules presented later here  follows an analysis of how two rules are compared.

4. Comparing two rules   Two rules are compared using their quality measures.  Let I be the sub-set of measures chosen by the expert, supposed to be increasing functions of the quality of the rules, e(I) their difference thresholds (for each measure the largest difference deemed negligible by the expert). The difference dI between two rules r1 and r2 for a measure I element of I, is defined by:       Then the result of the comparison of two rules is: 1. r1 and r2 are similar if ? I ? I, dI(r1,r2)=0.

2. r1 is prevalent over r2 if ? I ? I, dI(r1,r2)?0, and  ? I ? I, dI(r1,r2)=1.

3.  r2 is prevalent over r1 if ? I ? I, dI(r1,r2)?0, and  ? I ? I, dI(r1,r2)=-1.

4. r1 and r2 are non-comparable if ? I,J ? I,  dI(r1,r2)=-1, dJ(r1,r2)=1.

With these definitions, each couple of rules corresponds to one and only one of these four cases.

An example of rules compared using this technique is in table 1. As this example shows, this choice of a fuzzy and multidimensional comparison cannot equip the set of AR?s with a satisfactory algebraic structure (absence of transitivity).  Consequently an algorithmic operating mode has been chosen for the meta-rules.

Here are now the definitions of four first meta-rules determined by the principle P4.

Consider a set I of three quality measures which could be support, confidence, difference1, with respective thresholds of 10, 0.1, 0.2, and the following five rules with the corresponding values of measures : r1(10; 0.77; -0.08), r2(17; 0.70; 0.10), r3(25; 0.59; 0.26), r4(29; 0.65; 0), r5(15; 0.72 ; -0.15), the following results2 are obtained in table 1:   Table 1. A comparison of the 5 rules 2 by 2    The table shows the non-transitivity 3 of the three relations : - r2 and r1 are similar, r1 and r5 too, but not r2 and r5.

- r2 is prevalent over r3, r3 over r4, but r2 is not prevalent  over r4 (r4 is prevalent over r2).

- r3 and r1 are non-comparable, and r1 and r4, but r3 is  prevalent over r4..

5. Definitions of four meta-rules   Meta-rule mr1 : order 1 contradiction Imagine a set of rules in which there are the  following two rules: ?Town girls pass and register? and ?Town girls fail and register?.  Anybody gifted with the least common sense is aware of a contradiction because the value of one attribute on the right side changes while the others do not.  The aim of the meta- rule mr1 is to eradicate this type of contradiction.

These two rules are expressed in the following formalised manner, linking 4 binary attributes: r1: G1L1 R1I1  ?Town girls fail and register.? r2: G1L1 R0I1 ?Town girls pass and register.? The four attributes are the following: - gender : G1=?girl?, G0=?boy? - location : L1=?town?, L0=?country? - pass : R1=?pass?, R0=?fail? - register : I1= ?register?, I0= ?do not register?  Definition of mr1 : Let {r1,r2} be a couple of rules, it is said to be incoherent for the meta-rule mr1 if r1 and r2 differ only by the value of an attribute on the right side. It is coherent for mr1 in the contrary case.

The corresponding set of rules to eradicate mr1({r1, r2}) is defined by :  1 The ?difference? of the rule A B is the difference between its confidence and the percentage of subjects which verify B among the set of N subjects.

2 Detailed calculation for the couple {r1,r4}: the differences of values with the three quality measures are (-19, 0.12, -0.08).  The deviation thresholds for these three measures being (10, 0.1, 0.2), as (-19<-10, 0.12>0.1, -0.2<=-0.08<=0.2), the result is (-1,1,0).  Since the first two differences are of opposite signs, r1 and r4 are non- comparable.

3 A relation R on a set E is transitive when, for all elements x,y,z of E, if xRy and yRz then xRz.

- ?    if {r1, r2} coherent.

- ?    if ({r1, r2} incoherent), and (r1 and r2 non-  comparable), and undemanding user.

- {r1, r2} if ({r1, r2} incoherent), and (r1 and r2  similar).

- {r1, r2} if ({r1, r2} incoherent), and (r1 and r2 non  comparable), and demanding user.

- {r1} if ({r1, r2} incoherent), and (r2 is prevalent  over r1).

- {r2} if ({r1, r2} incoherent), and (r1 is prevalent  over r2).

For example4, if rules r1: G1L1 R1I1, r2:  G1L1 R0I1, r3: G1L1 R1I0, r4: G1L1 R0I0 are considered, the resulting incoherent sets for meta-rule mr1 are {r1,r3}, {r1,r3}, {r2,r4}, {r3,r4}. If the expert is undemanding and if the result of their 2 by 2 comparisons is that which appears in table 1, r1 and r2 will be eradicated because similar, so mrl({r1,r2})={r1,r2}, r1 and r3 are non-comparable, so mrl({r1,r3})=?, r4 is prevalent over r2, so mrl({r1,r4})={r2}, and r3 is prevalent over r4, so mrl({r3,r4})={r4}. Grouping these sets makes it clear that rules r1, r2 and r4 are eradicated.

Meta-rule mr2 : order 1 redundancy  Consider the example of the following two rules: r1: G1L1 R1I1 ?Town girls pass and register.? r2: G1L0 R1I1 ?Country girls pass and register.?  Rule r1 brings information to which r2 adds nothing.  On the contrary it causes the loss of part if the value of the initial information concerning the setting.

The meta-rule mr2 aims to eradicate this type of redundancy. To define it the former meta-rule mr1 is re-used but ?left side? is substituted for ?right side?.

Meta-rule mr3 : order 1 local/global contradiction  Consider the example of the following two rules: r1: G1L1 R1I1, ?Town girls pass and register.? r2: G1 R0I1, ?Girls fail and register.?  Rule r1 brings information in a particular situation (the setting being town), and rule r2, being more general, contradicts it.

Sources of inspiration for knowing how to amend the situation can be the way statisticians deal with the problem of interpreting interactions between variables [13]. If the statisticians? generalised linear model is adopted and the rules are re-expressed in terms of interactions which are simplified for clarity, the following is obtained [5]:   4  Detailed calculation: rule r1 and rule r2 only differ by R1 which becomes R0 on the right side, so they are incoherent for mr1. In the table, the couple {r1, r2} corresponds to the three 0 quality differences so they are similar.  r1 and r2 must be eradicated.  Rule r1 and rule r4, however, differ by two variables on the right side.

They are not incoherent by mr1.

- r1 : AB C corresponds to the effect of the interaction of A and of B (level 2 interaction) on C.

- r2 : A nonC corresponds to the principal effect of A (level 1 interaction) on C.

Since C appears in r1 and nonC in r2, the two  effects contradict each other, provided nonetheless that both appear significant5, since only significant effects are taken into account by statisticians.  In this case, many statisticians would neglect the simple effect corresponding to rule r2 and only interpret the interaction (rule r1). If only one of the two is significant, it alone is interpreted and the other one discarded.

Let?s transpose as follows: when one of the two rules is prevalent over the other, it corresponds to a significant relation; and the other rule is eliminated.

When the rules r1 and r2 are similar, they are both deemed significant and the rule r2, with smaller number of attributes and therefore being the most ?general?, is dropped.

Definition of mr3 : Let {r1,r2} be a couple of  rules, it is said to be incoherent for the meta-rule mr3 if the set of attributes of the left side of either is strictly included in the set of attributes of the left side of the other, with the same values, and if the right sides have the same attributes, with the same values for all except one. If r2 has less attributes than r1 on the left side and so is more general mr3 ({r1, r2}) is defined by : - ?    if {r1, r2} coherent.

- ?  if ({r1, r2} incoherent), and (r1 and r2 non-  comparable) and undemanding user.

- {r2} if ({r1, r2} incoherent), and (r1 and r2  similar).

- {r2} if ({r1, r2} incoherent), and (r1 and r2 non  comparable), and demanding user.

- {r1} if ({r1, r2} incoherent), and (r2 is prevalent  over r1).

- {r2} if ({r1, r2} incoherent), and (r1 is prevalent  over r2).

Meta-rule mr4 : order 1 local/global redundancy Consider the example of the following two rules:  r1: G1L1 R1I1 ?Town girls pass and register.? r2: G1 R1I1 ?Girls pass and register.?  The meta-rule mr4 looks like mr3 but it aims to eradicate redundancy instead of contradiction. To define it, the former meta-rule is re-used but if the incoherent rules are similar or non-comparable, the   5 For a statistical definition of the word significant, the reader my refer to an abundant specialised literature.  It is enough for now to say that a significant event is one that cannot reasonably be attributed to chance.  When such an event occurs the statistical model indicates the variables to which it can be attributed.

most local rule (r1 for this example) is suppressed, and not to the most global one as in mr3.

6. Simultaneous action of meta-rules   The decision to eradicate a rule that is determined incoherent by a meta-rule is not immediately put into effect, rather the rule is annotated: a tag marks the imperativeness with which the rule may be suppressed.

After all the meta-rules have been applied, the rules without any tag are kept, the other rules, having one tag, or multiple ones, are considered for suppression.

This way of operating implies that the result obtained remains independent of the order in which the rules of the set are treated.  A rule, even if it has been tagged for suppressing at one stage, can still contribute to the dropping of another at the next stage. Looking at example 1 and imagining that for each of the sets {r2,r3}, {r3,r4}, {r2,r4}, a meta-rule for which they are incoherent can be found, as r2 is prevalent over r3, r3 is prevalent over r4, and r4 is prevalent over r2, the three rules will be dropped in the end, as common sense suggests.

7. Application  7.1. The dataset   The LUCAS0 dataset has been created in the framework of the Causality Challenge (www.causality.inf.ethz.ch) as a ?toy? workbench for adjusting and testing causality extraction methods, before dealing with real life datasets.

Figure 1 : Graph for LUCAS0 (see  www.causality.inf.ethz.ch for details).

We have not used the whole causality workbench, but the only LUCA0 training set, which consists of 2000 examples described by 12 binary variables such as anxiety, smoking, fatigue? The graphic model sketched in fig. 1 is a Bayesian network used for generating these examples. It has been designed so as     to mimic a few causes and consequences of lung cancer. One control variable (?Born in even day?) is related to none of the others.

7.2. Our objective.

We are not trying to tackle the complex problem of  causal relationships ? several ?manipulated? test sets are provided for this purpose in the Causality Challenge ? but the only problem of detecting the minimal set of relevant and non-redundant relations in the training set. Which translates, in this context, as representing the diverse links between variables as an association rule set cleansed and condensed by our meta-rules.

7.3. Our project scheme.

Our work has been structured as follows: the names  of the 12 variables have been condensed with the two first letters of their labels (see table 3) and an extra digit, 1 or 0, corresponding to the variable itself or its negation. We have extracted 100 000 association rules or so with support >= 100 and confidence >=0.1, and with a unique variable in their right size.

Table 3: Variable labels and their  abbreviations.

Al An At Bo Ca Co  Allergy Anxiety Attention Disorder Born an Even Day Car Accident Coughing  Fa Ge Lu Pe Sm Ye  Fatigue Genetics Lung Cancer Peer Pressure Smoking Yellow Fingers   We have then applied each of our meta-rules  independently on the resulting rule set. We chose the support and confidence as quality measures, and the thresholds for significant difference: 100 for support, 0.1 for confidence.

When a couple of rules is deemed consistent by a meta-rule, no digit is added to their tags. If it is not, the digits 0, 1, or ?nc? are appended to one or both in accordance to their degree of requirement for elimination: - the digit 1 if one of the two has much less credit  (appended to this rule), - the digit 0 if both are equally reliable as regard to  the meta-rule (appended to both rules in the case of mr1 and mr2, to the most general rule in the case of mr3, to the most local in the case of mr4).

- the digit ?nc? if they cannot be compared, appended in the same manner than the digit 0.

When all the meta-rules have been used on the  whole rule set, tags of each tagged rule are examined,  with a precedence order: the digit 1 is chosen if one digit 1 at least is present; if not, the digit 0 is chosen if one digit 0 at least is present; ?nc? is chosen in the remaining case if one digit ?nc? at least is present.

These tags are useful in the interpretative stage of our workflow: all the non-tagged rules remain for interpretation, but ?nc? rules may be useful if the set of non-tagged rules is judged defective. If it is still not enough, the rules tagged ?0? may be examined. For the occasion, the parameters of the meta-rules may be revised (relative weights of the different measures, difference thresholds).

7.4. Results   We have shown in table 4 below a few examples of  rule couples assessed as inconsistent according to one meta-rule or another, and their resulting tags.

The first column identifies the involved meta-rule; the next two columns list the couple of rules to be compared (between brackets: support and confidence).

In the fourth column are displayed the differences of quality measures, to be compared with the thresholds.

The fifth column sketches the results of the fourth: -1 if the first is by far lower than the second, 1 if the opposite is true, 0 else. The conclusion is drawn in column 6, translated in column 7 into the resulting digit, with the code of the rule to which it is attached.

The first row of the table 4 reads as follows: according to the mr1 meta-rule the couple of rules Yellow_Fingers no Lung Cancer and Yellow_Fingers Lung Cancer has been estimated inconsistent. The support of the first rule is 296 and its confidence is 0.189, far beyond the corresponding values of the second rule (1268 and 0.811). The differences overpass the values considered significant, as stated in the fourth column and sketched in the fifth one. The differences for this couple are marked (-1,-1) (see ?4), which means that the first rule is poorer than the second, due both to support and confidence. In column 6, we conclude that r2 prevails on r1, and in column 7 the ?1? tag is attributed to r1 (?first to be dropped? type).

The table 5 shows the results obtained when  tagging the 100,000 rules: each row of the table gives the cumulative count of the rules tagged for a possible elimination, dispatched by number of variables in the left part. Almost all of the rules (96,981) have been tagged ?1? because of contradiction (mr1 or mr3) or redundancy (mr2 or mr4), when compared to better rules. 268 rules have been tagged ?0? because of contradiction or redundancy as regard to rules of similar quality. The latter will be eliminated if one specially cares for consistency of the rule set.

At last, 69 high-quality rules remain, all of them with a single variable in each side; 61 non-comparable rules,  while supporting interesting information, may also be considered.

Table 4 : Five cleansing examples  Col1 Col2 : rule r1 (supp., conf.)  Col3: rule r2 (supp., conf.) Col4: index(r1)-index(r2)  Col5: Diff(r1,r2) Col6: comparison result Col7: tag  mr1 Ye_1  Lu_0 (296, 0.189) Ye_1   Lu_1 (1268, 0.811)  296-1268<=-100 0.189-0.811<=-0.1 (-1, -1) r2 prevails on r1 r1 : 1  mr2 Al_0  Lu_0 (202, 0.294) Al_1  Lu_0 (355, 0.363)  202-355<=-100 -0.1<0.294-0.363<0.1 (-1, 0) r2 prevails on r1 r1 : 1  mr2 Bo_0  Lu_1 (742, 0.727) Bo_1  Lu_1 (701, 0.716)  -100<742-701<100 -0.1<0.727-0.716<0.1 (0, 0) r1 and r2 similar  r1 : 0 r2 : 0  mr2 Ge_0  Lu_1 (1171, 0.680) Ge_1  Lu_1 (272, 0.975)  1171-272>=100 0.680-0.975<=-0.1 (1, -1)  r1 and r2 not comparable  r1 : nc r2 : nc  mr3 Lu_0  Ye_0 (261, 0.469) Lu_0 ; An_1  Ye_1  (188, 0.699) -100<261-188<100 0.469-0.699<=-0.1 (0,-1) r2 prevails on r1 r1 : 1     Table 5 : Cumulative effects of tagging the set of 100,000 rules with four meta-rules.

Variable number in the left side of the rule  Initial rule number 1 2 3 4 5 6 7 8 9 10 total 506 3872 12786 22385 24486 18970 10208 3483 650 33 97379    Meta-rule, tag  Number of rules tagged for a possible suppression  mr1 1 212 1203 2641 3334 2773 1544 536 100 7 0 12350 0 60 526 1502 2040 1862 1318 670 228 40 0 8246 nc 0 0 0 0 0 0 0 0 0 0 0  mr2 1 314 2556 7800 12490 11273 6047 2034 360 23 0 42897 0 52 609 2896 6192 9790 10946 7277 2762 486 0 41010 nc 32 205 539 785 633 339 112 18 1 0 2664  mr3 1 321 2906 9173 14506 13150 7476 2788 614 70 2 51006 0 46 338 1878 4775 8206 9553 6534 2520 446 0 34296 nc 61 223 528 760 615 335 112 18 1 0 2653  mr4 1 360 3809 12704 22305 24459 18970 10208 3483 650 33 96981 0 32 47 82 80 27 0 0 0 0 0 268 nc 45 16 0 0 0 0 0 0 0 0 61    Final Result Number of rules tagged for a possible suppression  69 0 0 0 0 0 0 0 0 0 69   Final evaluation: comparing the 69 final rules with the original model  Strong similarities follow from the comparison: - The control variable Be: ? Born_an_Even_Day ?  appears nowhere in the rule set.

- Bi-directional rules link the main variable ? Lung  Cancer ? to Coughing, Smoking, Fatigue, Yellow_Fingers with a support greater than 1200 and a confidence of more than 0.80; similarly it is linked to Anxiety with a support of about 1000 and a confidence around 0.70. It is also strongly linked to Car_Accident (s=1111, c=0.77).

- Bi-directional relations connect Car_Accident, Coughing, Fatigue, with a support superior to 1000 and a confidence beyond 0.75.

- The same type of relations link Smoking, Anxiety and Yellow_Fingers with a support superior to 1000 and a confidence beyond 0.70; each one involves Car_Accident with the same quality.

Additional knowledge follows from the association rules: - Lung Cancer implies no Allergy, no  Attention_Disorder, no Peer_Pressure, with a     support not far from 1000 and a confidence of about 0.65.

- Smoking or Car_Accident implies no Allergy, with the same quality as stated above.

- No Genetics is implied by Smoking, Anxiety, Fatigue et Yellow_Fingers, with a support exceeding 1000 and a confidence around 0.85.

Missing elements: - The link between Genetics et Lung Cancer is just  pointed out by Ge_1  Lu_1  (272, 0.975) tagged as ?nc?, as shown in table 4 as an example for the metarule mr2.

- The same is true between Allergy and Coughing, the rule Al_1  Co_1  (614, 0.895) being tagged ?nc?.

- The links between Genetics and the other variables are just pointed out negatively, as stated in the above subsection.

- The only interaction embedded in the model, the one of Smoking + Genetics on Lung Cancer does not come to light, because the corresponding rule Ge_1 ; Sm_1  Lu_1, (s=213, c=1) has been tagged ?nc?, being appreciated non-comparable to Ge_1  Lu_1  (272, 0.975) by the mr4 metarule.

8. Comments and discussion   Starting from 100,000 association rules, 69 ones  remain after our cleansing process. The differences between the generating model and this operational and condensed rule set concern essentially 1) the direction of the rules, which seldom contradicts causality, 2) the loss of a few links with small support despite of strong confidence, such as Genetics  Lung Cancer, 3) the loss of interaction rules, with two variables at the left side and one at the right one (Smoking + Genetics vs.

Lung Cancer).

The first point is a general problem for knowledge extraction from observational data. Mere observation seems not enough for establishing causal links, and experimental manipulation of some variables seems unavoidable, as in statistical ?design of experiment?, or as in the variants embedded in the test sets of the Causality Challenge [11].

As the second point is concerned, the meta-rule could be changed in order to favour confidence instead of support. In this example, the choice of support could eliminate rare illnesses or effects, which is questionable in a medical context?  Concerning the third point, the non-identification of an interaction follows from the choice of measures ? chosen in this first application because of their simple computation and clear interpretation ? and from the  choice of the difference thresholds: these options prevent small interaction effects, such as the one built in the data, from being detected.

As a whole, our method retrieves the essential part of the structure of the data, but an effort remains to propose to the user different choices of parameters.

9. Assessment and Perspectives   Our global mining method is incompatible with an  incremental cleaning procedure, as follows from section 6 above; the essence of our method is to consider a rule set as a whole first, before the cleansing phase. It is to be noticed that this feature enables us to cleanse sets of rules obtained by the fusion of rule sets extracted from different sets of subjects for the same properties, or suggested by an expert.

The meta-rules presented here are easy to use by  anybody since they rest on common sense.  They apply to the complete set of rules extracted from data and the only requirement for these rules is their having a non- zero support.  They reduce the size of the set by eliminating contradictions and redundancies.  We also pointed out that the user had to choose parameters such as extra quality measures or different thresholds, according to the type of knowledge he/she wants to mine in the data. He/she should be helped in this matter by being suggested various parameter sets adapted to diverse situations. One perspective of research in this area is the progressive building of a coherent cleansing system, developing its generality and enhancing the performance and scalability of the corresponding software.

One criticism against this technique may be the use of properties with negations, which greatly weigh down the research algorithms with the frequent itemsets and association rules if the properties with negations are not planned for from the start. There are, however, numerous cases where coding leads to both the property and its negation being present in the base (for example coding the variable ?Gender? into two variables ?female?, ?male?). In some other cases coding produces a series of properties which form a partition of the subjects determined by the various values of one and the same property (for example, coding the quantitative variable ?Blood Pressure? into ?Normal BP?, ?High BP?, and ?Low BP?). This coding into more than two categories also produces incoherencies in the sets of rules, as it is an extension of the case of a property and its negation which has been dealt with in this chapter.  The meta-rules must then be re-written in a more generalising manner.  This is another direction research can take.

10. Acknowledgments   The authors are deeply indebted to Isabelle Guyon and her team for designing the problematics of the Causality Challenge, and for providing the Lucas0 test data.

11. References  [1] Agrawal, R. Srikant, H,  Fast algorithms for mining association rules in large databases, Research Report RJ 9839, IBM Almaden Research Center, San Jose, California, June 1994.

[2] Bastide Y., Data mining : algorithmes par niveau, techniques d'implantation et applications, Doctoral dissertation, Universit? Blaise Pascal, Clermont-Ferrand, 2000.

[3] Botta M., Boulicaut J.-F., Masson C., Meo R. : A Comparison between Query Languages for the Extraction of Association Rules. DaWaK 2002, 1-10  [4] Cadot, M. (2006). Extraire et valider les relations complexes en sciences humaines : statistiques, motifs et r?gles d'association. Doctoral dissertation, University of Franche-Comt?, France. Available online at <http://www.loria.fr/~cadot/cadot_these_2006.pdf>  [5] Cadot, M., Maj, J.-B., & Ziad? T. (2005). Association Rules and Statistics, in J. Wang (Ed.), Encyclopedia of Data Warehousing and Mining (pp. 74-77). Hershey, US, Idea Group Publishing.

[6] Chen Q. Mining Exceptions and Quantitative Association Rules in Olap Data Cube, Master Thesis, Simon Fraser University, 1999  [7] Fabris C.C., A.A. Freitas. Discovery of surprising patterns by detecting occurrences of Simpson's paradox.

Research and developpement in intelligent systems XVI (Proc ES99. The 19th SGES Int. Conf. on Knowledge-based systems and applied artificial intelligence). 148-160.

Springer-Verlag, 1999.

[8] Fu Y., Discovery of multiple-Level Rules from large Databases, Master Thesis, Simon Fraser University, 1996  [9] Guigues J.L. et Duquenne V. (1986) , Familles minimales d'implications informatives r?sultant d'un tableau de donn?es binaires, Math. Sci. Hum. n?95, 5-18  [10] Guillet F., Hamilton H., (2007) Quality Measures in data mining, Springer.

[11] Guyon I., Causality Challenge #1: Causation and Prediction, 2008, http://www.causality.inf.ethz.ch/challenge.php   [12] Hoc J.-M., L'analyse planifi?e des donn?es en psychologie. PUF, Paris, 1983.

[13] Howel D.C., Statistical Methods for Psychology, Duxbury, A Division of International Thomson Publishing Inc., 1997  [14] Jakulin A., Attribute Interactions in Machine Learning Master's thesis University of Ljubljana, Slovenija, 2003  [15] Luxenburger M., Implications partielles dans un contexte, Math?matiques, Informatique et Sciences humaines, n?113,  35-55, 1991  [16] Pearl J., Causality models, reasoning, and inference, Cambridge University Press, 2000, 267 - 279  [17] Simpson, E. H., The interpretation of interaction in contingency tables, Journal of the Royal Statistical Society, Series B, 13, 238-241, 1951.

[18] Suzuki E., Kodratoff Y., Discovery of Surprising Exception Rules Based on Intensity of Implication. Second European Symposium on Principles of Data Mining and Knowledge Discovery. Springer-Verlag, London, UK. Source Lecture Notes In Computer Science. 1998, p. 10-18.

[19] Toivonen H., Klementtinen M., Ronkainen P., Hat?nen K., Mannila H., Pruning and Grouping Discovered Association Rules, ECML'95  [20] Winer B.J., Brown D.R., Michels K.M., Statistical principles in experimental design (third edition), New York: McGraw?Hill, 1991  [21] Zaki, M., Mining Non-Redundant Association Rules, Data Mining and Knowledge Discovery, 9, 223-248, 2004, Kluwer Academic Publishers, Netherlands.

