A Case Study on Entity Resolution for Distant Processing of Big  Humanities Data

Abstract?At the forefront of big data in the Humanities, collections management can directly impact collections access and reuse. However, curators using traditional data management methods for tasks such as identifying redundant from relevant and related records, a small increase in data volume can significantly increase their workload.   In this paper, we present preliminary work aimed at assisting curators in making important data management decisions for organizing and improving the overall quality of large unstructured Humanities data collections. Using Entity Resolution as a conceptual framework, we created a similarity model that compares directories and files based on their implicit metadata, and clusters pairs of closely related directories. Useful relationships between data are identified and presented through a graphical user interface that allows qualitative evaluation of the clusters and provides a guide to decide on data management actions. To evaluate the model?s performance, we experimented with a test collection and asked the curator to classify the clusters according to four model cluster configurations that consider the presence of related and duplicate information. Evaluation results suggest that the model is useful for making data management action decisions.

Keywords: Collections Management; Entity Resolution; Distant Processing; Digital Humanities

I.  INTRODUCTION One of the challenges introduced by the big data  phenomenon in the context of Digital Humanities is that traditional data management methods are not suitable for increasing amounts of digital data. The concepts of ?close reading and distant reading,? in much use in current Digital Humanities discussions, relate to this problem. They refer respectively to the methods and cognitive demands involved in the close study of a work or passage of text, (such as that done by traditional literary critics like Harold Bloom, who finds distant reading ?absurd.? See [1]) versus the possibility of studying aggregates of digital texts using computational analysis methods (Moretti; Matthew Jockers prefers to call this ?macroanalysis.? See [2]). A similar problem applies to Humanities data management in reference to collection processing activities such as organizing, describing, accessing and preserving digital collections. Loosely borrowing on these concepts, we can say that there are close and distant data processing methods; the former understood as linear review methods traditionally used by Humanities scholars and archivists, and the latter as computational methods that narrow large amounts of information to render a meaningful representation. We  contend that both; distant and close processing are needed for making sense of big Humanities data.  The problem relates to the mismatch between close processing and the amount of data available for curation. Just a small increase in the amount of data makes time consuming for scholars and curators to read a text, or to look at image, or video files, one after another.  On the other hand, close processing may be needed for fine grain analysis of the contents.

Ideally, close processing can be preceded by distant processing methods to present the curator with a synthesis involving valid patterns found in the data for making research data management decisions.

Research data management has the goal of making data and information accessible throughout the duration of the project and to its disposition or long term archiving [3].

Managing data translates into making decisions about what actions to pursue such as what to keep, discard, merge, and organize. In projects with large teams or especially long lifecycles, increasingly typical in Humanities, researchers commonly share, copy and paste, reuse, select, duplicate, and transform data in response to research questions or as solutions to specific problems, but seldom document their actions along the way. As data are analyzed, new datasets are collected or produced as by-products, and these are added into the research loop, creating new relationships that may not be obvious or explicitly documented. Data provenance may get blurry or lost entirely in the midst of redundancy and disorder. Furthermore, for data to transition to archival collections, data provenance, inter-relationships, and transformations need to be documented or inferred by the archivists.

The goal of this project is to assist users in overcoming these challenges with large unstructured Humanities datasets. Using Entity Resolution (ER) as a conceptual framework [4], we developed a three-stage method that combines both distant and close processing. After the first stage which involves metadata extraction and pre- processing, the second computational stage as distant processing narrows and groups the metadata into clusters that the user can evaluate within a graphical interface. At the post-processing stage, the user reviews the results and decides what data management actions to pursue as close processing (e.g. discard, merge, reorganize, re-evaluate or leave data as is).

ER encompasses a variety of methods developed within different disciplines to resolve cases of data ambiguity and      repetition, to link relevant information points, and to improve the overall quality of the data [4]. ER is a well- known problem in Artificial Intelligence (AI). It specializes in finding duplicate records and locating records that might refer to the same entity such as a person, address, or any other information point. The problem that we address in this project is similar to the applications of ER in AI, but it presents considerable differences as well. In AI, ER aims to identify records of the same thing but with different partial information, the entity is clear in that case. In this project, by contrast, we do not have a fixed concept of entity, but we aim to find connections between sets of data that may or may not contain duplicate information.

For this study we used an archaeological collection generated over decades of research and publication activities by the Institute of Classical Archaeology (ICA) at the University of Texas at Austin. This collection, amassed by several generations of research staff and collaborators from a variety of disciplines, is fairly typical of a large archaeological research project. While the case study is focused on the ICA collection?s particular formation history research and processing requirements, the approach is widely applicable to any large and unstructured data collection that contains redundant data alongside data that need to be assessed and re-organized for study, archiving, and dissemination.

We created a similarity model that allows directories and files to be compared based on their implicit metadata and that clusters closely related pairs of directories. We call implicit metadata the terms and filenames that users create to label their data as they go about their work, while explicit metadata refers to that which is created following descriptive metadata standards. Knowledge from the curator about the collection is incorporated in the first and third stages of the method. Useful relationships between data are found based on a scoring system assigned to numerical series and tags found in the implicit metadata. Results, presented through a graphical user interface, allow qualitative evaluation of the clusters and consequently making decisions about data management actions. To evaluate the model?s performance we classify the clusters according to four model cluster configurations that consider the presence of related and duplicate information. Finally, an action score is applied to each cluster as an indicator of the curator?s confidence to make decisions about data management actions.



II. BACKGOUND AND RELATED WORK  A. Formation and Processing of Unorganized Data Aggregations  Problems making sense and processing collaborative, and un-ruled aggregations of data in networked or virtual organizations emerged early on in the history of digital information management and archiving. In Thirty years of Electronic Records, Linda Henry describes the early efforts  at the National Archives and Records Administration (NARA) to appraise unorganized materials accumulated in multiple personal computers. Her narrative points to the lack of appropriateness of traditional methods used by archivists to approach this task  [5].  For cases of chaotic groups of records within unmanaged environments whose access can only be understood by the creator, the UK Public Records Office recommends leaving them out of inventories as long as the information is duplicated elsewhere [6].

Many of the reasons why these aggregations are chaotic derive from the way in which they originate and evolve. In her description of the formation process of an organizational archive dating from the mid 80?s to the mid 2000?s, Esteva mentions that, despite the fact that a shared network drive was implemented for file sharing and exchange, each staff member kept and discarded files idiosyncratically and nobody could find each other?s information [7]. Using ethnographic methods, Boticelli studied the records derived from collaborative research projects and found that the authorship and origin of the records are difficult to pinpoint due to changes in organizational structure and functions of the projects as well as changes in the value of the data as the research proceeds [8]. In the case of archaeology datasets, Trelogan explains that taking care of data management throughout the research project and beyond, places a burden on the research team [9]. In her study of digital preservation practices of archaeologists and art historians, Beaudoin describes faculty image collecting habits as resulting in disconnected, largely invisible, ?hoards? or ?silos? of data that not only replicate past work, but are largely invisible as part of the scholarly record [10]. Across these papers the commonality is that faculty, research staff, and archivists are overwhelmed by the belief that they are incapable of dealing with large, unstructured collections of digital data.

Preliminary findings from the ChartEX project, that develops methods for analysis of digitized medieval manuscripts while examining researchers response to them, suggest that to conduct their work, researchers need to combine both distant and close data examination methods [11]. In this project we provide that kind of complementary approach. Our aim is to alleviate the burden on Humanities researchers and archivists trying to make sense of large varied datasets by providing a distant processing model that also allows for close processing of more manageable chunks of information.

B. Data and Collection?s Organization  Xu and Esteva have experimented with various computational analysis methods for purposes of understanding data and archival collections? organization and aiding their processing. To find the stories, as records related to a same project or event, amongst the text documents of different staff members in an organization, they developed a text mining method to find similarities between text segments [12]. Using the file system metadata of that same dataset, they created a treemap visualization to     uncover individual staff recordkeeping practices across time [13]. The same team developed an interactive visual analytics framework that allows conducting collections structural and functional analyses, and includes a data mining method to infer four types of data organization criteria [14, 15]. With a focus on managing GIS archival data, Heard and Marciano created an application that identifies geographic data and allows the user, via a map- based interface, to explore metadata and records from a ?top down? or ?bottom up? including a geographical perspective [16]. The work presented here fills a gap in big data processing in the Humanities, in that it focuses on data organization considering the presence of duplicate and redundant information present in the collection.

C. Entitiy Resoultion  Entity resolution is originated from a classic problem in database research for duplicate records detection [4, 17]. In large database system in the real word, there are duplicate representations of the same object, also known as ?entities? in relational databases. Those duplicate records may not be exactly the same or share a common key reflecting their connection in the system. The value of each record could be only partially matched to other records or present certain errors that make detecting those duplicates a difficult task.

A common example is when multiple different formats of the same address are registered in a database. This problem is also related to record linkage, which is to identify records referring to the same entity in different databases.

A typical entity resolution process includes: data preparation, which transforms data in a uniformed model; field matching, which breaks records by field and defines similarity models between fields; and duplicate record detection, which utilizes data mining techniques such as clustering and learning classification, to identify possible duplicated records [4, 18]. Over the years, there have been a number of studies in this area [17, 19, 20]. ER application has also broadened into fields like social network analysis and web data mining [21-24].

Until now, ER has not being used in the context of collections management and curation. Our work focuses on utilizing the general ER framework to help identify related records. There are several differences between our conception and the traditional ER approach. First, most of the existing works focus on the structured data source, such as a database, or metadata from web documents. In this project, we are dealing with less structured data that also lacks uniform and consistent metadata. Secondly, the entity is generally clearly defined in ER, such as a product, or a person etc. In this project, the entity is not explicitly defined.

We consider each sub-directory in the collection as a representation of some abstracted concept or theme that might be important for record keeping and should be taken into account in any curatorial decisions. Hence, the detection of duplicated representations of the same entity is translated to the detection of sub-directories that might be  related to the same theme or concept. In this exploration, we focused on data modeling and similarity scores.



III. CASE STUDY COLLECTION As a test case we used a collection generated by the  Institute of Classical Archaeology (ICA), which has been conducting research in Greek agricultural territories since 1974. ICA?s work has included dozens of excavation projects in southern Italy and Ukraine, in addition to intensive field surveys, campaigns dedicated to site and object conservation, and a huge variety of related studies.

The resulting digital collection consists of 5TB of data (over one million files) generated by hundreds of researchers from many different countries and disciplines, each using distinct recording methods and technologies. It contains digitized and born-digital photographs, drawings, maps, plans, and notes as well as more complex datasets including GIS data, 3D models, and relational databases.

Throughout its long history, parts of the collection have been copied, transferred, and amalgamated from every available form of detached media, personal computers, and servers, and it has suffered a great deal from duplication and corruption in the process. In addition, there is a general dearth of descriptive metadata and, although some smaller projects have achieved a degree of consistency, there have been no systematically adopted principles for file naming or organization throughout the entire collection. Implicit descriptive metadata has been inconsistently applied in the form of filenames and directory labels that encode things like names or initials of places and people, abbreviations and acronyms, dates, site codes, and subjects, which provide the only information pointers.

In previous work, the lifecycle of the case study research collection was modeled into three stages: 1) the collection of new primary data (e.g. photography and illustrations of objects), to 2) its study and synthesis (e.g.

quantification and analysis of a particular class of artifact), and finally 3) to its publication and dissemination [15]. The motivation for the current project was to locate all relevant data related to one excavation, and, after having reduced as much irrelevant or duplicated data as possible, prepare it for archiving and presentation in print together with an online companion that will include the entire dataset.

The sample used here contains over 100,000 files, most of which are images of varied formats, either scanned from analog media or captured digitally, and spread across 3033 directories. The implicit metadata takes the form of names given to files and directories by the researchers, photographers and scanning technicians in the course of their work, and includes non-standardized terms and numbers that change over time.



IV. METHOD AND IMPLEMENTATION Due to the diversity of file formats in this collection, a  major problem is that there is no way to resolve redundancy with direct content-based comparison methods. Exact     duplicates can be detected by finding file checksums, but checksums also indicate c some duplication is done deliberately, so based on checksums alone may be mislead modification in a file (e.g. minor edits an result in a different checksum, even though the file may be considered a duplicate.

complex in the case of image data. Using im software for identification of duplicate im bring together different related images, nor for purposes of grouping related data in d (e.g. spreadsheets and text documents).  Fu wise content comparisons among files are expensive and not practical for large-scale approach focuses on the available implicit directory level to return clusters that may files. The curator further evaluates if the res are useful.

In a nutshell, our method includes a natural language processing (NLP) and methods. We first tokenize each term i structure and submit a list of terms to t determines which ones are meaningful for organizational needs. After pre-processing, represented by a set of tokens that appea within that directory. To relate and compar developed a scoring model that considers tags, series and structural information. Usin comparison between any two directories yie quantifies the similarity between two proceed with a pair-wise comparison a directories in the test collection. Next, we f directories with low similarity as indicated vector and conduct cluster analysis with directories. Figure 1 shows an overview of t  Figure 1: Overview of the processing workflow  Tags Series D  es with identical corrupt data, and discarding files ding. A minimal nd resizing) will h for the curator, This gets more  mage recognition mages would not r would be useful different formats urthermore, pair- computationally collections. Our  t metadata at the y contain related sultant groupings  a combination of d data analysis in the directory the curator, who r the collection?s each directory is  ar in all the files re directories, we s three features:: ng this model, the elds a vector that directories. We  amongst all the filter out pairs of  d by the resultant h the remaining the workflow.

A. Data Pre-processing and Mode  The raw input data is the hier entire collection including all the d generated the structural file using a utility named ?find.? We adopted OpenNLP package (http://opennlp.

common separators to generate tok numbers that appear in the filename  One of the challenges presente identify directories that contain s different, may be related by proven For example, a set of image files o user or project may have been, versioned by other users at diffe approach is to utilize the numbers a in the naming conventions to i However, since numbers or sequen may not mean anything, we sequences, and terms within a direc interested first in identifying if a directory.  A series could be numbe by a camera, or numeric codes used of series is not trivial because numb present in different formats and valu Definition 1 and introduce Algorithm  Definition 1: A series is an orde numerals (or equivalent) i.e. (a1, a n>3. Furthermore, the difference b consecutive numerals are within a g Algorithm 1: Series Detection 1.   sort the input sequences, 2.   init: Index i 0 ; series S  empty 3.   For a value ai in the list; 4.     if S is empty 5.       S?  (ai, ai+1, ai+2) 6.     If S is not empty 7.          S?  S U ai 8.  Check if S? is a series based on the c 9.     if S? is a series, 10.        i  i+1 11.   else, 12.       i  i+1 and S  S? 13.   Continue with line 3   In this study, we used a heuris  the series score as following:    The d is calculated as definitely indicates a series, a scor probably a list of categorical numbe with high value of d could also b  Directory path  eling  rarchical structure of the directories and files. We a Linux operating system d a tokenizer from the apache.org/), which uses  kens as well as to extract es and directory labels.

ed by this collection is to sets of files that, while  nance, function, or theme.

originally created by one , reused, modified and erent time periods. Our and sequences embedded identify these relations.

nces in an individual file consider all numbers, tory. Specifically, we are series, exists in a given ring sequences generated d in filenames. Detection bers for example, may be ues. We define a series in m 1 for series detection.

ered set of at least four a2, a3, ?, an) and where between intervals of two given threshold ?.

computed series score  tic measure to determine  .

. A value of 1.0 of d re of less than 1.0 means ers. And a set of numbers be a series with a large     difference between any two consecutive m based on the knowledge we had about the this could be just random sequential numb photographic camera but with no bearing on  Tags are the tokens extracted from the are generally very short and may be idiosyn noise we selected tags based on thei occurrences in the directories. Next, we as familiar with the collection, to highlight go from the list of frequent tags. Good tag information such as locations, dates, nam abbreviations. Some examples of good tags are: Metaponto, CH (for Chersonesos), an Bad tags are those that, though commonl indicative of the functions and provenance Some examples of those are: music, friends,  In a collection, each directory may number of tags depending on how many f within and how varied their names are. To computation costs and based on studying th of tags distribution, we fixed the number o directory. The five tags were selected following rules:  ? analyze the tags in files in the immedia ? in the first position is the most frequen ? in the next four positions are the most ? if there are not 4 good tags then fill in  the most frequent tags ? if there are not 5 unique tags in the cur  fill in the remaining positions with UN When identifying series and tags, only th  and directory labels immediately inside analyzed. Previous labels in the full path directory are not considered, and the childr further down the tree are not considered eith  B.  Overall Similarity Scoring Model  After each directory has been analyzed, pair need to be compared. The scoring model fo directories consists of the following three fe 1. lists of series identified inside a directory 2. top five tags exctracted inside a directory 3. directory path of the directory  A tag score for each directory pair calculating the number of matches be directories? ?top 5 tag? vectors.  A positiv position of the vector adds 1 to the t maximum score of 5.  UNKNOWN tags ar same, so the tag score can be high even if tag matches, for example, considering: [a, UNK] [b, c, UNK, UNK, UNK], score=4 matches, but since we match UNK?s to ea matches are counted. A series score fo directories is calculated as follows: ? 0=no series overlap at all  means. However, e test collection, bers placed by a n similar content.

filenames which  ncratic. To reduce ir frequency of sked the curator, ood and bad tags s contain useful  mes, and project s in the collection nd IT (for Italy).

ly used, are less of the collection.

, lost, and files.

have a different  files are included reduce noise and  he unique number f tags to five per  d based on the  ate directory nt tag frequent tags these spots with  rrent directory, NKNOWN he names of files a directory are of the analyzed  ren in directories her  r-wise directories or comparing two eatures: y y  is computed by etween the two ve match in each tag score, for a re considered the f only one actual , c, UNK, UNK,  4. Here, only ?c? ch other, 3 more or each pair of  ? 1=some series overlap (any ran series of the same number of di directories)  ? 2=complete series overlap (both exact same number of series, an the same size, min, and max) For the structural comparison,  suffix scores between the paths of The prefix score is the number o directory labels in the path that ar directories, starting from the left.  T but starts from the right.  One can longest common prefix or suffix b comparing each directory name characters in a string.

We calculate the overall simila pair of directories as follows. For vector: [tag score, series score, pre The scores are normalized and tran best score, and the highest score is score for the directory pair is calcul weight vector to obtain the weighte score vector. Lower scores mean mo similar. To group directories and mean clustering to group the direc groups. K values of 21, 50 and experiment. For each k value, we times and chose the one with the errors as the output to present to the  C. Clustering Analysis and Evalua   Figure 2. Screenshot of the GUI showing ho can be closely evaluated by the curator.

Using the similarity scoring cluster analysis with a standa algorithm. For evaluation, for calculated its compactness and su help users access the clustering re graphical user interface shown in Fi  nge overlap between igits between the two  h directories contain the nd each of the series has  we calculate prefix and f each pair of directories.

f consecutive individual e the same between two  The suffix score is similar view these scores as the  between two strings, but in a path instead of  arity score between each each pair we construct a efix score, suffix score].

nsformed so that 0 is the the worst. The similarity lated by multiplying by a  ed sum of elements in the ore similar.  Zero is most present this we used k-  ctories into a k value of 125 were used in our  e ran the algorithm five lowest sum of squared  e evaluators.

ation   ow the contents of each cluster  g model we conducted ard k-mean clustering each cluster we also  um of square errors. To esults we implemented a igure 2.



V. RESULTS EVALUATION AND A The test collection consists of 3,033  with various nested sub-directories. Comp requires running 4.6 million pair-wise co computed value distributions based on features in logarithmic scale. The result Figure 3.

The tag feature includes the top 5 tags sel directory. The tag feature comparison i number of common items found between Therefore, the result ranges from 0 (no mat (exactly the same set of tags). For the seri value means that there is no overlap (or n between the two directories. Value 1 mean overlap between the two series, and value 2 directory might have a series that is a supe from the other directory. Figure 4 indica cases, the majority of the directories have n top 5 tag selection (88.8%) and no relation (93.8%).  This means that we selected directories which have highest scores in bo features. In this set, there are 415 directorie has the exact set of top 5 tags and strong s with at least one other directory. Not directories account for about 13.7% of directories in the test collection.

Figure 3. Tag and series features value distribution.

A. Evaluation Criteria  Our evaluation criteria included classification model to typify the cluster scoring system to indicate the usefulnes cluster. We mentioned that it is import redundancy be automatically discarded, represent significant, intentional decisions m creators.  In the process of reducing decisions about whether to delete, merge leave data untouched must be weighed in rest of the data within which it is stored. Fo defined four different clustering models th and duplicate data and used those to compa results. The four models are shown in Fig clusters share a set of duplicate data; 2) one          0 1 2 3  nu m  be rs  (l og  sc al  e)  Scores  Tag F  Series  ANALYSIS directories each  putationally, this omparisons.  We  tags and series ts are shown in  lected from each is based on the  two directories.

tches found) to 5 ies feature, the 0 no series feature) ns that there is an 2 means that one erset of the series ates that in both no overlap in the ship in the series d the subset of oth tag and series es, each of which series connection te that the 415 total number of    d a clustering rs and an action s of a resultant  tant that not all as some may  made by the data redundancy, the e, reorganize, or context with the  or this reason, we hat share related are our clustering gure 4: 1) pair of e cluster contains  a subset of duplicate data; 3) P related, but not duplicate data; 4) c by duplicate data. A fifth clustering not having related nor duplicate d useful result. For the results eva result was compared to the mo containing one or more of them.

Figure 4. Cluster classification model with fi  We then proceeded to score th scoring system allowed us to performance of the model in relat actions. The scoring represents the the curator in making a data mana cluster; 1= some confidence, but ne and 2=complete confidence, take ac  A curator actively working w reviewed the clustering results after clustering types and clarifying the scoring system. Note that the evalu (See Figure 2), only involves revie labels and file names, not reading contents. At this point, the curator d data management action to pursue, notes about the characteristics of improve or refine the model.

B. Quantitative Analysis of Cluste  We reviewed the three sets of different k-value (21, 50, and 12 which one better achieved the chal redundant information for action merging all data related to one pro photographic campaign, publication each set provided different results.

4 5  Feature  s feature    Type 1: two clusters share a subset of duplicates  Type 2: a clu    Type 3: two clusters contain useful related info but no duplicates  Type are e    Type 5: Bad cluster; nothing relat  Feature  Pair of clusters contains cluster is formed entirely g model was identified as data and thus as a non- aluation, each clustering odels and classified as  ive clustering types.

he clustering results. This determine the overall  ion to data management e level of confidence of gement action (0=ignore eds further investigation;  ction).

with the test collection r being trained to identify e meaning of the action uation done with the GUI ewing directory structure, g or observing the files did not define the type of , but was asked to make the resulting clusters to  ering Comparisons  clustering results with a 5), with an eye toward llenges of: a) identifying and b) identifying and  oject (e.g excavation site, n, etc.). We observed that  one cluster contains ster of duplicates      e 4: two clusters entirely duplicates  ed or useful     Figure 5. Comparison of different k values in relation t   Figure 5 shows a comparison of the d clusters, indicating the percentage of ind clusters labeled by the curator within the di Because each clustering result was classifie one or more clustering types, the sum of t individual types may exceed 1. In fact, indicates that the clustering is less decisive it contains mixed cluster types. Hence, a smaller clusters can further separate out reduc the number of multiple types in eac the clustering results, the type 3, whi containing related directories but no duplica most common occurring.  We observe that t type 5 clusters, labeled as useless, decrease increase. This indicates that with highe clustering results become more ho consequently, more helpful for the cu decisions.

Figure 6. Comparison of action decisions and their con corresponding k value clusters.

Figure 6 shows a comparison betw decisions made by the curator, their assoc levels, and the corresponding k value cluste each column shows the number of clu decision made. The confidence level of shown in different colors.  0 indicates no and 2 indicates high confidence in the action  When the k value is small, the cluste mixed types of clustering models within. H with scoring 1 will need close processing.

0.5   1.5   2.5  21k 50k 125k  R A  TI O   Pe rc  en ta  ge   to clustering types.

different k value dividual types of ifferent k results.

ed as mapping to the percentage of  a higher value to the curator as large number of those cases and  ch cluster.  In all ich are clusters ated data, was the the percentage of es as the k values er k values the omogenous and urator to make   nfidence in relation to  ween the action ciated confidence ers. The length of usters with each each decision is confidence at all n decision.

ers often contain  Hence, all clusters . Instead, for the  k125 clustering results, we observ action and no action decisions narro  C. Qualitative Evaluation of the C  The collection?s curator compl of the clusters focusing on their co in general, across the different k v (types 1-4) presented data in ways patterns that would have been imp linear review. She was able to deriv and to create new organizationa clusters. We describe some of obse findings in relation to data ma processing practices.

Large sectors of the collection up as types 1 and 4 clusters, were backups that could be immediately the backups were located in deeply opportunity to observe this trend in have presented itself using close nested directories are confusing to r  A number of type 2 clusters w the curator. While these were in other files, they represented delibe creators and, in several cases, allow the provenance of files selected out directory (either to be processed, re research project). These duplicates be described with their provenan would otherwise be lost if they were  The curator identified a number clusters that never would have occu In more than one cluster, files from campaign were identified as relate auto-generated by a digital camera, separated in two very different di archive and labeled differently.

algorithm also discloses different la case due to sequencing series that al of the relationship between the fi same photographic campaign, photographer.

The most ?successful? result w value of 125. Although all three useful, and very different, clusters, terms of confidence scores (See Fig a score of 2, an immediate decision close processing. The clusters with useful, in that they indicate the ne and as a guide for what to modify the algorithm. In relation to the lat that, considering the amount of seq collection, the series score needs t the comparison, and that a dynamic would be useful to data organization  type-5  type-4  type-3  type-2  type-1  Score 2  Score 1  Score 0  ve that the ratio between ows down (70 vs. 49).

Clustering Results.

leted a qualitative review ontent. She observed that alues, the useful clusters that allowed discovering ossible to detect through ve a list of ?action items? al groups based on the ervations to illustrate the anagement and archival  n, most of which showed identified as useless old  y deleted. In every case, y nested directories. The the collection may never processing methods, as  review.

were of direct interest to fact identical copies of  erate choices by the data wed for reconstruction of t and copied into another eused, or fed into another were left as is, but could nce information, which e found out of context.

r of related data as type 3 urred as being connected.

m the same photographic ed because of file names , although they had been irectory locations in the  This means that the ayers of similarity, in this llowed for reconstruction iles as belonging to the or sharing the same  was the set that used a k k value sets contained  it performed the best in gure 6). For clusters with n could be made without h scores of 1 were also eed for close processing  y in a second iteration of tter, the curator observed quencing numbers in this to have less relevance in c review of the good tags n goals.

The qualitative evaluation was instructive for: a) establishing priorities for close processing (those action items with confidence score =1), b) reducing unnecessary redundancy, c) informing a deeper understanding of the contents of the archive and d) indicating what needs to be refined in a next iteration of the algorithm. The exercise of reviewing three sets of clusters took a total of ~8 hours. Not only did this ?distant? method speed up the process of reviewing and tidying the archive in comparison to reviewing the collection item by item, it also provided significant insight into the formation process and embedded meaning in the collection. The curator noted that actions could be determined after close processing of the cluster results, including directives like ?delete directory,? ?merge with directory Y after deleting duplicates?, or ?leave as is.?

VI. CONCLUSIONS AND FUTURE WORK As a preliminary attempt to apply general ER framework  and techniques in the realm of collection curation, our initial efforts focused on practical benefits for a specific collection.

Many aspects of this presentation are specifically tied to the test collection and utilize heuristics from that collection?s curator. On one hand, this is necessary due to a lack of prior knowledge or prior models applicable to this problem. On the other hand, the heuristic knowledge, such as series distribution and customized tags lists, simplify the computations and the complexity of the processing workflow. Still, we think that using ER as a framework for data curation processes is a viable approach that can be generalized to other collections with additional learning methods to improve its robustness and effectiveness.

There is an important part missing from the work presented here, namely the scope of the results in relation to the entire collection. Understanding how complete is the representation provided by the distant processing is not yet possible because there is no suitable benchmark set.

Creating such a benchmark is not trivial, but is something we will strive for in future work.

In the future, we want to investigate what kind of results might be produced by further modifying the algorithm as a consequence of observations during the first clustering iteration evaluation and of the emergence of new requirements as the data are cleaned, reorganized, and filtered. We also plan to visualize the cluster review process including other metadata such as identical checksums, and file format identification that can help the curator validate his cluster evaluation and improve action decision making.

