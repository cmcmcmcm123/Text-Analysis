Wavelet Network based Online Sequential Extreme  Learning Machine for Dynamic System Modeling

Abstract? Wavelet network (WN) has been introduced in many applications of dynamic systems modeling with different learning algorithms. In this paper an online sequential extreme learning machine (OSELM) algorithm adopted as training procedure for wavelet network based on serial-parallel nonlinear autoregressive exogenous (NARX) model. The proposed model used as system identification for nonlinear dynamic systems. The main advantage of OSELM over conventional algorithms is the ability of updating network weights sequentially through data sample-by-sample in a single learning step. This attains good performance at extremely fast learning. The initial kernel parameters of WN played a big role to ensure fast and better learning performance. Simulation of the proposed scheme applied to nonlinear dynamic systems validates that WN-OSELM is superior in terms of identification accuracy and fast learning ability compared to NN-OSELM.

Keywords? Neural Network (NN), Extreme Learning Machine (ELM), Nonlinear ARX Model, Waveleons, Dilation, Translation.



I.  INTRODUCTION Wavelet network is an ordinary feed-forward neural  network with wavelet functions in the hidden layer nodes. It holds same properties of neural networks and wavelets decompositions. The hidden layer nodes, whose activation functions drawn from an orthogonal wavelet family called Waveleons. Wavelet network successfully applied in system identification applications, for instance black-box identification of nonlinear dynamic systems  [1], and a new class wavelet netwok based upon forward orthogonal least squares (OLS) algorithm [2]. However, the scope focused on the training algorithms that used for online training in real time applications instead off-line algorithms like back-propagation, which requires a heavy computation load and learning scheme is based on batch mode which is not applicable for online learning [3]. An on-line identification scheme proposed by [4] based on recursive weight learning algorithm with the growing network technique to adjust the weights so that the identified model can adapt to variations of the characteristics in the system. For fast training of the wavelet neural network, a PID back-propagation algorithm proposed in [5] try to solve two major limitations of the original BP algorithm, which are low speed and local minimum problem. Moreover, a learning algorithm called extreme learning machine (ELM) proposed for single-hidden layer feed forward neural networks (SLFNs) [6] with batch-mode learning algorithm based on fixed network  architecture where the network kernels assigns arbitrarily.

However, ELM learning method has verified noble capabilities to resolving regression and classification problems [6]. Lately, ELM techniques have received great attention in computational intelligence and machine learning communities, as it provides better generalization performance at extremely fast learning speed beside the ability of appraising the network parameters in a single learning step [7]. Wavelet neural networks with the extreme learning machine was introduced in [8], as a composite bounded non-constant piecewise activation wavelet function, where a differential evolution and extreme learning machine algorithm proposed to find the optimal network parameter and reduce the number of hidden nodes used in the network. On the other hand, an online sequential extreme learning machine (OSELM) developed based on ELM with feed forward neural networks in a unified way to deal with industrial applications that the training data come one by one or chunk-by-chunk not in batch mode [9]. In this paper, wavelet network with OSELM is proposed to estimate nonlinear dynamic systems based on parallel-serial NARX model.

Section II introduces the WN-NARX model structure, Section III presents the OSELM algorithm for WN-NARX model, Section IV discusses the simulation results of the proposed scheme on nonlinear dynamic systems and Section V concludes the paper.



II. NARX MODEL USING WAVELET NETWORK The nonlinear autoregressive and exogenous inputs  (NARX) models are useful idea for nonlinear system identification problems. Their structures construct from modest building blocks of linear dynamic system and nonlinear static transformation, where, these combinations can take a number of ways to map model output by available well-known scheme, like Wiener and Hammerstein models and others [10]. In general, the nonlinear autoregressive and exogenous inputs (NARX) model can expressed with the following nonlinear equation,      where  f  is unknown nonlinear mapping functions, ny refer to the number of lagged feedback predictor sequences y(k) (output), nu to the feedforward predictor   u(k)  (input) and e (k) is the fitting residual  assumed to be bounded and uncorrelated with the inputs. In exercise, many types of functions, such as  ( ) (1)         )()(),...1( , )(),...1()(   keunkukuynkykyfky +????=     polynomials splines, kernel functions, and other basis functions can be selected to represent the nonlinear mapping functions in model (1). Moreover, wavelet networks adopted to represent a mapping function because of its decompositions properties and localization ability in both time and frequency domain [2].

Wavelet network in NARX model can be expressed by expanding each functional component of the model into a weighted sum of the wavelet expansions,        where, w refer to the wavelet network output layer weights, and ?(t,d,X ) to the wavelet activation functions for m hidden layer nodes while, t  and  d  are translation and dilation vectors induce a multiresolution analysis for the wavelet function. In (2) and (3), X represents n number of regressors and n = ny + nu.

In this work a serial-parallel NARX model structure is used as shown in Figure1, where the real plant outputs are used for prediction of the future model?s outputs so that the stability and approximation of the network is guaranteed  [11].

Fig. 1. Seiral- Parallel WN-NARX

III. WN-NARX BASED ONLINE SEQUENTIAL-ELM The performance of ELM provides the necessary  background for the development of online sequential extreme learning machine OS-ELM for WN, where the learning procedure consists of two phases, namely an initialization phase and a sequential learning phase:  1) Initialization phase Initializing wavelet mother function parameters namely  dilation and translation is a critical issue, since this may make some wavelets too local (small dilations), which could make the components of the gradient of the cost function remarkably small in areas of interest and consequently effect on speed of convergence and the accuracy of learning algorithm  [12]. For that reason, it is important to choose good initial parameters since there will be no changes once they are initialized because OS-ELM based on fixed structure network [9]. In this paper, the initializing WN parameters done with density function and recursive algorithm, where the procedure based on the input  domains of input space where the wavelets are not zero [13].

Consider a network with form of (2), and a domain defined by [a, b] containing values of the jth samples of the input vectors

X. Now, consider the waveleon i, then the dilation parameter di defined as the inverse of the scale of the concerned wavelet is di = Si-1, and considering the scaling for the first waveleon as the center of the parallelepiped then scaling S1= 0.5 (a- b). To initialize t1, a point p selected between a and b so it divides interval [a, b] into two parts. To find the point p, the ?density? functions ? estimated from available input output observations [x, f(x)] are used such that the point p to be the center of gravity of domain [a, b] where,  ) (4                               )()( /)(  /)( xd  a  b xxpa  b dxdxxdf  dxxdf ?(x) ? ?=  ?  =  Recursively, in each sub-interval [p, b] same initializing procedure applied for t2, S2, t3, S3, and so on for all waveleons.

However, the number of data required to fill up appropriate matrix should be at least equal to the number of hidden nodes [9].

2) Sequential learning phase The WN-NARX structure in (2) with K arbitrary distinct samples of (Xj, yj) ? RK, j = 1, ?,K, can generate an single estimated output for jth samples from weighted sum of waveleons output as stated below,  ) (5                            ]).,,( .......  ).,,([ 1111 mjmmmjj wXdtwXdty ?++?=  Since sequential learning algorithm based on fixed network architecture, then (5) can be written compactly as,  (6)                                                                                                                    ?HY = where, Y is an (K?1) output vector, H is (K?m)   hidden layer nodes (waveleons) output matrix, and ? is the (m?1) weight vector.

:    ,      :   , ),,(),,(  ::: ),,(....),,(      ? ? ?  ?  ?  ? ? ?  ?  ?  = ? ? ?  ?  ?  ? ? ?  ?  ?  = ? ? ?  ?  ?  ? ? ?  ?  ?  =  kmmmkk  mm  y  y Y  w  w  dtXdtX  dtXdtX H ?  ??  ??    Base on ELM algorithm theories [6], when the number of training samples equals to the number of hidden nodes, i.e.

K=m (H is square matrix), and all hidden node parameters are independent from training datasets, an analytically and simple way the output weights ? can be calculated by inverting H in a single step realizing zero training error. However, when the number of training samples is greater than the number of hidden nodes (K>m), one can calculate the estimated weights ? by using a pseudo inverse of H to give a small nonzero training error |e|>0 which is the difference between estimated and the real output. To minimize this error, the norm of error that include the estimated weight, ?, as a cost function F,    By considering that the estimation problem in (7) becomes a linear in the parametric problem, the least squares methods may be used to solve the cost function and estimate the weights [14] , thus the solution for (7) is  (8)                                                                             Y?                                              ?H?=  (7)                                              ?    min)?(                         YHF ??=?  (3)    )]( ,... )1(  ),(  , )( ,...., )1( [  )(2      ,,     ,    ),,(  ),(  unkukukuynkykyX  RXdteXiditi? m  i iwwXf  k  k kiikkk  ????=  ?+? =  =    where,   H ? is pseudo inverse of H.  The equation given by (8) can now be rewritten as below:    Now, for the given WN-NARX structure in (2) with K arbitrary distinct samples, assume a chunk of initial training data set {(  , ) for =1, 2?  } is applied with m  hidden layer nodes, where  m <  , then,    thus,    where,         Now, if another chunk of data arrived {(  , ) ? = +1, +2? + }, then the minimizing problem (7) become,  (12)                                                 min)(    1 ?? ?  ?  ? ? ?  ? ??  ?  ? ? ?  ? =  Y Y  H H  F ? ?  and same as (8) and (9)  (13)    11? ?  ?  ? ? ?  ? ? ?  ? ? ?  ? = ?  Y Y  H H  )(T T  1?  where,  (14)         1 ?  ?  ?  ?  ? ?  ?  ? ? ?  ? ? ?  ? ? ?  ? ? ?  ? =  H H  H H  T T        To present ?0 as a function of ?1, (14) can state as:    and by (11) we can get  (15)                                                                                                                         1  1 0 1 HHTT T+=  Now, rewrite (13),  )  ()(                                         11  11 ? YHYHT T  T  0? += ?  and multiply the first term in the bracket by (T0T0-1)  )    ( 11   11 ? YHYHTT)(T T  T  0? += ??  and using (10), then (15),  )    ) ( ()(  )   ()(               110 111   11 ? ?? YHHHTTYHTT TTT ??? +?=+= ??  finally,   En route for generalizing of the previous arguments, assume an s sequences of the data set {(Xjs ,yjs) ? js = Ks +1, Ks +2?. Ks +Ks+1} are available, where Ks and K 1  +s  are arbitrary distinct samples, then a recursive algorithm for updating the least- squares solution written as follows:    where, then  and    To find (Ts+1)-1 Woodbury formula in [15] can be used. Note that instead of chunk-by-chunk, the training data are received one by one, which means K=1 and hence the weights can be updated sample by sample. Finally, the proposed OS-ELM algorithm can be summarized in the following four steps:  1) initialize WN parameters as described in section III.1.

2) for the first data sequence (s = 0), determine ?s and Ts  using (17) and (18) for the K samples.

3) for the next data sequence (s= s+1), determine Ts+1  and ?s+1 using (19) and (20).

4) set ?s = ?s+1 and Ts = Ts+1   go to step3 until all training data finish.



IV. SIMULATION EXAMPLES The simulation based on comparison between WN-OSELM  and NN-OSELM applied to three nonlinear dynamic systems, all the trained and the tested data samples for the input output database are carried out in MATLAB 7.11 environment running in an ordinary PC with 2.27 GHZ CPU, and the hidden nodes activation functions are selected to be derivative of Gaussian function for WN and sigmoid function for NN. The serial-parallel NARX inputs are selected to be two feedback predictors (ny =2) from the real outputs of the plants, and one forward predictor (nu =1) from the inputs. The simulation carried out in two stages for both NN and WN identification models: stage one is the best fit performance analysis and stage two is network size analysis which are described respectively below. For the sake of simplicity we denote NN to NN- OSELM and WN to WN-OSELM.

Stage 1: Best Fit Perfermances In this stage, the simulation is done with seven hidden  layer nodes for both NN and WN applied to the following dynamic systems to determine the best fit performance:  A. Magnetic Levitation: The equation of motion for this system is      where y(t) is the movement displacement of the magnet ball, i(t) is the current that controls the electromagnet flux, M is the mass of the magnet ball, g is the gravitational constant, the factor ? is a tacky friction parameter, and ? is a field power constant. The simulation result in Figure 2 shows notable  )(                                 (9) ? 1 YHHH TT? ?=  (11)                                                                      )  (                                          000 HHT T=  1  1         1 :, : , ),,(),,(  ::: ),,(....),,(  ?  ??? ? ? ?  ?  ?  ? ? ?  ?  ? =  ? ? ?  ?  ?  ? ? ?  ?  ? =  ? ? ?  ?  ?  ? ? ?  ?  ?  ??  ?? =  KkmmmK mmKK  mm  y  y Y  w  w  dtXdtX  dtXdtX H ?  [ ]                                                                            1100  101 HHHHH  H HHT TTTT +=?  ?  ? ? ?  ? =  (17)                                                               )(                                         1? sYsHsTs T? ?=  (18)                                                                  )(                                       sHsHsT T=  (19)                                                                                                        1 1 1 +++ += sHsHsTsT T  (20)                          )      (  ) (                       ??? 1 1 1 11 1 ssHsYsHsTss ??? T +++?++ ?+=  (16)                                              )   ()(                                0 11110 1 ??? ??? HYHT T1 ?+= ?  (21)                        )y( )( )(   )y(     dt td  M ?  ty ti  M ?g  dt td ??  ? ?  ? ? ????  ? ?  ? ? ?+?=  :    ,:  , ),,(),,(  ::: ),,(....),,(    1 00         0 ?  ??? ? ? ?  ?  ?  ? ? ?  ?  ?  = ? ? ?  ?  ?  ? ? ?  ?  ? =  ? ? ?  ?  ?  ? ? ?  ?  ?  ??  ?? =  Kkmmm0K mmKK  mm  y  y Y  w  w  dtXdtX  dtXdtX H ?  (10)                                                                 )(                                  ? 00100 YHT T? ?=    differences in best fit performances, where WN reached 99.80 % of best fit and NN reach 87.20 %. Note that this system has fast dynamic response yet WN was capable to extract plant dynamics.

Fig. 2. WN and NN based OS-ELM for Magnetic Levitation system  B. Continuous Stirred Tank Reactor (CSTR):The dynamics of the CSTR system can describe by,        where l(t) is the fluid level, r(t) is the concentration result at the output, ?1(t) is the flow rate of first liquid feed source with concentration C1= 24.9, and ?2(t) is the flow rate of the second feed with concentrations C2 = 0.1. Similarly, the simulation result in Figure 3 displayed that, WN much better than NN, where the best fits was 91.66 %, while NN shows 80.13 %.

Note that NN suffers from sharp discontinuity ends, while WN with decompositions properties a superb estimate capability with sharp discontinuities system responses.

C. Robot Arm: The system consists of a single-link robot arm that controls the movement of the arm. The equation of motion dynamics is,      where m(t) is the angle of the arm, and T is the DC motor torque. However, the simulation result in Figure 4 shows better best fit and good estimation with 99.66 % for WN and 95.45% for NN. Note that WN still higher.

Fig. 3. WN and NN based OS-ELM for CSTR system    Fig. 4. WN and NN based OS-ELM for Robot Arm system  For more investigation on stability performance,  the training errors are shown in Figure 5 for the robot arm system; it is obvious that the error decline gradually for the NN with none soothe convergence rippling with each training sample unlike WN, where it was stable all the line. As a result from previous tests, it is obvious that WN can offer excellent approximation ability turn out to be factual even for functions with sharp discontinuities.

Fig. 5. The Training errors for Robot Arm system  Stage 2 : Network Size The simulations are applied to the three dynamic systems  with different numbers of neurons and waveleons in the hidden layers, while the numbers of regressors are fixed in all tests and the trained data are fed sample by sample (K=1). On behalf of that, a comparison illustrated according to training time and normalized mean square error (NMSE) and tested with 7, 15, and 25 numbers of hidden nodes. For the first evaluation with 7 hidden nodes, Table1 shows that WN is less NMSE error and more training time than NN for all systems, where WN is taking more time in about 0.1- 0.2 seconds.

TABLE I.  TRAINING TIME AND NORMALIZED MSE FOR 7 HIDDEN NODES OF WN AND NN BASED OS-ELM  Magnetic Levitation CSTR Robot Arm  NN WN NN WN NN WN  Training Time  (second) 0.1248 0.3120 0.3744 0.4368 0.2340 0.4212  NMSE 0.1426 0.0023 0.1528 0.0661 0.0244 0.0018  0 200 400 600 800 1000 1200 1400 1600 1800 2000 -1          Samples  O ut  pu ts     Plant  NN  WN  Best Fit NN = 87.20 % WN= 99.80 %  0 500 1000 1500 2000 2500 3000 3500 4000 19.5   20.5   21.5   22.5   Samples  O ut  pu ts      Plant  NN WN  Best Fit NN = 80.13 % WN= 91.66 %  0 200 400 600 800 1000 1200 1400 1600 1800 2000  -0.6  -0.4  -0.2   0.2  0.4  0.6  Samples  O ut  pu ts      Plant  NN WN  Best Fit NN = 95.47 % WN= 99.66 %  0 500 1000 1500 2000 2500 3000 3500 4000 -0.04  -0.02   0.02  0.04  0.06  0.08  0.1  0.12  0.14  Samples  T ra  in in g  E rr or    NN WN  610 615 620 625 630 635  -0. 04  -0. 02   0. 02  0. 04  0. 06      (22)                                       )(.20 )()()(              21 tlt?t?dt tld  ??+=  ( ) ( ) ( )  (23) )(1 )(  )( (t)  )( )( (t)  )( )(    tr tr  tl ?  trC tl  ? trC  dt trd  + ???+??=  (24) )(  2   ))((sin10 )(   T dt  tdm tm  dt tmd  +????=    The same results obtained in Table2 for 15 hidden nodes, where WN shows better accuracy but not much difference than NN. The enhancement in training time of WN is clear where the differences reduced within 0.01- 0.1 seconds, meaning that WN with prior initializing tries to find optimality due to number of waveleons.

TABLE II.  TRAINING TIME AND NORMALIZED MSE FOR 15 HIDDEN NODES OF WN AND NN BASED OS-ELM  Magnetic Levitation CSTR Robot Arm  NN WN NN WN NN WN  Training Time  (second) 0.5148 0.5616 1.1076 0.9516 1.0920 1.0764  NMSE 0.0746 0.0601 0.0825 0.0353 0.0074 0.0069  In Table3 for 25 hidden nodes, WN shows better NMSE in robot arm and CSTR systems with less learning time comparing to NN in all three systems. This means that WN reached almost optimal waveleon numbers with robot arm and CSTR systems. However, NN showed better identification accuracy then WN in magnetic levitation system, and this could be the reason of over fitting tricky of WN with larger number of waveleons than the optimal number.

TABLE III.  TRAINING TIME AND NORMALIZED MSE FOR 25 HIDDEN NODES OF WN AND NN BASED OS-ELM  Magnetic Levitation CSTR Robot Arm  NN WN NN WN NN WN  Training Time  (second) 0.9204 0.5772 1.5288 0.9984 1.5132 1.1700  NMSE 0.0136 0.0601 0.0390 0.0353 0.0106 0.0069  From the above results, it?s clear that WN is less sensitive to the network size than NN and the reason is a good initialization procedure for wavelet parameters, while the initialization of NN done randomly. Moreover, the wavelets decomposition propriety helped WN to approximate output of the plants precisely giving better identification of the system dynamics.

On the other hand, the time consuming for training varied with number of hidden nodes, and WN reached optimal performance in best fit and training time with specific number of waveleons no matter what size it is. In another word, WN is able to spend less training time than NN if the number of waveleons is chosen correctly.



V. CONCLUSION Wavelet Network based on fixed structure serial-parallel  NARX model with online sequential extreme learning machine has been applied to nonlinear dynamic systems as system identification scheme. The OSELM algorithm is adopted as training algorithm for wavelet network.

Simulations have been carried out with proposed model on three nonlinear dynamic systems. The updating weights was done online sequentially, sample-by-sample and, for all the  simulated dynamic systems.  The results show fast learning ability with good system identification accuracy for WN- OSELM preserving better stability and convergence over NN- OSELM in most tests.  The overall results show better acceptable training time, and could be applied in real time applications if real input and output observations are available.

However, further investigation is required in order to implement the algorithm experimentally, since selecting the number of hidden nodes (waveleons) acts as a big rule in WN- OSELM training time, because it may lead to fatal estimations and much more time consuming.

