Hypothesis Testing Based Knowledge Discovery in   Distributed Multiple Data Sources

Abstract?In the past several years, most data mining researchers focus on data mining from single data source. Nowadays, data mining from multiple data sources is a new problem in Web environment and is also an efficient technique for solving knowledge discovery in distributed databases. A new method for mining multi-data sources is presented in this paper. By sharing knowledge patterns discovered in other similar data sources, hypothesis testing is employed for verifying whether the patterns are also suitable for local data source or not. So that can improve the efficiency of KDD greatly. Finally the effectiveness of this method is analyzed and experimental result is given. This method can be extended as an efficient data mining algorithm in case of apriori hypothesizes are provided. And it can be also used for incremental data mining.

Keywords- multiple data sources, hypothesis testing, knowledge sharing, knowledge discovery

I.  INTRODUCTION With the availability of inexpensive storage and the  progress in data capture technology, many organizations haves created ultra-large databases, and this trend is expected to grow. As an important way to obtain knowledge from mass data, data mining (also called knowledge discovery in databases, KDD) has been brought into focus. Increasing number of scholars have engaged in this area since 1990s, and achieved considerable fruits. However, the traditional research of KDD mostly aimed at centralized data source. With the rapid development of network technology, it is possible to realize the remote accessing of data and large-scale information sharing. Large amount of distributed and multiple data source information systems appeared. In practice, KDD systems need more and more deal with distributed multiple data source and incremental data [1,2,3]. In the environment of distributed multiple data sources, KDD faces some new problems as follows:  i) Usually, there are many similar data sources in distributed database system. Take a big chain-store enterprise for example, affiliated agencies in different regions usually build local database, which is similar to each other. Does the knowledge discovered in one database equally applicable to the others?

ii) In distributed data source environment, different data sources may be used by many users. Different users often have  customizing requirements about the knowledge discovered in database, such as support, confidence, etc. If one user had run data mining algorithm and got some rules, does these rules meet another user?s demand?

Above problems will be discussed in this paper, and an effective solution is given.



II. RELATED WORK In order to discover knowledge in multiple data sources or  distributed databases, the following methods are often used:  A. Centralized Mining To collect data from various data sources, and establish a  data warehouse or data mart, then run the data mining algorithms in local synthesized database [4,5]. This method can realize exhaustive mining through full search in all data records, but its cost is too high. Because KDD faces mass data, the spatio-temporal complexity of mining algorithm is usually very high and is sensitive to the amount of records. At the same time, the cost of data communication, data conversion, data pre-processing, etc. used to establish centralized database are also huge. In addition, information security, data confidentiality and other issues may also be involved in.

Therefore, centralized mining is not an idea solution.

B. Partition Method To break the data mining task of multiple data sources into  three subtasks: i) classify the multiple data sources; ii) mining each database separately; iii) synthesis mining knowledge from similar databases [6,7]. Meanwhile, multi-agent technology has also been brought into distributed KDD systems. The system sends data mining agent to each site for data mining, then sends back the mining results to local for synthesis [8,9,10]. All the data records have to be processed through these methods, the complexity can not be significantly reduced, and the synthesis of pattern is also lack of effective algorithm at present [11].

C. Mining on Sample Space The research on sample detection method for data mining  has been proposed since several years ago. Tovienven[12] proposed a random sampling approach to mining association rules. Firstly, mining the sample space, then check the results of the remaining data. This algorithm has higher efficiency.

However, the method is applicable to the mining of single, homogeneous database. Jong[13] and other researchers have proposed a variable precision association rule mining method.

Firstly, obtain necessary knowledge from some samples by sampling techniques, and then use this information to mine the entire database in a heuristic way. The methods mentioned above are all faced to the single data sets which can not be used for distributed situation. Reference [14] proposed a distributed variable precision association rule mining algorithms SMDM.

Firstly, randomly sample a certain capacity of data at each site, mine the sample data, and then re-use meta-learning strategies to obtain the final results. This method has higher efficiency but the accuracy of results decreases sharply. Meanwhile, it is considerable difficult for meta-learning and verification.



III. KNOWLEDGE DISCOVERY METHOD BASED ON HYPOTHESIS TESTING  In fact, the multiple data sources of the same enterprise or the same field are so similar that the implicated knowledge in these data may be shared to some extent. As a result, KDD system could improve the efficiency of knowledge discovery by knowledge-sharing mechanisms. We can use the knowledge discovered from database D to guide the knowledge discovery process in a similar database D?. We just need to test the knowledge and determine the applicability in D?, then reduce the repetitive work. It is more effective than traditional data mining. If testing all the related records in database is avoidable, the efficiency of knowledge discovery can be further improved.

Based on this idea, knowledge from another similar data sources can be seen as a hypothesis in the local database, and the records in local database are used to evaluate the validity of the knowledge. So we can transform the knowledge judging problem into hypothesis testing problem. It is similar to product acceptance: rules (hypothesis) can be regarded as acceptance criteria, each record in local database looks as a product. The products consistent with the criteria are qualified, otherwise are rejected. If the rejection rate is too high, it means that the hypothesis does not conform to the database, which can not commendably describe the data. So it should be refused. If the rejection rate is low, the rule is adopted. In order to efficiently determine the hypothesis, we can sample from the database reasonably to reduce the capacity of subsample. Then we just need test the subsample space to verify the hypothesis.

The traditional methods of sampling inspection mainly include three modes: unitary, duplex and sequential. Though the requirement of sub-sample capacity for unitary mode is larger than the other two, the process is simple. Since the testing process discussed in this paper is not complicated and not very sensitive to subsample capacity, unitary mode is a feasible method.

Unitary sampling inspection means to sample only once, then makes a decision to accept or reject according to the observations results. It can be divided into two types, quality based inspection and the amount based inspection. The problem discussed in this paper is to check whether the data record is consistent with a given rule, so quality based inspection is adopted.

In the quality based inspection, two criteria of rejection rate is usually set: p0 and p1, and p0?p1. p0 is called acceptable quality level (AQL), and p1 is called limit quality level (LQL).When the rejection rate p?p0, receive this batch of product; When p?p1, refuse this batch of products. Suppose the subsample capacity is n, and k stands for the number of inferior product in subsample. We need to choose suitable integers n and c to make the error rate as small as possible while making the following judgment: "When k ?c, this batch of product is eligible; When k>c, this batch of product is ineligible." None sampling inspection methods can guarantee the conclusion of testing to be entirely correct. But we can pre- set a upper limit probability of getting wrong conclusion, and determine n and c by the pre-set limit. Then we can assure that the conclusion gotten from the sampling inspection to be correct in a high probability. In mathematical statistics, there are two kinds of mistake[15]. We call ?refusing a correct hypothesis mistakenly? as the first mistake, and call ?accept a wrong hypothesis mistakenly? as the second mistake. Suppose the upper limit probability that we make the first mistake is ?, and the upper limit probability that we make the second mistake is ?, this is a hypothesis testing problem. For a hypothesis H0 : p ? 1-Cr, Cr is a pre-set lower limit of reliability. Now we test H0 and the requirements are as follows: when p?1-Cr, the probability of committing  the first mistake does not exceed ?; when p>1-Cr, the probability of committing the second mistake does not exceed ?.

For the population whose rejection rate is p, the number of population is N, the number of reject is M. We randomly select n products, suppose D is the number of reject. Then when N is a finite, D obeys hypergeometric h(n, M, N):  ( ; , , )  M N M  k n k P D k N n p  N  n  Np Nq  k n k N  n  ?  ? = =  ? =  ? ?? ? ? ?? ? ? ?? ?  ? ? ? ? ? ?  ? ?? ? ? ?? ? ? ?? ?  ? ? ? ? ? ?  (1)  When N is infinite, D obeys binomial distribution, since the amount of data which KDD dealt with is very large, binomial distribution can be used to approximate. In this case, the probability of that the reject in the sub-sample is not more than c is as follows:   1 1  ( , , ) (1 )  (1 )  c k n k  k  c n c  p  n L p n c p p  k  n n x x dx  c  ?  =  ? ?  = ?  ? = ?  ? ? ? ? ? ? ? ? ? ? ? ?  ?  ?                       (2) The requirements of the sampling plan can be expressed as  follow:      ( , , ) 1  ( , , )  L p n c p p  L p n c p p  ?  ?  ? ? ?  ? > ? ? ?  (3)  Equation (2) showed that L is a monotonic decreasing function of p, then n and c which satisfies (3) can be obtained by solving the following equations:    ( , , ) 1  ( , , )  L p n c  L p n c  ?  ?  = ?  = ? ? ?  (4)  The solution of function (4) is complex. During the practical application, n and c are solved through a special mathematics table, which is more convenient for computer processing.

The above sampling method depends entirely on the parameters n and c, it is also known as (n, c) sampling plan.

And the function L (p, n, c) is called operating characteristic function of (n, c) sampling plan, as shown in Figure 1:                Figure1. Sketch map of operating characteristic function.

Take association rules as example, assume that we have obtained a rule from the database D', R: yxxx n ?? ?21 , now we verify whether it is equally effective in database D.

Suppose the data set which contains items x1?x2???xn is S, and the data records don?t comply with the rule in S is SR.

Suppose Rp S S= , then the confidence of the association rule R is equal to 1-p. Take S as the population, randomly sample n data records from S as subsample to test rule R.

Suppose the number of records which don?t complied with R in subsample is k. It is reasonable to accept R with the smaller k, and it is reasonable to refuse R with the bigger k. Suppose the lower limit confidence of accepting R is Cr, then if 1p Cr? ? , accept R; if 1p Cr> ? , refuse R. The most convenient approach is to suppose 0 1 1p p Cr= = ? , 1? ?= << , then determine the integers n and c according to the above equation (4), and make sure the sampling plan. But unfortunately this kind of sampling plan does not exist. Because take 0 1 1p p Cr= = ? into equation (4), then we get 1? ?= ? .It is unable to meet the accuracy requirements: , 1? ? << . As can be seen from figure 1, when n and c are unchanged, if , 1? ? << , then 1 0p p is rather big. While p0 is closer to p1, then the curve in the diagram between p0 and p1 may be steeper. Which indicated the larger subsample n, high- efficiency test can not be achieved.

In practice we can pre-set Cr0 and Cr, and Cr0>Cr. Cr is defined as the lower limit confidence of rule R. Suppose the confidence of rule R is CR. If CR?Cr0, it means that R has higher reliability and is acceptable; if CR<Cr, it indicates that the credibility of R is low and should be denied. Suppose AQL=1-Cr0, LQL=1-Cr, Cr0>Cr ? 1? ?= << , (n, c) is determined by (4) for testing rule R. If the testing result is accepting R, then the probability of CR<Cr is smaller than ?, re-testing is not needed; if the result is rejecting R, the probability of CR>Cr0 is smaller than ?, but it is still possible that the value of CR is between Cr0 and Cr. Thus R may still be an accepted rule. In this case, whether completely testing in the whole population space is needed, is decided according to our actual necessaries. For the high confidence rules, this approach only needs a small amount of samples to test; completely testing is needed only in case of very lower confidence rules.

Because the testing approach in this paper is carried out through data comparison by computer, it is not very sensitive to the subsample capacity. So Cr0 can be made adequately close to Cr, thereby reducing the risk that mistakenly refuse such rules whose CR is between Cr0 and Cr. For the data of random distribution, the inspection process does not require a specific sampling plan and data pre-processing. It only need compare such related data records with the rule one by one. If it is found the rule can be accepted or rejected, then stopped.



IV. EXPERIMENTAL RESULTS AND ANALYSIS In order to demonstrate the correctness of the algorithm, we  have done several experiments with association rules. The experiment data set is generated by Assocgen[16]. The number of transactions in database D and D? is 10,000 respectively.

The association rules are generated by algorithm mentioned in [16] and the algorithm is implemented with C++. Set min_sup=2%, and min_conf=60%, we get 19 association rules from database D?. Then we verify these 19 rules in database D.

Set ?=?=0.05. While hypothesis testing, we set Cr0=0.85, Cr=0.7, and get the sampling plan (n, c) ?(506, 122). The testing result shows that: in eleven rules whose confidence are more than 70%, there are only two rules are refused; and the other eight rules whose confidence are less than 70% are all refused. In this testing, there are only 506 records to be checked. It means the method proposed in this paper has higher efficiency.

Because the hypothesis is made by the requirement of users, the parameters Cr0 and Cr are set by the user. Then different users may set the confidence of rules flexibly in need of their tasks.  As for the support of rules, hypothesis testing methods can also be used to determine whether it is satisfied the user?s requirements (the specific approach shall not be repeated here). Thus, the hypothesis testing method can solve the second problem proposed in section I.

This method is also applicable to other types knowledge discovery tasks such as sequential patterns, classification rules, etc. It only needs to pre-process the data correspondingly with the knowledge type. Furthermore, the method can also be used to verify conjectural knowledge. Hypothesis derived from background knowledge or prior knowledge, or given by human experts, can also be verified by this method.

p ?   1-?  L  p0 p1      What?s more, the method we proposed is valid on the multiple data sources which are similar, but not always effective to those multiple databases of different quality.



V. CONCLUSIONS AND FUTURE WORK Data mining from multiple data sources is a new problem in  Web environment and is also an efficient technique for solving knowledge discovery in distributed databases. A new method of knowledge discovery from multiple data sources is proposed in this paper. It shares knowledge found in other similar data source to determine the validity of knowledge in the local database through sampling test. The experimental results show that the method is effective and can significantly improve the efficiency of knowledge discovery.

The hypothesis testing based knowledge discovery method proposed in this paper is also an effective solution to incremental data mining. In the real world, new data records added into an application database (such as a sales database), to some extent, is generally similar to original data. Under this circumstance, it is necessary to determine the impact of new added samples, and to make sure how to re-arrange subsample size and the sampling strategy. Specific research findings will be given in future.

