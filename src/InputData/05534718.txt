MapReduce as a Programming Model for  Association Rules Algorithm on Hadoop

Abstract? As association rules widely used, it needs to study many problems, one of which is the generally larger and multi-dimensional datasets, and the rapid growth of the mount of data. Single- processor?s memory and CPU resources are very limited, which makes the algorithm performance inefficient. Recently the development of network and distributed technology makes cloud computing a reality in the implementation of association rules algorithm. In this paper we describe the improved Apriori algorithm based on MapReduce mode, which can handle massive datasets with a large number of nodes on Hadoop platform.

Keywords-Association rules; Apriori; Hadoop; MapReduce;KDD

I. INTRODUCTION Known as knowledge discovery in databases (KDD), data  mining can automatically extract useful hidden information from the datasets which are massive, noise, and vague. Data mining, as an efficient way to extract the knowledge, has draw attention from both the industrial and research communities.

Association rule is one of the important themes and essential of data mining which can find out the relationship between item sets in the database. We can use the interesting association relationships which are extracted among huge amounts of data.

But, the discovery of association rule is a direct mass-oriented database system that often has hundreds of properties and millions of records, contains a complex relationship between data tables, and remains a time-consuming process. This will inevitably lead to a great surge in search of dimension and space.

Traditional association rule mining algorithms encounter many difficulties when deal with massive data. For example the traditional Apriori algorithm [13] faces the following difficulties: It spends so much time dealing with particularly large number of candidate sets since candidate (k + 1) itemsets are constructed through the self-join of frequent k-itemsets. Its growth is exponential. To get all frequent sets, it needs to repeatedly scan the database. As Apriori employs an iterative approach, which incurs high I/O overhead for scanning, there are certain limitations regarding in handling of transactions in large database.

Considering above problems, parallel association rule algorithms with high performance are best choice. Currently proposed parallel algorithms are CD (Count Distribution) CaD (Candidate Distribution) and DD (Data Distribution) [3]. These algorithms have major weaknesses at some respects of Communication and synchronization. Now MapReduce model can be used in Machine Learning on Multicore[5] where has many advantages to solve these problems. A key benefit of MapReduce is that it automatically handles failures, hiding the complexity of fault-tolerance from the programmer [11]. In this paper we use the MapReduce model which provides a parallel design pattern for simplifying application developments in distributed environments to improve the Apriori algorithm as to obtain a better parallel performance. This model can split a large problem space into small pieces and automatically parallelize the execution of small tasks on the smaller space [6].

We use it to reduce the communication overhead without having to take into account the synchronization operations between nodes. The contribution of this paper includes:  ? An improved parallel Apriori was proposed. We calculate the count of the candidate items based on Mapreduce framework to achieve parallelism which significantly improved the efficiency. After the redesign of map and reduce function, through the iterations of invoking map and reduce function, then gets frequent itemsets.

? Using Apache?s MapReduce implementation ?Hadoop, we have compared the performance of the improved parallel Apriori algorithm on the Hadoop platform using different nodes.



II. BACKGROUND: ASSOCIATION RULES, MAPREDUCE AND HADOOP  A. Association Rules Let 1 , 2 m{ , . . . , }I I I I=  be a set of items.

Let 1 , 2{ , . . . , }t nD B t t=  be a database of transactions, where each transaction it is a set of items such that it I? . Given an item set X I? , a transaction T    contains X if and only if X T? . An association rule is an implication of the form X=>Y, where X I? , Y I? and X Y = ?? .The Support and  Confidence are common indicators to measure strength of association rules. To select interesting rules from the sets of all possible rules, constraints on various measures of significance and interest can be used. The best-known constraints are minimum thresholds on support and confidence. The support supp(X) of an itemset X is defined as the proportion of transactions in the dataset which contain the itemset. The confidence of a rule is defined as  ( ) s u p ( ) / s u p ( )c o n f X Y p X Y p X? = ? [4].

A user-specified minimum support and a user-specified minimum confidence are needed in the Association rules at the same time. It consists of two steps. First, minimum support is used in order to find all frequent itemsets in a database. In a second step, applying the minimum confidence and these frequent itemsets, we can form rules. The first step should be played more attention to improving as the second step is straight forward. Apriori is applied to count the support of itemsets with a breadth-first search strategy and generate candidate itemsets to accomplish the first step.

B. MapReduce As a data parallel model, MapReduce is a patented software  framework introduced by Google to support distributed computing on large datasets on clusters of computers [1].

Known as a simple parallel processing mode, Map-reduce has many advantages: such as, it is easy to do parallel computation, to distribute data to the processors and to load balance between them, and provides an interface that is independent of the back- end technology [10]. MapReduce is designed to describe the process of parallel as Map and Reduce. The user of the MapReduce library expresses the computation as two functions: Map and Reduce [2].

Map, written by the user, takes an input pair and produces a set of intermediate key/value pairs. The MapReduce library groups together all intermediate values associated with the same intermediate key I and passes them to the Reduce function.

The Reduce function, also written by the user, accepts an intermediate key I and a set of values for that key. It merges together these values to form a possibly smaller set of values.

Typically just zero or one output value is produced per Reduce invocation. The intermediate values are supplied to the user's reduce function via iterator. This allows us to handle lists of values that are too large to fit in memory.

The Map and Reduce functions are both defined with respect to data structured in (key, value) pairs. MapReduce provides an abstraction that involves the programmer defining a ?mapper? and a ?reducer?, with the following signatures [9]:  Map::(key1) ? list(key2,value2)  Reduce::(key2,list(value2) ? list(value2)  C. Hadoop Hadoop is a popular open source implementation of  MapReduce, a powerful tool designed for deep analysis and transformation of very large data sets which is inspired by Google?s MapReduce and Google File System [2]. It enables applications to work with thousands of nodes and petabytes of data. Hadoop uses a distributed file system called Hadoop Distributed File System (HDFS) which creates multiple replicas of data blocks and distributes them on compute nodes throughout a cluster to enable reliable, and has extremely rapid computations to store data as well as the intermediate results [6]. Hadoop schedules map and reduce tasks to distributed resources, which handles many tough problems, including parallelization, concurrency control, network communication and fault tolerance [8]. Furthermore, it performs several optimizations to decrease overhead involved in the scheduling, network communication and intermediate grouping of results.



III. MAPREDUCE FOR APRIORI  A. Description of the traditional Apriori Apriori is a traditional algorithm for association rule and  designed to operate on database containing transactions. The main idea of Apriori is producing candidate items, and then scanning the database so as to decide whether they meet the count. It use a "bottom up" approach, frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data.

The algorithm terminates when no further successful extensions are found. It can be shown as follows:  Input: D, (Database of transactions), min_sup (minimum support threshold),  Output: L, (frequent itemsets)  Method:  (1) 1L =find_frequt_1-itemsets(D);  (2)for(k=2; 1kL ? !=? ;k++){  (3) kC = Apriori_gen ( 1kL ? ,min_sup); (4) for each transaction t?D{//scan D for counts  (5)   tC =subset( kC ,t); //get the subsets of t  (6)       for each candidate c? tC (7)     c.count++;  (8)    }  (9) kL ={c? kC |c.count>min_sup} (10) }  (11)return L=L k? kL ;  The function of Apriori_gen( 1kL ? ,min_sup) is presented in the paper [7].

B. MapReduce for Apriori The Apriori consists of two steps: the first is generating all  frequent itemsets, the second is generating confident association rule from the frequent itemsets. We deploy MapReduce to parallel the first step and have the data stored in the file in order to deal with in the Hadoop Distributed File System (HDFS).

We use Hadoop components to perform job execution, file staging and workflow information storage and use the files replace the database to store datasets. In the files each line can be seen as a transaction, each item is separated with white space (a space or Tab). The datasets in files are split into smaller segments automatically after stored in HDFS and the map function is executed on each of these data segments. At first, the candidate sub item are put out with the counts number after the execution of map function, then the frequent itemsets are found after the execution of map function.

Input/output Map function  Reduce function  Input:  key/value pairs  Key:Line No.; Value one row of  data  Key: candidate item sets;  Value:1  Output :  key/value pairs  Key: candidate itemsets;  Value:1  Key:  frequent subitems;  Value:  support  Fig.1. key/value pairs for map function and reduce function  Fig.2. Dataflow of parallel Apriori  The definition of key/value is the first step to implement the function of MapReduce. The pairs of key/value are shown in Fig.1, using the key/value pairs, we can deign the simple map function and reduce function [12]. The map function mainly collects the count of every item in candidate itemsets and the reduce function prunes the candidate itemsets which have an infrequent sub pattern. Each frequent item is generated through one execution of map and reduce function. After stored in HDFS, the datasets are split into smaller segments and then transformed to datanodes. Map function is executed on these  data segments and produces <key, value> pairs for every records it encounters. The framework groups all the pairs, which have the same item and invokes the reduce function passing the list of values for candidate items. The reduce function adds up all the values and produce a count for the candidate item as a one-time synchronization. This algorithm?s advantage is that it doesn?t exchange data between data nodes, it only exchanges the counts. In every scan, each map function generates its local candidate items, then the reduce function gets global counts by adding local counts. Fig.2 shows the data flow and the phases of the MapReduce framework for the improved Apriori algorithm. Multiple iterations of MapReduce computations are necessary for the overall computation. An iteration of the MapReduce produces a frequent itemset. The iteration continues until there aren?t frequent itemsets further.

Parallel Apriori algorithm is shown as follows:  Input: D(data in HDFS), min_sup(minimum support threshold),  Output: L, frequent itemsets  Method:  (1) 1L =find_frequt_1-itemsets(D); (2)for(k=2; 1kL ? !=? ;k++){  (3)   kC =candidate_gen( 1kL ? ,min_sup); (4) for each row of data t?D{//scan D for counts  (5)    tC =  map();//get the subset of the candidate itemsets (6)    }  (7) kL =Reduce();// get the subset of the frequent itemsets (8) }  (9)return L=L k? kL ;

IV. EXPERIMENTS In our experiments, we used Hadoop version 0.20.0,  running on a cluster with 9 machines (1 master, 8 slaves). Each machine has two single-core processors (running at 2.60GH) and 4GB memory. The experiments datasets generated by IBM Data Generator are changed in the form of transactions as in the database.

We use speedup as criterion to measure our algorithm?s superiority. When more than one node is used, computation and transfer is performed in parallel, and the speedup is over one. In the case of more than one data node, speedup increases with the increase of number of data in certain circumstances, which can prove the high efficiency of the parallel Apriori based on MapReduce. Dataset also mainly determines the performance of the parallel algorithm. If we experiment with the small datasets, its performance turned out to be lower for the reasons; extra communication time occupying a large proportion compared to the total execution time. This is easily predicted from experiments where we noticed that the more data a node processes, the less significant proportion becomes    the communication time. The opposite effect is simply seen here, larger datasets would have shown even better speed up characteristics. Clearly the parallel algorithm scales very well, being able to keep speedup almost larger as the datasets and data nodes sizes increase as show in Fig1. Like the CD algorithm, the algorithm's communication overhead is O (|C| ?n) at each phase, where |C| and n are the size of candidate itemsets and number of data sets, respectively [3].

1 2 3 4 5 6 7 8        Number of datanode  S p  e e d u  p  Fig. 3.  speedup of parallel Apriori (Blue:100,000 transactions, Green:1000,000 transactions,) support = 1%).



V. CONCLUSION In this paper, we mainly address the challenge of using the  MapReduce model to parallelize Apriori. Hadoop, the underlying cloud computing framework, can handle many tough problems, including parallelization, concurrency control, network communication and fault tolerance. The novelty of the algorithm lies in the ability of parallel Apriori to take full advantage of what Hadoop can provide. It can be easily applied to many commodity machines to deal with mass data without consider the synchronization problem.

