Optimal Reduction of Large Image Databases for Location Recognition

Abstract  For some computer vision tasks, such as location recog- nition on mobile devices or Structure from Motion (SfM) computation from Internet photo collections, one wants to reduce a large set of images to a compact, representative subset, sometimes called ?keyframes? or ?skeletal set?. We examine the problem of selecting a minimum set of such keyframes from the point of view of discrete optimization, as the search for a minimum connected dominating set (CDS) of the graph of pairwise connections between the database images. Even the simple minimum dominating set (DS) problem is known to be NP-hard, and the constraint that the dominating set should be connected makes it even harder.

We show how the minimum DS can nevertheless be solved to global optimality efficiently in practice, by formulating it as an integer linear program (ILP). Furthermore, we show how to upgrade the solution to a connected dominating set with a second ILP if necessary, although the complete method is no longer globally optimal. We also compare the proposed method to a previous greedy heuristic. Experi- ments with several image sets show that the greedy solu- tion already performs remarkably well, and that the optimal solution achieves roughly 5% smaller keyframe sets which perform equally well in location recognition and SfM tasks.

1. Introduction  Location recognition from an image taken by the user is a useful application of computer vision, even more so now that practically all mobile phones and tablets are equipped with cameras. The state-of-the-art methods to build such a service [10, 4] rely on the ?big data? paradigm?large pre- computed image databases which reside on computer clus- ters, to which user images must be submitted via Internet or cellular networks to query for the location. With increasing computational power and storage capacity of the mobile de- vices, it becomes possible to run such an application even locally, without network access.

For the described mobile application of location recogni- tion, the compactness of the pre-computed database is cru- cial, but the need for a compact, yet meaningful subset of big image data is not restricted to this particular use case.

Mobile robotic SLAM [6] and short- or wide-baseline se- quential visual odometry methods [22, 29] often rely on a set of keyframes to robustify the computation and/or to overcome the drift of the estimated camera poses over time, by performing loop-closing. Compactness of the keyframe set is important for the efficiency of loop-closing, which must share limited processing power with the actual per- frame SLAM computation. Note that the image selection procedure is even more complicated in the SLAM case be- cause of the on-line nature of the computation.

The contribution of this paper is twofold. First, we ex- plore optimal methods for reducing a large, redundant in- put image set by selecting the most relevant representatives via solving the same graph problem as [14]. Secondly, we compare the selections made by the proposed near-optimal selection algorithm with the output of the existing subopti- mal greedy algorithm used by [14] and show that the pro- posed method gives more compact image subsets while the performance of the location recognition task remains un- changed. We also show that the difference between the sizes of the subsets obtained using suboptimal and optimal methods is rather small: while the proposed approach is arguably more principled, our results vindicate the widely used greedy heuristic, but have a slight edge for applica- tions with strict storage constraints.

The bulk of visual location recognition techniques re- lies on image retrieval based on visual words [27]. Fur- ther advances using either a more compact data represen- tation [5] or achieving higher efficiency thanks to vocab- ulary trees [23] allow for location recognition from city- scale databases [26]. Recently, methods have appeared which offer precise positioning of a given query image [15, 25]. These methods provide the camera pose w.r.t. a pre- computed 3D point cloud, instead of just an approximate localization. Once the point cloud is geo-referenced, a pre- cise location on the Earth can be obtained [19, 12].

DOI 10.1109/ICCVW.2013.93    DOI 10.1109/ICCVW.2013.93    DOI 10.1109/ICCVW.2013.93     Figure 1. Unconnected dominating set denoted by black vertices.

We use location recognition as a benchmark for reduc- ing big image databases to representative subsets. Our aim here is to decrease the size of the keyframe set by revisit- ing the problem of selecting relevant representatives from a large image collection. To our best knowledge, no system- atic attempts have been made so far to select such image subsets optimally, by formally minimizing the size of the subset that has the required properties. Note however, in other contexts the formulation of the minimum dominating set problem as an integer linear program has been known for several years [16].

Most techniques for reducing an image set rely on the same idea, namely to first compute pairwise distances within the database, and then prune it such that for ev- ery image there is a sufficiently near representative in the keyframe set (or equivalently, a representative for each clus- ter of images). The differences lie mostly in the proxy used to estimate the distance. One possibility is to employ global image descriptors, e.g. GIST [24]. Although this approach was successfully used to speed-up Structure from Motion (SfM) computation [17, 8], we believe that methods based on visual words are more suitable for the location recogni- tion task. On the other extreme, one could estimate similar- ity through the number of keypoint matches or even through the relative pose, however such approaches in most cases defy the purpose, as matching and pose estimation are com- putationally too expensive.

We use a technique based on visual words, similar to the one of [14]. Note however, our approach is generic and applies to any distance/similarity matrix. We show that thanks to modern integer linear program (ILP) solvers, it is possible to solve this graph problem optimally even for rather large image sets, and that there is a noticeable gap between the sizes of the optimal and greedy solutions. It is worth mentioning that also the widely used large-scale SfM  pipeline based on skeletal image sets [28, 1] greedily solves the same graph problem, so our results apply to city-scale SfM, too.

2. Method  Let us assume for the moment that a connected graph is available whose vertices are the input images, and in which an edge exists between two vertices if and only if the two corresponding images share a significant part of the view- field, i.e. are co-located. Then, to make a compact selection covering all views present in the image set one needs to se- lect a minimum subset of vertices, in such a way that (i) all the vertices in the graph are either in the subset or connected to it by an edge (meaning that every database image is co- located with at least one keyframe), and (ii) the subgraph induced by the selected subset is connected. In graph theory a subset that fulfills (i) is known as the minimum dominat- ing set (DS) of the graph, see Figure 1, while (ii) is called the minimum connected dominating set (CDS). Searching a CDS avoids database fragmentation, but makes the problem considerably harder. In some applications, such as finding a skeletal set for SfM, connectedness is mandatory, while in others it might be dropped or only encouraged rather than enforced.

Following [11], the minimum CDS problem is defined as follows. Given a graph G = (V , E), find a minimum size subset of vertices S ? V , such that the subgraph induced by S is connected and S forms a dominating set in G. In a graph with a dominating set D ? V , each vertex V ? V is either in the dominating set or adjacent to some vertex in the dominating set, V ? D ? ?V ? ? D : (V, V ?) ? E . We say that V is covered by V ? in the latter case. The problem of finding the minimum CDS is known to be NP-hard [9].

2.1. Minimum CDS formulated as ILP  By definition, to find the optimal solution to an NP-hard problem one has to evaluate an exponential number of can- didates in the worst case. Nevertheless, a large class of NP-hard problems can be formulated as integer linear pro- grams (ILPs); and for many instances of such ILPs, modern solvers can find the optimum quickly. The general ILP for- mulation reads:  minimize cTx subject to Ax ? b  lj ? xj ? uj, xj ? Z (1)  The high efficiency of ILP solvers is achieved thanks to the following strategy: (i) obtain a lower bound of the objec- tive function in low polynomial time by solving the relaxed LP problem, where the integer constraint is dropped; and (ii) use the lower bound in branch-and-bound type meth- ods to quickly rule out large portions of the search space.

Figure 2. Four components of the subgraph of G induced by D denoted by color ellipses are connected by adding two yellow ver- tices into S . These two vertices form the solution of Program 3 for a 3? 5 matrix D (the union of the sets C and C? for the three gray edges of the corresponding maximum spanning tree of the con- structed weighted graph contains four 1-vertex and one 2-vertex bridges), see text.

Although this can in the worst case still lead to an exponen- tial number of candidates, the actual convergence times are quite short for our ILPs as will be shown in Section 3.4.

We believe that, unfortunately, it is not possible to for- mulate the search for the minimum CDS as an ILP, because of the comprehensiveness of the connectivity constraint. On the contrary, the ILP formulation of finding a (possibly dis- connected) minimum DS is straightforward [16]. We thus resort to the same strategy also used by the greedy formula- tion (see Section 2.2), namely first find the minimum domi- nating set D. If the subgraph induced by D is connected,D is at the same time the minimum connected dominating set of G. Otherwise, we need to include additional vertices in S ? D. While one can also select these ?connecting? ver- tices in a one-shot optimization, the total CDS is no longer guaranteed to be the globally most compact one.

To prevent excessive fragmentation of the found mini- mum dominating setD, and thereby increase the chance that the subgraph induced byD is already connected, we require even the selected vertices to be covered, so the minimum possible size of a component is 2 and not 1. The connectiv- ity of D can be further improved by requiring each vertex to be covered by more than one vertex. We call the num- ber of covering vertices of a keyframe set its cover number (CN).1 This changes the minimum component size to 3 or even more, and often directly yields a minimum CDS, at the cost of obtaining larger keyframe sets, see Section 3.

1Note the relation to the k-nearest neighbor strategy: a cover number of Q guarantees that at least Q useful neighbors are available for localization, respectively pose estimation.

To formulate the search for the minimum dominating set as an ILP, we construct an n ? n matrix A where n is the number of vertices of G and Aij = 1 ?? (Vi, Vj) ? E with the remaining elements of A being zero. By zeroing the diagonal of A, we require the selected vertices to be covered also. Our program then reads:  minimize 1Tx subject to Ax ? b  0 ? xj ? 1, xj ? Z (2)  The desired cover number can be set through parameter vec- tor b. The solution of the program is a Boolean vector x of length n, indicating the membership of vertices in the set D. The minimized objective value is the number of vertices in the DS.

If the subgraph induced byD consists of multiple, say m (disconnected) components, then for every such component there exists another one to which it can be connected by adding at most two additional vertices, thanks to the prop- erties of the dominating set [11]. We examine all pairs of components to find vertices which would form such 1- vertex or 2-vertex bridges, and then try to find the smallest subset of vertices that need to be added to D to obtain a connected graph.

First, to simplify the task, the (m ? 1) pairs of compo- nents which are going to be connected are pre-selected. For each pair of components i and j, the set of feasible 1-vertex bridges Cij and the set of feasible 2-vertex bridges C?ij are found. Second, a weighted graph is constructed whose ver- tices are the individual components, and whose edges are the potential connections, weighted according to numbers of feasible bridges: the weight of a given edge is either the size of C?ij (if Cij is empty) or the size of Cij plus the size of the largest C? in the entire graph otherwise. By selecting the (m ? 1) pairs forming the maximum spanning tree of this graph, the pairs of components which can be connected us- ing a 1-vertex bridge are preferred. The secondary criteria then is the number of feasible 1-vertex (or 2-vertex) bridges, because this increases the chance that some of these bridges could potentially connect other component pairs, too. Then, a second ILP is constructed:  minimize cTy subject to Dy ? 1  0 ? yj ? 1, yj ? Z (3)  where D is a (m?1)?k matrix, k is the size of the union of sets C and C? of the pre-selected component pairs, and c is a vector of length k which is 1 for 1-vertex bridges and 2 for 2-vertex bridges. The minimized objective function then represents the number of added vertices. The entries of D are Dij = 1 if the i-th component pair would be connected by the j-th bridge, and zero otherwise. The constraints en- sure that all (m ? 1) components pairs will be connected,     (a)  (b)  Figure 3. (a) Illustration of the computation of Algorithm 1 (Step II.). Two black vertices have been added to D so far, the gray vertices are already covered. Note that three more vertices need to be added to construct the dominating set whereas the opti- mal solution (b) consist of three vertices in total.

resulting in a subgraph of G with a single component, i.e. a CDS, see Figure 2.

2.2. Greedy Algorithm  A (suboptimal) greedy solution to the minimum CDS problem is given in [11], see Algorithm 1. Being greedy, the algorithm inherently has polynomial time complex- ity. Moreover, its approximation ratio is in the worst case ln?+ 3, with? the maximum vertex degree in the graph.

The greedy algorithm selects one vertex at a time, pick- ing the one that covers the largest number cV of uncovered vertices, see Figure 3(a). Note that this strategy is subopti- mal, Figure 3(b). If the desired cover number is greater than one, the algorithm can be extended by introducing fractional coverage. Each not fully covered vertex contributes as many times to the value of cV as it needs to be further covered.

Thereby, the selection prefers vertices whose neighbors re- ceived less coverage from the previously selected vertices.

In our experience this works better than counting all not yet fully covered vertices equally, as proposed originally.

Once the greedy selection has found a DS, it is connected to a CDS with another greedy search procedure. For all component pairs, one checks whether a connection by a 1- vertex bridge or 2-vertex bridge is possible. Then one itera- tively selects the bridge which connects the highest number of components. For a graph consisting of m components, at most (m ? 1) iterations are needed to obtain the final D. This can be rather slow for highly fragmented graphs because graph components need to be updated after each iteration.

Algorithm 1 Approximate min CDS computation [11]  Input G = (V , E) Unweighted undirected graph.

Output S Vertices of the min CDS of G.

I. Label all vertices V ? V white.



II. SetD := {} and repeat until no white vertices are left: 1: For all black vertices V ? V set cV := 0.

2: For all gray and white vertices V set cV := num-  ber of white neighbors of V .

3: Set V ? := argmax  V cV .

4: Label V ? black and add it into D.

5: Label all neighbors of V ? gray.



III. Set S := D and connect components of the subgraph induced by D, by adding at most 2 vertices per com- ponent into S.

IV. Return S.

2.3. Constructing the Graph  Until now, we have assumed that the graph G represent- ing the co-located images is available. In fact, constructing such a graph properly, i.e. by matching all image pairs and verifying pairwise geometries, is rather costly. We use a standard image indexing technique instead to obtain a proxy more efficiently [27]. Images are resampled to 3Mpix res- olution, and a visual vocabulary of 200,000 visual words trained on images of city landmarks is used to quantize SURF [2] features detected in the database images. The en- tries of the similarity matrix are dot products of the images? tf-idf vectors, and the edges of graph G can be obtained by thresholding the similarity matrix.

Special care must be taken to ensure there is a solution: if the graph G is not connected, no CDS exists. Moreover, already the DS is infeasible if G has small isolated com- ponents whose vertex count is lower than the desired cover number. To bypass this issue, we pre-process the graph and recursively remove uncoverable vertices until the problem is solvable. Note, this step can already significantly deci- mate the database size if a high similarity threshold is used.

3. Experiments  Three landmark image sets, each consisting of roughly 4,000 images, were downloaded from Flickr [32] as re- sults for queries ?di Trevi?, ?duomo milano?, and ?old town square prague?. The variability of the image sets increases as DITREVI represents a single wall, DUOMO represents a single building, while OLDTOWN represents a larger area with multiple buildings. Only images which were available in at least 0.75Mpix resolution were used.

(a)  (b)  (c)  Figure 4. Sample images from the three datasets downloaded from Flickr [32]: (a) DITREVI, (b) DUOMO, (c) OLDTOWN.

To evaluate location recognition using the selected sub- sets of images, 300 images from each image set were ran- domly set aside as query images and the rest of the images form the datasets of sizes 3,110, 3,104, and 3,255 images respectively, see Figure 4.

Two application scenarios were explored with the held- out query images. First, location recognition based on im- age retrieval and subsequent geometric verification, using the selected keyframe sets. And second, pose estimation of the query images w.r.t. 3D models reconstructed from the keyframe sets.

3.1. CDS Computation  Connected dominating sets were computed for the three datasets independently, using both the ILP solver and the greedy algorithm. The corresponding graphs G were ob- tained using the method described in Section 2.3 for three different thresholds of the image similarity, namely 0.075, 0.1, and 0.15. Cover numbers (parameter vector b in ILP) of 1, 2, and 3 were tested. The sizes of the returned min- imum DS and CDS of G are displayed in Figure 5. It can be seen that more images are required for the most complex scene OLDTOWN than for the other two datasets.

As both methods first search for the dominating set, it is also interesting to see how many vertices had to be added to connect the DS into a CDS, ?Con? in Figure 5. We observe that, as expected, the fragmentation of the dominating set  CN 1 2 3  N um  be ro  f im  ag es  Greedy DS Greedy Con ILP DS ILP Con       0.0750.0750.075 0.10.10.1 0.150.150.15  (a)  CN 1 2 3  N um  be ro  f im  ag es  Greedy DS Greedy Con ILP DS ILP Con       0.0750.0750.075 0.10.10.1 0.150.150.15  (b)  CN 1 2 3  N um  be ro  f im  ag es  Greedy DS Greedy Con ILP DS ILP Con       0.0750.0750.075 0.10.10.1 0.150.150.15  (c)  Figure 5. Sizes of the connected dominating sets obtained by the greedy algorithm and by solving the ILP, with different thresh- olds for image similarity and different cover numbers. Datasets (a) DITREVI, (b) DUOMO, and (c) OLDTOWN.

is smaller when the similarity threshold is lower (and hence the graph is denser), and also when a higher cover number is chosen. In some cases the optimal DS was already con- nected, in which case it is also the globally optimal CDS.

In the remaining cases, and always for the greedy solutions, no optimality guarantee can be given. We also observe that the greedy DS often needs fewer additional vertices to turn it into a CDS; the DS returned by ILP are ? 5% smaller than those of the greedy method. That difference slightly decreases for the respective CDS solutions. Apparently the redundancy in the greedy solution means that it is closer to a CDS, and fewer vertices have to be added.

Image Retrieval - correctly localized images (of 300)  DITREVI Greedy ILP Thr / CN 1 2 3 1 2 3 0.075 140 151 160 137 144 156 0.100 153 165 160 156 156 163 0.150 171 169 169 169 174 172  DUOMO Greedy ILP Thr / CN 1 2 3 1 2 3 0.075 35 62 57 25 59 69 0.100 68 81 83 67 78 79 0.150 75 86 91 78 87 90  OLDTOWN Greedy ILP Thr / CN 1 2 3 1 2 3 0.075 82 102 113 81 94 107 0.100 119 111 123 112 121 123 0.150 121 123 131 124 124 135  Table 1. Evaluation of location recognition using image retrieval with geometric verification, with different thresholds for image similarity and different cover numbers.

3.2. Image Retrieval  Following the same procedure as for the dataset images, we extract SURF [2] features from the query images (re- sampled to 3Mpix resolution), quantize them with a pre- trained visual vocabulary, and represent images by tf-idf vectors [27]. Our database is formed by the union of the minimum CDS of all three image collections (DITREVI, DUOMO, and OLDTOWN). Similarity between each query image and all database images is computed as the scalar product of their tf-idf scores. The five database images hav- ing the highest score are passed to geometric verification with a homography [13]. The original SURF descriptors are matched with approximate NN-search [30] and the homog- raphy is then computed with RANSAC [7], with an error threshold of 4 pixels. Among those candidates for which the fit has at least 100 inliers and an inlier ratio better than 10%, the one with the highest inlier ratio is selected. The localization is considered correct if the selected candidate is from the same collection (i.e. the same landmark).

Results summarized in Table 1 show that, over all cover numbers and image similarity thresholds, the numbers of correctly localized images are roughly the same for the keyframe sets obtained with the greedy algorithm and those from the ILP solution, although the latter are more compact.

The numbers of incorrectly localized images were around 8 for DITREVI and around 11 for DUOMO and OLDTOWN.

Regarding storage, the footprint of the database is ? 70kB per image for the zipped tf-idf vectors, plus ? 500kB per image to store the image itself (for efficient on-the-fly extraction of SURF features). The size of the visual vocab- ulary, independent of the dataset size, is 108MB.

Pose Estimation - correctly localized images (of 300)  DITREVI Greedy ILP Thr / CN 1 2 3 1 2 3 0.075 188 206 199 176 210 218 0.100 213 226 230 221 224 230 0.150 222 229 230 235 230 234  DUOMO Greedy ILP Thr / CN 1 2 3 1 2 3 0.075 66 73 80 62 63 78 0.100 74 90 84 75 86 82 0.150 85 97 93 94 94 92  OLDTOWN Greedy ILP Thr / CN 1 2 3 1 2 3 0.075 65 84 85 63 64 76 0.100 85 122 151 85 n/a 148 0.150 134 154 152 133 153 134  Table 2. Evaluation of location recognition using pose estimation w.r.t. the 3D point cloud of the largest partial model, with different thresholds for image similarity and different cover numbers.

3.3. Pose Estimation  VisualSFM [31] was used to create 3D models of the selected keyframe sets. The sizes of the largest recon- structed partial models are small for low similarity thresh- olds, because with such strict thresholds the image collec- tions disintegrate into many small, unconnected fragments.

We note however that with one exception (OLDTOWN, im- age similarity 0.1, cover number 2) all 3D models are visu- ally correct, albeit incomplete?they capture only the most prominent part of the scene. Dataset DUOMO seems to be the most challenging one because several separate clus- ters of images exist, e.g. the frontal view, the building in- terior, and the view from the roof. Consequently, only the largest cluster?the frontal view?is reconstructed. The re- construction of OLDTOWN becomes quite complete when a sufficiently high threshold for image similarity is used.

We used VisualSFM ?resume? feature to test whether it is possible to connect a given query image to the con- structed landmark models. The numbers of correctly local- ized images summarized in Table 2 show that the results are again roughly the same for the keyframe sets obtained us- ing the greedy algorithm and those from the ILP solution.

In these experiments, not a single image was wrongly lo- calized (i.e. connected to the wrong landmark model). One can also see that for the easy DITREVI dataset, the results of localization using pose estimation are better than those obtained from localization using image retrieval. On the other hand, localization using image retrieval works better than via pose estimation for the most complex dataset OLD- TOWN, because of the difficulties of building 3D models of complicated scenes from reduced, overly sparse image sets.

CN 1 2 3  N um  be ro  f im  ag es  Greedy DS Greedy Con ILP DS ILP Con      0.050.050.05 0.0750.0750.075 0.10.10.1  Figure 6. Sizes of the connected dominating sets obtained by the greedy algorithm and by solving the ILP, with different thresh- olds for image similarity and different cover numbers for dataset DUBROVNIK.

Regarding storage, the most salient SIFT features ex- tracted by VisualSFM occupy ? 1MB per image. The size of the reconstructed 3D model is dependent on the scene type and the spatial distribution of cameras; for our datasets it was approximately 30kB per reconstructed image.

3.4. DUBROVNIK Dataset  Finally, the performance and scalability of the proposed selection method was tested using the publicly available DUBROVNIK dataset [18] which comprises 6,044 database and 800 query images, with known ground truth poses w.r.t.

the 3D point cloud constructed from the database images.

As only SIFT [20] descriptors are provided and not the orig- inal images, we trained a 200,000 word vocabulary from the database image descriptors. The sizes of the obtained mini- mum DS and CDS are shown in Figure 6.

Notice that (i) the graph is similar to the one of the OLD- TOWN dataset but the overall portion of selected images is nearly twice as large due to the even more complex layout of DUBROVNIK; and (ii) lower thresholds for image sim- ilarity were used because for higher thresholds the removal of uncoverable vertices during pre-processing already elim- inated the majority of the images.

Run times are reported for the minimum DS search of our multi-threaded MATLAB implementation of the greedy algorithm and MOSEK [21] ILP solver. Times were mea- sured on a single Intel(R) Core(TM) i7-3930K workstation with Linux 64bit operating system, see Table 3.

We point out that the run times of the greedy algorithm and the ILP solver are similar but there is a much higher variance in the run times of the ILP solver. Also, in general the greedy algorithm gets slower with higher cover number, whereas the time needed to solve the ILP stays the same or even decreases.2  2This need not be the case for other problems: when applying the pro- posed method to very sparse graphs we observed much longer run times of the ILP solver.

Greedy Thr / CN 1 2 3 0.050 7.43 s 11.71 s 13.98 s 0.075 10.07 s 13.10 s 11.87 s 0.100 4.79 s 5.21 s 3.84 s  ILP Thr / CN 1 2 3 0.050 48.24 s 4.20 s 8.63 s 0.075 5.96 s 15.30 s 0.83 s 0.100 0.26 s 0.21 s 0.25 s  Table 3. Run times (in seconds) of the search for the minimum DS of DUBROVNIK; multi-threaded MATLAB implementations of the greedy algorithm and MOSEK [21] ILP solver.

Pose Estimation - registered images (of 800)  DUBROVNIK Greedy ILP Thr / CN 1 2 3 1 2 3 0.050 679 724 729 669 705 731 0.075 719 737 734 717 736 732 0.100 644 687 680 650 658 677  Table 4. Evaluation of location recognition using pose estimation w.r.t. the 3D point cloud of the respective submodel, with different thresholds for image similarity and different cover numbers.

For each of the obtained subsets of images, a submodel was created by removing from the 3D model the unselected cameras, as well as 3D points which were left with less than two projections. Then, average SIFT descriptors of the 3D points in the submodel were computed using the remain- ing projections, and tentative 3D-to-2D matches between the submodel and each of the 800 query images were cre- ated with approximate NN-search. The poses of all query images with more than 20 tentative matches were estimated using P4Pfk (P3P with unknown focal length, [3]) in a RANSAC loop, with reprojection error threshold of 2 pixels and the maximum number of iterations limited to 200.

We use the same methodology as [18] and report the number of query images which attained at least twelve in- liers supporting the recovered pose, see Table 4. The num- bers of registered images are again roughly the same for the keyframe sets obtained using the greedy algorithm and those from the ILP solution, and also similar to the results presented in [18]. Notice that larger subsets do not always yield better registration performance.

Regarding localization accuracy, the median camera po- sition error is around 0.9m with quartiles 0.25m and 4.5m, independent of both the threshold for image similarity and the cover number; an exception being the smallest subset (image similarity 0.05, cover number 1) where localization is less accurate. Similar to [18], there are around 100 cam- eras which have position errors larger than 20m.

4. Conclusions  We have shown that the search for the minimum dom- inating set of a graph?the problem which needs to be solved when selecting a compact, yet meaningful subset of a large image collection?can be formulated as an integer lin- ear program, and solved efficiently by current ILP solvers.

When comparing the sizes of keyframe sets obtained by the proposed method with those obtained with the state-of-the- art greedy algorithm, we found a gap of about 5%. This result empirically vindicates the greedy method, which per- forms surprisingly close to optimal, and much better than what could be expected from the theoretical guarantees.

Apparently the structure of visual databases is favorable for greedy keyframe selection. Still, the optimal solution deliv- ers consistently smaller keyframe sets, which could be sig- nificant for application scenarios with strict memory limita- tions. As an application example, we have shown that there is no perceptible difference between the greedy and optimal keyframe sets in location recognition problems, both for lo- cation recognition based on image retrieval with geometric verification, and for pose estimation w.r.t. a pre-computed 3D point cloud.

Acknowledgements  The authors would like to thank Vojte?ch Franc for dis- cussions about the ILP formulation of the CDS problem, and all Flickr users whose images were used in the project.

