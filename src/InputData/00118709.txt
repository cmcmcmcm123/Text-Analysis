An Extended Model of the Neocognitron for Pattern  Partitioning and Pattern Composition*

Abstract  In this paper we propose two neuron growth rules: the neuron splitting rule and the neuron formation rule for self-organization of adaptive neural networks. Using these two rules we extend the model of the Neocognitron such that when presented with part of a stored pattern or a composite image consisting of many stored patterns, the pattern will be recognized properly. The extended model does pattern partitioning when it recognizes that the input image is a part of a stored pattern, and pattern composition or association when many stored patterns are presented at the same time. This extended Neocognitron is demonstrated by computer simulation. W e  suggest that the mechanism proposed in this paper is a plausible model for child cognitive development and useful for visual pattern recognition.

Introduction  The Neocognitron is a neural network model which was designed as a mechanism for visual pattern recognition by Fukushima [3][4]. In this model, several layers of cells are arranged in a cascade such that the receptive field of each layer increases with the depth of the layer. The whole process of pattern recognition using the Neocognitron can be divided into two serial stages: a training stage and a recognition stage. During the training stage, a set of stimulus patterns are presented repetitively, and the network learns these patterns by modifying the efficacies of synaptic connections in an unsupervised way.

Each cell in the deepest layer or back layer represents a learned pattern. During the recognition stage, if a pattern presented in the input layer fires a cell in the back layer, the model recognizes the input pattern as the stored pattern represented by the firing cell. Once training is  * Preparation of this paper was supported in part by grant no. lROl NS24926 from the NIH (MichaeI Arbib, PrincipaI Investigator).

finished, there is no further modification of synaptic weights in the Neocognitron. The elegant property of this process is that the pattern recognition is not affected by a shift in position or small distortion of the input pattern.

This constitutes the main feature of the Neocognitron, compared to the early Cognitron model proposed by the same author [ZI.

However, the Neocognitron performs pattern recognition on one pattern at a time. All patterns stored in the back layer are individual patterns and no relation can be established among these patterns. Also if we present a part of a learned pattern to the network, the typical response of Neocognitron is to recognize it as the whole stored pattern. This response may be appropriate for pattern classification, but it is not desirable for more accurate pattern recognition. In short, the Neocognitron is not yet a proper model for pattern recognition in a more complex visual context in which several patterns or a part of pattern are presented as a single visual stimulus. In his recent work, Fukushima proposed an extended model of the Neocognitron intended to respond to a composite figure consisting of multiple patterns [61[71. The performance of this extended model when presented with a composite image is to respond to each pattern separately.

This mechanism obviously neglects the wholeness of the presented composite pattern. That is, the network loses the information that these individual patterns are presented at the same time.

In this paper, we focus on the pattern recognition in a complex visual context using the Neocognitron, and propose two neuron growth rules: the neuron splitting rule and the neuron formation rule in order to do pattern partitioning and pattern composition respectively. With these two neuron growth rules, the Neocognitron can dynamically grow at the stage of recognition and thus enhance its recognition capability with practice.

11-267    Specifically, we extend the Neocognitron model in the following aspects:  when only part of a stored pattern is presented, the extended model partitions this part from the whole stored pattern.

when a composite image which consists of several stored patterns is presented, a new cell is formed in a composite pattern layer (UC-layer) to represent this image. The UC-layer is a new layer formed in the proposed model which connects to the back layer of the Neocognitron. Figure 1. Schematic diagram illustrating the synaptic co~ections  between layers in the Neocognitron (from [31) once the model stores a composite image, it associates all patterns in a composite image if we present a subset In the case of e=1 in Eq.(l), Ucel(kel,n) stands for U&), of the composite image. and we have Ke-l=l.  Here, ae(ke-l,v,ke) and be(ke)  Basic Structure of the Neocognitron  The Neocognitron is composed of a cascade of modular structures preceded by a input layer U0 Each of the modular structures is composed of two layers, a Us layer consisting of S-cells, and a UC layer consisting of C- cells. S-cells or C-cells in any single layer are further divided into many cell-planes, each of which detects a different optimal stimulus feature. It is assumed in Neocognitron that all the cells in a single cell-plane have  represent the efficacies of the excitatory and inhibitory modifiable (learning) synapses, respectively. Because all the S-cells in the same S-plane are assumed to have identical sets of afferent synapses, w(kel,v,ke, and b@p) do not contain any argument to indicate.the position n of the receptive field of cell Use(ke,n). The parameter re is a constant used to control the inhibition due to Vcel(n), which sends an inhibitory output to Use(kp,,n). The output of Vccl(n) is defined as:  inputs of same spatial distribution, and only the positions of the input cells are shifted in parallel depending on the positions of receiving cells. Figure 1 shows a schematic diagram illustrating the synaptic connections between layers. Each tetragon drawn with heavy lines represents an Splane or a C-plane, and each vertical tetragon drawn with thin lines, in which S-planes or C-planes are enclosed, represents an Slayer or a C-layer.

c cel(v)*Uc~l(kel, n+v) (2) k 1 = 1  VE st  vccl(n) =  The efficacies of the unmodifiable synapses ce-l(v> are determined so as to decrease m o n o t o ~ c ~ y  with respect to I v I, and to be normalized.

The connections from S-cells to C-cells are all Umodifiable. The output of a C-Cdl in the k-fh C-plane of the &th module is given by (3).

In the Neocognitron, U&) denotes the output of the input layer, where n iS  two two-dimensional coordinate vector indicating the location of the cell. The output of an S-cell of the k@h Splane in the e-th module is given by  where $[XI = max(x, 0).

11-268    x > o X I 0  where ~ [ x ]  =  The parameter a is a positive constant which determines the degree of saturation of the output.

The inhibitory cell V,e(n), which sends an inhibitory signal to the C-cell in order to form lateral inhibition, yields an output:  V,e(n) = - c C%(v).~~e(kt,n+v) (4) 1 % % kF1 VE De  In Eqs. (3) and (4), the efficacies of unmodifiable synapses de(v) are determined so as to decrease monotonically with respect to I v I in a manner similar to ce(v), and De indicates the connection area.

Two Hypotheses for neuron growth  It is widely accepted that animal learning behavior is due to the modification of synaptic weights. However, this may not be the only mechanism which underlies learning.

Blakemore and Cooper [l] observed that when kittens were reared in an abnormal environment, these animals did no have any cortical neurons responding to certain stimuli, and their cortices were smaller than those of normal animals. This suggests that the size of visual cortex is affected by visual experience during development. In accordance with their experiment, we suggest two hypotheses for the growth of neurons, by which experience can determine the g r o w t h  of an adaptive neural network. The two hypotheses are:  1. Neuron sulittinv rule  We call a synaptic pathway a prominent pathway if the efficacy of the synaptic connection is remarkably large.

A cell N is split into two cells if and only if a true subset of all of its prominent pathways have remarkable inputs. This can be described more clearly as: suppose N receives inputs XI, ..., x, through pathways W1, W2, ..., W, respectively, and the first m pathways are prominent, i.e., W1, W2, ..., Wm >> 0. If only a part of the inputs {XI, x2, ..., xm), say [xal, Xa2, ..., xak) are firing, then N is split into two cells: NI and N2, cell NI receiving {Xal, xa2, ..., Xak) as well  as {xm+1, ..., x,), N2 receiving {XI, x2, ..., xm] - {Xal, Xa2, ..., xak) as well as [xm+1, ..., xn). Both NI and N2 inherit all efferent connections from N. This splitting process is shown in Fig.2A.

If we consider all prominent inputs to N as individual features and the task of N is to combine these features to form a new pattern with a larger receptive field, this splitting rule states that if part of the pattern that neuron N is supposed to recognize appears in the visual field, this neuron will be split into two cells, one of which recognizes the presented part of the pattern while the other recognizes the rest of the pattern. This rule implies that the visual cortex will grow if the visual system memorizes complex visual patterns and then is presented with simpler patterns.

2. Neuron formation rule  A new neuron N is formed if and only if more than one neuron is firing at the same time in a particular layer.

Let yi(t) represent the output of a cell Ni at time t .  For neurons NI, N2, ..., Nk (k > 11, if yl(t) > 0, y2(t) > 0, ......, yk(t) >Oat some time t ,  a new cell N will be formed which  A  I x1 %. +- w?  ... ...

I Xm  B  T ... ...

Figure 2. A.Neuron splitting rule. If a true subset of all of the prominent pathways of N has remarkable inputs, N is splitted into two neurons: NI and Np.  B. Neuron formation rule. A new neuron N is formed if NI, N2, ... , Nk are firing at the same time.

11-269    receives NI, N2, ..., Nk as its inputs. This formation process is shown in Fig.2B.

If we consider each neuron NI, N2, ..., Nk as representing a different feature, the newly formed neuron N represents the pattern consisting of all the features represented by NI, N2, ..., Nk. The cell N reflects the concurrence of features represented by NI, N2, ..., Nk, because it is formed only when these neurons fire at the same time.

In the following sections we will apply these two rules to the Neocognitron in order to endow it with the extended recognition capabilities in a complex visual context.

Pattern Partitioning  The extensions proposed in this paper influence the recognition stage, i.e. after learning is finished. We only apply the neuron splitting rule in the last module where the receptive field of each cell is almost as large as the entire input layer. Suppose USL is the last Slayer and UCL is the last C-layer. Each cell in USL receives input from one area of cells in each plane of UCL-1. Each such plane represents a different feature. To use the neuron splitting rule, each C-plane in layer UCL-1 is considered a single input pathway to USL, and the input connection whose weight has been strengthened during learning stage is considered a prominent pathway. The S-plane in USL will be split into two S-planes if only some of the prominent features are presented. Since each C-plane pairs with an S- plane, once an S-plane is split the corresponding  output is chosen as a candidate for the representatives. If two or more candidates appear in a single S-plane, only the one yielding the largest output among them is chosen as the representative from that S-plane. Quantitatively,  UsL(kL', n') = maxn(maxkL(USL(kL, n))) (5)  2. Detect Implicit Features  Each representative USL(kL', n') represents a detected feature of the input pattern. Presynaptic inputs to any representative are features with smaller receptive fields which will be integrated in the representative cell.

Before applying the splitting rule, we determine which inputs to a representative cell are prominent features and whether the prominent features are explicit or implicit. A prominent feature is exp2icit if this feature is present in the current input pattern. Otherwise the prominent feature is called implicif feature.

Discover prominent features: for any plane kL-1 in layer UCL-1, according to (11, the expected contribution of plane kL-l to a representative UsL(kL', n') is given by  Hence, kL-l is a prominent feature if and only if Ce(kL-1, kL', n') > 0.

Discover implicit features: for any plane kL-1 in layer UCL-1, according to (l), we compute its actual contribution to USL(kL', n') as  C-plane splits too. More precisely, the following steps are taken during pattern partitioning.

1. Choosinv Representatives  Given Ce(kL-1, kL', n') and Ca(kL-l, kL', n'), an implicit feature satisfies Ca = 0 and Ce > 0, recorded as f(kL-1) = 0, and an explicit feature satisfies Ca > 0, recorded as f(kL-l) = 1.

3. Splitting Process To partition a pattern, the part of a stored pattern  being presented in the input layer must be determined.

The presented pattern can be determined using the same procedure as choosing representatives during training stage. From each %column in the layer UsL. in to the stimulus pattern the S-Cd yielding the largest  If there is a plane kL-1 such that f(kL-1) = 0, splitting is done as follows: create a new Splane k i '  which copies plane kL' and its inputs and outputs, also create a corresponding C-plane which receives inputs from the  11-270    kL"-th plane of USL in exactly the same way as the kL'-th plane of UC- receives inputs from the kL'-th plane of USL.

In plane kL' of USL, set aL(kL-l, v, kL') = 0 if f(kL-1) = 0; in plane kL" of USL: set aL(kL-1, v, kL") = 0 if f(kL-1) = I. In addition, some modifications are needed to maintain the original hierarchy of the Neocognitron.

Every time the splitting process is executed, a new plane is created. In this way the Neocognitron grows dynamically with experience, yielding a new form of self- organization for neural networks, different from synaptic adaption. Once a new plane in USL is generated, the number of stored patterns is increased by 1, thereby expanding the storage capacity of the Neocognitron.

Pattern Composition  In this section we will study how to extend the Neocognitron so that it can give desired response when presented with more than one pattern. We call this kind of input a composite image. A new layer, called the composite layer (Uc for short), is added to the Neocognitron as the deepest layer which connects to layer UCL. Each cell in the UC layer represents a composite image and is assumed to be binary. With the Uc layer, this model can recognize and associate composite images appearing in the input layer.

1. ComDosite Imaee Creation  The neuron formation rule is used to create cells in UC layer. In the Neocognitron, if more than one stored pattern is presented to the input layer, following the lateral inhibition rule of (4) which is expressed as an arithmetic mean, there will be more than one cell firing in layer UCL, each of which corresponds to each single pattern, provided that the strength of a single pattern is larger than the arithmetic mean of all inputs to the UCL layer.

Suppose, at a certain time, the group of firing cells in UCL are (Cl, ..., Cn). Following the neuron formation rule, if n > 1, a new cell C in UC is formed which receives (Cl, C2, ..., Cn) as its inputs. C represents the composite image (Cl, Cp, ..., C,,). Define input(Cj = [Cl, C2, ..., C J .

2. Composite Imaee Recognition  Here we suppose that only when all its inputs are firing a cell in UC layer is activated. This is expressed in the following formula:  v c E U, (C fires iff image 2 input(C)) (8)  Hence there is a possibility that more than one cell fires when an image is presented. If so, it implies that the current image contains more than one previously stored image. If there exists a C such that (input(C) = image), the current input image doesn't form any new cell in UC layer.

3. Association  Each cell in the UC layer represents a composite image, in order that the extended model can recall the whole image if only part of a composite image is presented, we introduce recurrent connections from the uc layer to layer UcL for each connection from UCL to UC.

Fukushima proposed an extended model of the Neocognitron which can exhibit association 151. The association proposed in this paper differs from his in that our association is among different patterns and his association is within a single pattern.

Association appears if and only if there is no firing cell in layer UC and no new cell is created. This occurs when the current input image is a part of some stored  image. In this case, the system associates the current image with the whole stored image. Because an image is formed when a number of input patterns are presented concurrently, it is possible that a certain input image is contained by more than one stored image. A competition appears among these stored images to select only one image. Two factors are considered crucial for this kind of competition: (1) how much the stored images resemble the current image; (2) how often the stored images have been presented before. Let Q1,12, ......, In) represent the set of competing composite images. We measure:  (9)  Where t(I$ is the number of times that image Ii has been presented before. Hence we can find an Ik such that strength&) 2 strength&), i = 1, ..., n, and input(Ik) will be recalled in layer UCL.

Computer Simulation  The extended Neocognitron model with the mechanisms for pattern partitioning and pattern composition was simulated on a Sun workstation. The originally implemented Neocognitron was a seven layered network: U O ~ U S ~ ~ U C ~ ~ U S ~ - S U C ~ ~ U S ~ ~ U C ~ .

In this simulation, we implemented a four layered  11-27 1    network UC~US~+UC-+UC, where layer UC is the new layer added for pattern composition. These four layers are sufficient to demonstrate the extension proposed in this paper, because as we described earlier, pattern partitioning  . f * * * t * t  * * x XXx*  * X X X ' * X X X ' * x XXX' * * f * (I * * I)  uc2: t *** / f * t t ' X  f ' X * x  + * x  * I *  * .

**.*+***  * * * * * * f t  f xxx* * x X'  x X' * xXX* * * t *  .****lit+  *******.

XX *  x X' * x *  X ' xxxx*  * * ***.****  * * * * * * * * f xx  x X' x *  * X ' * XXXX' t .

It******  It..****  * * * * . f  * .

* * * * *****I*.

**i t**  I )****  t*ttt ***..

* X '  * x *  t *et*, t**t+ **e.* * t a t *  Figure 3. The two patterns shown in the uppermost layer are the stored patterns in the Neocognitron. Three features are detected in layer U C ~ ,  and pattern "10 and "2" activate two cells in Uc3 respectively.

and pattern composition only appear in the last layer.

Another simplification is that we only implemented four planes in each layer.

We assume that during the training stage, two training patterns "10" and " 2  shown in the upper layer of Fig3 are presented repeatedly in layer U0 and stored in the modifiable synaptic weights. We assume further that three features "l", "0, " 2  are recognized respectively by the first three C-planes in Uc2, as shown in Fig.3.

In Fig.4 we test a main feature of the Neocognitron: shift-invariance. Pattern "2" is presented at four different positions of the input layer. The model recognizes all four presentations as the same stored pattern "2.

****.*** *Ill.*.** * * M *  ' X X  ' X X ' X X '  * x * x  ' X  " f x 1 'XXXX *xXXx * * *tt.t**t ********  ***.**** xx  x X' X '  X ' * XXXX' t * * " * " *  ******** * *  x x + + x x*  X ' x *  f XXXX' a+*** .**  uc3: **tt* til.,. "**** *I**.

* t * x .  .

*.*.e ****I e*.*. *.*.*  Figure 4. When pattern "2" is presented at four different positions to the input layer, as shown in the upper part of this figure, the same cell in layer Uc3 is activated.

Fig.5 shows pattern partitioning. If we only present pattern "l", our model detects that "I" is a part of the stored "lo", and the pattern partitioning procedure will split "10" into two separate patterns: "1" and "0". The planewhich represents "10" is splitt into two planes, which represent "1" and "0" respectively. If "1" is later  uc2: ttt***t*  * x  f ' X  f * x  f ' X  *  t t  . * *.**..*.

..It*  I*..****  t .

f XXX' * x X' f x X' * XXX' * * .t**t..t  **t*t.t*  t f  * f * . I  t f  * * f ,  *,*.****  /,***..* f f  f .

t .

* * I .

. * t*..t.*t  * f t f f  * f f * /  .*ft.

t f  * f  f .

f * t * .  I**** f.tff  Walt inq u n t i l  i n p u t  p a t t e r n  i s  ready  uc2: *t*.t**t ' X  + ' X  * ' X  f * x  f . f  * I  .***.e*.

**.****.

t t  ( I f  t t  f t  * t  f .

.**.*.**  .*.f*f.*  1 .

f f  f f  t .

t .

f f  **t.t*tt  .*..*.If  * .

* f  t +  * .

* f ( I f  ...*.**.

uc3: f * f * f  l t f t f  I*.** ff...

' X '  . t 1 f * * e * .  t i t l i t  .**+* ** . *e  Waiting u n t i l  i n p u t  p a t t e r n  is ready  ttt*.*.*  * XXX' * x X' * x X'  XXX' t 1  f t  f t f f f * . / f  f * f f f * f I  . * I *  * I  f f  f .

f .

****..*.

t.*..t.t . f f t  t f  * * ( I *  f .

**. tl...

Figure 5. Pattern partitioning. upper: When pattern " 1 0  is presented, the first cell in U C ~  is activated; middle: when a part of "10, 'I", is presented. lower: When pattern "0, the rest of the original pattern "10, is presented.

presented one cell in Uc3 is activated, and if "0" is presented another cell in Uc3 is activated.

Figd shows pattern composition. After the pattern partitioning process for the pattern "lo", if we present composite images " 0 2  "12", according to the neuron formation rule, two cells in UC are generated to represent these composite pattems. If "02  is presented again, the cell representing it in UC fires, which shows that the extended model can recognize the composite image.

Fig.7 shows associative memory. The pattern "102" was presented earlier and stored as a composite image.

Now "0' and "10" are presented. As shown in the Figure, no cell in Uc fires because these patterns are not whole composite patterns. However these patterns activate the association process. The whole stored pattern "102" is recalled in layer Uc3.

Discussion  The proposed extension to the Neocognitron based on two neuronal growth rules provides a mechanism for pattern recognition and self-organization in the brain.

Generally speaking, the neuron splitting rule implements a mechanism for specialization of general pattern  11-272    I . . . ** . ( .  ...e. f t f  ...*.*/. ti , .****  Waiting u n t i l  input p a t t e r n  i s  ready  uc2: . I . * * . * .  .* .*...* ***,***.

. X  * I f f X X ' ' X  * * t * x x .

' X  . f f * X ' . X  * * f * x *  * * XXXX' t f .

f f "  f 1  I  I t*( .* . . .  . . f t f t * .  I..*tt**  Figure 6. Pattern composition. upper: After splitting of pattern "10, when we present a composite image "02"; middle: when we present another image "12; lower: when a previously presented image "02" is presented again.

memories while the neuron formation rule provides a mechanism for generalization of specific pattern memories that occur in images. Because these two processes are critical for cognitive development in children [8], we suggest that these two rules might be used in child cognition development , during which the experience may affect the formation of nervous system directly.

With the mechanisms proposed in this paper, the extended model enhances the pattern recognition ability of the Neocognitron considerably. Pattern recognition in a complex context in the sense that both parts of a stored pattern and composite images can be recognized appropriately is now possible. Another feature of this model, in contrast to the Neocognitron, is that it combines learning and pattern recognition. Learning and pattern recognition is a whole process and cannot be separated in the human cognitive process.

ASSOCIATED ****I ..ff* I**** *..** t t * t * ' X *  ' X *  * * 1 ' X '  uc3: ** f t .  *.**I * * * * *  (1.1.. * t t * *  Waiting u n t i l  input p a t t e r n  is ready  I . .*****  * * * xxx* * x X' * x X'  XXX' * * (I.*.*..*  ****.*.* .*t*ttt* * * I t t .

(I * *  t f t f  I  f t ,  (I  t *  I * * * * * . *  tl.*tft*  Figure 7. Image association. upper: After splitting of pattern "10, when a composite image "102" is presented, three cells in Uc3 are activated, so a cell is created in UC. middle: when pattern " 0  is presented; lower: when pattern "10" is presented.

Reference  C. Blakemore and G. F. Cooper, "Development of the brain depends on the visual environment," Nature (Lond.), vol. 228, pp.477-478,1970.

K. Fukushima, "Cognitron: A self-organizing multilayered neural network," Biol.Cybern., vol. 20, pp.121-136,1975.

K. Fukushima, "Neocognitron: A self-organizing neuron network model for a mechanism of pattern recognitiron unaffected by shift in position," Biol.

Cybern., vol. 36, pp.193-202,1980.

11-273    [4] K. Fukushima and S. Miyake, "Neocognitron: A self- organizing nerual network model for a mechanism of visual pattern recognition," In: Cooperation and Competition in Neural Networks, S.Amari and M.A.Arbib (eds). Springer-Verlag, Berlin Heidelberg New York, 1982.

[5l K. Fukushima, "A hierarchical neural network model for associative memory," Biol.Cybern., vol. 50, pp.105- 113, 1984.

[6] K. Fukushima, "A neural network model for selective attention in visual pattern recognition," Biol.Cybern., vol. 55, pp.5-15,1986.

PI K.Fukushima, "A neural network model for selective attention," in Proceedings of the IEEE first Diego, 1987, pp.II.11-18.

[8] J. Piaget, The Principles of Genetic Epistemology.

Wolfe Mays, London, Routledge & Kegan Paul, 1972.

