Locally Smoothed Modified Quadratic Discriminant Function

Abstract?Modified quadratic discriminant function (MQD- F) is a state-of-the-art classifier for handwriting recognition.

However, the big gap between accuracies on training and testing sets indicates that MQDF has a good capability to fit training data but the generalization performance is not promising. To solve this problem, we propose a new mod- el called locally smoothed modified quadratic discriminant function (LSMQDF) by smoothing the covariance matrix of each class with its nearest neighbor classes. LSMQDF can be viewed as a regularization to avoid over-fitting. The covariance matrix estimated by local smoothing is more accurate and robust. LSMQDF can be also viewed as an extension of the global smoothing method, namely regularized discriminant analysis (RDA). Experiments on both offline and online Chinese handwriting databases demonstrate that: with local smoothing, the accuracy on training set is decreased (over-fitting avoided), and the accuracy on testing set is improved significantly and consistently (generalization improved).



I. INTRODUCTION  The modified quadratic discriminant function (MQDF) [1] has been applied prevalently and successfully in handwritten Chinese character recognition [2] [3] over the past 25 years.

However, the large number of free parameters of MQDF usually lead to over-fitting of the training data. To solve this problem, this paper proposes a local smoothing technique as a regularization to obtain a more robust and accurate estimation of the covariance matrix. The local smoothing can alleviate the over-fitting problem and improve the gen- eralization performance of MQDF.

A. Quadratic Discriminant Function (QDF)  In Bayes decision theory, the quadratic discriminant function (QDF) is derived from the assumption of class- conditional Gaussian distribution  p(x|i) = exp [? 12 (x? ?i)???1i (x? ?i)]  (2?) d 2 |?i| 12  , (1)  where ?i ? Rd and ?i ? Rd?d are the mean vector and covariance matrix of class i respectively. Considering a M -class classification problem, a pattern x is classified into the class of maximum a posterior (MAP) probability x ? argmaxMi=1 p(i|x) = p(i)p(x|i)p(x) , where p(i) is the prior probability and p(x) is the mixture density function.

When assuming equal prior probabilities, the MAP decision  rule becomes x ? argmax p(x|i) which is equivalent to x ? argmin? log p(x|i). Therefore QDF is defined as:  fQDF(x, i) = (x? ?i)???1i (x? ?i) + log |?i| , (2) which is actually a distance metric between x and class i:  x ? class arg M min i=1  fQDF(x, i) . (3)  The performance of QDF is highly dependent on the computation of the inverse matrix ??1i . The covariance matrix ?i is usually singular and under-estimated due to the estimation errors and the sensitivity of the estimation to small sample size. Therefore QDF can not achieve high accuracy in real applications. To achieve an accurate and robust estimation of ??1i , many improvements have been proposed, among which the modified quadratic discriminant function (MQDF) [1] is the most effective one for large category Chinese handwriting recognition.

B. Modified Quadratic Discriminant Function (MQDF)  By utilizing eigen-decomposition algorithms, the covari- ance matrix can be diagonalized as ?i = ?i?i??i , where ?i = diag[?i1, ? ? ? , ?id] with ?ij ? R+, j = 1, ? ? ? , d being the eigenvalues (ordered in decreasing order) of ?i, and ?i = [?i1, ? ? ? , ?id] with ?ij ? Rd, j = 1, ? ? ? , d being the ordered eigenvectors. Inserting the eigen-decomposed covariance matrix into QDF, we get fQDF(x, i) =  ?d j=1  ?ij  [??ij(x? ?i)]2 + ?d  j=1 log ?ij . The minor eigenvalues are usually prone to be underestimated due to small sample size, therefore MQDF [1] replaces the minor eigenvalues (?ij , j > k) with a constant ?i to stabilize the generalization performance  fMQDF(x, i) = k?  j=1  (  ?ij ? 1  ?i  )[ ??ij(x? ?i)  ]2  +  ?i ||x? ?i||2 +  k? j=1  log ?ij + (d? k) log ?i, (4)  where k denotes the number of principal axes. The above derivations utilize the properties of ??i ?i = I and ||x ? ?i||2 =  ?d j=1[?  ? ij(x ? ?i)]2. Compared with QDF, MQDF  involves only the principal eigenvectors and eigenvalues,   DOI 10.1109/ICDAR.2013.11     therefore, both the storage of parameters and the compu- tational complexity are reduced. More importantly, high- er classification accuracy can be obtained by setting the minor eigenvalues to be a class-independent constant as ?1 = ?2 = ? ? ? = ?M = ? 1Md  ?M i=1  ?d j=1 ?ij and using  cross-validation to select ? from [0, 1] on the training data.

Although MQDF is a state-of-the-art classifier for Chinese  handwriting recognition, many improvements have been proposed in the past 25 years. We first give a summariza- tion of the improvements on MQDF in Section II. After that, the motivation and definition of the newly proposed locally smoothed MQDF are described in Section III. The experimental results on both offline and online Chinese handwriting datasets are presented in Section IV, and the concluding remarks are drawn in Section V.



II. RELATED WORKS  Many improvements have been proposed for MQDF from different aspects, such as discriminative training, memory reduction, instance selection and generation, ensemble learn- ing, and discriminative feature extraction.

Because MQDF is a generative model, Liu et al. [2] proposed to use the minimum classification error (MCE) [4] criterion for discriminative training of MQDF. After that, the sample separation margin criterion [5] and perceptron criterion [6] were also adopted for this purpose. The re- training of MQDF based on instance importance weight- ing [7], instance selection [8] and virtual instance generation (mirror image) [9] can be also viewed as some kinds of discriminative training. Discriminative training of MQDF can directly optimize the classification boundaries, therefore, high classification accuracy can be achieved especially when the size of training data is large.

The memory requirement of MQDF is usually very large (e.g. 120MB for a 3,755-class and 160-dimension problem).

To reduce the memory requirement of MQDF, Long and Jin [10] proposed to use the vector quantization and split quantization techniques to compress the parameters (mean- s, eigenvectors and eigenvalues) of MQDF. Modeling the inverse covariance matrices by the expansion of some tied basis matrices was proposed by Wang and Huo [11]. The precision constrained Gaussian model was combined with the minimum classification error (MCE) training criterion to simultaneously compress the parameters and improve the accuracy [12]. With parameter compression, the high accuracy MQDF classifier can be embedded into some hand- held devices such as mobile phones and tablet computers.

There are also many other improvements of MQD- F derived from different viewpoints. The kernel MQDF was proposed by Yang and Jin [13] to extend MQDF from original feature space to kernel space (implicit high- dimensional space). The ensemble learning methods such as cascade classifier training [14], boosting [15] and pairwise discrimination [16] were adopted to improve the accuracy  of MQDF. The graphical lasso method was used to estimate a sparse inverse covariance matrix ??1i for QDF [17]. The normalization of the determinant of the covariance matrix was shown to achieve better classification accuracy [18]. The asymmetric Mahalanobis distance [19] can be viewed as an extension of the symmetric Gaussian distribution. Combin- ing MQDF with the discriminative feature extraction [20] can further improve the performance.



III. LOCALLY SMOOTHED MQDF Although MQDF is a generative model, it has a very high  accuracy on training set (over 99%). However, the accuracy on testing set is much lower (around 89% and 93% for offline and online Chinese handwriting recognition). This indicates that MQDF has a good capability to fit training data but the generalization performance is not promising.1  The covariance matrix estimated by maximum likelihood (ML) is usually under-estimated due to small sample size, and the large number of free parameters gives MQDF a strong memory of training samples (over-fitting). To improve the generalization performance of MQDF, we propose a new model of locally smoothed modified quadratic discriminant function (LSMQDF) by smoothing the covariance matrix of each class with its nearest neighbor classes. In the following sections, we first give a description of the ML estimation, and then present the details of LSMQDF. After that, we compare LSMQDF with a global smoothing method.

A. Class-wise ML Estimation Given a training dataset {xij ? Rd} (i = 1, ? ? ? ,M and  j = 1, ? ? ? , ni), where ni is the number of training samples in class i, and M is the number of classes. Let xij denotes the j-th training sample from class i. The function of the negative log-likelihood of the data from class i w.r.t. the mean ? and the covariance matrix ? can be defined as:  NLL(?,?, i) = ? ni? j=1  log p(xij |i)  ? ni? j=1  (xij ? ?)???1(xij ? ?) + ni log |?| .

(5)  The maximum likelihood (ML) estimation of ?i and ?i is conducted class-by-class (i = 1, ? ? ? ,M ) as:  {?i,?i} = argmin ?,?  NLL(?,?, i) . (6) By solving this convex optimization problem, the ML esti- mation is:  ?i =  ni  ni? j=1  xij , (7)  ?i =  ni  ni? j=1  (xij ? ?i)(xij ? ?i)? . (8)  1This explains why discriminative training of MQDF can only improve the testing accuracy slightly.

B. Locally Smoothed MQDF The class-wise ML estimation is proven to be sensitive  to small sample size (small ni), and the estimation error in covariance matrix (8) is more serious than the estimation error in mean vector (7). To achieve a robust estimation, we propose a local smoothing method:  ??i = argmin ?  (1? ?) NLL(?i,?, i) +  ?  K  ? j?KNN(i)  NLL(?j ,?, j) , (9)  where KNN(i) denotes the K nearest neighbors of class i from the remaining classes (defined by the Euclidean distance between the class means). The first term of (9) is the ML estimation and the second term is a local smoothing of the covariance matrix with the nearest neighbor classes. The ? ? [0, 1] is a tradeoff parameter. Because the estimation of the class-mean is accurate enough, ?i is fixed as the ML estimation (7). The new covariance matrix ??i is estimated to maximize the likelihood of not only class i but also the neighboring classes. Therefore, both the samples from class i and the neighboring classes are used simultaneously to achieve a robust estimation. This is based on the assumption that the covariance matrices of neighboring classes are close to each other, which is satisfied in handwriting recognition, because similar characters usually have the same types of distortion (variation).

The optimization problem of (9) is also a convex problem which has a closed-form solution:  ??i =  (1? ?)ni?i + ? 1K ?  j?KNN(i) nj?j  (1? ?)ni + ? 1K ?  j?KNN(i) nj  , (10)  where ?i is the ML estimation (8). From this definition, we can find that ??i is a smoothing of ?i with the neighboring classes ?j : j ? KNN(i). Therefore, ??i should be more accurate than ?i especially when ni is small. By using ?i (7) and ??i (10) to train the MQDF classifier (Section I-B), much better generalization performance can be achieved, and we denote this method as LSMQDF. With local smoothing, some discriminative information of the training data is lost, but the new covariance matrix ??i will be more robust.

Therefore, although the accuracy on training set is decreased (over-fitting avoided), the accuracy on testing set will be improved (generalization improved).

C. Local Smoothing V.S. Global Smoothing LSMQDF can be viewed as an extension of the global  smoothing method, namely regularized discriminant analy- sis (RDA) [21], which constrains the range of parameter values by interpolating the class covariance matrix with the common covariance matrix ?0 and the identity matrix I:  ??i = (1? ?)[(1? ?)?i + ??0] + ??2i I, (11)  where ?0 = ( ?M  i=1 ni?i)/( ?M  i=1 ni), ? i =  d tr(?i), and  0 ? ?, ? ? 1. With appropriate values of ? and ? empirically selected, RDA can improve the generalization performance, and some special cases are included: (i) o- riginal QDF: ? = ? = 0; (ii) linear discriminant function (LDF): ? = 0, ? = 1; and (iii) Euclidean distance: ? = 1.

Improvements can be achieved when using ?i (7) and ??i (11) to train the MQDF classifier (Section I-B), and we still denote this method as RDA for simplification.

RDA is a global smoothing method, while LSMQDF is a local smoothing method. When the number of classes is large, global smoothing can not achieve any improve- ment, while local smoothing is still effective to improve the accuracy. This is because we can not assume all the covariance matrices to be close to each other, and contrarily, the neighboring classes are more likely to have the same covariance matrices. The superior performance of LSMQDF is also verified by experiments in the following sections.



IV. EXPERIMENTS  We evaluated the performance of LSMQDF on two 3,755- class Chinese handwriting databases [22]: the offline hand- writing database CASIA-HWDB1.1 and the online hand- writing database CASIA-OLHWDB1.1. Both of them con- tain handwritten Chinese characters from 300 writers (240 for training and 60 for testing). Each writer has about 3,755 characters (one for each class). The extracted feature data can be downloaded from our website.2  A. LSMQDF for Offline Handwriting Recognition  For representing an offline character sample, we extract features from gray-scale character images (back-ground e- liminated) using the normalization-cooperated gradient fea- ture (NCGF) method [23]. The original feature dimension- ality is 512 which is further reduced to 160 by Fisher linear discriminant analysis (FDA).

For the LSMQDF method, we set K = 10 and ? = 0.5 for (10). Sensitive analysis of these parameters will be shown in the following sections. For the RDA method (11), we have ?0 = I (the common covariance matrix is I in the FDA transformed subspace), and now the RDA can be simplified as ??i = (1??)?i+??2i I and we report the best performance of RDA on testing set by searching ? from 0 to 1.

The accuracies of MQDF, RDA and LSMQDF on both training set and testing set with various numbers of principal eigenvectors are shown in Figure 1(a). We can find that: (i) with either global smoothing (RDA) or local smoothing (LSMQDF), the training accuracy is decreased (over-fitting avoided), and the testing accuracy is improved (generaliza- tion improved); (ii) LSMQDF outperforms RDA consistently and significantly on the testing set; and (iii) compared with the baseline MQDF, LSMQDF is effective to improve the  2http://www.nlpr.ia.ac.cn/databases/handwriting/Download.html     97.5  98.5  99.5  93.5  94.5  95.5  96.5  97.5  ur ac y? (% )  MQDF?Training MQDF?Testing  RDA?Training RDA?Testing  88.5  89.5  90.5  91.5  92.5  A cc LSMQDF?Training LSMQDF?Testing  87.5  5 10 15 20 25 30 35 40 45 50 55 Number?of?principal?eigenvectors  (a)  89 5   90.5   88.5   89.5  ac cu ra cy ?(%  )  86.5   87.5  Te st in g? MQDF  LSMQDF   50 60 70 80 90 100 110 120 130 140 150 160 170 Dimensionality  (b)  89.5  90.5  87.5  88.5  g? ac cu ra cy ?(%  )  85.5  86.5  Te st in g MQDF  LSMQDF  84.5  40 60 80 100 120 140 160 180 200 220 240 Number?of?training?samples?per?class  (c)  90.2  90.4  89 6  89.8   g? ac cu ra cy ?(%  )  MQDF  LSMQDF  89.2  89.4  89.6  Te st in g   3 5 7 9 11 13 15 17 19 21 23 25 27 29 Number?of?nearest?neighbors?in?LSMQDF  (d)  90.3  90.5  89.7  89.9  90.1  g? ac cu ra cy ?(%  )  MQDF  LSMQDF  89.3  89.5  89.7  Te st in g  89.1  0.1 0.2 0.3 0.4 0.5 0.6 0.7 Value?of??? in?LSMQDF  (e)       cu ra cy ?(%  )  MQDF?Training MQDF?Testing RDA Training RDA Testing    95A cc  RDA?Training RDA?Testing LSMQDF?Training LSMQDF?Testing   5 10 15 20 25 30 35 40 45 50 55 Number?of?principal?eigenvectors  (f) Figure 1. (a) Different models for offline handwriting recognition; (b) MQDF v.s. LSMQDF on different dimensions; (c) MQDF v.s. LSMQDF with different number of training samples; (d) LSMQDF with different numbers of K in (10); (e) LSMQDF with different values of ? in (10); (f) Different models for online handwriting recognition.

testing accuracy, e.g., from 89.53% to 90.27% when the number of principal eigenvectors is 50.

B. Varying Dimensionality  By using FDA to reduce the dimensionality from 512 to different subspaces, we compare the performance of MQDF and LSMQDF (K = 10 and ? = 0.5 for (10) and the number of principal eigenvectors is 50). The results are shown in Figure 1(b). When the dimensionality is small, the difference between MQDF and LSMQDF is not significant, but with the increasing of dimensionality, the difference becomes larger and larger. This is because the curse of dimensionality, i.e., in the high-dimensional space, the training data are not sufficient to get an accurate parameter estimation, and in this case, smoothing is an important tool for improving the generalization performance.

C. Varying the Size of Training Data  We also compare the performance of MQDF and LSMQDF with different number of training samples. In this case, we fix the dimensionality as 160, use K = 10, ? = 0.5 for (10), and set the number of principal eigenvectors as 50. The results are shown in Figure 1(c). We can find that: LSMQDF can improve the testing accuracy consistently and significantly, especially when the number of training samples is small (e.g. the accuracy is improved from 84.79% to 86.80% when there are only 60 training samples per class).

D. On Selection of K In this section, we evaluate the performance of LSMQDF  by varying the number of nearest neighbors K in (10).

Other parameters are fixed as the same of Section IV-C.

The results are shown in Figure 1(d). When K = 3, the testing accuracy of LSMQDF is already higher than MQDF, and with the increasing of K, the performance is further improved. When 10 ? K ? 30, the accuracy is nearly not changed. Therefore, the performance of LSMQDF is not sensitive to the selection of K.

E. On Selection of ? We also evaluate the performance of LSMQDF by varying  the values of the tradeoff parameter ? in (10). Other param- eters are fixed as the same of Section IV-C. The results are shown in Figure 1(e). When ? = 0.1, the accuracy of LSMQDF is higher than MQDF, because the information of neighboring classes is incorporated into the estimation of the covariance matrix. With the increasing of ?, the performance is further improved. However, when ? > 0.5, the accuracy is decreased due to the over-smoothing. Therefore, in practice, we should pay attention to the value of ?, and ? = 0.5 is shown to be a good choice in our experiments.

F. LSMQDF for Online Handwriting Recognition In this section, we use LSMQDF for online handwriting  recognition. For representing an online character sample,     we use a benchmark feature extraction method [24]: 8- direction histogram feature extraction combined with pseudo 2D bi-moment normalization (P2DBMN). We also add the direction values of off-strokes (pen lifts) to real strokes with a weight of 0.5 [25]. The settings of different parameters are the same as Section IV-A.

The results are shown in Figure 1(f). Although MQDF can achieve much higher testing accuracy for online data (compared with offline data), LSMQDF is still effective in improving the generalization performance, e.g., from 93.22% to 93.70% when the number of principal eigen- vectors is 50. The local smoothing of LSMQDF again outperforms the global smoothing method of RDA.



V. CONCLUSION  To improve the generalization performance of MQDF, we proposed a locally smoothed modified quadratic dis- criminant function (LSMQDF) by smoothing the covariance matrix of each class with its neighboring classes. The idea of local smoothing is simple and effective. In the future, we will explore the local smoothing from the theoretical viewpoint such as Bayesian learning [26]. Using the parameters of LSMQDF as initialization for discriminative training [2], and combining LSMQDF with improved dimensionality reduction methods [27] can further improve the accuracy.

ACKNOWLEDGEMENTS  This work was supported in part by the National Natural Science Foundation of China (NSFC) Grant 60933010 and National Basic Research Program of China (973 Program) Grant 2012CB316302.

