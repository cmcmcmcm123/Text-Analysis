Guiding Intelligent Testbench Automation Using  Data Mining and Formal Methods

Abstract?Achieving coverage closure is consistently identified as one of the most difficult challenges during the functional verification of today?s HW designs. Constraint random testing as well as coverage directed test generation (CDTG) techniques have been proposed previously with different degree of success. This paper presents a framework for speeding up the coverage closure of the design under verifications (DUV) using state of the art verification techniques. The framework starts with random simulation of the DUV followed by frequent pattern mining of simulation data to extract some valid design constraints. Simulation coverage database is analyzed and the coverage holes are identified and directed to the formal verification step, formal analysis is used to prove the unreachability of some coverage holes during simulation run. Formally proven unreachable cover items as well as automatically extracted design constraints are then fed as test template specification to direct the intelligent testbench generation to rapidly achieve the coverage of previously uncovered corner cases. Our experimental results demonstrate the effectiveness of the proposed approach in closing the coverage loop for a set of today?s RTL designs.



I. INTRODUCTION Functional Verification is the process that ensures  conformance of the design to its specification. It continues to be one of the most expensive and time consuming step in the design process. Practical functional verification relies on extensive simulation to the design under verification DUV, the completeness of simulation based verification is measured mainly by the final coverage achieved by the simulated tests. Generating effective tests for complex industrial designs has always been a challenging problem that can take one of the following forms: Directed tests that are written to cover corner cases and basic features of a design, they are written by verification engineers and require a very good understanding of the design.

Constrained-random test generation produces a large number of controlled random tests where verification targets will be eventually covered. Limitations of the constrained-random approach led to the development of coverage-directed test generation (CDTG) techniques.

These techniques dynamically analyze coverage results and automatically adapt the test generation process to improve the coverage. In general the verification team does not write tests fed to the simulator by hand, instead automatic tools for stimulus generation are used.

New intelligent automatic stimulus generator tools take a specification of test template or a set of directives as an input to specify the kind of stimulus desired. The generator generates a large number of distinct well-distributed test program instances that comply with the user?s specification [1]. Although intelligent verification tools are able to derive many tests for the DUV from a single, compact, high-level model. Building a single model that represents design specification is a manual task that requires a well understanding about verification intension and greatly increases the chance of human error in the testbench development process for highly complex designs.

Apparently any effort to automatically narrowing down the test specification scope by feeding information about current covered items, valid design input constraints and formally proven statements about the unreachable scenarios will help generating effective tests specification with increased ability of fast coverage closure.

The main contribution of this paper is the introduction of a complete verification framework that complements the comprehensiveness of the state-of-the-art intelligent testbench generation techniques with the power of formal verification methods to achieve fast and effective coverage closure for the design under verification. It combines coverage directed test generation (CDTG) by-feedback with CDTG by-construction methods as appropriate during the coverage closure cycle. Our proposed approach defines a complete automated work flow to transfer the knowledge acquired from the initial random simulation runs in the form of design constraints ,using data mining techniques, aside with coverage-holes information that have been extracted from the deep analysis of the simulation coverage databases to automatically identify test directives that guide the intelligent testbench generation processes. Formal methods are utilized to prove the correctness of the automatically extracted design constrains as well as formally prove the unreachability of some coverage holes and hence generating associated coverage exclusions that help accurate calculation of the final coverage results.

Usually the algorithmic test bench generation ignores any knowledge about the design implementation and focus only on exploring the state space of design variables as identified by verification team, however our experimental results indicate that the proposed automatic generation of test directives that respect learned design constraints, directed to fill the active coverage holes and ignore   2015 10th International Design & Test Symposium (IDT)     formally prove unreachable cover items resulted in an effective fast coverage closure to many of today?s HW designs as will be indicated in the experimental results. The rest of the paper is organized as follows. Section II briefly reviews some of the coverage-directed test generation methods as well as background about data mining techniques that are used in the proposed framework and intelligent testbench automation ITBA. Section III formulates the problem and explains our proposed framework. Section IV demonstrates the feasibility of our approach against a group of industrial designs. Finally, concluding remarks and future work directions are given in section V.



II. RELATED WORK & BACKGROUND  A. Coverage Directed Test Generation CDTG is a simulation-based verification methodology  that aims to reach coverage closure automatically by using coverage data to direct the next round of test generation toward producing tests that increase coverage [2]. There are two main approaches to CDG: one is by construction and the other is based on feedback. Examples of the former often require a formal model of the DUV, which is used to derive directives for test generation that accurately hit a specific coverage task [3]. [4] Proposes using BDD-based symbolic techniques for describing and traversing the implementation state space to drive the test suites.

Generating directed test cases using a high-level design specification with design signal relationships and causality was introduced in [5]. The work in [6] generates test programs to detect all the faults in the functional fault model of a graph-theoretical presentation that captures the structure and behavior of a wide range of pipelined processors. In contrast, CDG by feedback employs machine learning methods to close the loop between coverage analysis and test generation. This closure is achieved by learning from the existing tests and the achieved coverage some cause and effect relationships that are then utilized to construct new tests for a test generator in such a way that coverage closure is achieved faster and more reliably.

Recent works proposed various techniques to learn from the simulation results. These approaches employ a variety of learning techniques such as Bayesian Networks [7], Markov Models [8], Genetic Algorithms [9] and Inductive Logic Programming [10].

B. Data Mining Algorithms : An Overview Data Mining is the nontrivial extraction of implicit, previously unknown, and potentially useful information from large data sets or databases. Data mining has become a vital emerging application in the field of functional verification since a large amounts of data can be stored and processed during design verification. Previous work has explored the automatic generation of design properties from simulation trace using data mining techniques. Many of these attempts are inspired by a prior work for the automatic detection of software program invariants using the dynamic analysis of program runs [11]. IODINE [12] is the first attempt to infer dynamic invariants for hardware designs by hypothesizing a set of candidate invariants across one or more variables in the design. GoldMine [13]  is a methodology for the automatic assertion generation which involves the use of static analysis for the RTL of the design under study combined with a data mining decision tree based supervised learning algorithm. In [14], the authors build up an effective assertion extraction approach based on the mining of frequent episode in a sequence of events. Our work in [15] proposes a framework that combines searching for known assertion via templates with frequent and sequential pattern mining, while constraining the search by some knowledge about the design. Our approach was different from GoldMine in the utilization of constrained sequential data mining techniques alongside with the association rule mining methods which are directed by design specific knowledge. The Miner step proposed in this paper differs from the work presented in [15] by many means. First the mining is directed to extract the frequent patterns between inner design signals and design input ports only to improve design controllability.

Additionally the rule generation step is modified to respect rule coverage which is identified as a function of the number of literals in the generated constraint, the width of those literals and the original rule support. Finally rule merging is added to reduce the number of final generated constraints as will be explained in the mining step section III.A.

C. Intelligent Testbench Automation  Intelligent Test bench Automation (ITBA) becomes a hot topic as an advanced verification techniques that generates optimal sets of tests to improve both productivity and effectiveness. ITBA is an algorithmic based test generation program that is aware of the valid test space, the engineer?s verification targets within that space, and the current state of the design to efficiently achieve the verification goals. Many EDA Vendors provide ITBA solutions that are graph-based for test specifications and recently a new Accellera Standards Committee has been proposed for Graph-Based Test Specification Standard has been proposed as an attempt for standardizing test specification language across different EDA vendors [16][17]. The graph-based specification format is not new to verification, it is based on the standard Backus-Naur Form (BNF) meta-language grammar construct, pioneered by IBM in the 1970s, and has been used by many companies to automate compiler testing [17].The graph itself describes a test space at a higher level of abstraction, which can be used to automate the test implementation. The proposed framework utilizes Mentor ITBA Tool, InFact, which allows verification engineers to describe the graph in the form of rule file that contains stimulus definitions and consists of one overall BNF. The overall BNF consists of one or many (action & meta-action) symbols that are serially connected to specify the required test scenario. The meta-action allows specifying a defined range from which the values of generated stimulus are extracted. The action node maps to a function that is executed when the action node is traversed during the simulation.

2015 10th International Design & Test Symposium (IDT)      Figure 1:  a. Rule-Based Stimulus Description, b.

Corresponding Graph View  Figure 1 illustrates a simple example for memory read/write sequence stimulus using a rule based specification and how it is mapped to the corresponding graph-based view. ITBA tool will generate a test stimulus to cover the specified sequence using the valid ranges of design variables read_addr & wr_addr as declared in the rule specification.



III. PROPOSED VERIFICATION FRAMEWORK   Figure 2 : Atomatic Coverage Closure Framework   Figure 2 describes the major building blocks of the proposed work flow. It starts by the design being simulated to generate the trace file in VCD format, which is passed to the miner engine to extract design constraints. Minded constraints properties are verified by formal verification, proved properties are considered as real design constraints.

Coverage-hole analysis is done to identify which cover items are deviated from their pre-settled coverage goal.

Formal verification is used to identify unreachable coverage targets as coverage exclusions. Proven constraints as well as to-cover items are then forwarded to ITBA engine which generates test sequences that target coverage holes, and respect design constraints that automatically generated from mining the initial simulation runs. Formally proven coverage exclusions are then annotated to the achieved coverage results to provide a final coverage summary about the DUV. To illustrate the proposed solution, a simple example, Figure 3, will be introduced to show up step-by-step of problem formulation, constraints generation, formal verification, coverage hole analysis, algorithmic based stimulus generation and coverage closure after the annotation of proven unreachable cover-items as coverage.

Figure 3: RTL of Design Under Test with Coverage  Item   A. Mining Simulation Data for Constraints Extraction At the core of this approach is a data mining engine that  analyzes initial random simulation data and extracts input constraints. These constraints are then used to control internal design signals and hence enhance the ability to modify a test-bench to cover specific verification goals.

The data mining step starts by data preparation and simulation trace encoding. Figure 4, shows a simulation trace  of the example design that contains six signals and running from time t1 to t12. The signals changes combination at rising edge (or falling edge) of the clock cycle is seen as a simulation event ?e?.

Figure 4: Example: Simulation Trace  For instance, at time t2 the design signals (in, out, pos, a) having the following values combination (in=1, out=0, pos=3?b001, a=1) which will be encoded as simulation event e2.The simulation trace  is parsed to build up a list of signal-value pairs ( ), Figure 5.a, which defines pairs of <s, v> where s is the signal name and v is its value. The list will store for each target signal the values associated for it that have been detected during the simulation run. For example in Figure 3, the design signal ?pos? has 4 associated values with it {3?b000, 3?b001, 3?b010, 3?b011} those are the only values that have been assigned to signal ?pos? during simulation run. The (signal, value) pairs list will be used in encoding simulation events to transaction database format for further mining steps.

2015 10th International Design & Test Symposium (IDT)      Figure 5:  a. List of signal-value pairs ( ), b. Simulation  Trace Alphabet , c. Simulation Events Sequence E  Frequent pattern mining requires the extraction of Trace Alphabet ( ) from the simulation trace. The Trace Alphabet  is a list of distinct un-timed events associated with the frequency of their occurrence in simulation trace  as shown in Figure 5.b. This encoding is particularly useful when used to represent the idle or waiting cycles in a simulation trace where the same event typically may not changing for a long number of cycles. For Example the simulation event e2 is the same during t2, t6, t10. Further Step is done to encode this events based schema into transaction database, which in turn will be used by frequent pattern miner.

Mining Sequential Patterns in simulation trace needs the construction of Simulation Events Sequence ?E? Figure 5.c which is an ordered sequence of simulation events separated by the corresponding time difference between them ( e). Within 2 consecutive simulation events if the signal value does not change from its previous value, it will not be added as a part of the current simulation events. For instance at t4 only ?pos? signal has a value change from its previous value at t3, 3?b010 -> 3?b011, so pos=3?b011 will be the only signal in the simulation event sequence at time t4.Simulation Events Sequence ?E? is then covered to the customer transactions dataset which is a set of tuples (CID; TID; Data_Item) for sequential miner algorithm [18].

The mining step is divided into two main independent phases; the first one is the mining of the frequent patterns using association rule data mining algorithm [19].

Typically frequent mining algorithms will try to exhaustively produce all possible correlations between items. To restrict the number of generated rules, we apply several constraints. The first constraint is only rules with 100% confidence will be encountered in the final rule merging and evaluation step. We identify rule coverage measure which incorporates the coverage of each frequent item with respect to the entire input space of its corresponding design signal. Rule evaluation and pruning steps make use of the concept of support and confidence from data mining domain: For example support (A) is the proportion of instances in the simulation data that contains ?A?, while confidence (A=>B) is the estimate of conditional property P (B|A). The frequent mining algorithm has been modified to calculate the rule final support as a function of the rule original support, the number of frequent items in the generated rule and the rule coverage measure. This modification helps in generating rules with better information gain. Rule merging and reduction step has been modified to merge rules with the same antecedent and support but having different consequent by using the ?OR? operator. So that the two  rules (r1: A, B => C & r2: A, B => D) with same support will be merged to (r: A, B=> C||D). The redundant rules in the form of (r1: A, B=>C & r2: A, C => B) are also eliminated when they are having the same support and 100% confidence.

At sequential pattern mining step the miner identifies all the frequent sequences in the simulation trace that span across multiple simulation cycles. Those frequent sequences are processed over the simulation trace events sequence to extract the exact timing differences between its subsequences. Then it starts building up a list of timed frequent sequences, which are then used to generate a group of candidate rules implication pairs. Rule pairs are built up as subsets from the originally mined timed frequent sequences. Rule generator looks for antecedents and consequents across the entire simulation trace to gain some knowledge on whether the whole timed frequent sequence implies the re-occurrence of it through the entire simulation time.

B. Formal Verification & Coverage Hole Analysis Questa Formal, is used as the formal verifier of the minded design constraints. Figure 6 illustrates some of the propositional as well as temporal properties that have been extracted as design constraints and proven by formal verification step.

Figure6: Example of Automatically Extracted  Constraints  The Coverage hole analysis is done using the API Layer of Unified Coverage Interoperability Standard (UCIS). In June 2012, Accellera released the UCIS as an open, industry standard to allow the coverage data models interchange of a variety of coverage producers [20].

Figure 7 : UCIS APIs for Coverage Holes Extraction  The UCIS data model is a universally recognizable data model for all coverage information with API Layer that standardizes the way data is written or queried from this data model. Figure 7 demonstrates how APIs are used to extract coverage information from a simulation coverage  2015 10th International Design & Test Symposium (IDT)     database to identify coverage holes for next coverage closure step.

C. Automatically Generated Test Specification for ITBA The coverage goal for the given example is described using System Verilog Functional Coverage Syntax [21] as shown in Figure 8a. It is a cross coverage between 3 variables namely (out, pos, a) with iff (! rst) conditional guard for the cross coverage calculation. Cross coverage of a set of N coverage points is defined as the coverage of all combinations of all bins associated with the N coverage points, that is, the Cartesian product of the N sets of coverage point bins [21]. For example the covergroup in Figure 8a identifies a cross coverage between two 1?bit variables (a, out) and one 3?bits variables (pos) System Verilog implicitly creates a coverage point for each variable. (a, out) will both have a cover point with 2 bins namely auto[0]..aut[1] while (pos) will have a cover point with 8 bins, auto[0]..auto [7]. The cross of (out, pos and a) (labeled out_XX_pos_XX_a), therefore, has 32 cross products, and each cross product is a bin of out_XX_pos_XX_a.

Figure 8: a. Cross-Coverage Declaration, b. Illegal  Cross Points Combinations   The real difficulty in defining cross coverage goals is to get the exclusions correct, and this can be really difficult when the variables in the cross coverage have some constraints that determine the relationships that must be maintained between them. If these exclusions are not properly identified then the ability to accurately judge the actual coverage achieved becomes impossible since the missing coverage may contain a significant percentage of illegal cases. To illustrate the difficulty let?s consider our simple three variable example: Our coverage goal is to test all of the legal combinations of all three variables (out, pos and a). The definition of the legal cross coverage goals is not trivial even for this case, since we have to determine which combinations of (out, pos, a) are unreachable. Analyzing RTL implementation of the design Figure3, the illegal combinations turn out to be the ones listed in Figure 8b.

This leaves 5 valid combinations out of a total of 32, which would mean that if the exclusions are not specified then the maximum coverage attainable is 15%.

The proposed frame work passes the automatically extracted design constraints from the mining of the simulation data to the ITBA tool, Questa inFact, which  respects those constraints and can provide an accurate count of the legal solutions in a cross. So if the following automatically generated constraint as explained in the data mining step ,?constraint assum_all {if(!rst && (pos == 3'b001 || pos == 3'b010 || pos == 3'b011 )&& a == 1'b1 ) {out == 1'b0;}}?, is given to the ITBA, it will guide the tool to identify correct unreachable bins as indicated in Figure 9a and will result in the suppression of the test paths that violates the above constraint, i.e tests for cross coverage combination in which out==1?b0 where pos ==3?b000..3?b111 and a= 1?b0 or tests in which out == 1?b0 where pos==3?b000..3?b111 and a==1?b1 as indicated in Figure 9b. So respecting such constrain resulted in the suppression of 15 test scenarios that are unreachable for the DUV and focus to cover more meaningful coverage targets.

As more constraints are added the more the ITBA will be guided for better tests generation.

Figure 9:  a. Constraints-aware Cross-Coverage Declaration,  b. Ignorable Cross-Coverage Combinations by ITBA Step  Finally Formal reach-ability analysis is used to identify unreachable coverpoints after ITBA step. The formal verifier reads the coverage results from ITBA simulation coverage database, then it leverages the formal engines under the hood to flag the coverage items that are unreachable so it can be  safely ignored and list them in coverage exclusion file that can be used to refine final coverage calculations. As a conclusion the initial simulation coverage of the running example from which the design constraints have been extracted was 65.6% which was boosted to 87.5% after the algorithmic test stimulus generation phase and finally applying formal exclusions resulted in final coverage of 98.5%.

2015 10th International Design & Test Symposium (IDT)     Table  1: Progress of Coverage Ratios Increase During the Proposed Framework

IV. EXPERIMENTAL RESULTS  In this experiment, the proposed verification framework has been exercised against a group of industrial designs downloaded from Opencores [22], or obtained from a set of in-house regression test suites Table 1 lists some information about the designs under study such as the number of design modules, design size in LOC (line of code) and the number of design covergroups and coverbins.

Table 1 illustrates the number of design constraints that have been extracted during the mining step and have been verified/proven by formal verification. It demonstrates the coverage results progress from the initial random simulation runs to the tests coverage after constraints- guided test stimulus generation step and finally the impact of applying formal exclusions (formally proven unreachable cover items) to the ITBA simulation runs and how each step contributes in the coverage closure of designs under verification.



V. CONCLUSION Due to the increasing complexity of modern circuit  designs, a complementary set of functional verification techniques should work together to assure complete and fast verification cycle. This paper introduces a complete framework to accelerate coverage closure for the DUV, which makes use of the initial simulation data analysis to identify frequent patterns as design input constraints and to extract coverage holes. The coverage holes as well as formally proven un-reachable coverage goals are used to automatically identify test specification to guide ITBA for better test scenarios and effective coverage closure. Our experimental results show the feasibility of proposed framework to achieve good increments in coverage closure cycle using many industrial designs.



VI. REFERENCES  [1] S. Fine, A. Freund, I. Jaeger, Y. Mansour, Y. Naveh, A. Ziv ?Harnessing Machine Learning to Improve the Success Rate of 1344-1355, November 2006.

[2] C. Ioanides , K. I. Eder ?Coverage-Directed Test Generation Automated by Machine Learning-A Review?, ACM Transactions on Design Automation of Electronic Systems , Vol 17, Jan. 2012 .

[3] S. UR, S., Y.YADIN, ?Micro architecture coverage directed generation of test programs?, In Proc. of the Design Automation Conference (DAC), pp. 175?180,1999  [4] D. Geist, M. Farkas, A. Landver, Y. Lichtenstein, S. Ur, Y Wolfsthal. ?Coverage-Directed Test Generation Using Symbolic Techniques? , Formal Methods in Computer-Aided Design, pp. 143- 158, 1996  [5]  Nina Saxena, Jacob A. Abraham, Avijit Saha. ?Causality based generation of directed test cases?, In Proc. Asia and South Pacific Design Automation Conference, pp. 503-508, 2000  [6] P. Mishra, N. D. Dutt, ?Functional Coverage Driven Test Generation for Validation of Pipelined Processors? , In Proc. Design Automation and Testing in Europe, pp. 678-683, 2005  [7] S. Fine, A. Ziv., ?Coverage directed test generation for functional verification using bayesian networks?, In Proc. Design Automation Conference, pp. 286-291, 2003.

[8] I. Wagner, V. Bertacco, T. Austin, ?Microprocessor verifcation via Computer-Aided Design of Integrated Circuits and Systems, Vol. 26, pp. 1126-1138, June 2007.

[9] G. Squillero,?MicroGP An evolutionary assembly program generator?, Genetic Programming and Evolvable Machines, Vol 6, pp. 247-263, Sept. 2005.

[10]  H.-W. Hsueh, K. Eder, ?Test Directive Generation for Functional Coverage Closure Using Inductive Logic Programming,? In Proc.

High- Level Design Validation and Test Workshop, pp. 11-18,2006  [11] J.H Perkins, and M.D Ernst,?Efficient Algorithms for Dynamic Detection of Likely Invariants?, Proc. ACM SIGSOFT Symposium on the Foundations of Software Engineering,  pp. 23?32, 2004  [12] S. Hangal, N. Chandra, S. Narayanan, and S. Chakravorty, ?Iodine: a tool to automatically infer dynamic invariants for hardware designs?, in the Proc. of Design Automation Conference(DAC), pp. 775?778,  [13] S. Vasudevan, D. Sheridan, S. Patel, D. Tcheng, B. Tuohy, D.

Johnson, ?GoldMine: Automatic assertion generation using data mining and static analysis?, In Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 626-629, 2010  [14] P.H Chang, Li.C Wang,?Automatic assertion extraction via sequential data mining of simulation traces.?, Proceedings of the 15th Asia South Pacific Design Automation Conference( ASP- DAC), pp. 607-612,2010  [15] E. El-Mandouh, A. G. Wassal, ?Automatic Generation of Hardware Design Properties from Simulation Traces.?,  Proceedings of  IEEE International Symposium on Circuits and Systems (ISCAS), pp.

2317-2320,2010  [16]  ?Test Specification Language ? the past, present and the future?, http://www.cvcblr.com/blog/?p=857 , 2014  [17] ?Mentor Graphics Proposes New Accellera Standards Committee for Graph-Based Test Specification Standard ? https://www.mentor.com/company/news/mentor-accellera-standards- graph-based-test-specs , 2014  [18] J. Ayres, J. Flannick, J. Gehrke, T. Yiu, ?Sequential Pattern mining using a bitmap representation,? In Proc. Knowledge Discovery and Data Mining (KDD , pp. 429-435, 2002  [19] C. Borgelt,?Efficient Implemantation of Apriori and Eclat.?,In FIMI: Proceedings of the IEEE ICDM Workshop on Frequent Item Set Mining,2003  [20] Accellera Organization, Inc. Unified Coverage Interoperability Standard (UCIS). Accellera Organization, Inc.

