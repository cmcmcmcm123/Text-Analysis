User-Oriented Modelling of Scientific Workflows  for High Frequency Event Data Analysis

Abstract? Whether it is research scientists in computational physics, astronomy, genomics or financial services, all these varying disciplines have been challenged by the analysis of Big Data.  They are all required to perform multi-step analysis tasks to turn this data into actionable insight, from which critical decisions can be made.   Two data processing models that have rapidly evolved in the past decade to support data analysts are Complex Event Processing and Scientific Workflows.  Our research proposes a hybrid approach, which extends scientific workflows, to incorporate the handling of event-streams.  This model not only aims to provide a more efficient approach to analysing high frequency event streams, but also facilitates conceptual modelling of processes - to enable domain experts to build abstract, exploratory analysis processes in a user-friendly manner without the concerns of underlying technology, and transparently maps them to concrete solutions at run-time.



I. INTRODUCTION Today, many areas of sciences are dealing with massive  information streams or ?Big Data?.  Big Data is characterised by several dimensions ? Volume (exponential growth rate), Velocity (increasing frequency of data generation),  Variety (complexity and diversity of  data types).  These characteristics pose varying challenges to domain scientists conducting data analysis.  Our research focuses on handling of temporal events, a subset of Big Data characterised by high frequency event streams.  The industry has increasingly adopted Complex Event Processing (CEP) as a solution to fast and efficient processing of these event streams in real-time.

CEP focuses on detecting patterns of interest on event streams that are correlated in a causal, spatial or time dimension process, analysing their impact and acting upon them.

However, CEP solutions have several drawbacks.  Lack of  useability, inadequate provenance support and difficulty in reproducibility of analysis results make CEP models unsuitable to model complex scientific analyses and domain scientists often use scientific workflows to represent complex sets of computations.  Our research explores a hybrid approach, called as Scientific Event Processing (SEP) that extends scientific workflows to incorporate the handling of high frequency event streams, that is aimed at providing data analysts a more efficient way of conducting event analysis.

This paper is organized as follows:  Section II draws a  comparison of existing data processing techniques and establishes the motivation for our work. Section III provides  an architectural overview of our proposed system.  Section IV discusses on how our proposed system can be validated.

Section V concludes this paper.



II. BACKGROUND This section explores the relative merits and demerits of  both data analysis models and looks at how an integrated approach that unifies the two models would be beneficial.

A. Complex Event Processing (CEP)  CEP [1] has emerged as a model to respond to high frequency data streams.  CEP engines look for patterns and relationships between unrelated events that signal new opportunities or critical threats and trigger a reaction in response to the detected pattern.  For example, high-frequency financial data is increasingly becoming more available to both market participants and researchers.  Several types of events relevant to the financial market have been identified such as an OrderEvent (a market participant submitting an order to the market),   TradeEvent ( a timed action that relates to exchange of products between two market participants), which have been used to study market behaviour [15].

Despite its wide adoption in the industry, CEP solutions are  not without their limitations.  Currently, a wider platform that is able to integrate diverse technologies to support variety of functional and non-functional requirements is still a subject of research, as CEP technologies today are stand-alone, proprietary solutions [18].  Different CEP engines all have different characteristics, significantly varying in the manner they react to event patterns and applying decisions. CEP engines rely on an Event Pattern Language (EPL) to allow domain users to define event patterns that are matched against event data.  Again, different language styles have been adopted by different CEP products as summarised in Table 1.

TABLE I SUMMARY OF EVENT PATTERN LANGUAGE STYLES  Programming Language Style  Technology  Stream-Oriented (SQL Extensions)  ESPER [3],  Oracle CEP [4]  Rules-Oriented Drools [4], Tibco Business Events [5] Agent-Oriented Spade [6] Imperative-Script Style Apama [7]  supervised by Fethi A. Rabhi     Current CEP solutions are only semi-automated. Data analysed by event processing systems is still required to channel through several pre-processing and post-processing tasks.  What we need is to build an infrastructure that allows the entire spectrum of processing phases from data acquisition, filtering, assimilation to high-end computation tasks to be captured into a single Event Processing Network (EPN), a model that allows different components of the event processing architecture implemented with different technologies to be plugged in with minimal effort.

B. Scientific Workflows Scientific Workflows have emerged as a paradigm to  represent and manage complex distributed scientific computations [2].  Scientific data analysis constitutes a pipeline of several analytical tasks such as querying single or multiple data sources, transforming data (e.g. cleaning, enriching, aggregating, summarizing), analysing correlations, detecting patterns and testing or discovering models.    Each step in the analysis process is a process or computation to be executed, which is typically achieved by end-users through a software program or statistical tool such as SPSS, SAS, and MATLAB or Business Intelligence (BI) tool.  Currently several workflow projects such as Taverna [8], Wings [9], Kepler [10] have been demonstrated in a variety of scientific applications.  A complete survey describing the state-of-the- art in workflow technologies is discussed in [11].

Scientific workflows promise acceleration of scientific discovery through automation of computation tasks and providing support for provenance information, ensuring reproducibility of analysis processes and consistent results and sharing of analyses.  However, current workflow projects focus only on orchestration and execution of a sequence of tasks to process large data sets, and are not suited to process complex entities like event.  The article in [12] further distinguishes between event-based and work-flow based processes.  An event-based approach is seen as a more dynamic solution, as it allows domain users to respond to situations in the context of each specific event and respond appropriately to each.  The context of a specific transaction determines the route chosen for that specific transaction.

While a workflow based approach is seen as being more restrictive to domain users because of its prescriptive nature.

It starts with a flow in mind and activities that happen are only secondary to the flow.

A challenge inherent across both CEP and scientific  workflows including notable open-source projects Esper, Taverna, is that the technologies are not tailored to be user- friendly.  Domain users would benefit from a more graphical based abstract event processing model, rather than a programmer based model, that would allow them to tailor their own event processing networks.

We look at a motivating case-study that involves a fictional  analyst interested in detecting unusual price movements in  trade data for Telstra exchange on the Australian Stock Exchange (ASX) in the year 2011 and explore possible reasons behind the price jumps [15].   From a user?s perspective, the conceptual model is fairly simple - select an event source, detect abnormal price jumps, visualise the price jumps.  Realisation of this conceptual model however involves the use of complex price detection algorithms and comprises multiples steps: (1) Acquisition of stock data (2) Transformation of data to a format compatible to underlying CEP engine (3) Price Jump Detection (4) Visualisation.

Detection of price jumps can be realised using a CEP engine, but other pre-processing and post-processing tasks requires manual coordination, using a variety of tools.  CEP alone cannot achieve complete automation of the above process.

Switching between different CEP engines or configuration of different price jump algorithms makes the analysis task more time-consuming. Reproducibility of analysis results also presents a challenge.  Use of scientific workflows might seem a better solution to solve reproducibility and automation, but domain users are again confronted with multiple tools and technologies that require tremendous human effort.  To orchestrate the multiple steps of analysis into a workflow requires bridging the data flow between the different components in a seamless manner.

The wide spectrum of technologies available across both  CEP and workflow engines has itself become an obstacle to domain-experts from purely focusing on the science.   As non- technical domain experts are the real participants of the data analysis process, a data analysis framework from a user?s perspective must be designed.  Domain experts are not technical, but they know their data.  They should have the flexibility to freely design an abstract, conceptual model of their analysis process, which is purely tied to the functional behaviour of the process and captures user?s view point without specifying the underlying resources that actually implement the process.  They should be able to seamlessly interoperate with diverse tools and use existing functions and packages.  They should be able to build their process in an incremental and iterative fashion. They need to perform analysis in a collaborative manner.

What we need are ?hybrid processes? that have work-flow like structures extended to incorporate event processing.  Such an integrated solution can capitalise on the merits of both paradigms.  It would preserve the reproducibility, automation and transparency of scientific workflows and also benefit from the non-prescriptive and powerful expressive nature of event- based approaches.  It is also imperative that the ?hybrid processes? be defined at a layer of abstraction, where domain experts can conceptually orchestrate a network of event processing components into a process model, that is stable and sustainable over a rapidly changing technology.

Our research realises this vision through a framework  called as Scientific Event Processing (SEP).  SEP is a novel approach that adds a new dimension to current scientific workflow practices through the incorporation of event     processing.  By adopting the principles of Service Oriented Architecture (SOA) [13], SEP also allows users to build conceptual models of their analysis process that can be easily mapped to a variety of technology solutions.  SOA is an architectural approach that organises software components into interoperable, standards-based services (service-enabled technologies) that can be assembled together to create a solution to meet a particular business need. The next section presents an architectural overview of our proposed system.



III. OUR PROPOSED APPROACH Scientific Event Processing (SEP) is a proposed hybrid  solution that extends scientific workflows to include the processing of temporal event-streams, in a way that preserves the current principles of the scientific workflow model.

A. Event and Event Pattern Concepts An Event Instance is ?anything that happens or is  contemplated as happening? [1].  An event instance belongs to an Event Type. All similar event instances are grouped under the same event type, for example the stream of event instances from a blood pressure monitor sensor.  A Temporal Event Stream is a set of event instances that are ordered for example, on a timestamp.  An Event Pattern is an abstract template that matches Event Instances based on their content.  An event processing engine applies an Event Pattern on an event stream to detect and generate Event Pattern Instances (EPI).

B. Design Overview The following key design principles are proposed in  support of this solution.

(1) Creation of abstract service components  Users should be able to build an abstract model of their event analysis process.  This is accomplished by defining abstract service components (ASC).  These components are purely functional entities from a user?s viewpoint and can be easily mapped to a choice of suitable service-enabled technologies.  Based on the model described in [15], three types of abstract service components are defined as shown in Table II, depending on the type of function carried out on the event stream.

TABLE II TYPES OF ABSTRACT SERVICE COMPONENTS (ASC)  ASC Role of abstract service component Event Import Service  Extract Event Instances (EI) from event sources Event Source  EI  Event Processing Service  Filter : Produces a subset of events, based on some filtering critieria,  eg:- first 100 instances  EI + Filter Expression  EI Transformation (Tx) : Takes input event instances and outputs event instances that are function of the input event  instances, eg:- translation, aggregation, merging  EI +Tx  Function  EI Pattern Detection:  EI + Pattern Template  Matched EPI Event Export Service  EPI  or EI  Event Consumer  (2) Composition of abstract service components into an abstract event processing network  Domain scientists will be able to build a conceptual model of their event analysis process from the set of abstract service components defined earlier as shown in Figure 1.  This model depicts an abstract event processing network (EPN) and the nodes represent abstract service components.

Fig. 1  Conceptual Modelling of Analysis Process  Composition rules will be defined to ensure that only meaningful event processing processes can be constructed.

Examples of valid rules are shown in Table III.

TABLE IIII EXAMPLES OF VALID COMPOSITION RULES  Composition Rule Validity Import  Filter Pattern Match  Export  Valid  Import1  Merge Export Invalid ? Merge must have two input sources  Import1, Import 2  Merge Transform  Export  Valid   The abstract EPN is a crucial element in our design, as it  facilitates modelling the event processing at a layer of abstraction independent of underlying technology.  The abstract EPN will be captured in a template that is based on SCA (Service Component Architecture) Assembly Model specifications [14].  SCA provides a model for composing applications that follow the principles of SOA and provides a set of specifications that describe how components implemented with different technologies can be composed together in a standard way.   Concrete service definitions will be added to this abstract template to generate a concrete workflow during the mapping phase.  These process templates are persisted to allow them to be re-used.  As domain experts examine event sources to engineer new workflows, pre- existing templates will be suggested to end-users.  Users will have the flexibility to re-run the pre-existing models or customise components in the template to explore new models.

These process templates will assist in both consistent reproducibility of analysis results and also accelerate the pace of scientific discovery.

(3) Mapping of abstract event processing network to a  concrete workflow Abstract service components defined in a process template  will be mapped to service-enabled technology solutions.

Mapping is accomplished by maintaining a catalogue of services in a service repository.   To select candidate services, each service in the catalogue will be associated with a service type (as defined in Table II) that will be matched against the type of the abstract service component.  The service definitions of the selected concrete services are added to the abstract process template to generate a concrete SCA composite workflow model.  This SCA composite can now be executed on any application server that supports a SCA run- time.

Event streams flowing between the abstract service  components are based on a common Canonical Data Model.

Software components called adapters are created where appropriate to transform data to and from the canonical model.

Three types of adapters will be defined ? (1) input adapters (2) output adapters (3) bridge adapters. The system architecture for our proposed system is shown below.

Fig. 2 SEP System Architecture  C. Validation Using our proposed framework, the case-study presented in  Section II is modelled as shown in figure 3.

Fig. 3 An event analysis process to detect price jump events in Telstra stock

IV. CONCLUSIONS Currently, no research has successfully unified event  processing and scientific workflows, and provided the ability to model workflow processes in a user-oriented manner.

Siddhi [16], a stream-oriented CEP engine, represents the closest work to our research, but user-oriented modelling is not dealt with in Siddhi.  Categorisation of abstract services in  this paper is inspired by the work in ADAGE [14].   ADAGE is an architectural pattern to allow non-IT domain experts to effectively express analysis processes.  This paper extends ADAGE by formalising the concept of ADAGE abstract services using SCA specifications and their mapping to concrete services and also introduces the idea of reusable, flexible processes from a user?s perspective through use of process templates.

In summary, this research proposes a novel approach to  integrating handling of temporal event streams, using scientific workflows that is intended to not only provide domain users a more efficient approach to analyzing event streams, but also enable domain expert users to build flexible, abstract models of their event analysis process and transparently maps them to concrete solutions.  The next phase in our research is to build an initial prototype and validate the prototype using case-study discussed in Section II.

