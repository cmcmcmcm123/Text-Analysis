Robot: An Efficient Model For Big Data Storage Systems Based On Erasure Coding  Chao Yin 1 , Jianzong Wang3, Changsheng Xie 12 , Jiguang Wan 12? , Changlin Long 1 and Wenjuan Bi 1

Abstract?it is well-known that with the explosive growth of data, the age of big data has arrived. How to save huge amounts of data is of great importance to both industry and academia. This paper puts forward a solution based on coding technologies in big data system that store a lot of cold data. By studying existing coding technologies and big data systems, we can not only maintain the system's reliability, but also improve the security and the utilization of storage systems. Due to the remarkable reliability and space saving rate of coding technologies, importing coding schema in to big data systems becomes prerequisite. In our presented schema, the storage node is divided into several virtual nodes to keep load balancing. By setting up different virtual node storage groups for different codec server, we can ensure system availability.

And by utilizing the parallel decoding computing of the node and the block of data, we can also reduce the system recovery time when data is corrupted.

Additionally, different users set different coding parameters can improve the robustness of big data storage systems. We configure various data block m and calibration block k to improve the utilization rate in the quantitative experiments.

The results shows that parallel decoding speed can rise up two times than the past serial decoding speed. The encoding efficiency with ICRS coding is 34.2% higher than using CRS and 56.5% more than using RS coding equally. The decoding rate by using ICRS is 18.1% higher than using CRS and 31.1% higher than using RS averagely.

Keywords-distributed file system; erasure coding; big data; robustness; availiabilty; cloud storage

I.  INTRODUCTION Along with the development and popularity of internet  technologies, information is playing a significant role in a symbol of digital data. In the perspective of enterprises, loss of data could even be destructive; therefore the demand for dependability of data storage system rises up. With the rapid growing number of data, the storage scale has reached TB, PB, even ZB level, shown as tremendous development. Jim Gray [1], a Turing award winner, dictates that the new data amount from the Internet is expected to quadruple in every 18 months compared with all the old data existed. Thereby, traditional information storage scheme, which keeps the data in a centralized server, is no longer a satisfying solution to current demand for big data storage. Moreover, local storage technology will encounter resistance when the extension.

Some factors, such as costs, techniques and communication,  let data storage to be the main bottleneck to prevent the development  of information technologies.

In such situation, distributing data for storage and extending from local to remote, for instance cloud storage has been an inevitable tendency. Distributed storage takes outstanding advantage of both storage and transmission technologies, presenting unparalleled superiority in data security, storage volume, disaster backup and recovery.

For the storage of big data [8, 9, and 10], problems that remain to be solved are as follow:  ? Capacity Capacity can reach the scale of PB/ZB level. As a result, mass data storage systems should have ability for scaling. Meanwhile, the online expansion method must be convenient enough and decrease the degradation serving time;  ? Delay: Big data applications should be real-time, especially involved with online trading or financial related applications. Date-intensive computing has the SLA (Service Level Agreement) requirements. In addition, the popularity of server virtualization has led to a stringent demand for the high IOPS;  ? Security and Privacy: Access control for big data is also a hot topic for research. Big data applications lead to concern about security, especially for private data of company or individual  ? Cost: To save the cost, we need to make every device work more efficiently and avoid over- provision. Due to the widely implementation of coding, data de-duplication and cloud storage technologies in storage field, big data storage applications can be more effective and valuable  This paper is focus on the big data backup systems.

Currently erasure coding and deduplication technologies are used in big data storage frequently. Well known cloud storage systems like GFS [2], HDFS [3], Amazon S3 [4] and Ceph [5] all use replication to provide data redundancy.

Comparing the two redundancy technologies, Erasure code is more suitable for big data backup system since it requires less storage for maintaining reliability. We have developed an optimized erasure coding algorithm, called ICRS (Improved CRS), to store the backup data. The algorithm is improved based on original CRS [6] and Reed-Solomon [7] algorithms. By optimizing the determinant of a matrix, the number of identity element ?1? in CRS matrix is decreased dramatically. The performance of the system is also improved by increasing the speed of matrix operations.

The underlying architecture of our system used decentralized storage systems because of its target      requirements. First of all Robot is targeted mainly at backup applications where there are no updates frequently. Secondly, Robot is built for latency sensitive applications that require at least 99.9% of read and write operations to be performed quickly. Different from Pastry [25], Scality Ring [26] and Chord [11] which route requests through multiple nodes, Robot can be characterized as a zero-hop DHT, where each node maintains enough routing information locally to route a request to the appropriate node directly. This is because multi-hop routing increases variability in response times, thereby increasing the latency at higher percentiles.

We have used consistent hash algorithm to store data in distributed system. In comparison to these systems, a key- value store is more suitable in this case. First, it is intended to store relatively small objects (size < 1M). Second, key- value stores are easier to configure on a per-application basis.

The adoption of Hash not only improves the efficiency of looking up, but also guarantees that node addressing will not conflict.

In the current era of big data, the storage of data backup should make sure reliability and minimize storage overhead.

Our aim is to reduce storage consumption and improve the system performance.

The main contributions of ours paper are listed as below: ? Research and design ring system architecture, an  extension of symmetry, which should favor decentralized techniques over centralized control.

We imported Peer to Peer systems into big data system so that the system will not crash because of one or more nodes? damage.

? Store data in the mechanism including virtual nodes and physical nodes. We built corresponding relationship between them to ensure that data can be stored after coding and fast recovery when data is lost.

? Presented a coding scheme -ICRS (Improved CRS), which is based on CRS that can reduce the encoding and system down time. The testing result shows that the coding efficiency when using ICRS is 34.2% higher on average than using CRS and 56.5% higher on average than using RS. The decoding efficiency when using ICRS is 18.1% higher on average than using CRS and 31.1% higher on average than using RS.

The rest of this paper is organized as follows. The architecture and the design of Robot are introduced in Section 2. Section 3 gives the experimental evaluation and results. Section 4 shows related works. We make conclusions in Section 5.



II. ROBOT  A. The Design of Robot Most existing systems use triple replication methods to  provide reliability. But cold data are barely used by users, so the utilization of space is very slow. Considering the feature of cold data, system can use code methods to provide reliability to optimize the storage usage.

Coding is one of the key features in distributed system.

The common distributed system can provide high reliability but the storage space usage is overwhelming waste with much money investment. We are aim to import code methods by replacing duplication schema and two indexes are our optimizing targets in our system: Storage Space Utilization and Repair Time.

Figure 1.  The architecture of Robot  Figure 1 show the architecture of Robot, which consists of two components: coding servers and data servers. Coding servers is responsible for choosing suitable storage group, encode or decode data and store important information such as metadata. Date servers are composed by virtual nodes, which are located in the ring structure. These virtual nodes store original data. The ring structure guarantees loading balance and randomly distributed on each virtual node. The communication on data servers rely on the message exchange between virtual nodes instead of unique metadata.

The import of ring structure can be advanced compared with the traditional duplication situation and address the cold data storage problems in the big data environment in order to save the space. Due to the low read frequency, the demand of response time is lower than regular system. So we deploy erasure code method to provide reliability and optimize storage usage. Furthermore, by dividing physical nodes into virtual nodes, keep the system load balancing and prove repair speed up by utilizing parallel computing. After receiving the request of users, system will select a virtual node to build coding server based on the storage demand of users, every virtual node use its own storage group. All data of the coding servers will store in this storage group. The data on the coding servers will be first encoded and then store on data servers. When the physical node fails, system will read data from several storage groups concurrently to repair it. So our design can provide better performance on both security and repair speed.

B. The Disturbution of  Nodes The ring structure and hash method can balance the load  and promote the I/O speed. In the storage system, the     physical node is divided into several virtual nodes according to the computing and storage size. Then these virtual nodes are placed on the ring complying with hash method. In Robot, each coding server has several virtual nodes, and one virtual node can only be placed on one physical nodes, one physical node can have multiple virtual nodes, as figure 2 shows.

Figure 2.  The relation among nodes  The number of the virtual nodes on one physical node depends on the storage size of the physical node. Physical nodes with different size storage will have different number of virtual nodes. Every virtual node has it unique number, the different virtual nodes on physical node will be hashed according to IP address and port number. The result of hash operation will act as the ID of the virtual node and decide the position of the virtual node in the ring.

Physical nodes are numbered according to the joining order, first as No.1, second as No.2?nth as No. n. Then we carry out hash operation to the IPv6 address and port number of the physical nodes. Then choose 64 bits as the number of the virtual nodes and sort them in order. After sorting, we place these virtual nodes on the ring according to their number. This method can build a mapping between virtual nodes and physical nodes, which has good uniform property.

The amount of the nodes in the ring is related to the number and storage size of physical nodes. If Mi is the storage size of the No.i physical node, n is the data size, K is the required size of virtual nodes. The amount of the virtual nodes nr_vnodes:    _ / n  i i  nr vnodes M K =  =  (1)  C. ICRS Code We know that the encode time of CRS code depends on  the encode matrix. We have developed ICRS code to accelerate the rate of encoding and decoding. The performance of CRS code directly depends on the number of 1 in the Cauchy matrix. So we use matrix transform to build Cauchy matrix with less 1 in it.

The optimized Cauchy matrix uses less calculation so it has better performance in the repair process. Besides, to the situation when m=2, we gives all optimal Cauchy matrix when w 32. Here, w  means binary words of a fixed length.

There are 3 steps:  (1) Build basic Cauchy matrix M: do matrix transform M[i,j] = 1/( i ( m+j) ), the division is calculated in finite field and plus is normal plus calculation.

(2) Set all the number in the first row to 1: To column j, divide every number in this column by M [0, j] in finite field.

(3) Optimize the rest row: reduce the amount of 1 in the rest line is main goal of this operation. We implement it by using enumeration. Divide the row by every number in the row and choose the situation with least 1.

Through these steps we can significantly improve the performance of encoding/decoding process. Here is an example of the optimizing process for situation where k = m = w = 3.

Here is the original Cauchy matrix:    6 7 2 5 2 7 1 3 4  (2)  Optimize the first row:    1 1 1 4 3 6 3 7 2  (3)  Optimize the rest row: enumerate every number in the rest row and select the best situation. For the second row, we try 4, 3, and 6 and count the number of 1. The results are 12, 11 and 16. So we choose the 3 to optimize the second row.

Then we divide the last row by 3.

1 1 1 5 1 2 1 4 7  (4)  This optimized matrix have 34 ?1? in it, much less than the 46 before optimizing process. Using this method can have better encoding/decoding performance than CRS and RS code, and the reading and writing speed is much better than CRS and RS.

D. Data Division In Figure 3, we can see this part first divide big request  into several fixed-sizes blocks and put them in the data pool.

After the data pool is filled up, system encodes the data and adds parity data then send encoded data block to virtual nodes in related storage group. When user requests the original data, decoded data from related virtual node and return them to user.

Figure 3.  The flowchar of data division.

The encoded process is as following: ? Get data requests from the coding server.

? Divided the data request with certain size and each  block has its own number BLOCK_ID, consisted with request block number and sub-block number.

At the same time, system will add the information of the sub-block into the metadata, such as size, amount, BLOCK_ID, version.

? Send data into buffer pool whose size is k. When the pool is filled up, trig the encoded process.

? Encode data block, use erasure code such as ICRS, CRS etc. to encode the data and add parity block.

Add the code type into metadata.

? Send the encoded data to virtual nodes.

If we adopt M as the data size of the coding server,  block_m as the size of data block, en_data as the amount of the data block, en_parity as the amount of parity block, then we can get the following equation.

The encoded times encoder_num is:  _ = _ * _  Mencoder num block m en data  (5)  Total storage size total_m is   * __  _ M en paritytotal m  en data =  (6)  Fault-tolerant ability of system Q is  _Q en parity=  (7)  This method of encoding/decoding data improves the discreteness of data. Every sub-block can be accessed independently. Furthermore, the repair process can be highly paralleled.



III. EVALUATION METHODOLOGY  A. Experimental Setup In the sections, we mainly discuss about the evaluation  and analysis of the implementation part.

All the tests are based on a platform with a CPU (Intel  Xeon CPU E5606 @ 2.13GHz) and RAM memory (16GB  DDR3). Test mode of all the tests is data filling and recovery.

We built a simulation for the performance estimation of all the processes in data encoding, decoding and node recovery.

B. System Encoding and Decoding Time Test 1) Encoding and Decoding Time of Data Chunks  After selection of storage group, coding servers could encode and coding the user data to selected storage group. If the data is corrupted, then decoding and recovery is needed.

When encoding or decoding, Vandermonde code and Cauchy code are suitable in distributed system due to its fine fault tolerance and dispensability. Therefore we compare these two codes with ICRS in the tests.

In the encoding process, user data is randomly generated, divided into chunks, encoded and stored into virtual nodes.

The division and size of the chunks, data chunks, parity chunks and total encoded data size differ, the time of encoding and decoding differ. Figure 4 presents encoding and decoding time in different data chunk sizes in two encoding methods. (Decoding time here mainly refer to recovery time when data is corrupted).

Figure 4 shows the encoding and decoding time pattern in a single encoding group when data chunk number is 5 and parity chunk number is 2. Chunks size ranging from 1M to 10M is the only variable parameter. From the figure we could know that encoding time rises as chunk size increases.

Compared to encoding time, decoding time of a single chunk has better efficiency. Meanwhile, because ICRS and CRS adopts bitmap to convert multiplication into XOR operation and requires lower encoding or decoding time than RS code, while decoding time of ICRS is obviously shorter than the CRS because of coding optimization. ICRS is more suitable to be deployed on distributed system for decoding work. It is also obvious from the figure that decoding time is less than encoding time, so encoding time in systems need attentions.

Figure 4.  Encoding and decoding time in single encoding group(k=5,  m=2)  Figure 5 shows the encoding and decoding time pattern in a 5-chunk encoding group when chunk size is 1M and parity chunk number ranges from 2 to 8. As the parity chunks in an encoding group increase, encoding time rises.

More parity chunks can tolerate more faults, which means if higher fault tolerance level is needed more encoding time is required. However, the decoding time is more stable.

Decoding time in RS, CRS and ICRS has no noticeable fluctuation, which indicates that increase in parity chunks numbers will not lead to decoding time increase of a single chunk. We could also indicated that ICRS performs better than RS and CRS in both encoding and decoding time, suggesting ICRS is more adaptive to efficient and time- demanding system than the others.

Figure 5.  Encoding and decoding time (size=1M, k=5)  Figure 6 shows the encoding and decoding time pattern in an encoding group when chunk size is 4M; parity chunk number is 2 and data chunk number ranges from 3 to 9. As the data chunks increase, order of magnitudes of the encoding time rises from 10 seconds to 100 seconds. ICRS code still has a lower encoding time than RS and CRS.

Unlike encoding time, decoding time presents a smaller fluctuation. That indicates encoding process has larger overhead than decoding and encoding time is more susceptible by data chunk number k. ICRS remain relatively stable and high efficiency than RS, when k changes.

Figure 6.  Encoding and Decoding Time(m=2, size=4M)  Figure 4, 5, 6 show different result of encoding and decoding time in an encoding group when data chunk number, parity chunk number and chunk size change.

Through these comparison tests, it can be clear seen that encoding time changes when data chunk number, parity chunk number and chunk size change. The relation between recovery time of a data chunk and chunk size is the most  obvious. Bigger chunk size needs more decoding time. When data chunk number in an encoding group increases, decoding recovery time increases, while the increment is less than the case when chunk size increases. In these factors, parity chunk number is the least influential one. Decoding time is less than encoding time and it fluctuates less. Besides, ICRS has a higher overall encoding and decoding efficiency in distributed system than CRS and RS. From Figure 4, 5, 6, we can see that the coding efficiency when using ICRS is 34.2% higher on average than using CRS and 56.5% higher on average than using RS. The decoding efficiency when using ICRS is 18.1% higher on average than using CRS and 31.1% higher on average than using RS.

2) Encoding and Decoding Time of Nodes Encoded data chunks are stored on virtual nodes and if  any data chunk is corrupted the chunk will be decoded, as Section II describes. Node tests of encoding and decoding includes encoding time test of massive data and test on failure of physical nodes.

Figure 7 is an encoding time chart when size is 1M, numbers are 600. Data chunk number in an encoding group ranges from 3 to 9 and the time for generating random data is included. Encoding time of both codes does not fluctuate very much as data chunk number increases. It is because that more data chunks in an encoding group lead to fewer encoding groups. Although encoding time of each group increases, there are fewer groups. Therefore, no significant growth appears. Apart from that, ICRS code has a better encoding performance than CRS and RS code.

Figure 7.  Encoding time(size=1M, num=600)

IV. RELATED WORK Erasure code has been widely used in present distributed  backup storage system.

It has been applied in many large-scale distributed  storage systems, including storage systems at Facebook and Google.  The updated Google File Systems- GFS2 (Colossus) [14] has imported the Reed-Solomon encoding in action.

Lots of works on designing efficient erasure codes or improving performance from certain aspects have mushroomed all around the world.  Compare to ICRS in this paper, the performance of RS is much lower.

LRC [15], used in Windows Azure Storage, reduces the number of erasure coding fragments that need to be read when reconstructing data fragments that are offline, while still keeping the storage overhead low. The basic of LRC is using extra fragments to construct more parity. As a result, fragments in a calibration chain are less than ICRS.

RS Codes, including ICRS, are Maximum Distance Separable- MDS Codes [16], which require minimum storage overhead for given, fault tolerance. There are also other erasure codes, such as Hover codes [17]; Weaver codes [18] and X-code [19], belonging to MDS codes but these codes are suitable for disk arrays. Jianzong Wang presented a quantitative evaluation model for different redundancy schemes from eight aspects: Availability, Reliability, Storage Overhead, Performance, Network Bandwidth, Energy Consumption and Dollar Cost in order to help the coding selection. [22, 13]. Another schema [12] showed that erasure coding used appropriately can improve system performance and save energy.



V. CONCLUSIONS Big Data has become a challenge for many companies  around the world. Many enterprises are looking for better ways to organize, manage and store their machine, application and user generated data that is quickly expanding to petabytes and Exabyte in size. This paper has studied the situation of present big data backup system and a solution to improving the storage utilization based on erasure code. We put forward a distributed backup system mechanism based on coding. This mechanism applies coding technique to distributed system in which large amounts of cold data are included to protect data and realize load balance. The ICRS (Improved CRS) method can improve performance by taking advantage of its high speed of CODEC. By using parallel recovery method, the repair time of system has been reduced.

The system's load balance is improved by using virtual node.

Results show that it is very fast to complete CODEC by using ICRS.

There is still much work to do in the future which mainly contains two directions. First, to reduce the decoding time further, we can optimize the present ICRS. Second, as is shown in related work data deduplication is of great value in storage of big data. How to combine coding technique with data deduplication in Robot is a research direction.

