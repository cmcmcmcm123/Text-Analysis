Smart Intermediate Data Transfer for MapReduce on  Cloud Computing

Abstract?MapReduce is a programming model proposed by Google to process large datasets in clusters. However, MapReduce often needs to transfer much intermediate data among nodes, which is harmful to performances of an application. MapReduce can be enhanced by using the proposed Smart Intermediate Data Transfer (SIDT) in the runtime system to smartly arrange intermediate data.

Although SIDT does not reduce intermediate data to the minimal size in comparison with other intermediate data arrangement procedures such as Huffman coding, bzip2, and gzip, MapReduce is proved to get a better performance from SIDT than from others in the experiments of this paper.

Keywords?SIDT; MapReduce; Intermediate Data; Cloud Computing

I.  INTRODUCTION MapReduce [1] is a programming model proposed by  Google to process large datasets in clusters. MapReduce has become the important technology on cloud computing [2] [3] because it allows programmers to easily develop applications in clusters. For applications, all MapReduce needs is programmers to prepare two functions (a.k.a. Mapper and Reducer) [1] that will be automatically distributed over nodes in clusters at runtime by the runtime system.

When the runtime system works for applications, it provides Mappers with input data and transfers intermediate data [1] outputted by Mappers to Reducers. However, the runtime system often consumes much bandwidth in transferring intermediate data among nodes, which is harmful to performances. Because intermediate data is just for Reducers, the runtime system should have a proposal specially designed for MapReduce to reduce bandwidth consumption. Ideally, the runtime system can use such a proposal to reduce bandwidth consumption in transferring intermediate data without costing much CPU time as a tradeoff.

In this paper, Smart Intermediate Data Transfer (SIDT) is proposed to reduce bandwidth consumption in transferring intermediate data from Mappers to Reducers. SIDT smartly arranges intermediate data when the runtime system moves intermediate data from Mappers to Reducers. SIDT improves performances without negative impacts on application results.

SIDT keeps MapReduce intact to have high portability among platforms. In the experiments, SIDT outperforms the runtime systems that use Huffman coding [4], bzip2 [5], gzip [6], or no  special arrangement [1] (a default in the existing runtime systems) to handle intermediate data.

The remaining parts of this paper are organized as follows.

Section 2 is the MapReduce background. Section 3 introduces SIDT. Section 4 has experiments. Section 5 concludes this paper.



II. BACKGROUND MapReduce [1] mainly has two stages, i.e. the Map stage  and the Reduce stage, in the progress of application execution.

In the Map stage, MapReduce needs Mappers to process input data and output intermediate data in a format of key and value pairs. MapReduce relies on the runtime system to save intermediate data in different intermediate files according to a hash function based on the key of intermediate data. In the Reduce stage, MapReduce uses Reducers to process intermediate data and output parts of application results.

Between the two stages, MapReduce relies on the runtime system to choose appropriate nodes, e.g. the idle ones, to run Reducers and transfer intermediate data from nodes on where Mappers run to nodes on where Reducers run.

Word Count [1] is an application often used to explain MapReduce. Word Count has a Mapper to parse a document having words separated by space characters. For each word, Word Count outputs a pair of ?word? and ?1? as the key and value pair in the Mapper. In the Reducer, Word Count processes intermediate data grouped into different words and sums up the values associated with the same word as the count of the word. Finally, Word Count gets outputs collected by the runtime system from all Reducers and has the counts of all words appearing in the document as the application result.

Although the runtime system may partition input data into different blocks before giving Mappers the block data and sort outputs of Reducers for an application in some MapReduce prototypes, we do not discuss the extra data process issues because they are beyond the scope of this paper. In this paper, we focus on how to reduce bandwidth consumption in transferring intermediate data from Mappers to Reducers, because reducing bandwidth consumption usually benefits performances and needs to be cared for nodes connected to each other with limited bandwidth in a cluster.

DOI 10.1109/CLOUDCOM-ASIA.2013.97    DOI 10.1109/CLOUDCOM-ASIA.2013.97

III. SMART INTERMEDIATE DATA TRANSFER (SIDT)  A. Overview A typical runtime system handles intermediate data as the  way in the upper part of Fig. 1 and uses a hash function with a key as its parameter in order to determine the intermediate file for saving the key and value pair. When the intermediate file reaches a certain size, a typical runtime system sends it to a Reducer on an idle node over networks. Generally, a typical runtime system directly saves intermediate data produced by Mappers in intermediate files without any further processing.

Fig. 1, SIDT Overview  In the lower part of Fig. 1, SIDT uses SIDT Encoder at one side to translate intermediate data into SIDT Metadata before sending it to nodes over networks. SIDT can save bandwidth because SIDT Metadata usually is smaller than intermediate data. At the other side, SIDT uses SIDT Decoder to restore SIDT Metadata to intermediate data before handing over it to a Reducer. SIDT uses an algorithm specifically designed for processing intermediate data without costing much CPU time.

Accordingly, SIDT can improve performances by reducing bandwidth consumption in networks.

B. SIDT Encoder SIDT Encoder translates intermediate data into SIDT  Metadata that has a smaller size. SIDT Encoder arranges intermediate data and targets redundant keys appearing in it.

SIDT Encoder slightly uses CPU time to greatly shorten intermediate data size in order to reduce bandwidth consumption when intermediate data is transferred among nodes.

Fig. 2, SIDT Encoder Components  SIDT Encoder has four components as shown in Fig. 2, i.e., Key Lookup Module, Table Update Module, Data Flush Module, and Key Lookup Table. SIDT Encoder builds Key Lookup Table that uses a different key as a record entry and a list of records having values associated with the same key.

SIDT Encoder uses Key Lookup Module to compare keys in intermediate data and those in Key Lookup Table. SIDT  Encoder calls Table Update Module to add a new record entry and related records to Key Lookup Table for a new key and value pair found by Key Lookup Module in intermediate data.

SIDT Encoder uses Data Flush Module to both create SIDT Metadata according to Key Lookup Table and save it in intermediate files.

Fig. 3, Translating Intermediate Data into SIDT Metadata  SIDT Encoder translates intermediate data as shown in Fig.

3. SIDT Encoder updates records in Key Lookup Table for each pair of key and value found in intermediate data, and creates a record entry with the newly found key on demand.

Next, SIDT Encoder merges the values associated with the same key in Key Lookup Table and translates them into SIDT Metadata. If a key or value has the separate character that is designed for distinguishing a key and each of its values from SIDT Metadata, SIDT Encoder appends an extra separate character to the original one. Finally, SIDT Encoder uses a newline character to separate keys and their values as groups.

C. SIDT Decoder SIDT Decoder restores SIDT Metadata to intermediate data  before handing over it to the runtime system. SIDT Decoder extracts keys and their values from SIDT Metadata and translates them back to the original format of intermediate data.

In other words, SIDT Decoder executes the reverse procedures in Fig. 3. After SIDT Decoder restores and hands over intermediate data to the runtime system, the runtime system is not aware of the intermediate data translation made by SIDT Encoder for transferring intermediate data over networks.

Fig. 4, SIDT Decoder Components  SIDT Decoder has only two components as shown in Fig.

4, i.e. Group Scan Module and Data Restore Module. SIDT Decoder uses Group Scan Module to locate a key and its values in SIDT Metadata. Next, SIDT Decoder uses Data Restore Module to restore the key and its values in the format of SIDT Metadata to a series of key and value pairs in the format of intermediate data. Technically, SIDT Decoder uses Data     Restore Module to restore intermediate data according to the reverse procedures in Fig. 3.



IV. EXPERIMENT  A. Configurations   Fig. 5, Experiment Configurations  We use PHP (short for Hypertext Preprocessor) [7] to develop a runtime system named PHPMR because PHP is a widely-used general-purpose scripting language especially suited for Web development. We make PHPMR capable of: 1) reading input files from a local disk, 2) locating an idle node to run Mappers, 3) distributing input files over Mappers, 4) locating an idle node to run Reducers, 5) transferring intermediate data from Mappers to Reducers, and 6) collecting outputs from Reducers as the application result. For experiments, we use PHP to develop SIDT and two applications as well to work with PHPMR at nodes. We make PHPMR call SIDT Encoder just before saving intermediate data in intermediate files. Conversely, we make PHPMR call SIDT Decoder before sending intermediate data to Reducers.

Besides observing performances of PHPMR with SIDT, we observe performances of PHPMR that uses Huffman coding [4], bzip2 [5], gzip [6], or no special arrangement (i.e. the native case) for handling intermediate data. To this end, we simply replace SIDT Encoder and SIDT Decoder with the encoding and decoding functions of other intermediate data arrangement procedures in different experiments. In experiments, we prepare several identical PCs connected to each other via Gigabit Ethernet. We detail their configurations in Fig. 5.

B. Overhead Breakdown We want to observe SIDT overheads. To this end, we  develop a dummy application that has a Mapper and a Reducer both to do nothing more than outputting identical data when receiving input data or intermediate data. We provide the dummy application with an input file that has totally different words to occupy 80 bytes. In Fig. 6, we show the overhead of each procedure used in PHPMR with SIDT for the dummy application to process the input file. For comparison, we also show native overheads of certain procedures without SIDT in Fig. 6.

We observe that the dominant factor is the native overhead of the job partition procedure but it will not affect performances as much as the procedures of reading input data and getting intermediate data because they are in proportion to the quantity of data processed by an application. We note that  SIDT hardly has a negative impact on performances and furthermore may have a chance to improve performances. For example, we note that the emit function with SIDT can outperform the native emit function about 30.79% (i.e. (0.867- 0.6)/0.867) because SIDT can arrange intermediate data to reduce its size in order to shorten the delay of disk I/O in a node.

Fig. 6, SIDT Overhead Breakdown  C. Performance of Word Count   Fig. 7(a), Word Count Performance on Repeated Input Data with Different  Mapper Numbers  In the following subsection, we observe performances of two typical applications when different intermediate data arrangement procedures are taken. For comparison, we also observe the case that takes no special arrangement for handling intermediate data and refer to it as the native case. We test an application with different numbers of Mappers and Reducers and limit each PC in the cluster to running either a Mapper or a Reducer, in order to clearly observe the performance impacts of Mapper and Reducer numbers and bandwidth consumption.

In order words, we test an application with one Reducer and     multiple Mappers, and then we test the application with one Mapper and multiple Reducers.

At runtime, we provide an application with input data from a PC as the role of the Master without using any distributed file system. We respectively input repeated data and non-repeated data. We let PHPMR automatically and arbitrarily distribute Mappers and Reducers of an application to PCs in a cluster over networks. Finally, we let PHPMR collect outputs of Reducers from all PCs in the cluster. We observe time and intermediate data that an application costs in the entire execution procedure, i.e., the reception of input data, the process of data with Mappers and Reducers, and the collection of outputs from Reducers.

Fig. 7(b), Word Count Performance on Repeated Input Data with Different  Reducer Numbers  First, we observe performances of Word Count [1] because it is a canonical application often used in performance evaluation of MapReduce runtime systems. In Word Count, we program a Mapper to emit to disks a string ?word 1? for each word in input data provided by PHPMR. Besides, we program a Reducer to merge groups of words in intermediate data collected by PHPMR from the corresponding Mappers according to a key-length-based hash function. Finally, we record the execution time and bandwidth consumed by transferring intermediate data among nodes. We test the application respectively with a 100 MB input file having repeated data and a 100 MB input file having non-repeated data.

In Figs. 7(a) and 7(b), we observe that increasing Mappers and Reducers both can improve performances of Word Count in processing repeated input data. Because Word Count can process more input data with multiple Mappers, we observe in Fig. 7(a) that increasing Mappers can get more performance gains than increasing Reducers. We observe that SIDT outperforms other intermediate data arrangement procedures, although SIDT does not compress intermediate data to the  minimal size. For example, we observe in Fig. 7(b) that SIDT can use 1 Mapper and 7 Reducers to improve 40% (i.e. (188- 111)/188 = 41%) performance of the native case but gzip only can improve 3% (i.e. (188-183)/188 = 3%) performance.

Although bzip2 and gzip can compress intermediate data to a size much smaller than SIDT, we observe that their high computation overheads can not make them get performances better than SIDT. Similarly, we note that Huffman coding seriously degrades performances of the native case due to the high computation overhead.

Fig. 8(a), Word Count Performance on Non-Repeated Input Data with  Different Mapper Numbers       Fig. 8(b), Word Count Performance on Non-Repeated Input Data with Different Reducer Numbers  When testing Word Count with non-repeated input data in Figs. 8(a) and 8(b), we observe that SIDT works as efficiently as other intermediate data arrangement procedures, even though SIDT generates intermediate data similar to the native case. Because a Mapper can not generate intermediate data compressible to SIDT when processing non-repeated input data, we are not surprised by the phenomenon that SIDT arranges intermediate data similar to the native intermediate data in transmission. Although bzip2 and gzip can compress intermediate data very well, we observe that their performances just are slightly improved due to high computation overheads.

Nevertheless, we show that SIDT sill can maintain performances in processing non-repeated input data even though Mappers can not generate intermediate data compressible to SIDT. According to Figs. 7 and 8, we note that all intermediate data arrangement procedures can get better performances in processing repeated input data than non- repeated input data, because the runtime system costs less time in collecting the application result of repeated input data than that of non-repeated input data.

D. Performance of Quick Sort   Fig. 9(a), Quick Sort Performance on Repeated Input Data with Different  Mapper Numbers  In this subsection, we observe performances of Quick Sort [8] with different intermediate data arrangement procedures. In Quick Sort, we implement a Mapper to process input data and save intermediate data into different intermediate files according to the digit of a number. We implement a Reducer to sort its corresponding intermediate data with the Quick Sort algorithm [8]. We expect that the application costs much CPU time in Reducers because Mappers merely classify numbers according to their digits. We test the application respectively  with a 100 MB input file having repeated data and a 100 MB input file having non-repeated data.

Fig. 9(b), Quick Sort Performance on Repeated Input Data with Different  Reducer Numbers  According to Figs. 9(a) and 9(b), Quick Sort can get a better performance by increasing Reducers than by increasing Mappers because increasing Reducers can alleviate computation overheads. When processing repeated input data, Quick Sort gets the best performance with SIDT by saving much bandwidth consumption in transferring intermediate data.

For example, we observe in Fig. 9(b) that SIDT can use 1 Mapper and 7 Reducers to improve 30% (i.e. (218-153)/218 = 30%) performance of the native case, which is much better than other intermediate data arrangement procedures. Although bzip2 and gzip can reduce intermediate data more than SIDT, Quick Sort hardly gets many performance improvements from them because they cost much time in compressing and decompressing intermediate data.

We show results of processing non-repeated input data in Figs. 10(a) and 10(b). According to the curves, we know that increasing Reducers still improves performances much more than increasing Mappers when Quick Sort processes non- repeated input data. Although SIDT does not reduce intermediate data to the minimal size because no intermediate data is compressible to SIDT, we observe that SIDT still maintains performances close to bzip2, gzip, and the native case. According to Figs. 9 and 10, we observe that Quick Sort gets better performances with all intermediate data arrangement procedures in processing repeated input data than in processing non-repeated input data, because Reducers cost more time to search arrays and sort non-repeated numbers.

In brief, we show that SIDT is a practicable intermediate data arrangement procedure and better than other intermediate data arrangement procedures such as Huffman coding, bzip2, and gzip in experiments because: 1) SIDT can compress intermediate data to a certain degree but outperform other intermediate data arrangement procedures when processing     repeated input data; and 2) SIDT can maintain performances close to bzip2, gzip, and the native case without degrading performances when processing non-repeated input data.

Fig. 10(a), Quick Sort Performance on Non-Repeated Input Data with Different  Mapper Numbers   Fig. 10(b), Quick Sort Performance on Non-Repeated Input Data with Different  Reducer Numbers

V. CONCLUSION In this paper, we propose Smart Intermediate Data Transfer  (SIDT) to reduce bandwidth consumption in transferring intermediate data from Mappers to Reducers. We use SIDT to arrange intermediate data when the runtime system moves intermediate data from Mappers to Reducers. By compressing intermediate data without costing much CPU time, we use SIDT to successfully improve performances without negative impacts on application results. Because of keeping MapReduce intact without any modification, we make SIDT and MapReduce applications portable to various platforms. For verifying practicability of SIDT, we implement SIDT on a MapReduce runtime system to help Word Count and Quick Sort process repeated input data and non-repeated input data. In the experiments, we show that SIDT outperforms other intermediate data arrangement procedures such as Huffman coding, bzip2, and gzip in processing repeated input data and non-repeated input data because: 1) SIDT compresses intermediate data to a certain degree but outperforms other intermediate data arrangement procedures when processing repeated input data; and 2) SIDT maintains performances close to bzip2, gzip, and the native case without degrading performances when processing non-repeated input data.

Accordingly, we are convinced that SIDT can improve performances of MapReduce applications in clusters by reducing intermediate data without negative impacts on application results.

