

A Probabilistic Approach to Apriori Algorithm  Vaibhav Sharma                      M.M. Sufyan Beg Computer Science Department                Department of Computer Engineering Institute of Technology and Management Jamia Millia (A Central University) Gurgaon, Haryana, India              New Delhi, India shvaibhav@yahoo.com                        mbeg@jmi.ac.in   Abstract We consider the problem of applying probability concepts to discover frequent itemsets in a transaction database.

The paper presents a probabilistic algorithm to discover association rules. The proposed algorithm outperforms the apriori algorithm for larger databases without losing a single rule. It involves a single database scan and significantly reduces the number of unsuccessful candidate sets generated in apriori algorithm that later fails the minimum support test. It uses the concept of recursive medians to compute the dispersion in the transaction list for each itemset. The recursive medians are implemented in the algorithm as an Inverted V-Median Search Tree (IVMST). The recursive medians are used to compute the maximum number of common transactions for any two itemsets. We try to present a time efficient probabilistic mechanism to discover frequent itemsets.

Keywords: Data mining, KDD, association rules, frequent itemsets, probability, statistics, apriori algorithm.

1. Introduction The exponential increase in disk space and availability of cheap storage has contributed to the accumulation of large quantities of data. Knowledge Discovery in Databases (KDD) is the process of extraction of frequent patterns from database. KDD is an important tool to transform the vast amount of data into information. KDD is also referred to as Data Mining. In different contexts, data mining is referred as knowledge extraction, data analysis, data dredging, data fishing and data snooping. Data mining finds its application in marketing, surveillance, fraud detection, scientific discovery, pattern recognition and customer analytics. Data mining techniques were classified on the basis of databases to be mined by Chen et al [7].

Culotta et al. proposed an integrated supervised machine learning method that learns both contextual and relational patterns to extract relations [8]. A wide variety of data mining techniques have evolved but they all share the same basic features [11],[12],[15].

Data Mining is a very challenging task. The most time consuming step in data mining is the discovery of frequent itemsets. Itemsets that satisfy the criterion of minimum  support are called frequent itemsets. Minimum support gives a lower limit to the occurrences of a set of items in a given database. Agrawal et al. proposed a mathematical model to address the problem of mining association rules in a transaction database [2]. Association rule discovery is the most important aspect of data mining [13]. Various techniques have been developed to discover frequent itemsets in a database. But Apriori algorithm[1] was the first algorithm to discover the frequent itemsets in a time efficient manner. It was the first algorithm that significantly improved the data mining techniques to identify frequent itemsets. Apriori algorithm is explained in Section 2 of the paper. Different techniques have been used by different researchers to implement apriori algorithm. Monte Carlo simulation techniques using statistical methods have been successfully applied to apriori algorithm [10]. Olson and Wu also correlated Monte Carlo simulation techniques with data mining concepts [14]. Bodon proposed a very fast implementation technique for apriori algorithm [5]. Bodon also proposed a trie based apriori algorithm for mining frequent itemset sequences [6].

The paper proposes a probabilistic approach to apriori algorithm. Using concepts of probability and statistics, the paper presents an implementation faster than the apriori algorithm. The structure of the proposed algorithm is kept similar to the apriori algorithm for ease of understanding.

Though the proposed algorithm inherits many concepts from apriori algorithm, but its mechanism for mining frequent itemsets is completely different. It is based on the fact that not all (k-1)-itemsets have equal probability of generating k-candidate sets with high support. This is opposite to the concept of apriori algorithm which gives equal priority to all the itemsets. The proposed algorithm uses concept of ?bimodal distribution? to classify the itemsets into two groups. The first group contain itemsets that have very high support relative to others. The second group contain itemsets that have moderate support. The itemsets in both the groups are already above the minimum support. The itemsets in the second group are further processed to identify those combinations of itemsets that will not generate high support candidate sets. These low support combinations are later pruned from the possible candidate sets. The algorithm provides a time efficient method to discover frequent itemsets in a transaction database.

DOI 10.1109/GrC.2010.69     The paper is divided into 6 sections. The Second Section gives only a brief account of apriori algorithm. The Third Section discusses the key concepts used in the proposed algorithm. The proposed algorithm is presented in detail in Section 4. A detailed explanation along with pseudocode is provided in this section. The Fifth Section analyse the performance results for the proposed algorithm and compares them with the apriori algorithm. The paper concludes with the Sixth Section giving directions for future research.

2. Apriori Algorithm The general structure of the apriori algorithm is given below. The apriori algorithm involves multiple passes. For k-passes, it scans the database k times. In the first database scan large 1-itemsets with frequency at least equal to minimum support are generated. A subsequent pass through the algorithm is a two step process. In the first step, lexicographically similar large itemsets Lk-1 are combined to generate candidate sets Ck using the apriori_gen function. The second step counts the frequency of each candidate set in Ck. This step scans the entire database. This counting operation can be performed in a faster way by using a candidate hash tree. After the counting operation, all the candidate sets with support less than minimum support are pruned to give large k-itemset.

These two steps are repeated until no more large itemsets are formed. The application of this counting technique was extended beyond transaction databases in [3].

L1 = { large 1-itemsets }; for ( k=2; Lk-1 ?  ; k++) do begin Ck = apriori_gen (Lk-1); for all transactions t  D do begin Ct = subset ( Ck, t ); for all candidates c  Ct do c.count ++; End Lk = { c  Ck | c.count ? minsup} End Answer = Lk ; 3. Key Concepts used in the proposed algorithm Probabilistic and statistical concepts applied in the proposed algorithm identify and eliminate candidate sets that will otherwise fail in the minimum support test. The single database scan and the data sets used to store information after the scan are also discussed.

3.1. Bimodal Distribution A multimodal distribution is a mixture of two or more different unimodal distributions. A bimodal distribution is a specific case of multimodal distribution with only two  modes depicted as two distinct peaks (local maxima) in the probability density function.

Condition for bimodality in a probability density function states: ?A mixture of two normal distributions with equal standard deviations is bimodal only if their means differ by at least twice the common standard deviation.? The concept of bimodality is used in the function classify of the algorithm.

The apriori algorithm combines all the lexicographically suitable (k-1)-itemsets with equal probability to generate k- candidate sets. During candidate generation, there is no differentiation among (k-1)-itemsets. But statistically, (k-1) -itemsets with large support value have relatively higher probability of generating k-candidate sets with larger support values. The (k-1)-itemsets with high support values are independently present in relatively large number of transactions, and this contributes towards higher probability for any two of them being present together in a transaction. Using bimodal distribution in the probability density function for support, we try to classify the (k-1)- itemsets into two groups ? HIGH and MEDIUM based on their probability to generate k-candidate sets with large support values. This is discussed in more detail in Section 4.3.

3.2. Recursive Medians as a measure of dispersion for a given distribution Median divides a distribution into two new distributions with equal number of elements. If we, recursively, find out all the possible medians for a list, then, we can say with conformity that only one element is present between any two successive medians. This fact becomes complicated when the distribution is large and many values in the distribution turn out to be medians. But still it holds true.

The concept of recursive medians was first given in Median of Medians Algorithm [4]. This concept is widely used in Selection algorithms and Quick sort. Median of Medians Algorithm is not used as such in the proposed algorithm. It is used in a much simpler way and with little modification in the proposed algorithm.

The recursive medians concept is used in the function low_probable_combine_itemsets. In this function, recursive medians are calculated for a list of transactions against an itemset. It is used to find the maximum number of exactly same transaction ids for any two transaction lists within well defined boundaries. Recursive medians are used to evaluate dispersion of elements in a given distribution. Any two successive medians indicate a range containing only one element. This concept when used repetitively with all the possible medians for a distribution deduces the maximum number of elements present within a given range. Though, it cannot predict the exact number of ?same? elements for two distributions but it gives a maximum limit to the number of elements that can be ?same?. This concept is explained in detail in Section 4.5.

3.3. Inverted V-Median Search tree (IVMST) It is a V-shaped binary search tree where splitting nodes hold the value of recursive medians calculated for a distribution.

The concept of recursive medians in the context of algorithm is implemented in the form of an Inverted V-Median Search Tree. The root node of the IVMST holds the first median value for the distribution. The root node is the only binary node with two children. Rest of the nodes are unary with only one child. IVMST follows the basic principle of binary search tree. The left subtree of a node contains nodes with keys less than the node?s key. The reverse is true for the right subtree of a node. Thus, leftmost and rightmost leaf nodes hold the smallest and largest median value respectively. The implementation details of IVMST are given in Section 4.5.

3.4. Single Data Scan Unlike apriori algorithm which performs k data scans for k passes, the proposed algorithm performs only one data scan.

The database is not used to count the support for the k- candidate sets after the first pass. Rather, the information is collected in the data sets L1, C1 and C1. The information stored in L1, C1 and C1 is modified in further passes. For small values of k, the size of data sets Lk, Ck and Ck is larger than the database. But for large values of k, the size of data sets becomes significantly smaller than the database. This feature may appear similar to AprioriTid, but its implementation in the proposed algorithm is completely different. The format of the entries in L1, C1 and C1 is different from that in AprioriTid. The format for these data sets is given in Table 1.

Instead of storing information in form of transactions (done in AprioiTid), we store information in form of itemsets along with the corresponding transactions in which they are present. The first field in C1 represents the 1-itemset and the second field is a list of all the transaction ids which contain that itemset. C1 is same as C1 except that it contains the count of all the transaction ids as ?support? in its second field. This way of storing information is inherent to the success of the algorithm as it proves to be time efficient in later passes.

Also, this format is a key requirement, keeping in mind the probabilistic approach of the algorithm.

Table 1: Format of Data Sets  Data Set Format L1 {1-itemset} C1 {1-itemset, list of transaction ids} C1 {1-itemset, support}  4. Algorithm This section discusses the proposed algorithm in detail. It is divided into 6 sub sections. The first sub section explains the notations used in the algorithm. The second subsection presents the general structure of the proposed algorithm. Sub sections 4.3 - 4.6 explain the various functions invoked in the algorithm.

4.1. Notations used in the algorithm The algorithm uses various data sets to perform different operations. Also, values returned from the functions called from the main need to be stored in appropriate data sets. The notations used in the algorithm are given in Table 2.

Table 2: Notations  Data sets Format of each entry  Lk-1 {Large (k-1)-itemset} Ck-1 {Large (k-1)-itemset, list} Ck-1 {Large (k-1)-itemset, support} MEDIUM {(k-1)-itemset, support}  MEDIUM {(k-1)-itemset, list}  CkNew {k-candidate set}  LOW  {k-candidate sets} (Created from the lexicographic  association of two (k-1)-itemsets MEDIUM)  CkList {k-candidate set, list} HIGH_SUPPORT { support1, support2,?, supportn }  MEDIUM_SUPPORT { support1, support2,?, supportn }  4.2 Proposed Algorithm  L1 = Large 1 itemsets  C1 = Large 1 itemsets, support C1 = Large 1 itemsets, list For k 2 ; Lk  ; k++} do begin MEDIUM = classify(Ck-1); //see Section 4.3 For all c  Ck-1 do begin For all m  MEDIUM do begin If ( m.itemset = c.itemset ) Then Insert  c. itemset, c. list  into MEDIUM; End End CkNew = apriori_gen( Lk-1 ); //see Section 4.4 LOW=low_probable_combine_itemsets(MEDIUM);  //see Section 4.5 Delete all k-candidate sets present in LOW from CkNew; CkList = gen_candidates_with_list ( Ck-1, CkNew ); //see Section 4.6 For all c  CkList do begin If ( c.count (list)  minsup ) Then Lk =  c. itemsets Ck =  c. itemsets, c. list Ck =  c. itemset, c. count list End if End End Answer := Lk;      L1, C1 and C1 are the data sets obtained after the database scan. A subsequent pass through the algorithm, say pass k, consists of 4 phases. The first phase is the classification phase. In this phase, the large itemsets Lk-1 from previous pass are classified in two groups on the basis of bimodality in probability density function for support. These two groups are HIGH and MEDIUM. The second phase calls the apriori_gen function and generates k-candidate sets from lexicographic association of large itemsets Lk-1. These k- candidate sets are stored in CkNew. The next phase is performed in two steps. First, the large k-itemsets present in MEDIUM are processed in the function low_probable_combine_itemsets to identify those k- candidate sets that will fail in the minimum support test.

These k-candidate sets are stored in data set LOW. In the second step, all the entries present in LOW are deleted from CkNew. In the last phase, the list of transactions corresponding to each k-candidate set in CkNew is generated using function gen_candidates_with_list by intersection of the list of all its immediate (k-1) subsets present in Ck-1. Pruning of all the k- candidate sets with support below minimum support gives the large k-itemsets. This process is repeated until no new large k-itemsets are produced by the previous pass.

4.3 Classification of Large (k-1)-itemsets In the function classify, Ck-1 is passed as an argument. The data set Ck-1 contains support for each large (k-1)-itemset.

This function generates a probability density function (pdf) for support. We check the condition for bimodality in this probability density function. If such a condition exists, we divide the support distribution into two groups ? HIGH_SUPPORT and MEDIUM_SUPPORT. The point of division is given by the condition of bimodality.

HIGH_SUPPORT contains all the support values above the point of separation and MEDIUM_SUPPORT contains all support values below it.

Select max_val  MAXIMUM (c.support ); min_val  MINIMUM ( c.support ); mean  AVG ( c.support ); where c  Ck-1; For all c  Ck-1 do begin difference += ( c.support ? mean ) ^ 2; If ( c.support = max_val ) Then Pop c from Ck-1; Insert c.support into HIGH_SUPPORT; Else If ( c.support = min_val ) Then Pop c from Ck-1; Insert c.support into MEDIUM_SUPPORT; End  standard_deviation = ? (difference Total Transactions? ); standard_difference = 2*standard_deviation;   lean = ? 1 c. support  - ? 1 c. support ; Total number of Transactions  /* c  Ck-1 and lean is a measure of Skewness*/ While ((AVG(HIGH_SUPPORT)-AVG(MEDIUM_SUPPORT)  standard_difference) AND MEDIUM_SUPPORT  ) /*Condition for Bimodality*/  If ( Ck-1  ) Then Pop c from Ck-1 ; Insert c.support into HIGH_SUPPORT; Else Pop c from MEDIUM_SUPPORT; Insert c.support into HIGH_SUPPORT; End while loop Pop the last support value inserted into HIGH_SUPPORT; If ( HIGH = NULL) Then If ((lean < 0) OR (lean = 0 AND (min-minsup  max_val-mean))) Then HIGH_SUPPORT = Ck-1; ElseIf((lean > 0) OR (lean = 0 AND (min-minsup < max_val-mean))) Then HIGH_SUPPORT = NULL; End If MEDIUM_SUPPORT = Ck-1 ? HIGH_SUPPORT; Update MEDIUM_SUPPORT to contain only distinct values; For c  Ck-1 do begin For m  MEDIUM_SUPPORT do begin If ( c.support = m.support ) Then Insert c.itemset, c.support into MEDIUM; End End  The data sets HIGH_SUPPORT and MEDIUM_SUPPORT may contain the same value multiple times. This is because more than one itemsets may have same support. This function also keeps track of skewness in the pdf.

MEDIUM_SUPPORT contains all the support values present in Ck-1 but absent in HIGH_SUPPORT. MEDIUM is constructed from MEDIUM_SUPPORT. MEDIUM contains all those itemsets present in Ck-1 whose support is present in MEDIUM_SUPPORT.

4.4 Candidate Generation The apriori_gen is exactly similar to the candidate generation function used in apriori algorithm. It involves two steps to generate k-candidate sets from lexicographic combination of (k-1)-itemsets. The first step is the join step.

It is followed by prune step. The first step uses all possible lexicographic combinations of Lk-1 to generate new k- candidate sets. The prune step removes a candidate set if any one of its immediate subset is not present in Lk-1.

apriori_gen ( Lk-1) Join step Insert into CkNew select p.item1, p.item2,?, p.itemk-1, q.itemk-1 from Lk-1 p, Lk-1 q where p.item1 = q.item1, p.item2 = q.item2,?, p.itemk-1 < q.itemk-1; Prune step For all itemsets c  CkNew do begin For all (k-1) ? subsets s of c do If (s ? Lk-1) Then delete c from CkNew End End 4.5 Generating low probable combinations of (k-1)- itemsets In this function, all pairs of lexicographically similar (k-1)- itemsets (  MEDIUM) are processed to identify those pairs which will generate k-candidate set with support lesser than minimum support. Recursive medians are used to identify such candidate sets. These low support k-candidate sets are stored in the data set LOW. Pruning these unsuccessful k- candidate sets at an earlier stage results in increased performance of the algorithm. The average case and the worst case complexity for insertion as well as searching in IVMST is O(log n). Hence, IVMST presents a very efficient method to implement the recursive medians concept.

The first field of data set MEDIUM is the (k-1)-itemset itself and the second is a list of transaction ids in which that itemset is present. We denote the end limits of the overlapped region for transactions lists of the two lexicographically similar (k-1)-itemsets as START and FINISH respectively. Thus, START and FINISH are the boundary limits which denotes the region where the two (k- 1)-itemsets may have ?same? transaction ids. The ?same? transaction ids refer to transactions that contain both the itemsets.

The identification of the ?same? transaction ids is performed in two stages. In the first stage, we check whether the range of the boundary limits (represented by FINISH-START) is at least as large as minimum support. If the two itemsets fail in this test, the resulting k-candidate set is immediately placed in LOW and we proceed with the next pair of (k-1)-itemsets.

If the pair of itemsets passes this test, then we move to the second stage of testing. The second stage is implemented using the concept of recursive medians and IVMST. IVMST is created for each of the two itemsets. All possible medians for the list of transaction ids are represented as nodes in IVMST. Using the fact that successive medians have only one element in between, we predict the maximum number of ?same? transaction ids that can be present in the range {START, FINISH}. We again check whether the number of ?same? transaction ids will be at least equal to the minimum  support. If they fail to fulfill this criterion, then the resulting candidate set is pushed into LOW.

For all possible lexicographic k-candidate combinations among (k-1)-itemsets in MEDIUM do Let A and B be any two random lexicographically similar (k-1)-itemsets in MEDIUM Let [AB] be k-candidate set obtained from lexicographic association of A and B START  MAXIMUM (Min_TidA, Min_TidB); FINISH  MINIMUM (Max_TidA, Max_TidB); If (FINISH-START < minsup) Then Insert [AB] to LOW; Else Repeat for both A and B IVMSTX  Inverted V Median Search Tree (X); /* Depth of nodes on left subtree are taken negative */ /* Depth of nodes on right subtree are taken positive */ HLX  { node_depth; node  IVMSTX and node_value  START }; ValueLX  node_value (node_depth = HLX); HRX  { node_depth; node  IVMSTX and node_value   FINISH }; ValueRX  node_value (node_depth = HRX); For all nodes  IVMSTX do begin If (node_value  list AND node_value  RANGE{START, FINISH} ) Then XX  XX+1; End If (ValueLX  START AND ValueLX  list) Then XX  XX-1; If (ValueRX  FINISH AND ValueRX  list) Then XX  XX-1; ElementsX  ABSOLUTE(HRX-HLX) + XX; End Repeat; Maximum_same_list=MINIMUM (ElementsA,ElementsB); If (Maximum_same_list < minsup) Then Insert [AB] to LOW; End of If Else Statement End of for loop  4.6 Generating list of transaction ids for newly generated candidates This function generates the list of transaction ids for each of the k-candidate set present in CkNew. We use the property that all possible (k-1) subsets for the k-candidate set are already present in Ck-1, as k-candidate sets with missing (k-1)-subsets were already pruned in the apriori_gen function. The list for     each of the k-candidate set is generated by intersection of the list (of transaction ids) of all its (k-1) subsets. The k- candidate set along with the list is pushed into the data set CkList. This is followed by pruning of all those k-candidate sets that do not have support at least equal to minimum support. Thus large k-itemsets are obtained.

gen_candidates_with_list ( Ck-1, CkNew )  For all c  CkNew do begin For all possible (k-1) subsets s of c do Insert { itemset,list } into CkList where itemset = c.itemset AND list = si.list  // s  Ck-1 End End  5. Performance Study To confirm the time efficient approach of the algorithm, we implemented the proposed algorithm and the apriori algorithm on a 512 MB RAM Memory and 1.67 GHz Intel processor. We carried out a substantial performance evaluation and compared it with results obtained for apriori algorithm.

Table 3 compares performance of the proposed algorithm with the apriori algorithm for different minimum supports against different number of transactions. As minimum support decreases, the performance of the proposed algorithm increases and after a certain point, it performs better than the apriori algorithm. The relative performance also increases with increase in number of transactions. These results are also demonstrated in Figure 1. Total number of data items and average number of items per transaction are 30 and 15 respectively.

Table 4 compares the number of items in MEDIUM and LOW corresponding to the data values shown in Table 3.

The execution time is shown to compare the increase in performance with increase in number of candidate sets in LOW.

Figure 2 compares the number of elements in MEDIUM and LOW against different number of initial data items for a given database and minimum support. For this experiment, number of transactions in the database is 2K and minimum support is 20%.The proposed algorithm performs better with increase in the number of candidate sets present in LOW.

Figure 2 shows that the number of elements in MEDIUM and LOW increases with increase in number of data items.

Thus, the proposed algorithm performs better for large number of data items in the database.

5.1 Result Analysis The performance of the proposed algorithm is directly proportional to the number of passes over the algorithm, number of elements in LOW and data items in the database.

During initial passes, size of Lk, Ck and k is more than the database. Hence, initial passes are time consuming. But during the latter passes, these data sets become significantly smaller than the database. Larger minimum support generates less number of candidate sets and involves lesser passes over the algorithm. Thus, Figure 1 shows a dip in performance of the proposed algorithm for larger minimum support values. But, it performs better than the apriori algorithm as minimum support decreases.

Table 3: Execution time in seconds   Table 4: Execution time v/s size of MEDIUM and LOW    6. Conclusion and future directions The proposed algorithm presents a probabilistic approach to apriori algorithm. The proposed algorithm creates exactly the same number of rules as the apriori algorithm but in lesser time. The results confirm that the discovery of association rules in larger databases is faster in the proposed algorithm than the apriori algorithm.

Integration of probabilistic approach to data mining in distributed databases shall produce amazing results [9].

Hence, the effectiveness of the proposed algorithm needs to be checked on distributed databases. This algorithm was implemented on a synthetic database. Its application on real     world databases presents a future direction for further research.

