A Document Clustering Technique Based on Term  Clustering and Association Rules

Abstract?With development of internet and database technology, web mining has got more and more attentions from information science domain. This paper proposes a document clustering technique based on term clustering and association rules. In this technique, extract words from document collection firstly, then construct term clustering according to AMI(Average Mutual Information) between terms, document VSM(Vector Space Model) is represented by term clustering, and use association rules to mine document clustering. Experiment results show that performance and clustering quality of this technique are improved than those of traditional methods in the clustering process.

Keywords-term clustering; association rules; web mining; vector space model; document clustering

I.  INTRODUCTION In general, with difference of contents, web mining has  three types: web structure mining, web usage mining and web content mining [1]. Document clustering belongs to web content mining and content mining is to find some models or rules implied in document collection, if see the whole process of content mining is an I/O system, document collection D is the input and implied models or rules MR is the output, so the task of content mining is to find a shine upon ? from D to MR: ?D MR??? .

Document clustering process is different from document classification, it is not based on kinds of table or category system but documents themselves, namely, it uses documents to get the kind of document, the content and extension of the kinds and the whole category system are all completely determined by document collection which needs to do clustering. Now the commonly document clustering methods are hierarchical method, partitioning method, etc.



II. DOCUMENT FORMAL DESCRIPTION  A. Document Participle Process and Pretreatment Firstly do participle process to all documents in D, all terms  obtained have formed a term collection, then do pretreatment to the term collection, the main steps of pretreatment are:  ? According to far-between words table and inactive  words table, remove all far-between words and inactive words from term collection;  ? Remove some words which have high appearance in lots of documents, such as some auxiliary verb, preposition, etc.

? According to thesaurus to deal with some terms whose semantic are same or very related. choose formal terms in ?Chinese Classification Themantic Words Table? to represent these terms whose semantic are same or very related,  seeing the formal terms are equal to these terms and operation on formal terms are equal to on these terms, such as ?malingshu?, ?tudou?, ?yutou? are different names of potato, but only use ?malingshu? to represent potato in this paper.

B. Term Clustering Generally, there are some regulations in match process  between terms, the representation is that there are some certain regular match relationship between terms and others, there are two type of similar relationship between two terms: semantic similar and semantic relevance, the semantic similar is about synonym and parasynonym, this has been processed before, and using  AMI (Average Mutual Information) to represent relevance degree of two terms.

AMI? iT , jT ?= i j j2 j i j  n(T ,T ) n(T ) log  n(T ) n(T ,T ) +  i j j  j i j  n(T ,T ) n(T ) log  n(T ) n(T ,T ) + i j j  j i j  n(T ,T ) n(T ) log  n(T ) n(T ,T )   + i j j  j i j  n(T ,T ) n(T ) log  n(T ) n(T ,T ) , 1?i, j?m.

AMI? iT , jT ?is Average Mutual Information between  iT and jT ; jn(T ) is number of documents where jT  appears in D; i jn(T ,T ) is number of documents where both iT  and jT  appear in D; i jn(T ,T ) is number of documents where neither  iT  nor jT  appears in D; i jn(T ,T ) is number of documents     where iT  appears but jT  not in D; i jn(T ,T ) is number of documents where jT  appears but iT  not in D.

C. Term Weight This paper use Shannon?s reverse document frequency and  Dennis?s information and noise rate but do some modifications on these two methods to form term weight, use relative  frequency k, j  j  tf f  to replace absolute frequency k, jtf  in these  two methods? weight computing formula, then take the square root of the product of two weight value which are separately computed by these two modified weight-computing formula.

k , jW = idf inr k j k jw w?, ,  = k, j  j  tf f  ? m k,j j 2 2 j 2  j=1j j k, j  tf fn [(log )+1] [(log f ) - ( log )] d f tf  ? ??  idf k jw , is weight value of jT  in kD which is computed by  modified inverse document frequency; inr k jw , is weight value  of  jT  in kD which is computed by modified information and noise rate; t k, jf is total frequency of jT  in D; n is total number of documents in D; jd is number of documents which contain  jT  in D.

D. Document Vector Space Model Generally speaking, because terms in every term clustering  are similar or relative, they are used to describe some certain questions or objects in the same or related domain; in the other aspect, documents themselves are also the detail description of some certain questions or objects in the same or related domain, so use term clustering to form the document vector space mode is a good method.

kD =(< 1TC ? k,1r >?< 2TC ? k,2r >???< iTC ?  k,ir >???< tTC ? k,tr >??1?k?n.

k,ir is relevance degree between document kD  and term clustering iTC .



III. USE ASSOCIATION RULES TO MIN DOCUMENT CLUSTERING  A. Association Rules Let set O is a group of objects, each object is also a subset  of project set P, Let pO  is a subset of P, So we can define the support degree p(O )?  of pO  is number of objects which  contain pO  in O, let min_sup is minimum support degree, if  p(O )? ?min_sup, we say pO  is a frequent item-set. An association rule is an implication of the form pO ? qO , where pO ? P, qO ? P,and pO  qO = ? .Support degree of association rule pO ? qO  is defined as: sup? pO ? qO ?  = p q O O )  n ?(  (where, n is total number of objects in O);  confidence degree of this association rule is defined as: conf?  pO ? qO ?= p q  p  O O ) (O )  ?( ?  , let min_conf is minimum  confidence degree, if conf? pO ? qO ??min_conf, we say association rule pO ? qO is confidently.

B.  Document Similar Degree The square root of product of Jaccard coefficient and  Cosine coefficient is chosen to be document similar degree.

p qts(D ,D ) = p q p qjs(D ,D ) cs(D ,D )?  = 1 1t t t t  2 22 4 p,i q,i p,i q,i p,i q,i p,i q,i  i=1 i 1 i 1 i=1 (r r ) [ (r r r r )] [ (r ) (r ) ]  ? ?  = =  ? + ? ? ?? ? ? ?  p qjs(D ,D ) and p qcs(D ,D )  separately represents Jaccard coefficient and cosine coefficient; t is total number of term clustering in T; p,ir  is relevance degree between pD and  iTC ; q,ir  is relevance degree between qD  and iTC .

C. DHP Algorithm This technique uses DHP [4] algorithm(Direct Hashing and  Pruning) to min association rules, it is an advanced algorithm of Apriori, the using of hash technology of it has effectively reduced the number of candidate k-itemsets especially 2- itemsets, so this algorithm has obviously improved the performance of Apriori.

DHP algorithm:  Input: r(D TC)? , min_sup, large_num;  Output: k kL ,H ,k-itemsets;  After DHP algorithm mined initial document clustering, then whole document collection D is formed by different document clusterings which are not relevance or have very few similar degree with each other, and every document clustering is formed by documents which are similar with each other.

D=? 1DC , 2DC , ?, xDC , ?, dDC ?, 1?x?d.

xDC is the thx document clustering in D; d is total number    of document clustering in D.



IV. EXPERIMENT RESULTS The document collection is composed of documents which  come from CNKI (Chinese National Knowledge Infrastructure), there are 562 documents in   document collection.

In experiment, comparing new technique with k-means and k-medoids to test its effectiveness, then compute ADD (Average Difference Degree) and ASD(Average Similar Degree) of document clustering and cost of time of clustering process of these three different methods on five different document collections, the results are show in Figure 1.

The experiment results show that average difference degree and average similar degree of document clustering in new technique are higher than these of k-means and k-medoids, namely, the performance and quality of document clustering are better. Cost of time of these three methods are show in Figure 2, it show that cost of time of these three methods are all rise with increase of number of documents in document collection, but cost of time of our new technique is fewer and its rise speed is slower than other two methods, there are the results of using DHP algorithm to min association rules and using term clustering to form document vector space model which are all reduce cost of time of clustering process.

