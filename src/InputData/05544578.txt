Research on Mining Positive and Negative Association

Abstract - Positive and negative association rules are important to find useful information hided in massive data sets, especially negative association rules can reflect mutually  exclusive correlaiton among items. Despite a great deal of  research, a number of challenges still exist in mining positive and negative association rules. In order to solve the problem of  "difficult to determine frequent item sets" and "how to delete contradictive positive and negative association rules", the paper  presents a new algorithm for mining positive and negative association rules. The algorithm applies a new measurement  framework of support and confidence to solve the problems existing. The performance study shows that the method is highly  efficient and accurate in comparison with other reported mining methods.

Index Terms - Association Rules; Data Mining; Negative Association Rules; Positive Association Rules;

I. INTRODUCTION  Association rules provide a convenient and effective way to identifY and represent certain dependencies between attributes in a database. Association rules mining includes positive and negative association rules mining. In the traditional approach to find association rules, one merely thinks in terms of positive association rules: especially when determining the degree of support and confidence. Previous studies propose that negative association rules have high accuracy at handling massive unstructured data [1].

However, these approaches may also suffer some weakness.

On one hand, it is not easy to find interesting or useful frequent itemsets. Traditional techniques always select some simple criterions based on only positive frequent itemsets such as confidence, distance functions, rough set and so on, while ignoring the value of negative frequent itemsets. The simple pick may affect the mining accuracy.

On the other hand, a training dataset often generates a huge set of positive and negative association rules. It is challenging to delete contradictive positive and negative association rules efficiently [2].

To solve these problems, in this paper, we develop a new mining algorithm for positive and negative association rules.

Because the traditional approaches often are based on simple criterions.

Therefore, in this paper we will find the positive and negative association rules from training dataset and prune contradictory positive and negative association rules.

ZhangBo  School of Mathematics and Information Science Henan Polytechnic University  JiaoZuo, Henan Province China  zhangbocnu@sina.com  According to the new measurement framework of support and confidence, this paper presents a new algorithm for data mining. The final comparative experiment shows that the algorithm has a higher recall rate and precision rate, is feasible and effective [3].

This work makes the following contributions: (1) It proposes a new measurement framework instead of  relying on positive frequent itemsets for finding accurate frequent itemsets. This measurement framework uses a small set of high confidence positive and negative rules to determine the frequent itemsets, and experimental results show that this way is, in general, more accurate than other techniques;  (2) It proposes a new mining algorithm for collecting useful and non-contradictory positive and negative associative rules based on an improved correlation function.

The remaining of the paper is arranged as follows. Section IT revisits the general idea of association rules. Section m  presents the algorithm for generating frequent itemsets and determining positive and negative association rules . The experimental results on accuracy and the performance study on efficiency and scalability are reported in Section N .The  paper is concluded in Section V.



II. ASSOCIA nON RULES  This paper assumes that the training dataset is a normal relational table, which consists of N data objects described by L distinct attributes. Attributes can be categorical or continuous. For a categorical attribute, we assume that all the possible values are mapped to a set of consecutive positive integers. For a continuous attribute, we assume that its value range is discredited into intervals, and the intervals are also mapped to consecutive positive integers. So, we treat all the attributes uniformly in this study.

Let D be the training dataset. In the training dataset, Let I be the set of all items in D , that means every data object has some attributes following the form I={h i2, ? ? ?  , in} . We say that a data object d E D contains X ? I , a subset of items(called itemsets). A association rule is an implication of the form X ? Y where X ? I , and Y ? I . The number of data objects in D matching X and Y is called the support of the rule X ? y, denoted as sup(X ?J?The ratio of the number of objects matching X and Y versus the total number of objects matching X is called the confidence of the rule X ? Y, denoted as conf(X ?J? In general, given a training dataset, the task of  CCTAE2010    mining association rules is to find all association rules with high accuracy [4].

For example, if 80% of customers who have bought apples also buy oranges, i.e., the confidence of rule: apple-+oranges is 80%. To avoid noise, a rule is effective only if it has enough support. Given a support threshold and a confidence threshold, the method finds the complete set of association rules passing the thresholds [5].

In the training dataset, there also exists other association rules: X -+- Y, -X -+Y and -X -+- Y. The rule X -+- Y means the data objects which have itemset X do not have the itemsets Y. The rule -X -+ Y means the data objects which do not have itemsets X have the itemsets Y. The rule -X -+ - Y means the data objects which do not have itemsets X do not have the itemsets Y. These rules can be called negative association rules. The rule X -+ Y can be called positive association rule. The support and confidence of negative association rules can be defined as the support and confidence of positive association rule [6].



III. ALGORITHM FOR MINING POSITIVE AND NEGATIVE ASSOCIATION RULES  The algorithm in this paper consists of two phases: frequent itemsets generating and mining all association rules.

In the first phase: the algorithm computes the complete set of positive and negative frequent itemsets that pass the given support and confidence thresholds, respectively. Furthermore, it will prunes some contradictory itemsets between positive and negative frequent itemsets and only selects a subset of high quality frequent itemsets.

In the second phase: Furthermore, the algorithm will find all positive and negative association rules according to frequent itemsets computed in the first phase, also it will prunes some contradictory rules based on an improving correlation function.

A. Generating Frequent Itemsets So, the first step is to generate all the frequent itemsets by  making multiple passes over the data. In the first pass, it separately counts the support of individual positive and negative itemsets and determines whether it is frequent. So, we can get positive and negative frequent itemsets(P _FI and N _Fl). If there is an itemsets X not only existing in P _FI and also in N _FI, we should compare sup(X) with sup( -X) and delete the itemsets which one is small. In each subsequent pass, it starts with the seed set of itemsets found to be frequent in the previous pass. It uses this seed set to generate new possibly frequent itemsets, called candidate itemsets. The actual supports for these candidate itemsets are calculated during the pass over the data. At the end of the pass, it determines which of the candidate itemsets are actually frequent. So, we will compute frequent itemsets(Fl).

The algorithm of generating frequent itemsets is shown as follow: Algorithm 1 Input: N _FI and P _FI  Output: F (J)PJ=InitPass(P _FI), NJ=InitPass{N_FI); (2)P _Fh={flfEPJ), N_Fh={flfENJ }, (3)for(k=2; hJ!=NULL; k++) ( Sk=CandidateGen(P _Fh N_Fh Fk-J); for (each t ET) { for (each candidate s ES,J   { if (s is contained in t) (the number of s)++;  ) Fk={S ESk I sup(s?=minsup }; } (4)Return FI= Uh?  In this algorithm, there is an important function CandidateGenO which generates k-itemsets based on Fk.J. The code of it is shown as follow: Algorithm 2 Input: P _FI, N _FI, hJ Output: Sk (J)Sk=NULL; (2)for(allfi EFk_f, ,12 EN_FIJ or P _FIJ andh ! EhJ)  { s= {if, i2, ??? ik-2, ik-f, h }; Sk=SkU{S }; For(each (k-1)-subset s of s) ( if (s! ESk_J) delete s from Sk; } }  B. Generating Positive and Negative Association Rules Then, the next step is to generate positive and negative  association rules. It firstly finds the frequent itemsets contained in FI which satisfy min_sup and min_corif threshold. Then, it will determined the rules whether belong to the set of positve correlation rules P _ AR or the set of negative correlation rules N AR.

The algorithm uses the correlation between itemsets to find positive and negative association rules. The correlation between itemsets can be defined as:  (X Y) sup(X u Y)  corr , = --'--''----'-  sup (X)sup(Y) (1) X and Y are itemsets.

When corr()(, Y? 1, X and Y have positive correlation.

When corr()(, Y)= 1, X and Y are independent.

When corr(}{, Y)<1, X and Y have negative correlation.

Also when corr(}{, Y? 1, we can deduce that corr(}{,  Y)<1 and corr( -)(, Y)<1.

The algorithm of generating positive and negative  association rules is shown as follow: Algorithm 3 Input: training dataset T, min_sup, min_corif    Output: P_AR, N_AR (l)P _AR=NULL, N_AR=NULL;  (2)for (any frequent itemsets X and Yin FI) { if (sup(X -+ }j> min _sup and corif(X -+}j> min _conj) if( corr()(, }j> I) { P _AR= P _AR U{X-+Y};  } } (3)for (any frequent itemsets X and -Y in Fl)  ( if (sup(X-+ -}j>min_sup and corif(X-+ -}j> min_conj) if( corr()(, -}j<l)  { N_AR= N_AR U{X-+ -Y};  } } (4)for (any frequent itemsets -x and -Y in FI)  ( if (sup( -X-+ -}j>min_sup and conf( -X-+ -}j> min_conf) if( corr( -)(, -}j> I)  { N_AR= N_AR U{ -X-+ -Y};  } } (5) return P_AR and N_AR;  In this algorithm, we use a new method generates the set of frequent itemsets FI, In FI, there are some itemsets passing certain support and confidence thresholds. And the correlation between itemsets is used as an important criterion to judge whether or not the correlation rule is positve. Lastly, P _AR and N AR are returned.



IV. EXPERIMENTAL RESULTS  To evaluate the accuracy and efficiency of the algorithm, in this section, we report our experimental results on comparing the algorithm against the popular method: Apriori.

All the experiments are performed on a 2.20Hz Core PC with 10 main memory, running Microsoft Windows Server 2003.

Apriori was implemented by its authors, respectively. We choose a training dataset including 1000 objects which have 12 attributes.

In our experiments, min_corifis set to 50%. For min_sup, it is more complex, because min_sup has a strong effect on the quality. If min_sup is set too high, those possible rules that cannot satisfy min_sup but with high confidences will not be included, and also the rules may fail to cover all the training cases. In the experiments reported before, we set min_sup to 6%. The results are shown in Table I.

TABLE I EXPERIMENT RESULTS  ? Apriori  Rules  Positve Association Rules 199 Negative Association Rules 0 Total 199 Recall Ratio 91.2% Precision Ratio 89.2%  Algorithm in this paper  93.4% 90.5%  As can be seen from the table, the algorithm outperforms Apriori on recall ratio and precision ratio. Our Recall Ratio and Precision Ratio is higher than Apriori. So, it shows that the algorithm outperforms Apriori in terms of average accuracy and efficiency.



V. CONCLUSIONS  In this paper, we examined a number of problems that exist in current techniques of positive and negative association rules. A new algorithm is presented to generate all positive and negative association rules. The method has several distinguished features: (1) its frequent itemsets is performed based on positive and negative frequent itemsets, which leads to better overall accuracy; (2) it prunes contradictory positive and negative association rules effectively based on correlation between itemsets. Our experiment shows that the algorithm is highly effective at classification and has better average classification accuracy and efficiency in comparison with Apriori. In our future work, we will focus on building more accurate algorithm by using more sophisticated techniques and mining more useful positive and negative association rules.

