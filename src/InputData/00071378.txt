HETER0 ASSOCIATIVE NEURAL NETWORK  FOR PATIERN RECOGNITION

ABSTRACT  We present an Inter-Pattern Association (IPA) n e u r a l  n e t w o r k  m o d e l ,  in  w h i c h  b a s i c  l o g i c a l operations are used to determine the association among common and spec ia l  f ea tu re s  o f  reference pat terns  (i .e. ,  in ter-pat tern associat ion) .  Hetero- and  au to -as soc ia t ive  memory  a r e  syn thes i zed  by a p p l y i n g  a g e n e r a l i z e d  l o g i c a l  ru l e .  Compute r simulations for pattern recognition by using the IPA mode l  h a v e  s h o w n  a b e t t e r  p e r f o r m a n c e  and  a higher storage capacity than Hopfield model. A 2-D adaptive optical  neural network is  used to perform p a r a l l e l  n e u r o c o m p u t a t i o n s .  S i n c e  t h e Interconnection Weight Matrix (IWM) for IPA model has a tr i-state structure,  the dynamic range imposed on a Spatial Light Modulator (SLM) is rather relaxed, and the interconnect ions are  much s impler  than the Hopfield model.

In  this  paper,  we shall  present a neural  network model based on the association between reference patterns. First ,  th is  model determines the common and spec ia l  f ea tu re s  among pa t t e rns  by app ly ing logical operations on the pixels in the pattern space.

Based on the relationship between the special  and common features ,  two equivalent logical rules  are presented in Sec. I1 to  construct the excitatory and inhibitory interconnections in the network. In Sec.

111. the Inter-Pattern Association (IPA) model is c o m p a r e d  w i t h  t h e  H o p f i e l d  m o d e l  i n  t h e performance ve r sus  number of reference pa t t e rns in a noisy environment.  Computer s imulat ions of IPA model are conducted for  recognizing tools  of different orientations.  In Sec.  IV, a 2-D hybrid p a r a l l e l  o p t i c a l  n e u r a l  ne twork  i s  u sed  f o r  t h e implementation of IPA and Hopfield models.  The experimental  results show that IPA model performs more effectively than Hopfield model.



I. INTRODUCTION 11. IPA MODEL, AND ASSOCIATIVE MEMORY  The superior  capabili ty of human perception in p a t t e r n  r e c o g n i t i o n  h a s  s t i m u l a t e d  a g r e a t e n t h u s i a s m  a m o n g  n e u r a l  n e t w o r k  r e s e a r c h e r s .

Mathematical  modeling of various neural  networks has been developed during the past  several decades [l-41. However, for recognition of a given set  of r e f e r e n c e  p a t t e r n s ,  m a n y  n e u r a l  n e t w o r k a lgo r i thms  cons t ruc t  t he  In t e rconnec t ion  Weight Matrix (IWM) by emphasizing on the association of the elements within each reference pattern (i .e. ,  the i n t r a - p a t t e r n  a s s o c i a t i o n ) ,  w h i l e  p a y i n g  l i t t l e attention to  the association between patterns (i .e. , inter-pattern association). For example, in Hopfield type neural networks[5],  the outer-products of the reference patterns are added to form the IWM. This type of approach is  based on the assumption that r e f e r e n c e  p a t t e r n s  a r e  s i g n i f i c a n t l y  d i f f e r e n t .

However ,  in pract ice ,  the reference pat terns  are usually not independent,  and the differences among the patterns are often very small  (e.g. human faces, f inger pr ints ,  handwritten characters,  etc.). Thus it may create an unstable o r  ill-conditioned network.

Under these circumstances.  the special  features of t h e  p a t t e r n s  p l a y  a n  i m p o r t a n t  r o l e  i n  p a t t e r n recognition. Therefore it i s  necessary to  consider t h e  r e l a t i o n s h i p s  b e t w e e n  t h e  common  and  t h e spec ia l  f ea tu re s  among the  r e fe rence  pa t t e rns  in constructing the IWM.

T h e  in fo rma t ion  ob ta ined  from p ixe l s  in  the spec ia l  a r eas  of  a r e fe rence  pat tern i s  gene ra l ly more important  than that from the common areas ( i . e .  a r e a s  sha red  by more  than  o n e  p a t t e r n )  i n terms of constructing the associative memory matrix for a neural  network. The basic concept of IPA model  i s  to  de t e rmine  whe the r  t he  p i x e l s  in  t h e pattern space belong to  special o r  common spaces of the reference patterns,  and then set  the excitatory o r  i n h i b i t o r y  i n t e r c o n n e c t i o n s  a c c o r d i n g  t o  a logical relationship.

Figure 1 shows an example of a training set  that consists of three overlapping patterns A, B and C in space S 1  ( see  F i g . l ( a ) ) ,  and the i r  co r re spond ing desired output patterns A', B' and C' in space S2 (see F i g . l ( b ) ) .  T h e s e  p a t t e r n s  c a n  b e  d i v i d e  i n t o  7 subspaces; For instance, in space SI .  subspaces I, I1 and I11 are the special areas of patterns A, B and C.

respectively. IV, V and VI are the common areas of A and B. B and C, C and A, respectively, while VI1 is the common area of A, B and C. And the rest can be def ined as  an empty space a. Similar ly ,  for  the subspaces in S2. as  illustrated in Fig. l (b) .  The s u b s p a c e s  of  s i  and S2 can be expressed by the fo 1 Io w in g logical functions :     number of reference patterns. Similarly,  the logical operation in subspace X '  in S2  can also be written in the same form-, such as:  X = P A Q ,  (2') where P = p'1 A p'2 A . . . A pIn , (3')  Q = q l l  v 4 '2  v . . . v q'm . (4') The  p ixe ls  in  a r e a  X must  excite ( i .e. having  positive connections with) all the pixels in area P', inhibi t  (i.e. having  n 9 a t i v e  connections with) a l l the pixels in area Q' A P " ,  and have n o  connection with the remaining areas in S2.  For simplicity, the connection strengths ( i.e.

weights ) are  defined a s  '1' fo r  positive connections; '-1' f o r  n e g a t i v e  c o n n e c t i o n s ;  a n d  '0 '  f o r  n o connection. Thus  the IPA neural network can  b e constructed in a simple tri-state structure.

where P" is  defined by P" = p'1 v p'2 v . . . v PIn, ( 5 )  If patterns in  S 2  are  the same as  the patterns in S I .  then Rule A can be used to construct the auto- associative memory.

To i l lustrate  fur ther  the construction of  the IPA model ,  w e  s h a l l  s y n t h e s i z e  an  a u t o - a s s o c i a t i v e memory for A, B and C,  three 2x2 array patterns as shown in Figs. 2(a), (b) and (c), respectively. The pixel-pattern relationship is  given in Table I. I t  i s apparent that  pixel  1 represents the common feature of A, B and C, pixel 2 is  the common feature of A and B, pixel 3 the common feature of A and C, and pixel 4 represents the special feature of C.

By  a p p l y i n g  R u l e  A t o  t h e  t h r e e  r e f e r e n c e patterns A, B and C,  a tri-state neural network can be constructed, as  illustrated in Fig. 2(d). This is  a s ing le  layer  neural  network with 4 neurons at  the input end  and 4 neurons  at the output  end. Each neuron i s  matched t o  one  pixel of the input/output p a t t e r n s .  F o r  e x a m p l e ,  t h e  1 s t  i n p u t  n e u r o n co r re sponds  t o  p i x e l  1 o f  the  i n p u t  pa t te rn ,  and excites only the 1st output neuron. The  2nd input n e u r o n  ( t h e  c o r r e s p o n d i n g  p i x e l  b e l o n g s  t o patterns A anb  B) exci tes  both the 1st and the 2nd o u t p u t  n e u r o n s ,  w h i l e  i n h i b i t i n g  t h e  4 t h  o u t p u t neuron, which belongs t o  the special area of pattern C.

I t  i s  interesting to  analyze the s t ructure  of the IWM. Since the reference patterns are in 2-D form, the weight matrix becomes a 4-D matrix. We can partition the 4-D IWM into a 2-D submatrix array [ 6 ] , as  illustrated in  Fig.2(e). The  IWM can be divided into 4 blocks. Each block corresponds to  one output neuron. The  4 elements in  one  block represent the 4 neurons in the input end. For  example, s ince all the 4 elements in the upper-left block have value 1 ,  it indicates that  e i ther  one  of  the 4 input neurons can exc i te  t h e  1st  ouput  neuron .  In the upper-right block, as  another example, the 1st and 3rd elememts a re  0, the 2nd element has value 1,  and the 4th element is  -1. From this example we can determine t h a t  t h e  1 s t  a n d  3 r d  i n p u t  n e u r o n s  h a v e  n o connection to the 2nd output neuron, the 2nd input neuron excites the 2nd output neuron, and the 4 th input neuron inhibi ts  the 2nd output neuron.

In  order to simplify the algebraic operations, an equiva len t  rule ,  Rule  B, i s  developed as  fo l lows , w h i c h  c a n  b e  u s e d  t o  c o n s t r u c t  t h e  I W M  by examining the pixel-pattern relationships.

@ @  A' VII'  v VI  v V  111 111'  s1 s 2  ( a )  ( b )  Fig.1: ( a )  C o m m o n  a n d  s p e c i a l  a r e a s  o f  t h r e e r e f e r e n c e  p a t t e r n s ,  ( b )  c o r r e s p o n d i n g  d e s i r e d output patterns.

- I = A A ( B ~ ) ,  I ' = A A ( B ' V  e), -  I1 = B A (A v C). 11' = B' A ( A  v C'), I11 = C A (A v B), 111' = c' A (A' v B'), -  IV = (A AB)  A C  W = ( A  AB') AF, v = (B A C) A iC, v' = (B' A c) A iC, VI=(CAA)A%, W = ( C A A ' )  AFT,  (1)  vn = (A A B A C) A 5. VII' = ( A  A B' A c) AA where ' A I .  ' v ' and I - '  stand for the logic "AND" , "OR" and "NOT" operations, respectively.

When a pixel in area VI1 i s  on (i.e. the  pixel has a value '1'). i t  implies that there  is  an input, but we can not judge whether i t  belongs to  any of the A, B, and C patterns. Thus  this pixel can only excite the p i x e l s  w i t h i n  a r e a  V I I ' ,  f o r  w h i c h  i t  h a s  n o connection with the pixels  in  other areas belong to space S2.

i t  implies that the input pattern i s  not A, but we can not tell whether i t belongs to  pattern B or C. Thus this pixel should exci te  the pixels in areas V' and VII', but inhibit the pixels in  area 1'. Similarly, logical operations can also be applied to pixels in area IV or VI.

Fo r  t h e  c a s e  when a pixel i s  on  in  area I, i t implies that pattern A is  appeared at the input end.

Therefore ,  th i s  pixel  can  exc i te  a l l  the  pixels in pattern A' (i.e. areas 1'. IV', VI' and VII' ), but inhibit the pixels in areas ( B' v C' ) A T  (i.e. areas 11', 111' and V' ). Similarly, for the pixels in area I1 or I11 , they must excite area B' or C', and inhibit areas 1'.

111' and VI', or areas 1'. 11' and IV', respectively.

the descriptive logic can  b e summarized in  the following rule, which is  extented f o r  any number of  reference patterns: Rule A:  An arbi t rary subspace X in  input space S1 c a n always be represented by the following expression:  When a pixel in area V is  on,  In  view of eq.( l ) .

X = P A Q ,  (2) where P = p1 A p 2 A  . . .  A p n  , (3)  Q = q l  v 4 2  v . . .  V Sm . (4 ) p1. p2, . . . , pn. a re  input reference patterns that  occupy the subspace X , ql .  q2 . .  . . qm are  input reference patterns that d o  not belong t o  X, n and m are  two  pos i t ive  integers.  and n+m = M the total   - 7--    A  B  C  Input layer output layer  (4  1 2 3 4  1 1 1 0  1 1 0 0  1 0 1 1  Fig .2 :  C o n s t r u c t i n g  an au to -as soc ia t ive  memory matrix using IPA model: (a),  (b),  and (c) represent three 2x2 reference pat terns ,  (d)  a tr i-state neural network, (e) interconnection weight matrix.

T a b l e  I: reference patterns in Fig.2.

P ixe l -pa t t e rn  r e l a t ionsh ip  of  t he  th ree  R U k E L e t  u s  d e f i n e  D l . i  a s  a 2 - D  ma t r ix  t h a t  corresponds to the 2-D 'pixel-pattern array in table I, where 1 and i denote  the row and column number, and D i , i  as the  corresponding pixel-pat tern matrix for desired output patterns. Let di be the number o f patterns that are bright (i.e. in state '1' ) at the ith pixel ,  then i t  can be determined by summing the elements in the ith column of table I, i.e.,  di = c D,,i I I =1  d: = g,, I L1  Let us also define  Then we shall  now construct an IPA neural network by applying the following logical rules:  (1) If Ki, = di, this  means that the patterns sha r ing  the  ith pixel in S I  are included by patterns of which the corresponding patterns in S2 share  the j t h  pixel. thus pixel i in S1 should excite pixel j in  (2) If 0 < K.. < di, this implies that some patterns sha r ing  the i t h  pixel do not share t h e j r h  pixel, thus the i t h  pixel in S1 can not excite the j t h  pixel in S2.

s2;  'J  (3) If Kij =O, when d i  # 0, and d'.  z 0, which means that  no J  common pattern shares pixels i and j , in S1 must inhibit pixel j in S2;  when d i  = 0 and/or d'.  = 0, then pixel i must have no connection to pixel j .

We stress that ,  these logical rules for constructing the  IWMs are  r a the r  s t r a igh t  fo rward ,  which are suitable for computer implementations. Notice that, the IWMs computed by Rule B are equivalent  to those obtained by Rule A.

then pixel i  J  111. COMPARISON WITH THE HOPFIELD MODEL  It is the differences,  rather than the s imilar i t ies among pat terns ,  used fo r  pat tern recogni t ion.  For example, the outline of the eyes,  nose and mouth are c o m m o n  f e a t u r e s  i n  a l l  h u m a n  f a c e s .  P e o p l e dis t inguish d i f f e ren t  persons by the differences, rather than the detail of faces.

S i m i l a r  t o  m a n y  o t h e r  n e u r a l  n e t w o r k algorithms, Hopfield model constructs the IWM by c o r r e l a t i n g  t h e  e l e m e n t s  w i t h i n  e a c h  p a t t e r n , h o w e v e r ,  i g n o r i n g  t h e  r e l a t i o n s h i p s  among  t h e reference patterns. The IWM of Hopfield model for three reference patterns A, B and C can generally be expressed as  where ' ' stands for  the transpose of the vectors and I i s  the unit  matr ix ,  which makes the  weight matrix zero-diagonal.

If input pattern A is applied to the neural system, then the output would be  T = AAT + BBT + c c T  - 31 ( 9 )  V=TA = A (AT A) + B (BT A) + C (CT A) - 3A (10)  w h e r e  AT A r e p r e s e n t s  t h e  a u t o c o r r e l a t i o n  of pattern A, while BT A and CT A are the respective cross-correlation between A and B, A and C. If the d i f f e rences  between A ,  B and C a re  su f f i c i en t ly large, the autocorrelation of A, B or C would be much larger than the cross-correlation between them, i.e.,  Notice that ,  from eq. (10). pattern A has a larger weighting factor than patterns B and C. for which patterns B and C can thus be considered as noise. By choosing the proper threshold value,  pattern A can be reconstructed at the ou tpu t  end  of t he  neural network.

On the  o the r  hand,  if A, B and C are  c losely similar,  the inequalit ies of Eq. (11) are no longer he ld ,  s ince  the  we igh t s  of pa t t e rns  B and C are comparable with those of pattern A. Patterns B and C c a n  n o  l o n g e r  be  t r e a t e d  a s  n o i s e .  T h u s  t h e  AT AX-BT A, AT AX-CT A .  ( 1  1 )   7 1    threshold value for Hopfield model can not be easily d e f i n e d ,  and  t h e  neu ra l  ne twork  wou ld  become unstable.

Computer simulations of a 2-D neural network, with an 8 x 8 array neurons at the input and output ends have been conducted for both Hopfield and IPA models. The reference patterns considered are the 26 capital English letters,  lined up in sequence based on the similarities of the letters, ' B I ,  ' P ', ' R ' , ' F I, . .

. , and each l e t t e r  occupies  with an 8 x 8 array pixels.

F i g u r e  3 s h o w s  t h e  o u t p u t  e r r o r  r a t e s  a s  a function of reference patterns for Hopfield and IPA mode l s  unde r  noisy cond i t ions .  The  input  no i se levels are set at about 0%. 5 %  and 10%. respectively.

We not ice  that ,  Hopfield model becomes unstable when four patterns ' B '. ' P ', ' R ' and 'F' are stored in the IWM, wheras IPA model is quite stable with 12 s to red  l e t t e r s .  Under  the cond i t ion  of no i se l e s s input,  the IPA model can produce correct results for all 26  stored letters in the IWM, wheras Hopfield mode l  s t a r t s  mak ing  s ign i f i can t  e r ro r s  when t h e number  of  r e fe rence  p a t t e r n s  i s  i nc reased  t o  6 .

From Fig. 3 we see that the IPA model is  more robust and it has a larger storage capacity as  compared with the Hopfield model.

IPA model can be used to train neural networks to p e r f o r m  s h i f t  a n d  r o t a t i o n  i n v a r i a n t  p a t t e r n recognition, s ince the IPA neural network has large storge capacity and high robustness.

HoDf ield  0 10  20  Reference Pattern Number  Fig.3: Comparison of IPA and Hopfield models; input noise levels: I: 0%. 11: 5%,  and 111: 10%.

The neural network is trained to  recognize tools with different orientation, as shown in Fig.4. IPA model  i s  used to construct  an Hetero-associat ive memory of these tools.  The f i rs t  row in Fig.4(a) consists of 10 training patterns. The desired output must be the correct tool,  and always aligned in the up right direct ion,  as  shown in the second row of Fig.4(a). The interconnection weight matrix,  buil t by Rule B, are rather simple, as  shown in Fig.4(b), the dark parts are inhibitory weights (i .e. ,  -1). the bright parts are exitatory weights (i.e., + I ) ,  and the medium gray level represents no connection (i.e., 0).

F i g u r e  4 ( c )  s h o w s  s o m e  s i m u l a t e d  r e s u l t s performed by the IPA neural network. The patterns in the  f i rs t  and third columns are  input pat terns , e i the r  i ncomple t e  o r  with noise .  After  pa ra l l e l p r o c e s s i n g  by I P A  n e u r a l  n e t w o r k ,  t h e  o u t p u t p a t t e r n s  ( i . e . ,  t h e  p a t t e r n s  i n  t h e  2 n d  and  4 t h co lumns)  become comple t e  pa t t e rns  with co r rec t orientation.

Due to limited size (eg., 64 neurons), the neural network can  only be  t r a ined  to  recognize s imple patterns with limited orientation. However, if we build a neural network with aboutlOOxlOO neurans.

-T  Fig.4: Computer simulated results for  recognizing tools of different orientations: (a)  a set of training patterns, (b) an IWM generated with IPA model, (c) input/output results of the neural network.



IV. OPTICAL IMPLEMENTATION  O n e  o f  t h e  i m p o r t a n t  p r o p e r t i e s  o f  n e u r a l n e t w o r k s  i s  t h e  m a s s i v e  i n t e r c o n n e c t i o n  among large number of s imple processing elements  (i .e. , neu rans ) .  The  2 - D  and 3 - D  pa ra l l e l  p rocess ing capabili t ies of optical systems make it an important candidate  in solving the complex interconnect ion problems in neural networks [8,9,10].

An adaptive optical neural network[8] is used to implement IPA model.  The optical  architecture is illustrated in Fig.5. In this system a high resolution video monitor is  used to display the weight matrix which was constructed with the IPA model.  This p roposed  sys t em d i f f e r s  f rom t h e  ma t r ix -vec to r processor of Farhat and Psalt is[9],  for  which the position of the input SLM and the IWM have been exchanged. We note that this arrangement makes it p o s s i b l e  to  use a v i d e o  mon i to r  f o r  a s s o c i a t i v e m e m o r y  m a t r i x  g e n e r a t i o n ,  i n s t e a d  o f  a l o w  --7 -. .

T h u s  t h e  sequenc ia l  e l ec t ron ic  bot t leneck can  be al leviated to  some extend with the feedback loop.

Although displaying the  memory matr ices  using a v ideo  moni tor  i s  re la t ively s low,  however ,  t h e programming speed of IWM is not required to  match t h e  i t e r a t ion  speed  in  t h e  feedback  loop .  Wi th r e fe rence  to  a 1 0 2 4 x 1 0 2 4  p ixe l s  v ideo  m o n i t o r s (which are  available commercially), it i s  possible to build a parallel hybrid opt ical  neural network with 32x32 (i.e. 1024) neurons.

Fig.5: A 2-D hybrid optical neural network  resolution and low contrast  SLM. A lenslet  array consisting of NxN small lenses is  used to establish the opt ical  interconnect ions between the IWM and the input pattern, where a moderate sized SLM with NxN binary p ixe l s  i s  served a s  t h e  input  device.  As depicted in Fig.6, the l ight beam emitted for  each submatrix from the video screen would be imaged by a specific lens  of the lenslet  array onto the input SLM. Thus an NxN number of submatrices would be added onto the input SLM. The overall transmitted l ight through SLM can be  imaged at  the ouput plane t o  form an NxN ou tpu t  array dis t r ibut ion,  which represents t he  product of the 4-D matrix T and the 2- D input pattern. The  output pattern can be picked up  by an NxN photo-detector array for  thresholding and feedback iterations.

Video L e n s l e t  Input o u t p u t Monitor  A r r a y  Device De tec to r I I &  Fig.6: I l lustration of optical interconnections of the neural network.

To  form a closed loop neural network operation, the output  signals from the detector  array are  fed back to  the input SLM via a thresholding circuit .  It i s  apparent, by the intervention of a computer ,  the p r o p o s e d  o p t i c a l  n e u r a l  n e t w o r k  c a n  b e  m a d e adaptive.

We propose that a ferroelectric liquid crystal SLM would be used as the input device of the system.

Accordingly, i ts  contrast ratio can be a s  high as 125:l [ l l ] .  S i n c e  t h e  S L M  c a n  be  addressed in p a r a l l e l ,  t h e  d e t e c t o r  and  t h e  inpu t  a r r a y s  c a n communicate  in parallel .  Thus  the electro-optical f eedback  loop  c a n  be  per formed in a comple t e ly parallel  manner. This  arrangement would allow the system to operate at  high-speed asynchronous mode.

Fig.7: Experimental  demonstrat ions obtained with the proposed optical neural network of Fig. 5, using an LCTV as an input SLM: (a) three similar English letters as  reference patterns, (b  & c)  positive and negative parts of the IWMs of the IPA model, (d & e) positive and negative parts of the Hopfield model, ( f )  I n p u t  p a t t e r n  , S N R  = 7 d B ,  ( g )  p a t t e r n r e c o n s t r u c t i o n  u s i n g  I P A  m o d e l ,  ( h )  p a t t e r n reconstruction obtained with Hopfield model.

Since the resolution requirement of the lenslet array is  rather relaxing. An array of 32x32 lenses with 2.5 mm in diameter can provide at least 10 times the resolution of a commercially the TV monitor, which has a resolution of about 3 lines/mm.

However the alignment of the optical system is c r i t i c a l  f o r  t h e  m a t r i x - v e c t o r  o p e r a t i o n s .  T h e s u b m a t r i c i e s  on TV sc reen  have to  be p rec i se ly imaged onto the input SLM by the lenslet  array in superimposing position. Since the proposed optical neural network i s  essencially a close-loop feedback system, the precise alignment can be corrected by ad jus t ing  t h e  pos i t i on  of each  submat r ix  on TV screen by computer  intervent ion,  and the intensity of  the TV screen can also be adjusted.  Thus the proposed op t i ca l  system can  indeed perform in adaptive mode.

In  expe r imen t s ,  Hopfield and IPA models  a re chosen t o  perform pattern recognition using noisy input patterns. ' B ' , ' P ' and ' R ' are three letters stored in the weight matrix as  shown in Fig. 7(a).

The posi t ive and negat ive par ts  of IWMs f o r  IPA model are shown in Figs. 7(b) and (c),  while those for Hopfield model are displayed in Figs. 7(d) and (e).

In comparison between these two sets of IWMs it can be seen that the IPA model has two major advantages over the Hopfield model, namely:  1. less interconnections; and 2. fewer gray levels.

T h e  l a t e r  i s  s i g n i f i c a n t  b e c a u s e  the  IPA mode l requires  only 3 gray levels t o  represent the IWM, whereas the Hopfield model needs 2M+1 gray levels, where M deno tes  the number of s tored r e fe rence patterns.

The experimental results of these two models are obtained based on an input pattern ' B ' embedded in 30% random noise (SNR = 7 dB), as shown in Fig.7(f).

T h e  o u t p u t  p a t t e r n s  of  t h e  IPA mode l  and  t h e Hopf i e ld  mode l  a r e  shown  in F igs .7 (g )  and ( h ) respectively. Because of the curvature of the video monitor  s c reen ,  t he  output  resul ts  are  somewhat dis tor ted.  Nevertheless,  the results obtained from the IPA model have been shown better than those obtained from the Hopfield model. ,

V. CONCLUSIONS  We have i l lustrated IPA neural network model fo r  pattern recognition. By using a simple logical rule,  the common features and the special  features of the reference patterns can be obtained, and the pos i t i ve ,  nega t ive  or  no  in t e rconnec t ions  can be assigned to  each neuron. The IWM can be easily fo rmula t ed ,  which requires  merely 3 gray levels .

An adaptive optical  neural network is used to carry out the parallel  processing. Computer simulations and experimental  results have shown that the IPA model  can  pe r fo rm more e f f ec t ive ly  in  t e rms  of pat tern recogni t ion (among s imilar  pat terns)  than the Hopfield model. The basic reason is  that the IPA mode l  p u t s  more  e m p h a s i s  on  the  i n t e r - p a t t e r n relationships,  while the Hopfield model deals only the intra-pattern association.

ACKNOWLEDGEMENT  We acknowledge the support of the U. S .  Army Research office contract DAAL03-87-0147.

