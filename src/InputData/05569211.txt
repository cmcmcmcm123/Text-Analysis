

Abstract? On mining quantitative association rules and the segmentation of numerical attributes variables of ordered data,  Fisher cluster method can be used to determine the segmentation  range and the segmentation number of this variable value. The  method takes the data interval and data density into account.

Therefore it is of great significance to data pre-processing.

Keywords-quantitative association rules; cluster; numerical attribute

I. INTRODUCTION The mining of association rules is an important research  topic in the data mining field, which is initiated by Agrawal in 1993. It is often used to mine the tacit rules and associations between data, which can help the decision-maker to make correct and sensible decision.

Quantitative association rules refer to categorical attributes and numeric attributes, for example: scores and prices of goods belong to numeric attributes, sex and birthplace belong to categorical attributes. The discretization of numeric attributes is the key to quantitative association rules mining, and is determinative to quality of quantitative association rules mining. In this essay, cluster method is used to realize data partitioning. When ordered data are partitioned, the relative distances between data into account is very important.



II.  QUESTION DESCRIPTION A. Quantitative association rules Suppose I={i1,i2,?,im} is a data itemsets, and P is a  positive integers set. Iv represents set of I?P, and?x,v??Iv means that the value of attribute X is V. IR stands for set of { >< ulx ,, ?I?P?P| l?u. If x is variable of numeric attributes,  ul =  . If x is variable of categorical attribute. To the triad of >< ulx ,, ?I, we can use either the value among interval [l,u] to indicate numeric attribute x or the value l be to indicate categorical attribute x. This triad is known as an item. To  every RIX ? , set of {x| >< ulx ,, ?X} is abbreviated as  Attributes(X).

Suppose D is a records set, every record R is a set of  attributes values, namely VIR ? . Suppose every attribute only appears once in a record, namely attribute is single valued, not  multi-valued. If RqxXulx >?<?>?<? ,(,,  makes )uql ?? , record R is considered to include RIX ? .

Quantitative association rules can be abbreviated as the  implications of YX ? , RR IYIX ?? , and Attributes(X) ?Attributes(Y) = ?. If there are S records which contain YX ? in the records set D, then the support of rule YX ?  is S. If in the records set D, c?100% which contains X record also has Y, then the confidence level of rule YX ?  is C. If a record D is given, the task of quantitative association rules mining is to find all quantitative association rules, and the supports and confidence levels of these rules are higher than or equal to the least supports and the least confidence level that the users defined.

B. Correlative works In document [2], the question of quantitative association rules mining is initiated, and partial completeness is used to decide whether or not a numeric attribute is divided, and how to divide, and how much intervals which is appropriate. This can assure the intervals neither too big nor too small. At the same time, the least supports that the user defined is used to decide the extension of combination consecutive values/intervals. If the supports of intervals combination are more than the least supports the user defined, then the intervals combination stopped. Although partial completeness method is designed to aim at numerical data, this method only considers the general characteristic of numerical attribute of data, and neglects the measure of relative distance, the density of intervals, and distance of intervals between data. What?s more, in the case of data deflection, the partition effect is not perfect.

In document [2], ordered data?1, 2, 3?is equal to order data ?1, 20,300?in semantic. In document [3], data mining can be         defined as numerical rule like yx qxqx =?= , the predecessor and successor is a signal pair of <attribute, value>, and the predecessor of rule can be extended to uxl ??  which is similar to the method of dividing numeric attribute variable into interval. In document [4], when data are divided, only the order of ordered data is considered and the interval of data value is neglected. The classification method involved in document [6] is only applicable to the case when the variables are equaled equitable and independent. But in some cases, given variables or samples have a certain order, and this order can not be broken up. It can only be divided into many sections according to the order.

C. Reflection method If the value variation range of categorical attributes  variables and numeric attributes variables is small, the mining of quantitative association rules can be mapped to the mining of boolean association rules.

If the original numeric attributes correspond to multiple boolean attributes, then the quantities of these boolean attributes depends on the value domain of numeric attributes and their division methods. To the domain value of boolean of??attribute A?, ?value val??, if ?attribute A? in original record has ?value VAL?, then its corresponding boolean domain value is ?1?, otherwise the value is ?0?. If the value domain of a numeric attribute variable is very great, then it?s necessary to divided the value into many intervals, and then reflect every <attribute, interval> to the boolean attribute.

For example: part of attributes in table?are mapped  to table ?.

TABLE I.   ANALYSIS TABLE OF CLASS ADVISER TESTING  NO Positional  title  Professiona  teacher  Have teaching  duties or not  Valid  tickets  grade  Associate  professor yes yes 98% A  02 Lecturer no yes 100% A  03 professor no no 98% B  04 Lecturer yes no 20% A  05 Lecturer no no 40% B  06 lecturer yes no 90% B  07 assistant no no 69% C  ? ? ? ? ?  When values of numeric attributes are parted, there are  two questions: ?1?the least supports question. If the value of a numeric attributes variable is divided into many intervals, supports of every interval may be small. So some rules corresponding to these attributes cannot be produced.?2?the least confidence level question. When values are divided into some intervals, some information is lost. Only when certain data item of forerunner is a single value, some rules may have the least confidence level. With the increase of the interval quantities, more information may be lost.

TABLE II.  MAPPING TABLE OF A PART OF ATTRIBUTES VARIABLES OF  ANALYSIS TABLE OF CLASS ADVISER TESTING

III.  QUANTITATIVE ASSOCIATION RULE BASED ON INTERVAL  DISTANCE The section uses cluster method to divide data. When  ordered data are divided, it?s of great importance to pay attention to the relative distances among data. Ordered data are a set of data with linear sequence. Here, data division method in document [2] is called even depth segmentation, and data segmentation method that takes the relative distance among data and interval density into account is called interval segmentation  In table ?, even depth segmentation of valid tickets is compared with interval distance segmentation. In fact, intervals containing nearest data values are more significant than interval containing further data values.

N  O  Valid  tickets  20%..75  %  Valid  tickets  76%..

100%  profess  ional  teache  r  Non-  professi  onal  teacher  Have  teaching  duties or  not  Not  Have  teaching  duties  01 0 1 1 0 1 0  02 0 1 0 1 1 0  03 0 1 0 1 0 1  04 1 0 1 0 0 1  05 1 0 ? ? 0 1  06 0 1 1 0 0 1  07 1 0 0 1 0 1  ? ? ? ? ?         TABLE III.  CONTRAST TABLE OF EVEN DEPTH SEGMENTATION AND  INTERVAL SEGMENTATION OF VALID TICKETS  Even depth segmentation Interval segmentation  Segment  tag  Interval    Segment  tag  interval  1 [20?40%] 1 [20?40%]  2 [69?90%] 2 [69?69%]  3 [90?98%] 3 [90?90%]  4 [98?100%] 4 [98?100%]

IV. CLUSTER METHOD OF ORDERED SAMPLES: FISHER METHOD In the following, we will introduce simply fisher cluster  method, and the detail is in document [5]. Suppose that there are N variables, and they are classified into K catalogs. If these variables are equal, then we can prove that all the possible classifications areas follow:  R?n?k?= ni  k  ikk  i i  k c ?  = ? ?   )1( !

(1)  When these n variables are ordered, the unique method classified into K catalogs is as follows:  1),(  ? ?=?  k nCKnR                     (2)  Suppose that the ordered variable (samples)are in turn X1,X2 ,?,Xn, and every variable Xi(i=1, ?,n)is S dimension column vector. Because catalog quantities of ordered variable will linearly increase with catalog number K, we can get the first best solution in all classification methods.

Definition 1  Suppose that a certain categorization of variables  X1,X2,?,X  is { Xi,Xi+1,?,Xj },j?I,and its average value vector is :  ? =+?  = j  il lij xij  x   ?3? And the diameter of class { Xi ,Xi+1, ?,Xj },j?I is:  D(I,j)= ))(( ijl  j  il ijl xxxx ???  = .

It describes the differences between the variables in the variable range{ Xi,Xi+1,?,Xj },j?i. The fewer the D(I,j) is, the smaller the variables differences are. On the contrary, the more D (I,j) is, the bigger the differences among variables are, namely variables are more dispersion.

When S=1, the diameter of {Xi,Xi+1,?, Xj }can be expressed as:  D (I,j)= || iji  j  il xx ??  =                   ?4?  And ?  =+? =  j  il lij xij  x   .

Definition 2  ? In the non-confusion case in future, variable  iX  is abbreviated  as its suffix I.?Suppose that n variables is  classified into K catalogs, some classification is : P(n,k):{i1=1,i1+1,?,i2-1}{i2,i2+1,i3-1}?{ik,ik+1,?,n}, i1=1?i2?  ?? ki n? ?Then error function definition of this classification  is: e[P(n?k)]= )1,( 1  ?+  = ? j  k  j j iiD  (5) Consider the quadratic sum of total dispersion:  S (total )= )()'(  XXXX l  n  l l ???  =  = ),,()',,( 1111    XXXXXXXX jjjjjjjj  ij  j  iiiiIiiii  K  j  i  il i ?+??+? ????  =  ?  = ??  +    = )1,( 1  ?+  = ? j  k  j j iiD  + )((  1 j  k  j j ii ??  = + XX  jj ii ??1, )? (  XX jj ii  ??1, )  =e[P(n?K)]+eA[P(n?K)]                  (6)  And ?  = =  n  l lXn  X   is called general mean value;  eA[P(n,K)]is called quadratic sum among catalogs which reflects the difference among catalogs. When n,K is fixed , and S is a constant, e[P(n,K)] and eA[P(n,K)] vary with different classification. Obviously, when e[P(n,K)]is smaller, then eA[P(n,K)] is bigger, and the classification is more logical. So the best classification is a classification which makes e[P(n,K)] smallest.

Theorem 1.1 the recursion formula of [ ]),(min  -1 1 Knpe  nii k ?<<= is:  [ ]),(min -1 1  Knpe nii k?<<= =  [ ] )},()1,1(min{min -1 1  njDKjpe niinjk k  +?? ?<<=??       ?7?  Demonstration:  when k=2, obviously there is:   [ ])2,(min  -1 1 npe  nii k ?<<= = nji ?<<= -1 1 min  {D(1,j-1)+D(j,n)}  = nj?<2 min  {D(1,j-1)+D(j,n)}  = [ ])},()1,1(min{min  112 1 njDjPe  jinj +?  ??=?<         To K, there is also:  [ ]),(min -1 1  Knpe nii k ?<<= =  [ ] )},()1,1(min -1 1  niDKipe kknii k +??  ?<<=  = [ ] )},()1,1(min  1-1, 11 niDKipe kkiiinik kkk  +?? ??<<=?= ?  = [ ] )},()1,1(min{min  1-1 11 niDKipe kkiiinjk kk  +?? ??<<=?? ?  justified.

According to formula?7?, confirm jk, which makes the following formula correct.

[ ] )},()1,1(min 1-1 11  njDKipe kkjii kk +??  ??<<= ?  = [ ] )},()1,1(min{min  1-1 11 njDkjpe  jiinjK k +??  ??<<=?? ?  = )],([min  -1 1 knPe  nii k ?<<=  Then NO. K catalog is gotten, and GK={jk,jk+1,?,n},then confirm jk-1, which makes the following formula correct.

[ ] )}1,()2,1(min 111-1 121 ?+?? ????<<= ?? kkkjii jjDKjpekk  = [ ] )}1,()2,1(min{min  1-11 21 ?+??  ??<<=??? ? kjiijjK  jjDkjpe kk  = )]1,1([min  1-1 11 ??  ??<<= ? kjPe kjii kk   Thirdly NO. K-1 catalog is gotten, and GK-1={jk-1,jk-1+1, ?,jk-1}.with the similar method, NO.k-2 catalog can be gotten.

GK-1={jk-2,jk-2+1, ?,jk-2-1},?,NO.1 catalog G1= {j1=1,j1+1, ?,j2-1}. Obviously there is the following formula 8:  [ ]),(min)1,( -111 1  knPejjD niil  k  l l  k ?<<= +  =  =?? ?8?  We can draw a conclusion from the above, in order to get the best solution, we only need to calculate D(I,j) }1{ nji ??? and make appropriate judgement.



V. APPLICATION Table IV describes the classification of valid tickets in  table?. If the number of classification k can be not determined ahead of time, we can draw the curve of min e[P(n,k)]and k.

To calculate conveniently, valid tickets are magnified 10 times.

The main calculating process is as follows:     TABLE IV.  CLASSIFICATION  OF  VALID  TICKETS  When K=1, x =(2+ 4+6.9+9+9.8+10)/6=6.95  MIN E[P(6,1)]= )1,( 1    ?+ = ? l L  l JJD =15.9  When K=2, x 1==4.3   x 2=9.6 MIN E[P(6,2)]=6.4 With the same method, the following can be figured out: MIN E[P(6,3)]=4.1, MIN E[P(6,4)]=1.2 MIN E[P(6,5)]=0.2, MIN E[P(6,6)]=0  0. 5  1. 52 2. 5  3. 5  4. 5  5. 5  6. 5  7. 5  1 2 3 4 5 6    Figure 1.  Curve of Mine[P(n,K)] and K According to figure?, we know the curve turns at the  corner k=4?this means 4 is the best number of classification.

Although when K=6, the value of MIN E[P(6,6)]is the smallest, data  items may not  have enough supports, thereby some rules associated with these attributes can not be found, which loses the significance of quantitative association rules.



VI.  CONCLUSION When mining Quantitative association rules, if values of  numerical attributes variables is smaller, we can reflect the value directly in consecutive integers; if values of numerical attributes variables are evenly distributed, we can choose interval number to do even interval segment. To numerical attributes variables of ordered number, Fisher cluster method  Number of classification Classification cases  1 (2  4  6.9  9  9.8  10)  2 (2  4  6.9)  (9  9.8  10)  3 (2)  (4  6.9)  (9  9.8  10)  4 (2)  (4)  (6.9)  (9  9.8  10)  5 (2)  (4)  (6.9)  (9) (9.8  10)  6 (2)  (4)  (6.9)  (9)  (9.8)  (10)         can be used to determine segmentation range and segmentation quantities of the variable value, and reflect these intervals to consecutive integers; if supports of numerical attributes variables is smaller than the least supports the user defined, adjacent two intervals can be merged into an interval according to partial completeness method in document [2]. In a word, we can use corresponding algorithm to mine frequent itemsets, and get association rules from frequent itemsets in data mining.

