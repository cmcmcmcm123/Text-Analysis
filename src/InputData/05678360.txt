Associative classification over Data Streams

Abstract?Based on association rules, Associative classification (AC) has shown great promise over many other classification techniques on static dataset. However, the increasing prominence of data streams arising in a wide range of advanced application has posed a new challenge for it. This paper describes and evaluates AC-DS, a new associative classification algorithm for data streams which is based on the estimation mechanism of the Lossy Counting (LC) and landmark window model. We apply AC-DS to mining several datasets obtained from the UCI Machine Learning Repository and the result show that the algorithm is effective and efficient.

Keywords--data streams; associative classification; frequent itemsets

I.  INTRODUCTION Data-steam mining is a technique which can find valuable  information or knowledge from a great deal of primitive data.

Unlike mining static databases, mining data streams poses many new challenges [1].

First, each data element should be examined at most once.

It is unrealistic to keep the entire stream in the main memory.

Second, each data element in data streams should be processed as fast as possible. Third, the memory usage for mining data streams should be bounded even though new data elements are continuously generated. Finally, the results generated by the online algorithms should be instantly available when user requested.

A. Data Stream Let I={i1,i2,?,im} be a set of literals, called items. An  itemset is a subset of I. An itemset consisting of m items is called a m-itemset. Let us assume that the items in an itemset are in lexicographical order.  A transaction is a tuple, (tid, itemset), where tid is the ID of the transaction.

A transaction data stream DS={B1, B2, ? , BN , ?} be an infinite sequence of blocks, where each block is associated with a block identifier n, and N is the identifier of the latest block BN.  Each block Bi consists of a set of transactions, that is, Bi=[T1, T2, ?, Tk], where k>0. Hence, the current length of the data stream is defined as CL=|B1|+|B2|+?|BN|.

The frequency of an itemset, X, denoted as freq(X), is the number of transactions in B that support X. The support of X is  defined as fre(X)/N, where N is the total number of transactions received. X is a Frequent Itemset (FI) in B, if sup(X)>=minSupport, where minSupport (0<=minSupport<=1) is a user defined minimum support threshold.

Many algorithms have been proposed for mining data stream. According to the processing model on the data stream, we can classify the research work into three fields: landmark windows, sliding windows and damped windows. Manku and Motwani[2] proposed a single-pass algorithm, Lossy Counting, to mining frequent itemsets, and the algorithm is based on a well known Apriori-property. Yu et al. [3] proposed a algorithm, FDPM, which is derived from the Chernoff bound, to approximate a set of FIs over a landmark window. Li et al.[4] propose a single-pass algorithm, DSM-FI, to mine all frequent itemsets over the entire history of data streams. Pedro Domingos and Geoff Hulten [5] described and evaluated an algorithm, VFDT, to predict the labels of the records it received.

B. Associative Classification Let D be the dataset. Let I be the set of all items in D and C  be the set of class labels. We say that a data case di?D contains X ?  I, a subset of items, if X ? di. A class association rule (CAR) is an implication of the form X?c, where X ?  I, and c?C.

Bing Liu et al. [6] first proposed the AC approach, named classification based on association algorithm (CBA), for building a classifier based on the set of discovered class association rules. The difference between rule discovery in AC and conventional frequent itemset mining is that the former task may carry out multiple frequent itemset mining processes for mining rules of different classes simultaneously. Data mining in associative classification (AC) framework usually consists of two steps:  (1) Generating all the class association rules (CARs) which has the form of iset??c, where iset is an itemset and c is a class.

(2) Building a classifier based on the generated CARs.

Generally, a subset of the association rules was selected to form a classifier and AC approaches are based on the confidence measure to select rules [7].

Input: DS---a data stream in which each record has N items.

Swindow---the window size, Swindow=| Bi |.

minSupport--- Support threshold.

Output: M---a classifier with a lot of association rules whose confidence are great than 50% and support value great than minSupport.

Method:  Initial the rule memory  M=? Do  Read in a data block Bi={T1, T2, ?, Tk} m=0; Am=?         //Clear the set of candidate itemsets A  Am+1=Gen(Bi, Am)        // Generate n candidate frequent itemsets, // Itemset1, Itemset2,?, Itemsetn, //each Itemseti in A has 1 items.

While A?? For  i=1 to n  S=Supp(Itemseti)           //Calculate the support of Itemseti If  S>= minSupport  then M?M+ Itemseti           //Put the itemset into memroy Endif Endfor m=m+1 Am+1=Gen(Bi, Am)       //Generate the (m+1) generation A Endwhile M=Rank(M)                     //Rank rules by their confidence values M=Decay(M)                   //Decay the rules in memory While  Most of the algorithms shown above were used for finding frequent itemsets. Some of them were used to classify the data steams with a decision tree. In this paper, we present an approach to mine class association rules and then to make a classifier. Classifying a data stream with an association classifier is a newly explored area of research, which may be viewed as a further extension to the earlier work on mining frequent itemsets over data stream.



II. ASSOCIATIVE CLASSIFICATION ON DATA STREAMS  A. Problem Definitions A data stream is a massive unbounded sequence of data  elements continuously generated at a rapid rate. Due to the unique characteristics of streaming data, most one-pass algorithms have to sacrifice the correctness of their analysis results by allowing some errors. Hence, the True support of an intemset X, denoted by Tsup(X), is the number of transactions seen so far in which that itemsets occurs as a subset. The estimated support of the itemset X, denoted as Esup(X), is the estimated support of X stored in the summary data structure constructed by the one-scan approaches, where Esup(X)<=Tsup(X). An itemset X is called a frequent itemset if Tsup(X)>=Minsupport*CL.

Hence, given a user-defined minimum support threshold minSupport (0<= minSupport <=1) and a data stream DS, our goal is to develop a single-pass algorithm to classify the streaming data in the landmark windows model using as little main memory as possible.

Figure 1.  The proposed associative classification algorithm.

B. Associative Classification Algorithm for Data Stream We can see the completed algorithm from Fig.1. Our  algorithm accepts two user-specified parameters: a support threshold minSupport and a window size Swindow=| Bi |. Let  N denote the current length of the stream, i.e., the number of records seen so far. Every time received a record, our algorithm can forecast its class label based on the association rules extracted from the records before. Each of the rules has a estimated support Esup(X), whose value is great than minSupport.

For a given data block Bi, the first pass of the algorithm counts item occurrences to determine the frequent 1-itemsets.

As long as the set of 1-itemsets was not empty, the algorithm subsequently carry out the next pass to find frequent (m+1)- itemset. When the algorithm obtains all of the frequent itemsets, it will calculate the confidence of the rules and sort them in the memory. Then, if there is a request of classification, the classifier will predict the class label of a record. At the bound of block Bi, the memory rules will be pruned and the rules with low support value will be deleted.

C. Funtions and Data Structure 1) Data struture M  The data structure M is a set of entries of the form (itemset, fclass(1), fclass(2),?, fclass(i), t), where itemset is a subset of conditional attributes, fclass(i) is an interger representing the approximate frequency of class attributes i, and t is the number of the data blocks in which the itemset appeared firstly.

Initially, M is empty. Whenever a new association rule (itemset, class(i)) arrives, we examine M to see whether an entry mj already exists or not. If exists, we update the entry by incrementing its corresponding frequency fclass(i) by one.

Otherwise, we create a new entry of the form (set, fclass(1), fclass(2),?, fclass(i), t).  The parameter t is the number of the data block and fclass(x) is the frequency of class x.

fclass(x) =0 (class(x) ?class(i)) fclass(x) =1 (class(x) =  class(i))                                    (1)  2) Function Gen(Bx, Am) The Gen(Bx, Am) function takes as argument Am, the set  of all frequent m-itemsets. It returns a superset of the set of all (m+1)-itemsets. The function works as follows [8]:  a) Step 1, we join Am with Am: Insert into Am+1: Select p.item1, p.item2, ?, p.itemm, q.itemm From Am  p,  Am  q Where p.item1=q.item1,?, p.itemm-1=q.itemm-1, p.itemm<q.itemm;  b) Step 2, we delete all itemsets a?Am+1 such that some m-subset of a is not in Am: Forall  itemsets  a?Am+1 do Forall   m-subsets s of a do If   NOT (s?Am)  then Delete  a  from  Am+1;  3) Function Supp(Itemsetx) The function Supp(Itemsetx) was used to calculate the  support value of Itemsetx. The value of the function was calculated like this:  Supp(Itemsetx)= MAX(fclass(1), fclass(2),?, fclass(i))/N          (2) Where N is the number of current data block.

4) Function Rank(M) The function Rank(M) was used to sort the rules in M with  their confidence values. The confidence value of a entry in M was calculated like this:  Confidence(mi)=MAX(fclass(1), fclass(2),?, fclass(i))/ (fclass(1), +fclass(2), +?+, fclass(i) )            (3)  5) Function Decay(M) The function Decay (M) was used to delete some entries at  the boundary of a data block. If the expression below is true, the entry will be deleted from the memory M. N is the number of current data block.

MAX(fclass(1), fclass(2),?, fclass(i))+minSupport*(t-1) *CL  < MINSUPPORT*N*CL           (4)

III. EXPERIMENTS In this section, we provided a formal model of associative  classification and further examine how such algorithm can be applied in data stream mining. We have conducted an experiment on a 3.0 GHz Pentium PC with 512MB of memory running with Microsoft Windows XP to measure the performance of the proposed approach. The datasets used in the experiment are obtained from the UCI Maching Learing Repository[9].

In order to compare our algorithm with other classification algorithms and make the evaluation more credible and reliable, we choose some large datasets from UCI. The Entropy method was used in the progress of discretization of continuous attributes[10].

A. Effects of Parameters We investigated the effects of the different block sizes on  the effectiveness and efficiency of the algorithm and found that the accuracy of the algorithm was not affected obviously by it.

But the number of the rules found and the time cost were affected by it largely.

Figure 2.  The accuracy of the classification algorithm and the number of the rules found.

As can be seen in Fig.2, when the initial data block size was set to 1000, 2000, 3000, 4000 and 6000, and minimum support threshold was set to 10%, we got the similar forecast results.  However, the numbers of the found rules are very different. As the block size getting larger, the number of the found rules became smaller.

This is because that, although we delete some entries at the boundary of a data block, there still are some un-frequent rules in the memory and they will be deleted in the later decay step.

The large the data block size is, the less the number of un- frequent rules exists.

Figure 3.  The run time of the classification algorithm.

We further investigated the effects of different block sizes on the time cost by the algorithm and found that with the same dataset, as the increasing of the block size, the total run time becomes more and more long. With the different dataset, the run time was decided mainly by the number of attributes and the number of the items of the dataset.  We can see this from Fig.3, the numbers of the attributes of the four datasets, Nursery, Krkopt, Poke and Adult, are 8, 6, 10 and 14. And the numbers of the items of them are 27, 40, 85 and 127. Although, the support thresholds and the block sizes of them are the same, the run time of them are so different and it increased obviously with the increasing of the item and attribute.

Figure 4.  The effects of different support thresholds.

When the block size was set to 2000, we tested the performance of the algorithm with the different support thresholds. As it can be seen from Fig.4, with the decreasing of the support threshold value, the more accurate classification result we can get. But, at the same time, more classification rules will appear in our classifier, it means more run time.

B. Comparision with different algorithms We implemented the CBA algorithm [8] and AC-DS  algorithm for comparison. In the implementation of the CBA algorithm, the minimum confidence was set to 50%. In the implementation of the AC-DS algorithm, the confidence threshold was set to 50% too, and the data block size was set to    1000. The support thresholds of the two algorithms were shown in the Table.1.

Column 1: It lists the names of the 6 datasets.

Column 2: It lists the number of attributes in the dataset.

Column 3: It lists the number of classes in the dataset  Column 4: It shows the support threshold used in the algorithm.

Column 5: It shows the classification accuracy of the algorithm CBA.

Column 6: It shows the number of the records which had to been read in the memory by CBA.

Column 7: It shows the classification accuracy of the algorithm AC-DS.

Column 8: It shows the number of records in a data block which was read in the memory by AC-DS.

We test 6 datasets come from UCI Machine Learning Repository. As can be seen from table.1, the mean accuracy of the two algorithms is similar, but the memory used by CBA is obviously greater than AC-DS. Since CBA is a classification algorithm for static dataset and AC-DS is a classification algorithm for mining data streams, the more total time cost by AC-DS than CBA should be think acceptable as long as the two accuracy rates of them are similar and the memory cost by AC-DS is in a special limit.

TABLE I.   COMPARISION WITH DIFFERENT ALGORITHM  Dataset #attr#class#supp CBA  AC-DS #accu#mem#accu#mem  Adult 14 2 10% 83.7% 48842 82.1% 1000  Adult 14 2 5% 83.9% 48842 81.9% 1000  Adult 14 2 1% 85.3% 48842 83.7% 1000  Digit 16 10 10% 78.7% 10992 75.6% 1000  Digit 16 10 5% 86.0% 10992 83.4% 1000  Digit 16 10 1% 87.2% 10992 86.0% 1000  Letter 16 26 10% 59.2% 20000 62.8% 1000  Letter 16 26 5% 69.0% 20000 67.4% 1000  Letter 16 26 1% 70.1% 20000 68.6% 1000  Mushroom 22 2 20% 94.0% 8124 91.4% 1000  Mushroom 22 2 10% 96.4% 8124 93.2% 1000  Mushroom 22 2 2% 96.6% 8124 94.6% 1000  Nursery 8 5 10% 89.9% 12960 90.4% 1000  Nursery 8 5 5% 91.5% 12960 89.5% 1000  Nursery 8 5 1% 96.1% 12960 93.8% 1000  Krkopt,  6 18 10% 89.2% 28056 87.4% 1000  Krkopt, 6 18 5% 94.2% 28056 91.8% 1000  Krkopt, 6 18 1% 97.9% 28056 97.1% 1000

IV.  DISCUSSIONS AND CONCLUSION Since the transactions were not processed one by one, we  always try to read in the available main memory as many transactions as possible. So, we always select a large data block size to process. When the support threshold is fixed, the more the data block size is, the less combinatorial explosion of itemsets takes place.

One important fact we should notice is that our algorithm was designed to deal with dataset in which the all data was generated by a single concept. If the concept function is not a stationary one, in other words, a concept drift takes place in it, our algorithm will not output an accurate result.

This paper described an associative classification approach based on association rules for mining data streams. Empirical studies show its effectiveness in taking advantage of massive numbers of examples. AC-DS?s application to a high-speed stream is under way.

