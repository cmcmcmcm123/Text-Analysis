A Framework for Integrating a Decision Tree  Learning Algorithm and Cluster Analysis

Abstract? We proposed a modified decision tree learning algorithm to improve this algorithm in this paper. Our proposed approach classifies given data set by a traditional decision tree learning algorithm and cluster analysis and selects whichever is better according to information gain. In order to evaluate our approach, we did an experiment using program-generated data sets. We compared ID3 which is one of well-known decision tree learning algorithm to our approach about the recall ratio in this experiment.

Experimental result shows the recall ratio of our approach is similar than the recall ratio of a traditional decision tree learning algorithm. Though we can not show the advantage of our approach according to the experiment, we show it is worth using cluster analysis to make a decision tree. In future, we have to evaluate our approach according to cross- validation method using big and complex data sets in order to say the advantage of our approach. We think our approach is not good for all data set, so we try to find the situation which our approach is better than other approaches according to the experimental results. In addition to, we have to show how to explain a decision tree by our approach to keep the readability of a decision tree.



I. INTRODUCTION A decision Tree learning algorithm is one of well-  known supervised machine learning algorithms. The purpose of this algorithm is to make a decision tree from transaction data set. A decision tree shows a set of classification rules which show how to decide a class label of each data item based on attribute values. We can use these rules to classify new data items. Though there are some statistical methods or machine learning algorithms to extract rules from data set, for example, SVM (support vector machine [1]), ANN (Artificial Neural networks [2]), analysts and researchers often use this algorithm. One of reasons is that it is easy to understand a decision tree made by this algorithm. The other reason is that there are some tools we can use this algorithm, for example, Weka [3], R [4] and SPSS [5]. So some researches use this algorithm in many research filed, such as marketing, psychology and medical [6].

There are some issues about a decision tree learning algorithm, though analysts and researchers use this algorithm to analysis huge data. One well-known issue is that we need good training data set to use this algorithm.

The accuracy rate of a decision tree depends on training data set. If there is big difference between training data set and target data set we want to classify, a decision tree made from training data doesn?t work and the accuracy rate of it is not appropriate. However, it is not easy to prepare good training data set before learning. So we need  to improve a decision tree learning algorithm to manage the impact of training data set.

We had been trying to improve a decision tree learning algorithm. First approach was that we made a decision tree for a class label one by one [7]. In this approach, we rearranged a class label of each data items before making a decision tree. We selected a class label and replaced other labels to ?not this class?. Next, we made decision trees for each class. The number of decision tree is same as the number of class labels. When we predicted a class label of new data, we selected a class label which the prediction rate was highest in all decision trees. Our experimental results about this approach indicated the possibility that our approach improved a decision tree learning algorithm. However, we could not say that we solved issues about a decision tree learning algorithm.

Second approach was that we divide training data set and make a decision tree using divided data set [8]. In this approach, we think class labels of training data set were too rough. So we split given data set into some data subset using a class label. All data items in each dataset has same class label. Next, we divided each data subsets by a cluster analysis. Therefore, we divided each class into more detailed ones. We made a decision tree using training data set with more detail class labels. This experimental result indicated the possibility of our approach, but we could not say that we solved issues about a decision tree learning algorithm, too. Third approach was that we classified leaf nodes in a decision tree by cluster analysis to optimize the size of leaf nodes [9]. In this approach, we classified data set based on two relations. One is the relation between a class label and an attribute and the other is the relation between attributes. First relation is used in a traditional decision tree learning algorithm and second relation is used in cluster analysis. Experimental result showed the possibility that our approach was better than a traditional decision tree learning algorithm. So it is worth enhancing this approach to solve issues about a decision tree learning algorithm.

In this paper, we propose a framework for decision tree learning embedding cluster analysis. We think this approach based on our third approach. The difference between our third approach and the proposed approach is how to use cluster analysis. Though, in third approach, we use cluster analysis when we can not split data set by a decision tree learning algorithm, we use cluster analysis whenever we try to split given data set. In other words, we embed cluster analysis in a decision tree learning algorithm.

Next section describes our approach in detail. Section 3 describes the experiment for evaluation our research. We also describe discussion about our approach according to       the experimental result and future works in this section.

Finally, we conclude this paper in Section 4.



II. A FRAMEWORK FOR DECISION TREE LEARNING WITH CLUSTER ANALYSIS  A decision tree learning algorithm tries to divide data set based on an attributes. It doesn?t think two or more than attributes on same time. We think it is a week point of a decision tree learning algorithm. Our new approach tries to divide data set based on attributes using cluster analysis. Our idea is simple. Whenever we divide data set, we do it by a decision tree learning algorithm and cluster analysis and select better one. In order to compare the results of these algorithms, we use information gain [10] as criteria.  Information gain shows how much the entropy decreases if an attribute value is defined.  In other words, it shows which attribute is better to classify data set. We get information gain using formula (1), (2) and (3) .

? ?  ?  = =  =  ==?==  ?=  ?=  m  j  n  k jjj  n  i ii  Y  vYXPvYXPvYPYXH  PPXH  YXHXHgain  1 1   )|(log)|()()|(  )(log)(  )|()( (1)  (2)  (3)   Where X is given data set and Y is an attribute. H(X) is the entropy of X, H(X|Y) is the entropy of X when Y decided, n is the number of class in X, m is the number of Y?s value.

We show our proposed algorithm in Figure 1. We explain it as follow.

Input of this function is data-set and path. Output of it is a set of paths that includes a given path.

(1) If all data items in given data set have same class label,  this algorithm stops and returns path.

(2) This algorithm tries to split given data set by decision tree learning algorithm. This algorithm calculates the information gain of every attribute using the given data set. It splits the given data set into subsets using the attribute for which information gain is maximum value.

We regard these subsets as candidate subsets, i.e.

candidate child nodes. We show the maximum information gain as ?gaind?.

(3) This algorithm tries to split the given data set into clusters by cluster analysis as candidate data subsets, i.e. candidate child nodes. It calculates the information gain of clusters. We show this value as ?gainc?.

(4) If ?gaind? is more than ?gainc?, this algorithm accepts datasets by a decision tree learning algorithm as data subsets. It makes edges using the selected attributes value and an operator. Otherwise, it accepts clusters as data subsets. It makes edges centroid values of each cluster. It could add additional paths by adding a defined path to an edge related to data sets.

(5) This algorithm split every data subsets recursively.



III. EXPERIMENTS  A. Overview of our experiments In order to evaluate our approach, we compare the  performance of our approach to the performance of a traditional decision tree algorithm. At first, we make a decision tree using each approach and classify same data set by them. Next, we divide the number of data predicted correctly by the number of all data items to get the recall ratio of the decision tree. If the recall ratio of our approach is higher than the recall ratio of a traditional decision tree learning algorithm, we can say that our approach has advantage. We also compare these decision trees. We explain the outline of our experiment in next paragraph.

We make some data group using a program. Each data group has 60 data items which has 2 attributes defined by formula (4) based on the Box?Muller transform [12].

? ? ?  ? ? ? ?  ?  +? +?  =?? ?  ? ?? ?  ? mYX mYX  Z Z  )2sin()log( )2cos()log(    ?? ??   (4)   Where X and Y are probability values from the uniform distribution on the interval (0, 1] . The Box?Muller transform is a pseudo-random number sampling method for generating pairs of independent, standard, normally distributed random numbers, given a source of uniformly distributed random numbers. ? and m are parameters of this formula. ?  influence the range of value and m influence the center of distribution. We make 6 data groups in Figure 2.

When we evaluate our approach, we make pair of data  groups as data set.

To do this experiment, we implemented our approach by  C programming language. There are some decision tree learning algorithms and cluster analysis algorithms. We need to adjust our idea for selected algorithms. In this paper, we use ID3 [10] as a decision tree and K-Means [11] as cluster analysis. ID3 makes a binary decision tree, so we defined K of K-means as 2. When we divide data set by K-Means, we have to decide two seed data items.

function DTCL(data-set, path) if   the number of class label in data-set = 1   then return path else SetD ? Split given dataset by Decision Tree Learning Algorithm  gainD ? Information Gain( SetD) SetC ? Split given dataset by Cluster Analysis  gainC ?Information Gain( SetC) if  gainD > gainC  then n ? | SetD | data-subset[1..n]? SetD  edge[1..n]  ? a condition formula consists of selected the attribute and an operator  else n ? | SetC | data-subset[1..n]? SetC  for i=1 to n do edge[i]  ? centroid  of data-subset[i] endif for i=1 to n do DTCL(data-subset[i],  path+edge[i])  endif  Figure 1.  The Algorithm of Making a Decision Tree Embedding Cluster Analysis  M. Kuremts and H. Fujita ? A Framework for Integrating a Decision Tree Learning Algorithm and Cluster Analysis     We do it as following. At first, we find the attribute whose variance is the largest. Next, we select 2 data items. One has the minimum value of the selected attribute and the other has the maximum value of this attribute. We use these data items as seed for K-Means. In addition to, we calculate the distance between data items by Euclidian distance.

#1 (?,m)=(1,0)               #2 (?,m)=(1,1), (1,-1)    #3 (?,m)=(1,2),(1-2)   #4(?,m)=(0.5,0)        #5  (?,m)=(0.5,1),(0.5,-1)   #6 (?,m)=(0.5,2),(0.5,-2)  Figure 2.      6 data groups based on the Box-Muller transform  B. The Result of experiments We show the part of the result of this experiment as  Table 1. Each decision tree made by our approach in this table has nodes made by Cluster Analysis. ?ID3? and ?Our Approach? show the number of data item evaluated correctly and the recall ratio by each algorithm, respectively. ?CA Nodes? shows nodes made by Cluster Analysis in a decision tree. ?UPDOWN? shows the number of correct data by our approach minus the number of correct data by ID3.

TABLE I.

EXPERIMENTAL RESULT  Case#  Pair ID3 Our  Approach CA  Nodes UP  DOWN #1 #1,#2 80 67% 80 67% 0 0  #2 #1,#3 88 73% 88 73% 0 0  #3 #1,#4 87 72% 87 72% 0 0  #4 #1,#5 88 73% 88 73% 0 0  #5 #1,#6 90 75% 90 75% 0 0  #6 #2,#3 84 70% 84 70% +1 0  #7 #2,#4 91 76% 91 76% 0 0  #8 #2,#5 81 68% 82 68% +1 +1  #9 #2,#6 84 70% 84 70% 0 0  #10 #3,#4 90 75% 90 75% 0 0  #11 #3,#5 91 76% 91 76% 0 0  #12 #3,#6 82 68% 82 68% 0 0  #13 #4,#5 86 72% 86 72% 0 0  #14 #4,#6 90 75% 90 75% 0 0  #15 #5,#6 87 72% 87 72% 0 0   C. Discussion According to the result of this experiment, the recall  ratio of our approach is similar than the recall ratio of ID3.

There is one case where our approach is better than ID3.

However the difference between our approach and ID3 is very small. There are two decision trees have nodes made by cluster analysis. Though our approach increased the recall ratio in case #8, it didn?t change the recall ration in case #6.  Figure 3 shows the distribution of data items in case #8. In this figure, ?o? shows data items classified in correct class by both decision trees and ?n? shows data items classified in wrong class by both decision trees  We analyzed these decision trees. There is no difference between ID3 and our approach about data items? class label in case #6. Otherwise, three data items which classified in correct class with ID3 were classified in wrong class with our approach in case #8. We show these data items as ?i? in figure 3. Four data items which classified in wrong class with ID3 were classified in correct class with our approach in case #8. We show these data items as ?k? in figure 3. Figure 3 shows they are close. The experimental result says that the difference between a decision tree made by our approach and a decision tree made by ID3 is small. In order to check this, we classified all data groups with ID3 and our approach, respectively. ID3 algorithms classified 174 data items correctly and the recall ratio is 0.48. Our approach classified 175 data items correctly and the recall ratio is 0.49. The decision tree made by our approach has 2 nodes made by cluster analysis. We think our approach is better than ID3 according to data sets.

Figure 3.  Distribution of data items in case #8  To get the relation between our approach and data set, we compared the experimental result to the result of analysis using k-nearest neighbor algorithm (k-NN) [14].

We defined k as 3. Table 3 shows the result of comparison among two decision tree learning algorithms and kNN.

?kNN(#)? shows the number of data items classified correctly with kNN and ?kNN(recall)? shows the recall ratio of kNN. Bold letters shows kNN is better than decision tree learning algorithms. ?Average? shows the average of the entropy of 3 nearest data items of each data item and ?SD? shows the standard distribution of this entropy. If average is small, most of nearest data items are in same class. The correlation coefficient between the recall ratio and average is -0.82. The correlation coefficient between recall ratio and SD is -0.76. These values say that there is strong negative correlation between the recall ratio and these values. This result is within the scope of the assumption. If data items which have same data label gather, the entropy becomes small      and it is easy to split dataset. So, we can not find the explicit point for using our approach.

TABLE II.

COMPARISON AMONG DECISION TREES AND KNN  Case# ID3 Our kNN (#) kNN(recall) Average SD  #1 80 80 77 64% 0.54 2.37  #2 88 88 107 89% 0.18 1.41  #3 87 87 86 72% 0.47 2.22  #4 88 88 85 71% 0.45 2.18  #5 90 90 112 93% 0.06 0.82  #6 84 84 76 63% 0.48 2.25  #7 91 91 100 83% 0.25 1.65  #8 81 82 66 55% 0.52 2.34  #9 84 84 80 67% 0.38 2.02  #10 90 90 116 97% 0.03 0.58  #11 91 91 110 92% 0.08 0.96  #12 82 82 69 58% 0.56 2.42  #13 86 86 99 83% 0.23 1.57  #14 90 90 120 100% 0 0  #15 87 87 110 92% 0.15 1.29  Our experiment result shows it is worth using a cluster  analysis in a decision tree learning algorithm. However, we can not say our approach is useful. In order to show the advantage of our approach, we do some following works in future. One is that we have to evaluate our approach. We classified small and simple data sets and evaluated the recall ratio. We have to evaluate our approach by cross-validation method using big and complex data sets, i.e. open data sets in machine learning repository powered by University of California Irvine. We think our approach is not good for every data set, so we try to find the situation which our approach is better than other approaches according to the experimental results. In addition to, we have to show how to explain a decision tree by our approach. A decision tree made by our approach has nodes made by cluster analysis. We can not explain paths include these nodes in a same way. We should keep the readability of a decision tree, so we should ready how to explain paths in a decision tree. We continue to modify our approach based on the result of analysis of experimental results. We have to do spirally.



IV. CONCLUSIONS In this paper, we proposed a modified decision tree  learning algorithm to improve it. Our proposed approach classifies given data set by a traditional decision tree learning algorithm and cluster analysis and selects whichever has the greater information gain. In order to evaluate our approach, we did an experiment using program-generated data sets. We compared ID3 which is one of traditional decision tree learning algorithms to our approach about the recall ratio in this experiment.

Experimental result shows the recall ratio of our approach is similar than the recall ratio of ID3. Though we can not show the advantage of our approach according to this experiment, we evaluate it is worth using cluster analysis to make a decision tree. In future, we have to evaluate our  approach according to cross-validation method using big and complex data sets. We think our approach is not good for all data set, so we try to get features of data set which show our approach is better than other approaches according to the experimental results. In addition to, we have to show how to explain a decision tree by our approach to keep the readability of a decision tree.

