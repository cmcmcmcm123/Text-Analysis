Pattern Recognition, Informatics and Mobile Engineering, February 21-22

Abstract?The current utilization of the spectrum is quite inefficient; consequently, if properly used, there is no shortage of the spectrum that is presently available. Therefore, it is anticipated that more flexible use of spectrum and spectrum sharing between radio systems will be key enablers to facilitate the successful implementation of future systems. Cognitive radio, however, is known as the most intelligent and promising technique in solving the problem of spectrum sharing. In this paper, we consider the technique of spectrum sharing among users of service providers to share the licensed spectrum of licensed service providers. It is shown that the proposed technique rCloud computing is upcoming technology which has gained a lot of hype in the current world of I.T. Cloud computing is said to be the next big thing in the computer world after the internet. Though the benefits are huge, such a service is also relinquishing users? physical possession of their outsourced data, which inevitably poses new security risks towards the correctness of the data in cloud. In order to address this new problem and further achieve a secure and dependable cloud storage service.

We propose in this paper a flexible distributed storage integrity auditing mechanism, utilizing the homomorphic token and distributed erasure-coded data for dynamically storing data. The proposed design allows users or third party auditor to audit the cloud storage with very lightweight communication and less computation cost. The auditing result  ensures reliable cloud storage correctness ,and simultaneously achieves fast data error localization, i.e., finding which server is misbehaving in the fast rapidly changing its data in the cloud and  the user ?s application stored in cloud, it also supports efficient dynamic operations on outsourced data which is secured , including, deletion, and append ,block modification and resilient against Byzantine failure, malicious data modification attack, and even server colluding attacks.

Keywords- Error localization; data dynamics; Cloud Computing;  Data Integrity;  storage; Audit

I. INTRODUCTION Cloud is a large scale pool of computing service. The  Cloud helps enterprises are dynamically scalable abstracted computing infrastructure that is available on-demand and on a pay-per-use basis. This model not only saves the IT teams from investing heavily on infrastructure, but also shields them from the intricacies involved in infrastructure setup and management. The increasing network bandwidth and reliable yet flexible network connections make it even possible that    users can now subscribe high quality services from data and software that reside solely on remote data centers.

Although the cloud Infrastructures are much more efficient and reliable, powerful than our personal computing devices, it encounters range of both internal and external threats for data integrity still exist more in cloud [1] [2].

To make sure of the correctness of storage without the users possessing their own data, it is difficult to address all data security threats in cloud storage as all concentrated in single server scenario and not consider dynamically changing data and its operation. By using distributed protocols for maintaining storage correctness in the multiple server or peers [3]. We propose a concrete, flexible and effective scheme with explicit dynamic data support to maintain the integrity of the user data in the cloud. We use erasure- correcting code in the distribution of the file  in the cloud to avoid redundancies which increases the data dependencies. It overcomes the communication overheads of the traditional replication based techniques of file distribution.

Token utilization is used with distributed verification of the erasure?coded data which ensures the storage correctness and data error localization. The data corruption that has been detected during the verification of the correctness of the stored data is localized, which guarantee the data error localization simultaneously. It identifies the misbehaving server(s) [4].the verification is done without explicit knowledge of the data files.

We use third party auditing where the user can delegate them to check the integrity of data?s in the cloud storage and be worry free. The main contribution can be recapitulated as the following aspects:  ?  Compared to its predecessors they only provide binary results about the data storage status across the distributed servers, the protocol used in our work provides point of data error (i.e. Error Localization).

?  We provide secure and efficient dynamic operations on data blocks like update append and delete  ?  The security and performance analysis shows the proposed scheme is highly efficient and resilient against Byzantine failure, malicious data modification attack, and even server colluding attacks.

The rest of the paper is organized as follows: Section II introduces the system model, adversary model, our design, goal, and notations.  In section III we provide the detailed description of our scheme. Section IV deals with providing dynamic data operation support, followed by Section V which overviews security issues and performance analysis. Finally, Section VI gives the concluding remark of paper.



II. PROBLEM STATEMENT A. System Model  1. User: Users may be a person or an organization that have data to be stored in the cloud and rely on the cloud for data computation.

2. Cloud Service Provider (CSP): A CSP has significant resources and expertise in building and managing distributed cloud storage servers, owns and operates live Cloud Computing systems.

3. Third ?party Auditor (TPA): Who is expertise and capabilities that the user does not have and they are trusted to access and expose to the risk of cloud storage service for users.

Data storage in a cloud is where the user stores his data through a CSP into a set of cloud servers, which are running concurrently, cooperated and in distributed manner. The redundancy of the data can be employed with technique of erasure-correcting code to further tolerate faults or server crash as user?s data grows in size and importance [5].

Thereafter, for application purposes, the user interacts with the cloud servers via CSP to access or retrieve his data. In some cases, the user may need to perform block level operations on his data. The most general forms of these operations we are considering are block revise, erase, insert and affix.

It is of critical importance to assure users that their data are being correctly stored and maintains, as users no longer possess their data locally, That is, users should be prepared with security means so that they can make continuous correctness assurance of their data stored in Cloud Servers even without the existence of local copies. In case those users do not necessarily have the time, feasibility or resources to monitor their data, they can delegate the tasks to an optional trusted Third Party Auditor of their respective choices. In our model, we assume that the third party auditor behalf of the user communication channels between each cloud server and the user is authenticated and reliable, which can be achieved in practice with little overhead.

B. Adversary Model  All kinds of threats toward his cloud data integrity are found in the adversary model from the user?s view. The cloud data do not reside at user?s local site but at CSP?s address domain, these threats can come from two different sources: internal and external attacks. For internal attacks, a CSP can be self-interested, untrusted, and possibly malicious; it may also attempt to hide a data loss incident due to management errors, Byzantine failures, and so on. For external attacks, data integrity threats may come from outsiders who are beyond the control domain of CSP, for example, the economically motivated attackers. They may compromise a number of cloud data storage servers in different time intervals and subsequently be able to modify or delete users? data while remaining undetected by CSP. Therefore, we consider the adversary in our model has the following capabilities, the cloud data integrity is maintained by capturing both external and internal threats.

Continuously corrupting the user?s data files by adversary, in the individual storage servers can cause loss of integrity of data. The server can pollute the original data files by modifying or introducing its own fraudulent data to prevent the original data from being retrieved by the user. This corresponds to the threats from external attacks. In the worst case scenario, the adversary can compromise all the storage servers so that he can intentionally modify the data files as long as they are internally consistent. In fact, this is equivalent to internal attack case where all servers are assumed colluding together from the early stages of application or service deployment to hide a data loss or corruption incident.

C. Proposed CR Nodes Sensing  To design efficient mechanisms for dynamic data verification and operation and achieve the following goals:  (i) Storage accuracy: To ensure users that their data are indeed stored appropriately and kept intact all the time in the cloud.

(ii) Fast localization of data error: To effectively locate the mal- functioning server when data corruption has been detected.

(iii) Dynamic data support: To maintain the same level of storage correctness assurance even if users modify, erase or affix their data files in the cloud.

(iv) Dependability: to enhance data availability against Byzantine failures, malicious data modification and server colluding attacks, i.e. minimizing the effect brought by data errors or server failures.

(v) Lightweight: To enable users to perform storage correctness checks with minimum overhead.

D.  Notation and Preliminaries  F ? Data file to be stored. F is denoted as a matrix of m equal-sized data vectors, each consisting of l blocks.

A ? Scattering matrix used for coding.

G ? Encoded file matrix, which includes a set of  n = m + k vectors, each consisting of l blocks.

f ? Function, which is defined as f : {0, 1} ? key.

Figure 1. System Model   TPA Cloud  Data   TPA Cloud  Initialization Initialization  Challenge  Proof      ?key (.)- Pseudorandom function (PRF).

Ver-version number of individual block.

Sij-seed for PRF which depend on name, block index i,  server position j and version number.



III. SECURE DATA STORAGE IN CLOUD The users store their data in the cloud server which is no  longer available locally at the user side. Thus, the integrity and availability of the data files being stored on the distributed cloud servers must be guaranteed. One of the key issues is to effectively detect any unauthorized data modification and corruption, possibly due to server compromise and/or random Byzantine failures [4]. Besides, in the distributed case when such inconsistencies are successfully detected, to find which server the data error lies in is also of great significance, since it can be the first step to fast recover the storage errors. To address these problems, our main scheme for ensuring cloud data storage is presented in this section. The first part of the section is devoted to a review of basic tools from coding theories that are needed in our scheme for file distribution across cloud servers. Then, the homomorphic token is introduced. The token computation function we are considering belongs to a family of universal hash function, chosen to preserve the homomorphic properties, which can be perfectly integrated with the verification of erasure-coded data.

Subsequently, it is also shown how to derive a challenge response protocol for verifying the storage correctness as well as identifying misbehaving servers by third party auditing [7].

Finally, the procedure for file retrieval and error recovery based on erasure-correcting code is outlined.

A.  Preparation of file distribution   To tolerate multiple failures in the distributed storage system we use erasure?correcting code. The technique is to dispraise the data file F redundantly across a set of n=m+k distributed server. Reed Solomon erasure code is used  for creating k redundancy parity vectors from m data reconstructed from any m out of m+k data[5] .It can handle the failure  without any data loss. Vandermonde matrix is used for dispersal matrix A is derive from m x (m+k), this is the layout of parity vector. The A matrix is written after the transformation as (I/P), where I is identity matrix and P is the secret parity vector. By multiplying F by A, the user obtains the encoded file, where F is the files to be stored in the cloud.

The multiplication reproduces the original data file vectors of F and the remaining are k parity Vectors generated based on F.

B.    Token exactness  Verification of tokens is done in order to achieve data storage correctness and data error localization, the pre- computed verification tokens for each data files that stored in cloud. Before file distribution the user pre-computes a certain number of short verification tokens on individual; each token covers a random subset of data blocks. Later, when the user or the third party auditor makes sure the storage correctness for the data in the cloud, they challenge the cloud servers with a set of randomly generated block indices. After getting  assurance of the user it again asks for authentication by which the user is confirmed to be the authenticated user. Upon receiving assurance, each cloud server computes a short ?signature? over the specified blocks and returns them to the user. The values of these signatures should match the corresponding tokens pre-computed by the user. Meanwhile, as all servers operate over the same subset of the indices, the requested response values for integrity check must also be a valid codeword determined by a secret matrix.

Suppose the user wants to challenge the cloud server?s t times to make sure the correctness of data storage. Then, he must pre-compute t verification tokens for each function, a challenge key and a master key are used. To generate the ith token for server j, the user acts as follows:

I. Derive an arbitrary value i and a permutation key based on master permutation key.



II. Compute the set of randomly-chosen indices:

III. Calculate the token using encoded file and the arbitrary value derived.

Algorithm 1 Token Pre-computation  1. Procedure  2. Choose parameters l, n and function f;  3. Choose the number t of tokens;  4. Choose the number r of indices per verification;  5. Generate master key and challenge key;  6. for vector G(j), j ?1, n do  7. for round i? 1, t do  8. Derive i = f(i) and k(i) from master key .

9. Compute v (j)  10. end for  11. end for  12. Store all the vis locally.

13. end procedure  Blinding the each parity block is important after the token precomputing; this is done for protection of secret matrix P. we use HMAC for parity blinding. The encode vector is dispersed in the cloud server. This can be done by the user or delegate the challenging responsibility to third party auditor.

C.    Correctness Verification and Error Localization  The key requirement of Error localization is to eradicating errors in storage systems. Previous schemes do not explicitly consider the problem of data error localization effectively.

Thus it only provides binary results for the storage verification.  Integration of all correctness verifications, error localization in our challenge-response protocol is done. The determination of the correctness of the distributed storage, and also contain information to locate potential data error(s) with      response values from servers for each challenges. The procedure of the ith challenge-response for a cross-check over the n servers is described as follows:  i) The user reveals the i as well as the ith key k (i) to each servers.

ii) The server storing vector G aggregates those r rows.

iii) Specified by index k(i) into a linear combination R.

iv) Upon receiving R is from all the servers, the user takes away values in R.

v) Then the user verifies whether the received values remain a valid codeword determined by secret matrix.

Because all the servers operate over the same subset of indices, the linear aggregation of these r specified rows (R(1)i , . . . ,R(n)i ) has to be a codeword in the encoded file matrix.

If the above equation holds, the challenge is passed.

Otherwise, it indicates that among those specified rows, there exist file block corruptions. Once the inconsistency among the storage has been successfully detected, we can rely on the pre- computed verification tokens to further determine where the potential data error(s) lies in. Note that each response R(j) i is computed exactly in the same way as token v(j) i , thus the user can simply find which server is misbehaving by verifying the following n equations:  Algorithm 2: Correctness Verification and Error Localization  procedure CHALLENGE(i)  1. Recompute i = fl (i) and k(i) master key ;  2. Send {i, k(i) } to all the cloud servers;  3. Receive from servers R  4. for (j ? m + 1, n) do  5. R(j) ? R(j)?Prq=1 fkj (sIq,j)?_qi , Iq = _k(i)prp(q)  6. end for  7. if ((R(1)i , . . . ,R(m)i ) ?P==(R(m+1)i , . . .,R(n)i )) then  8.  Accept and ready for the next challenge.

9.  else  10.  for (j ? 1, n) do  11.  if (R ! =V ) then  12.  return server is misbehaving.

13.  end if  14.  end for  15.  end if  16.  end procedure        D. File  retrieval and  error recovery  Spot-checking is randomly done for storage correction assurance.  The user can always ask servers to send back blocks of the r rows specified in the challenge and regenerate the correct blocks by erasure correction. The newly recovered blocks can then be redistributed to the misbehaving servers to maintain the correctness of storage.

Algorithm 3: Error Recovery.

1: procedure  % Assume the block corruptions have been detected  among % the specified r rows;  % assumes s <= k servers have been identified  misbehaving;  2: Download r rows of blocks from servers;  3: Treat s servers as erasures and recover the blocks.

4: Resend the recovered blocks to corresponding  servers.

5: end procedure.

E. Third Party Auditing  Our protocol can support privacy-preserving Third party auditing. Encoding procedure in file distribution after blinding data vector, then the storage verification task can be successfully delegated to third party auditing in a privacy- preserving manner[8][9]. The following is the protocol.

1. The user blinds each file block data before file distribution k is the secret key for data vector is generated  2. Based on the blinded data vector, the User generates k parity vector via the secret matrix P.

3. The user calculates the ith token for server j as  previous scheme  4. The user sends the token secret matrix P, permutation and challenge key Kmaster key, and kchal to TPA for auditing delegation.

The blinding values in the servers are not taken by TPA response of the server are verified directly. As TPA does not know the secret blinding key there is no way for TPA to learn the data content information during auditing process. Thus the privacy-preserving third party auditing is achieved.



IV. PROVIDING DYNAMIC DATA OPERATION SUPPORT So far, we assumed that F represents archived data.

However, in cloud data storage, there are many potential scenarios where data stored in the cloud is dynamic, like electronic documents, photos, or log files etc. Therefore, it is crucial to consider the dynamic case, where a user may wish to perform various block-level operations of revise, erase and      affix to modify the data file while maintaining the storage correctness assurance.

The straightforward and insignificant way to support these operations is for user to download all the data from the cloud servers and re-compute the whole parity blocks as well as verification tokens. This would clearly be highly inefficient.

In this section, we will show how efficiently handle dynamic data operations for cloud data storage.

A. Revise Operation  In cloud data storage, sometimes the user may need to modify some data block(s) stored in the cloud, from its current value f to a new one; this is operation as data revise.

B. Erase Operation  Sometimes, after being stored in the cloud, certain data blocks may need to be erased. The erase operation we are considering is a general one, in which user replaces the data block with zero or some special reserved data symbol. From this point of view, the erase operation where the original data blocks can be replaced with zeros or some predetermined special blocks.

C. Append Operation  Adding blocks at the end of the data file when user like to extend, which we refer as data append. We anticipate that the most frequent append operation in cloud data storage is bulk append, in which the user needs to upload a large number of blocks (not a single block) at one time.

D. Update Operation  The user may need to modify some data block(s) stored in the cloud, after modification the block is updated. The random blinding information on parity blocks by subtracting the old and newly updated parity blocks. As a result, the secret matrix P is still being well protected, and the guarantee of storage correctness remains.



V. SECURITY ISSUES AND PERFORMANCE ANALYSIS In this section, we analyze our proposed scheme in terms  of security and efficiency. Generally, the checking scheme is secure if (i) There exists no polynomial-time algorithm that can cheat the verifier with non-negligible probability; (ii) there exists a polynomial-time extractor that can recover the original data files by carrying out multiple challenges-responses. We also evaluate the efficiency of our scheme via implementation of both file distribution preparation and verification token pre- computation.

A. Security Strength against Weak Antagonist  i) Identification Probability for Misbehaving Servers  We have shown that, if the antagonist modifies the data blocks among any of the data storage servers, our sample checking scheme can successfully detect the attack with high probability.

ii) Detection Probability against data modification:  In our scheme, servers are required to operate on specified list of tokens. These selected tokens greatly reduce the computational overhead on the server, while maintaining the detection of the data corruption with high probability. Note that if none of the specified r rows in the ith verification process are erased or modified, the antagonist avoids the detection  As long as the data modification is caught, the user will further determine which server is malfunctioning. This can be achieved by comparing the response values R with the pre- stored tokens v. The probability for error localization or identifying misbehaving server(s) can be computed in a similar way. It is the product of the matching probability for sampling check and the probability of complementary event for the false negative result.

Next, we consider the fake denial probability that R(j)=v(j) when at least one of z blocks are modified. Thus, the identification probability for misbehaving server(s) is predicted.

B. Security Strength against Strong Antagonist  We analyze the security strength of our schemes against server colluding attack and explain why blinding the parity blocks can help improve the security strength of our proposed scheme. Redundancy parity vectors are calculated via multiplying the file matrix F by P, where P is the secret parity generation matrix we later relies on for storage correctness assurance. If we disperse all the generated vectors directly after token pre-computation, i.e., without blinding, malicious servers that collaborate can reconstruct the secret P matrix easily: they can pick blocks from the same rows among the data and parity vectors to establish a set of m ?  k linear equations and solve for the m ? k entries of the parity generation matrix P. Once they have the knowledge of P, those malicious servers can consequently modify any part of the data blocks and calculate the corresponding parity blocks, and vice versa, making their codeword relationship always consistent. Therefore, our storage correctness challenge scheme would be damaged even if those modified blocks are covered by the specified rows, the storage correctness check equation would always hold. To prevent colluding servers from recovering P and making up consistently-related data and parity blocks, we utilize the technique of adding random perturbations to the encoded file matrix and hence hide the secret matrix P. We make use of a keyed pseudorandom function f with key k, both of which has been introduced previously.

C. Performance Evaluation  File Distribution Preparation is implemented for the generation of parity vectors for our scheme we use HMAC for parity blinding which improves cost. This experiment is conducted using JAVA on a system with an Intel Core 2 processor running at 1.86 GHz, 2048 MB of RAM and 250 GB Serial ATA drive. Thus the cost decreases when more data vectors are involved. The performance of our scheme is comparable and evens our scheme supports dynamic data operation while other are for static data only. Challenge Token Pre-computation: In our scheme we use fixed number of      verification on token t that is determined before file distribution; we can overcome this issue by choosing sufficient large t in practice.

CONCLUSION  In this paper, we studied the problem of data security in data storage in cloud servers. To guarantee the correctness of users? data in cloud data storage delegating TPA, we proposed an effectual and flexible scheme with explicit dynamic data support, including block revise, erase, and affix. We use erasure-correcting code in the file distribution preparation to provide redundancy parity vectors and guarantee the data dependability. Our scheme accomplishes the integration of storage correctness insurance and data corruption has been detected during the storage correctness verification across the distributed servers. Our scheme is highly efficient and resilient to Byzantine failure, malicious data modification attack, and even server colluding attacks.

We believe that data storage security in Cloud Computing, an area full of challenges and of dominant significance, is still in its infancy to be identified. We envision several possible directions for future research on this area. It allows Third Parity Auditor to audit the cloud data storage without demanding users? time, probability.

