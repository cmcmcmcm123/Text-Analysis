Keynote I

Abstract This talk will explore the use of Open Geospatial Consortium (OGC) standards in the context of Big Data requirements and analytics for accessing and processing geospatially-enabled content. A short description of the OGC is provided. This is followed by a discussion of how key OGC standards are or can be used for Big Data applications. The discussion of OGC standards and Big Data analytics is in the context of geospatial information and sensor fusion. Examples are provided. The talk concludes with a discussion of some of the key issues, such as provenance, quality, security and privacy, facing the use of geospatial data in Big Data applications.

Bio Dr. Carl Reed is currently the Chief Technology Officer and an Executive Director of the Open Geospatial Consortium (OGC). Dr. Reed is responsible for facilitating the OGC standards development process, chairing the OGC Architecture Board, and Chairing the OGC Planning Committee. Dr. Reed also participates in and collaborates with other standards organizations, including OASIS, NENA, W3C, ISO, and the IETF. As a result, Reed has contributed to numerous internet and web standards. During his tenure at the OGC, Reed has written numerous book chapters and articles and presented dozens of keynotes at geospatial/GIS conferences. Prior to the OGC, Reed was the vice president of geospatial marketing at Intergraph and pervious to that President of Genasys Americas. Dr. Reed received his PhD in Geography, specializing in systems architectures for GIS technology, from the State University of New York at Buffalo in 1980. In 1995 and in 2009, Reed was voted one of the 10 most influential people in the GIS industry. For his contributions to the geospatial industry, in 2009 Reed was inducted into the URISA GIS Hall of Fame.

xiv    Keynote II  COM.Geo 2013    A Fresh Look at Mobile Location Sensing   Dr. Jie Liu Principal Researcher & SERG Manager, Microsoft Research     Abstract GPS and WiFi in our smart phones and other mobile devices. However, continuous location sensing such as logging, tracking, and geo-fencing, consume too much energy and shorten device battery life. In this talk, we take a fresh look at location sensing, in both outdoor and indoor settings. For outdoor location, we dive into the principles of GPS receivers and show that by offloading GPS processing to the cloud, we can reduce the device side energy consumption by three orders of magnitude. For indoor location, we discover that commercial FM signals are good sources of location signatures that work better than WiFi signatures by themself, and works even better if combined with WiFi signatures. These low energy approaches enable always-there location services without users paying batter life penalty.

Bio Jie Liu is a Principal Researcher at Microsoft Research, Redmond, WA, and the manager of its Sensing and Energy Research Group (SERG). He is an ACM Distinguished Scientist. His research interests root in understanding and managing the physical properties of computing. Examples include timing, location, energy, and the awareness of and impact on the physical world. He has published broadly in areas like wireless sensor networks, mobile and embedded systems, ubiquitous computing, and energy efficient cloud computing. Dr. Liu is an Associate Editor of ACM Trans. on Sensor Networks, has been an Associate Editor of the IEEE Trans. on Mobile Computing, and has chaired a number of top tier conferences. Dr. Liu received his Ph.D. degree from Electrical Engineering and Computer Sciences, UC Berkeley in 2001. From 2001 to 2004, he was a research scientist at Palo Alto Research Center (formerly Xerox PARC).

xv    Keynote III  COM.Geo 2013    Unexplored 3D Worlds: The Futures of Focal Plane GIS    Mr. Mike Liebhold Senior Researcher, Distinguished Fellow  Institute for the Future    Abstract In this talk we explore some future impacts and challenges of combinatorial innovations in sensing, mapping and rendering and computing technologies enabling humans and machines to intimately interact with rich 3D geospatial data in a vertical focal plane view. We'll start with a tour through recent developments in sensor fusion, machine vision, and liquid data cloud supercomputing and then tour a few of  the more  interesting cartographic frontiers  including augmented reality,  robotic SLAM (Simultaneous Location and Mapping) and geolocated simulations.

Bio Mike Liebhold is a Senior Researcher and distinguished Fellow at IFTF.org, the Institute for the Future, focusing on the mobile and abundant computation, immersive media and geospatial web foundations for context-aware and ubiquitous computing. Previously, Mike was a Visiting Researcher, Intel Labs, working on a pattern language based on semantic web frameworks for ubiquitous computing. At IFTF MIke leads ongoing work in geospatial, and location services for companies like Intel, Nokia, Toyota, Daimler, Nissan, and Fujitsu, among others. In 2003 Mike was a producer and program leader for the Technology Horizons ?New Geography? Conference at the Presidio of San Francisco for technologists and strategic planners from top tier companies and the public to better understand the emerging geospatial information infrastructure. The event included The Fort Scott Locative Experience, a hands-on field exercise for conference attendees exploring a prototype geospatial web combining digital geodata and modern web hypermedia - deploying a prototype geo browser to read and write W3C and OGC standard data objects.

Prior to joining IFTF, Mike contributed to creation of GeoRSS, (the first web standard way to geocode web objects). Before that, during the late 1990s Mike worked on GPS enhanced precision agriculture in rural and remote regions. Mike is currently active in the AR, Augmented Reality community, launched the first ARdev camp, replicated worldwide, and was invited to join the W3C POI (Points of Interest) working group as an outside expert. Mike's work in geospatial computing began as an idea for a hypermedia atlas in the late ?70s leading to a Lab Director's role at Atari labs working with early MIT augmented reality artists, and authors of the Aspen movie map - the pre-eminent model for heads-up geography. Later, from 1983 -1993 at Apple?s Advanced Technology Group Mike created early hypermedia maps and lead work on the Terraform project - an early predecessor to a google earth-like computational framework, and on hyper-annotated video and augmented reality. In the ?80s Mike was instrumental in forging a partnership between Apple, the National Geographic society, and Lucasfilm to produce new geographic digital media.

Mike is a frequent speaker, has given keynotes at Where 2.0, Location Intelligence, URISA, NSDGIC and the UK Ordnance Survey conferences and has authored a number of papers, including one recently published in the Nieman Reports, the Harvard Journalism Review, entitled ?Digital Immersion: Augmenting Places With Stories And Information? and an earlier co-authored paper published in a special edition of the IEEE Journal on Pervasive Computing, ?Data Management in the World-Wide Sensor Web.? Most recently Mike was profiled in the 12/2011 Ericson Business Review in the cover story entitled ?Augmented Reality Check.?     xvi    Keynote IV  COM.Geo 2013    NASA World Wind Infrastructure for Spatial Data   Patrick Hogan NASA World Wind Manager, NASA    Kevin Montgomery, PhD CEO, Intelesense Technologies, Inc.

Sr. Researcher, Stanford University     Abstract Spatial information intelligence is a global issue that will increasingly affect our ability to survive as a species. Collectively we must better appreciate the complex relationships that make life on Earth possible. Providing spatial information in its native context can accelerate our ability to process that information. To maximize this ability to process information, three basic elements are required: data delivery (server technology), data access (client technology), and data processing (information intelligence). NASA World Wind provides open source client and server technologies based on open standards. The possibilities for data processing and data sharing are enhanced by this inclusive infrastructure for geographic information. It is interesting that this open source and open standards approach, unfettered by proprietary constraints, simultaneously provides for entirely proprietary use of this same technology.

Why World Wind? Over ten years ago NASA World Wind began as a single program with specific functionality, to deliver NASA content. But as the possibilities for virtual globe technology became more apparent, we found that while enabling a new class of information technology, we were also getting in the way.

Researchers, developers and even users expressed their desire for World Wind functionality in ways that would service their specific needs. They want to add their own features. They want to manage their own data. They told us that only with this kind of flexibility, could their objectives and the potential for this technology be truly realized. World Wind is a set of development tools, a software development kit (SDK) that allows a software engineer to create applications requiring geographic visualization.

Modular Componentry. Accelerated evolution of a technology requires that the essential elements of that technology be modular components such that each can advance independent of the other elements.

World Wind therefore changed its mission from providing a single information browser to enabling a whole class of 3D geographic applications. Instead of creating a single program, World Wind is a suite of components that can be selectively used in any number of programs.

World Wind technology can be a part of any application. Or it can be extended with additional functionalities by application developers. World Wind makes it possible to include virtual globe visualization and server technology in support of any objective. As open source, the world community can collectively collaborate in advancing this technology, and thereby continually benefit from optimization and increased functionality of this open source infrastructure.

Open Source + Open Standards = Accelerated Solutions. NASA World Wind is NASA Open Source software. This means that the source code is fully accessible for anyone to freely use, even in association with proprietary technology.

xvii    Facilitate Solutions. The ability to effectively deliver spatial data is an essential element of the US Executive Order 12906 for the National Spatial Data Infrastructure (NSDI). Open standards for data format facilitate data access. In the same manner, an open source 'standard' visualization tool facilitates the ability for others to generate spatial data solutions, proprietary or other. This open source technology for data access and visualization, also improves the ability for information intelligence, the analytical results, to be readily and more effectively shared. NASA World Wind open source technology provides the foundational tool for spatial data visualization and facilitates the creation and evolution of spatial data analysis and information exchange.

Bio Mr. Patrick Hogan currently manages the NASA World Wind development team, a group of world class engineers producing open source software that has received National awards and NASA Software of the Year for 2009/2010. During his 20 years with NASA, Patrick managed environmental programs and more recently the NASA Learning Technologies (NLT) program. NLT was an incubation 'tank' for technologies to move NASA content into education. NLT is where World Wind was born. Patrick, a former pilot, deep sea diver and high school science teacher, has a Master's in Earth Science and is a Registered Geologist in the State of California.

Dr. Kevin Montgomery is the Chief Executive Officer of Intelesense Technologies. He is also a Senior Researcher at the Center for Innovation in Global Health at Stanford University, and formerly the Director of the National Biocomputation Center there, where his team developed advanced technologies in medicine for NASA, DoD, NIH, and other clients. Dr Montgomery is a veteran of multiple startups, earned a Smithsonian Award in telemedicine, serves as a technical advisor to the DoD, and holds a PhD in Computer Engineering from the University of California. He has over 25 years of technical experience and 20 years of management experience leading high-performance teams in academia, government, and industry.

xviii    Keynote V  COM.Geo 2013    To the Edge of the Universe and Back Again: The Evolution of the WorldWide Telescope and  the Ideas That Inspired GeoFlow   Curtis Wong Principal Researcher, Microsoft Research     Abstract More than five years ago, WorldWide Telescope (WWT) was launched at the TED conference as the realization of a dream to build a high performance accurate interactive 3D model of the Universe populated by the highest resolution imagery of the heavens from ground and space based telescopes.

The goal of the project was to build an interactive spatial temporal data visualization environment that could empower kids of all ages to explore and understand the Universe.

Since that time WWT has garnered more than ten million users of all ages on every continent on Earth.

WWT features narrated interactive guided tours of the Universe produced by a wide spectrum of users from a 6 year old talking about the Ring Nebula to educators creating an interactive tour to help 6th graders understand the phases of the moon with a 3D simulation of the Moon's movement around the Earth, to Astrophysicists telling the story of the research to visualize the large scale structure of the Universe. WWT also has the capability to do large scale visualization and is installed in some of the biggest planetariums in the US in San Francisco, New York and Chicago. The Adler Planetarium's 80 megapixel digital dome is showing Cosmic Wonder the first fully interactive planetarium show produced and completely using WWT.

A private internal version of WWT was also used as a prototyping platform to explore challenges of high performance interactive geospatial and temporal data visualization. This research work inspired the Microsoft Office product group to create Project GeoFlow a geospatial temporal data visualization capability for Office Excel 2013 which is now in public beta preview.

This talk will cover some of the key ideas within WorldWide Telescope and how they are relevant to interactive geospatial data visualization and the development of ideas within GeoFlow.

Bio Curtis Wong, Principal Researcher at Microsoft Research, is responsible for basic and applied research in media and interaction. He has been granted more than 35 patents with 15 more pending. Recently, Curtis has led the effort to enable interactive spatial temporal data visualization as a broad capability for everyone to gain insight into the growing tide of data that is being generated from devices and services.

This work, codenamed Project GeoFlow, will be released as part of Excel 2013 later in the year and is Microsoft's first geospatial temporal data visualization application for the broad market.

Previously, Curtis conceived and developed Project Tuva in collaboration with Bill Gates to make the Messenger Series Lectures by acclaimed Nobel Prize winning theoretical physicist Richard P. Feynman freely available over the Internet. In 2008 Curtis fulfilled a lifelong goal to create the WorldWide Telescope (WWT), which is a free, rich interactive virtual simulation of the entire visible Universe to enable kids of all ages to explore and understand the Universe.

xix    Keynote VI  COM.Geo 2013    Big Data Storytelling through Interactive Maps   Dr. Jayant Madhavan Tech Lead, Google Fusion Table  Staff Software Engineer, Structured Data Research group at Google Inc.

Abstract Google Fusion Tables (GFT) is often used by data journalists to create interactive maps that are then  embedded in their news articles. These maps offer journalists the ability to overlay large geo-spatial datasets on Google Maps, customize their presentation and combine them with complementary datasets, without needing any software development skills beyond cut-and-paste. More importantly, they do not have to worry about any systems and scalability issues associated with visualizing large datasets nor supporting massive user traffic. In this talk, I will highlight the motivation and design underlying GFT, a web offering that brings easy-to-use data management in the cloud to data enthusiasts. Such users have interesting datasets, but not necessarily the technical expertise to manage their datasets. By relieving our users of the need to deal with systems issues, we let them focus on their storytelling and advocacy, tasks that better suit their interests and make better use of their expertise.

Bio  Dr. Jayant Madhavan is a member of the Structured Data Research group at Google Inc. He is broadly interested in enabling users make better use of structured data on the Web. He is currently the technical lead for Google Fusion Tables, a cloud data management solution. He was the Chief Architect at Transformic Inc., a data integration portal that was acquired by Google. He is a co-recipient of the Ten Year Best Paper Award at VLDB 2011. He received a Ph.D. from the University of Washington in 2005.

